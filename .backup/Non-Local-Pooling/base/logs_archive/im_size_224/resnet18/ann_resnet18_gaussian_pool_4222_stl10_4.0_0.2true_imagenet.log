
 Run on time: 2022-06-22 22:05:20.104368

 Arguments:
	 gpu                  : True
	 log                  : True
	 seed                 : 0
	 momentum             : 0.9
	 amsgrad              : True
	 dataset              : STL10
	 batch_size           : 8
	 architecture         : RESNET18_GAUSSIAN_POOL_4222
	 im_size              : 224
	 relu_threshold       : 4.0
	 learning_rate        : 0.01
	 pretrained_backbone  : 
	 pretrained_ann       : 
	 weight_decay         : 0.0
	 test_only            : False
	 epochs               : 300
	 lr_interval          : [180, 240, 270]
	 lr_reduce            : 5
	 optimizer            : SGD
	 dropout              : 0.2
	 kernel_size          : 3
	 dont_save            : False
	 visualize            : False
	 devices              : 0,1
 DataParallel(
  (module): NetworkByName(
    (net): ResNet_v2(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (downsample): Sequential(
            (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (pooling): GaussianPooling2d(
            kernel_size=4, stride=4, padding=0
            (ToHidden): Sequential(
              (0): AdaptiveAvgPool2d(output_size=(1, 1))
              (1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (3): ReLU()
            )
            (ToMean): Sequential(
              (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (ToSigma): Sequential(
              (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Sigmoid()
            )
            (activation): Softplus(beta=1, threshold=20)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (layer2): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (downsample): Sequential(
            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (pooling): GaussianPooling2d(
            kernel_size=2, stride=2, padding=0
            (ToHidden): Sequential(
              (0): AdaptiveAvgPool2d(output_size=(1, 1))
              (1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
              (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (3): ReLU()
            )
            (ToMean): Sequential(
              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (ToSigma): Sequential(
              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Sigmoid()
            )
            (activation): Softplus(beta=1, threshold=20)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (layer3): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (downsample): Sequential(
            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (pooling): GaussianPooling2d(
            kernel_size=2, stride=2, padding=0
            (ToHidden): Sequential(
              (0): AdaptiveAvgPool2d(output_size=(1, 1))
              (1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
              (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (3): ReLU()
            )
            (ToMean): Sequential(
              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (ToSigma): Sequential(
              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Sigmoid()
            )
            (activation): Softplus(beta=1, threshold=20)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (layer4): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (pooling): GaussianPooling2d(
            kernel_size=2, stride=2, padding=0
            (ToHidden): Sequential(
              (0): AdaptiveAvgPool2d(output_size=(1, 1))
              (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
              (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (3): ReLU()
            )
            (ToMean): Sequential(
              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (ToSigma): Sequential(
              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Sigmoid()
            )
            (activation): Softplus(beta=1, threshold=20)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
      (fc): Linear(in_features=512, out_features=10, bias=True)
    )
  )
)
 SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0
)
 Epoch: 1, lr: 1.0e-02, train_loss: 2.9776, train_acc: 0.1464 test_loss: 89.9845, test_acc: 0.1795, best: 0.1795, time: 0:01:00
 Epoch: 2, lr: 1.0e-02, train_loss: 2.3817, train_acc: 0.1704 test_loss: 27.3882, test_acc: 0.2248, best: 0.2248, time: 0:00:50
 Epoch: 3, lr: 1.0e-02, train_loss: 2.1627, train_acc: 0.2076 test_loss: 94.9506, test_acc: 0.2446, best: 0.2446, time: 0:00:50
 Epoch: 4, lr: 1.0e-02, train_loss: 2.0337, train_acc: 0.2400 test_loss: 2.2345, test_acc: 0.3054, best: 0.3054, time: 0:00:49
 Epoch: 5, lr: 1.0e-02, train_loss: 1.9766, train_acc: 0.2612 test_loss: 19.3825, test_acc: 0.3296, best: 0.3296, time: 0:00:50
 Epoch: 6, lr: 1.0e-02, train_loss: 1.9337, train_acc: 0.2646 test_loss: 83.2169, test_acc: 0.3166, best: 0.3296, time: 0:00:49
 Epoch: 7, lr: 1.0e-02, train_loss: 1.8903, train_acc: 0.2944 test_loss: 16.9683, test_acc: 0.3400, best: 0.3400, time: 0:00:51
 Epoch: 8, lr: 1.0e-02, train_loss: 1.8700, train_acc: 0.2956 test_loss: 3.9353, test_acc: 0.3766, best: 0.3766, time: 0:00:50
 Epoch: 9, lr: 1.0e-02, train_loss: 1.8482, train_acc: 0.3062 test_loss: 6.0073, test_acc: 0.3706, best: 0.3766, time: 0:00:51
 Epoch: 10, lr: 1.0e-02, train_loss: 1.8045, train_acc: 0.3170 test_loss: 4.6311, test_acc: 0.3811, best: 0.3811, time: 0:00:49
 Epoch: 11, lr: 1.0e-02, train_loss: 1.7994, train_acc: 0.3192 test_loss: 11.6579, test_acc: 0.4031, best: 0.4031, time: 0:00:50
 Epoch: 12, lr: 1.0e-02, train_loss: 1.7839, train_acc: 0.3254 test_loss: 1.8961, test_acc: 0.4134, best: 0.4134, time: 0:00:50
 Epoch: 13, lr: 1.0e-02, train_loss: 1.7481, train_acc: 0.3314 test_loss: 2.5312, test_acc: 0.3861, best: 0.4134, time: 0:00:50
 Epoch: 14, lr: 1.0e-02, train_loss: 1.7420, train_acc: 0.3384 test_loss: 2.7109, test_acc: 0.3919, best: 0.4134, time: 0:00:51
 Epoch: 15, lr: 1.0e-02, train_loss: 1.7312, train_acc: 0.3380 test_loss: 2.0983, test_acc: 0.3962, best: 0.4134, time: 0:00:50
 Epoch: 16, lr: 1.0e-02, train_loss: 1.6858, train_acc: 0.3620 test_loss: 7.9933, test_acc: 0.4269, best: 0.4269, time: 0:00:50
 Epoch: 17, lr: 1.0e-02, train_loss: 1.6868, train_acc: 0.3620 test_loss: 397.6787, test_acc: 0.4145, best: 0.4269, time: 0:00:49
 Epoch: 18, lr: 1.0e-02, train_loss: 1.6734, train_acc: 0.3818 test_loss: 446.2695, test_acc: 0.4325, best: 0.4325, time: 0:00:50
 Epoch: 19, lr: 1.0e-02, train_loss: 1.6468, train_acc: 0.3912 test_loss: 626.4470, test_acc: 0.3966, best: 0.4325, time: 0:00:49
 Epoch: 20, lr: 1.0e-02, train_loss: 1.6284, train_acc: 0.3924 test_loss: 21.8503, test_acc: 0.4426, best: 0.4426, time: 0:00:50
 Epoch: 21, lr: 1.0e-02, train_loss: 1.5962, train_acc: 0.4084 test_loss: 17.6466, test_acc: 0.4753, best: 0.4753, time: 0:00:51
 Epoch: 22, lr: 1.0e-02, train_loss: 1.5669, train_acc: 0.4132 test_loss: 3.2259, test_acc: 0.4825, best: 0.4825, time: 0:00:50
 Epoch: 23, lr: 1.0e-02, train_loss: 1.5477, train_acc: 0.4254 test_loss: 7.2170, test_acc: 0.4778, best: 0.4825, time: 0:00:49
 Epoch: 24, lr: 1.0e-02, train_loss: 1.5518, train_acc: 0.4242 test_loss: 6.1278, test_acc: 0.4796, best: 0.4825, time: 0:00:49
 Epoch: 25, lr: 1.0e-02, train_loss: 1.5444, train_acc: 0.4264 test_loss: 2.2899, test_acc: 0.4731, best: 0.4825, time: 0:00:50
 Epoch: 26, lr: 1.0e-02, train_loss: 1.5192, train_acc: 0.4356 test_loss: 15.3182, test_acc: 0.4935, best: 0.4935, time: 0:00:50
 Epoch: 27, lr: 1.0e-02, train_loss: 1.4661, train_acc: 0.4594 test_loss: 369.8869, test_acc: 0.4531, best: 0.4935, time: 0:00:49
 Epoch: 28, lr: 1.0e-02, train_loss: 1.5081, train_acc: 0.4414 test_loss: 5.2864, test_acc: 0.4944, best: 0.4944, time: 0:00:50
 Epoch: 29, lr: 1.0e-02, train_loss: 1.4620, train_acc: 0.4514 test_loss: 1.4940, test_acc: 0.5235, best: 0.5235, time: 0:00:49
 Epoch: 30, lr: 1.0e-02, train_loss: 1.4392, train_acc: 0.4726 test_loss: 46.1798, test_acc: 0.4329, best: 0.5235, time: 0:00:50
 Epoch: 31, lr: 1.0e-02, train_loss: 1.4940, train_acc: 0.4472 test_loss: 2.4998, test_acc: 0.4778, best: 0.5235, time: 0:00:49
 Epoch: 32, lr: 1.0e-02, train_loss: 1.4655, train_acc: 0.4556 test_loss: 5.4425, test_acc: 0.5009, best: 0.5235, time: 0:00:49
 Epoch: 33, lr: 1.0e-02, train_loss: 1.4129, train_acc: 0.4828 test_loss: 6.1008, test_acc: 0.5039, best: 0.5235, time: 0:00:49
 Epoch: 34, lr: 1.0e-02, train_loss: 1.3954, train_acc: 0.4898 test_loss: 4.1842, test_acc: 0.4899, best: 0.5235, time: 0:00:50
 Epoch: 35, lr: 1.0e-02, train_loss: 1.3935, train_acc: 0.4872 test_loss: 1.9501, test_acc: 0.5459, best: 0.5459, time: 0:00:50
 Epoch: 36, lr: 1.0e-02, train_loss: 1.3745, train_acc: 0.4934 test_loss: 1.8826, test_acc: 0.5481, best: 0.5481, time: 0:00:50
 Epoch: 37, lr: 1.0e-02, train_loss: 1.3516, train_acc: 0.5040 test_loss: 5.9549, test_acc: 0.5172, best: 0.5481, time: 0:00:50
 Epoch: 38, lr: 1.0e-02, train_loss: 1.3606, train_acc: 0.5020 test_loss: 1.4357, test_acc: 0.5665, best: 0.5665, time: 0:00:49
 Epoch: 39, lr: 1.0e-02, train_loss: 1.3204, train_acc: 0.5132 test_loss: 1.7092, test_acc: 0.5414, best: 0.5665, time: 0:00:49
 Epoch: 40, lr: 1.0e-02, train_loss: 1.3304, train_acc: 0.5170 test_loss: 1.5443, test_acc: 0.5730, best: 0.5730, time: 0:00:50
 Epoch: 41, lr: 1.0e-02, train_loss: 1.2965, train_acc: 0.5242 test_loss: 1.8261, test_acc: 0.5560, best: 0.5730, time: 0:00:50
 Epoch: 42, lr: 1.0e-02, train_loss: 1.2961, train_acc: 0.5272 test_loss: 1.6218, test_acc: 0.5425, best: 0.5730, time: 0:00:49
 Epoch: 43, lr: 1.0e-02, train_loss: 1.2671, train_acc: 0.5384 test_loss: 1.2500, test_acc: 0.5719, best: 0.5730, time: 0:00:49
 Epoch: 44, lr: 1.0e-02, train_loss: 1.2586, train_acc: 0.5398 test_loss: 1.3030, test_acc: 0.5541, best: 0.5730, time: 0:00:50
 Epoch: 45, lr: 1.0e-02, train_loss: 1.2352, train_acc: 0.5564 test_loss: 1.2711, test_acc: 0.5691, best: 0.5730, time: 0:00:50
 Epoch: 46, lr: 1.0e-02, train_loss: 1.2519, train_acc: 0.5438 test_loss: 1.2511, test_acc: 0.5653, best: 0.5730, time: 0:00:49
 Epoch: 47, lr: 1.0e-02, train_loss: 1.2281, train_acc: 0.5602 test_loss: 1.1236, test_acc: 0.6118, best: 0.6118, time: 0:00:49
 Epoch: 48, lr: 1.0e-02, train_loss: 1.2739, train_acc: 0.5338 test_loss: 1.2125, test_acc: 0.5690, best: 0.6118, time: 0:00:50
 Epoch: 49, lr: 1.0e-02, train_loss: 1.2649, train_acc: 0.5410 test_loss: 1.2926, test_acc: 0.5771, best: 0.6118, time: 0:00:51
 Epoch: 50, lr: 1.0e-02, train_loss: 1.1895, train_acc: 0.5742 test_loss: 1.5407, test_acc: 0.5735, best: 0.6118, time: 0:00:50
 Epoch: 51, lr: 1.0e-02, train_loss: 1.2061, train_acc: 0.5680 test_loss: 1.5079, test_acc: 0.5821, best: 0.6118, time: 0:00:50
 Epoch: 52, lr: 1.0e-02, train_loss: 1.1751, train_acc: 0.5708 test_loss: 1.3036, test_acc: 0.5887, best: 0.6118, time: 0:00:50
 Epoch: 53, lr: 1.0e-02, train_loss: 1.1638, train_acc: 0.5836 test_loss: 3.8600, test_acc: 0.5891, best: 0.6118, time: 0:00:50
 Epoch: 54, lr: 1.0e-02, train_loss: 1.1315, train_acc: 0.5848 test_loss: 1.1635, test_acc: 0.6014, best: 0.6118, time: 0:00:50
 Epoch: 55, lr: 1.0e-02, train_loss: 1.1482, train_acc: 0.5896 test_loss: 1.0214, test_acc: 0.6416, best: 0.6416, time: 0:00:50
 Epoch: 56, lr: 1.0e-02, train_loss: 1.1253, train_acc: 0.5914 test_loss: 1.1404, test_acc: 0.6190, best: 0.6416, time: 0:00:49
 Epoch: 57, lr: 1.0e-02, train_loss: 1.1232, train_acc: 0.5962 test_loss: 1.3318, test_acc: 0.5976, best: 0.6416, time: 0:00:49
 Epoch: 58, lr: 1.0e-02, train_loss: 1.1102, train_acc: 0.5958 test_loss: 1.0392, test_acc: 0.6355, best: 0.6416, time: 0:00:49
 Epoch: 59, lr: 1.0e-02, train_loss: 1.1072, train_acc: 0.5930 test_loss: 1.0426, test_acc: 0.6342, best: 0.6416, time: 0:00:49
 Epoch: 60, lr: 1.0e-02, train_loss: 1.1034, train_acc: 0.6006 test_loss: 1.0383, test_acc: 0.6419, best: 0.6419, time: 0:00:51
 Epoch: 61, lr: 1.0e-02, train_loss: 1.0885, train_acc: 0.6062 test_loss: 1.1321, test_acc: 0.6341, best: 0.6419, time: 0:00:50
 Epoch: 62, lr: 1.0e-02, train_loss: 1.0702, train_acc: 0.6134 test_loss: 1.1977, test_acc: 0.6368, best: 0.6419, time: 0:00:50
 Epoch: 63, lr: 1.0e-02, train_loss: 1.0744, train_acc: 0.6090 test_loss: 1.4301, test_acc: 0.5771, best: 0.6419, time: 0:00:50
 Epoch: 64, lr: 1.0e-02, train_loss: 1.0657, train_acc: 0.6168 test_loss: 1.2096, test_acc: 0.6296, best: 0.6419, time: 0:00:50
 Epoch: 65, lr: 1.0e-02, train_loss: 1.0569, train_acc: 0.6174 test_loss: 1.0833, test_acc: 0.6336, best: 0.6419, time: 0:00:50
 Epoch: 66, lr: 1.0e-02, train_loss: 1.0264, train_acc: 0.6288 test_loss: 0.9709, test_acc: 0.6584, best: 0.6584, time: 0:00:49
 Epoch: 67, lr: 1.0e-02, train_loss: 1.0199, train_acc: 0.6382 test_loss: 0.9762, test_acc: 0.6576, best: 0.6584, time: 0:00:49
 Epoch: 68, lr: 1.0e-02, train_loss: 1.0171, train_acc: 0.6262 test_loss: 1.0842, test_acc: 0.6334, best: 0.6584, time: 0:00:49
 Epoch: 69, lr: 1.0e-02, train_loss: 1.0364, train_acc: 0.6252 test_loss: 1.5683, test_acc: 0.6106, best: 0.6584, time: 0:00:49
 Epoch: 70, lr: 1.0e-02, train_loss: 1.0143, train_acc: 0.6326 test_loss: 1.4366, test_acc: 0.6379, best: 0.6584, time: 0:00:50
 Epoch: 71, lr: 1.0e-02, train_loss: 1.0189, train_acc: 0.6332 test_loss: 1.4101, test_acc: 0.6010, best: 0.6584, time: 0:00:49
 Epoch: 72, lr: 1.0e-02, train_loss: 1.0012, train_acc: 0.6416 test_loss: 1.2337, test_acc: 0.6279, best: 0.6584, time: 0:00:50
 Epoch: 73, lr: 1.0e-02, train_loss: 0.9943, train_acc: 0.6416 test_loss: 0.9422, test_acc: 0.6759, best: 0.6759, time: 0:00:50
 Epoch: 74, lr: 1.0e-02, train_loss: 0.9697, train_acc: 0.6526 test_loss: 1.1035, test_acc: 0.6399, best: 0.6759, time: 0:00:50
 Epoch: 75, lr: 1.0e-02, train_loss: 0.9641, train_acc: 0.6512 test_loss: 1.2941, test_acc: 0.6429, best: 0.6759, time: 0:00:50
 Epoch: 76, lr: 1.0e-02, train_loss: 0.9880, train_acc: 0.6428 test_loss: 1.1687, test_acc: 0.6494, best: 0.6759, time: 0:00:50
 Epoch: 77, lr: 1.0e-02, train_loss: 0.9586, train_acc: 0.6560 test_loss: 1.2288, test_acc: 0.6550, best: 0.6759, time: 0:00:49
 Epoch: 78, lr: 1.0e-02, train_loss: 0.9579, train_acc: 0.6570 test_loss: 0.9967, test_acc: 0.6781, best: 0.6781, time: 0:00:49
 Epoch: 79, lr: 1.0e-02, train_loss: 0.9472, train_acc: 0.6724 test_loss: 1.4400, test_acc: 0.6350, best: 0.6781, time: 0:00:50
 Epoch: 80, lr: 1.0e-02, train_loss: 0.9344, train_acc: 0.6716 test_loss: 0.9688, test_acc: 0.6767, best: 0.6781, time: 0:00:50
 Epoch: 81, lr: 1.0e-02, train_loss: 0.9093, train_acc: 0.6764 test_loss: 1.0983, test_acc: 0.6505, best: 0.6781, time: 0:00:51
 Epoch: 82, lr: 1.0e-02, train_loss: 0.9175, train_acc: 0.6708 test_loss: 0.9366, test_acc: 0.6786, best: 0.6786, time: 0:00:50
 Epoch: 83, lr: 1.0e-02, train_loss: 0.8966, train_acc: 0.6852 test_loss: 0.9837, test_acc: 0.6745, best: 0.6786, time: 0:00:50
 Epoch: 84, lr: 1.0e-02, train_loss: 0.9173, train_acc: 0.6712 test_loss: 0.9696, test_acc: 0.6789, best: 0.6789, time: 0:00:50
 Epoch: 85, lr: 1.0e-02, train_loss: 0.9340, train_acc: 0.6598 test_loss: 2.8753, test_acc: 0.5965, best: 0.6789, time: 0:00:49
 Epoch: 86, lr: 1.0e-02, train_loss: 0.8845, train_acc: 0.6848 test_loss: 0.9995, test_acc: 0.6771, best: 0.6789, time: 0:00:49
 Epoch: 87, lr: 1.0e-02, train_loss: 0.8545, train_acc: 0.6924 test_loss: 1.0450, test_acc: 0.6670, best: 0.6789, time: 0:00:49
 Epoch: 88, lr: 1.0e-02, train_loss: 0.8829, train_acc: 0.6708 test_loss: 1.0382, test_acc: 0.6683, best: 0.6789, time: 0:00:49
 Epoch: 89, lr: 1.0e-02, train_loss: 0.8668, train_acc: 0.6930 test_loss: 1.2402, test_acc: 0.6536, best: 0.6789, time: 0:00:51
 Epoch: 90, lr: 1.0e-02, train_loss: 0.8915, train_acc: 0.6798 test_loss: 0.9648, test_acc: 0.6736, best: 0.6789, time: 0:00:50
 Epoch: 91, lr: 1.0e-02, train_loss: 0.8903, train_acc: 0.6824 test_loss: 0.8538, test_acc: 0.7015, best: 0.7015, time: 0:00:50
 Epoch: 92, lr: 1.0e-02, train_loss: 0.8461, train_acc: 0.6952 test_loss: 1.0846, test_acc: 0.6696, best: 0.7015, time: 0:00:50
 Epoch: 93, lr: 1.0e-02, train_loss: 0.8670, train_acc: 0.6920 test_loss: 0.9640, test_acc: 0.6913, best: 0.7015, time: 0:00:50
 Epoch: 94, lr: 1.0e-02, train_loss: 0.8369, train_acc: 0.6958 test_loss: 1.0342, test_acc: 0.6589, best: 0.7015, time: 0:00:50
 Epoch: 95, lr: 1.0e-02, train_loss: 0.8414, train_acc: 0.6972 test_loss: 1.0062, test_acc: 0.6836, best: 0.7015, time: 0:00:50
 Epoch: 96, lr: 1.0e-02, train_loss: 0.8380, train_acc: 0.7010 test_loss: 0.8422, test_acc: 0.7133, best: 0.7133, time: 0:00:50
 Epoch: 97, lr: 1.0e-02, train_loss: 0.8334, train_acc: 0.6992 test_loss: 0.8177, test_acc: 0.7211, best: 0.7211, time: 0:00:50
 Epoch: 98, lr: 1.0e-02, train_loss: 0.8195, train_acc: 0.7110 test_loss: 1.0271, test_acc: 0.6725, best: 0.7211, time: 0:00:49
 Epoch: 99, lr: 1.0e-02, train_loss: 0.8150, train_acc: 0.7068 test_loss: 1.0556, test_acc: 0.6716, best: 0.7211, time: 0:00:50
 Epoch: 100, lr: 1.0e-02, train_loss: 0.8256, train_acc: 0.7040 test_loss: 0.9078, test_acc: 0.7025, best: 0.7211, time: 0:00:49
 Epoch: 101, lr: 1.0e-02, train_loss: 0.7856, train_acc: 0.7166 test_loss: 0.9505, test_acc: 0.6980, best: 0.7211, time: 0:00:49
 Epoch: 102, lr: 1.0e-02, train_loss: 0.7826, train_acc: 0.7246 test_loss: 1.0205, test_acc: 0.6916, best: 0.7211, time: 0:00:50
 Epoch: 103, lr: 1.0e-02, train_loss: 0.7950, train_acc: 0.7136 test_loss: 1.0015, test_acc: 0.6861, best: 0.7211, time: 0:00:50
 Epoch: 104, lr: 1.0e-02, train_loss: 0.7939, train_acc: 0.7208 test_loss: 0.9896, test_acc: 0.6939, best: 0.7211, time: 0:00:50
 Epoch: 105, lr: 1.0e-02, train_loss: 0.7938, train_acc: 0.7138 test_loss: 0.9250, test_acc: 0.6950, best: 0.7211, time: 0:00:49
 Epoch: 106, lr: 1.0e-02, train_loss: 0.7841, train_acc: 0.7216 test_loss: 0.9472, test_acc: 0.7065, best: 0.7211, time: 0:00:49
 Epoch: 107, lr: 1.0e-02, train_loss: 0.7717, train_acc: 0.7260 test_loss: 0.9038, test_acc: 0.7111, best: 0.7211, time: 0:00:49
 Epoch: 108, lr: 1.0e-02, train_loss: 0.7655, train_acc: 0.7380 test_loss: 0.9882, test_acc: 0.6976, best: 0.7211, time: 0:00:50
 Epoch: 109, lr: 1.0e-02, train_loss: 0.7698, train_acc: 0.7318 test_loss: 1.0092, test_acc: 0.6990, best: 0.7211, time: 0:00:49
 Epoch: 110, lr: 1.0e-02, train_loss: 0.7349, train_acc: 0.7392 test_loss: 0.9139, test_acc: 0.7075, best: 0.7211, time: 0:00:49
 Epoch: 111, lr: 1.0e-02, train_loss: 0.7464, train_acc: 0.7376 test_loss: 1.0136, test_acc: 0.7006, best: 0.7211, time: 0:00:51
 Epoch: 112, lr: 1.0e-02, train_loss: 0.7457, train_acc: 0.7372 test_loss: 0.8898, test_acc: 0.7225, best: 0.7225, time: 0:00:49
 Epoch: 113, lr: 1.0e-02, train_loss: 0.7471, train_acc: 0.7374 test_loss: 1.1218, test_acc: 0.6859, best: 0.7225, time: 0:00:49
 Epoch: 114, lr: 1.0e-02, train_loss: 0.7487, train_acc: 0.7398 test_loss: 0.9389, test_acc: 0.7067, best: 0.7225, time: 0:00:49
 Epoch: 115, lr: 1.0e-02, train_loss: 0.7514, train_acc: 0.7366 test_loss: 0.8547, test_acc: 0.7234, best: 0.7234, time: 0:00:50
 Epoch: 116, lr: 1.0e-02, train_loss: 0.7185, train_acc: 0.7464 test_loss: 0.9536, test_acc: 0.7156, best: 0.7234, time: 0:00:50
 Epoch: 117, lr: 1.0e-02, train_loss: 0.7227, train_acc: 0.7508 test_loss: 0.9402, test_acc: 0.7095, best: 0.7234, time: 0:00:49
 Epoch: 118, lr: 1.0e-02, train_loss: 0.7201, train_acc: 0.7470 test_loss: 1.0000, test_acc: 0.7036, best: 0.7234, time: 0:00:49
 Epoch: 119, lr: 1.0e-02, train_loss: 0.7211, train_acc: 0.7446 test_loss: 1.0292, test_acc: 0.7041, best: 0.7234, time: 0:00:49
 Epoch: 120, lr: 1.0e-02, train_loss: 0.6992, train_acc: 0.7506 test_loss: 0.8847, test_acc: 0.7268, best: 0.7268, time: 0:00:50
 Epoch: 121, lr: 1.0e-02, train_loss: 0.7096, train_acc: 0.7462 test_loss: 0.8604, test_acc: 0.7276, best: 0.7276, time: 0:00:50
 Epoch: 122, lr: 1.0e-02, train_loss: 0.6801, train_acc: 0.7600 test_loss: 0.8786, test_acc: 0.7155, best: 0.7276, time: 0:00:49
 Epoch: 123, lr: 1.0e-02, train_loss: 0.7055, train_acc: 0.7560 test_loss: 0.9495, test_acc: 0.7166, best: 0.7276, time: 0:00:50
 Epoch: 124, lr: 1.0e-02, train_loss: 0.6828, train_acc: 0.7536 test_loss: 0.9043, test_acc: 0.7147, best: 0.7276, time: 0:00:50
 Epoch: 125, lr: 1.0e-02, train_loss: 0.6624, train_acc: 0.7632 test_loss: 1.1288, test_acc: 0.6879, best: 0.7276, time: 0:00:49
 Epoch: 126, lr: 1.0e-02, train_loss: 0.6792, train_acc: 0.7610 test_loss: 0.9215, test_acc: 0.7254, best: 0.7276, time: 0:00:50
 Epoch: 127, lr: 1.0e-02, train_loss: 0.6757, train_acc: 0.7660 test_loss: 0.9461, test_acc: 0.7177, best: 0.7276, time: 0:00:49
 Epoch: 128, lr: 1.0e-02, train_loss: 0.6832, train_acc: 0.7602 test_loss: 0.8503, test_acc: 0.7369, best: 0.7369, time: 0:00:50
 Epoch: 129, lr: 1.0e-02, train_loss: 0.6632, train_acc: 0.7630 test_loss: 0.9831, test_acc: 0.7084, best: 0.7369, time: 0:00:49
 Epoch: 130, lr: 1.0e-02, train_loss: 0.6658, train_acc: 0.7642 test_loss: 0.9854, test_acc: 0.7106, best: 0.7369, time: 0:00:49
 Epoch: 131, lr: 1.0e-02, train_loss: 0.6518, train_acc: 0.7748 test_loss: 0.9372, test_acc: 0.7281, best: 0.7369, time: 0:00:50
 Epoch: 132, lr: 1.0e-02, train_loss: 0.6558, train_acc: 0.7630 test_loss: 0.9941, test_acc: 0.7130, best: 0.7369, time: 0:00:49
 Epoch: 133, lr: 1.0e-02, train_loss: 0.6528, train_acc: 0.7728 test_loss: 0.8881, test_acc: 0.7308, best: 0.7369, time: 0:00:49
 Epoch: 134, lr: 1.0e-02, train_loss: 0.6493, train_acc: 0.7722 test_loss: 0.8911, test_acc: 0.7326, best: 0.7369, time: 0:00:49
 Epoch: 135, lr: 1.0e-02, train_loss: 0.6576, train_acc: 0.7652 test_loss: 0.9860, test_acc: 0.7049, best: 0.7369, time: 0:00:50
 Epoch: 136, lr: 1.0e-02, train_loss: 0.6390, train_acc: 0.7770 test_loss: 0.9121, test_acc: 0.7430, best: 0.7430, time: 0:00:50
 Epoch: 137, lr: 1.0e-02, train_loss: 0.6485, train_acc: 0.7770 test_loss: 0.8750, test_acc: 0.7324, best: 0.7430, time: 0:00:49
 Epoch: 138, lr: 1.0e-02, train_loss: 0.6170, train_acc: 0.7820 test_loss: 0.9918, test_acc: 0.7149, best: 0.7430, time: 0:00:49
 Epoch: 139, lr: 1.0e-02, train_loss: 0.6313, train_acc: 0.7790 test_loss: 0.9549, test_acc: 0.7181, best: 0.7430, time: 0:00:49
 Epoch: 140, lr: 1.0e-02, train_loss: 0.6259, train_acc: 0.7778 test_loss: 0.9307, test_acc: 0.7248, best: 0.7430, time: 0:00:49
 Epoch: 141, lr: 1.0e-02, train_loss: 0.6167, train_acc: 0.7844 test_loss: 1.0110, test_acc: 0.7199, best: 0.7430, time: 0:00:49
 Epoch: 142, lr: 1.0e-02, train_loss: 0.6278, train_acc: 0.7808 test_loss: 0.8650, test_acc: 0.7391, best: 0.7430, time: 0:00:50
 Epoch: 143, lr: 1.0e-02, train_loss: 0.6139, train_acc: 0.7850 test_loss: 1.0437, test_acc: 0.7264, best: 0.7430, time: 0:00:49
 Epoch: 144, lr: 1.0e-02, train_loss: 0.5987, train_acc: 0.7846 test_loss: 0.9775, test_acc: 0.7250, best: 0.7430, time: 0:00:50
 Epoch: 145, lr: 1.0e-02, train_loss: 0.5942, train_acc: 0.7954 test_loss: 1.0382, test_acc: 0.7170, best: 0.7430, time: 0:00:50
 Epoch: 146, lr: 1.0e-02, train_loss: 0.6072, train_acc: 0.7840 test_loss: 1.0079, test_acc: 0.7180, best: 0.7430, time: 0:00:50
 Epoch: 147, lr: 1.0e-02, train_loss: 0.6055, train_acc: 0.7856 test_loss: 1.0680, test_acc: 0.6944, best: 0.7430, time: 0:00:50
 Epoch: 148, lr: 1.0e-02, train_loss: 0.6021, train_acc: 0.7868 test_loss: 1.0388, test_acc: 0.7181, best: 0.7430, time: 0:00:49
 Epoch: 149, lr: 1.0e-02, train_loss: 0.6044, train_acc: 0.7906 test_loss: 0.8909, test_acc: 0.7351, best: 0.7430, time: 0:00:50
 Epoch: 150, lr: 1.0e-02, train_loss: 0.5894, train_acc: 0.7980 test_loss: 0.9793, test_acc: 0.7159, best: 0.7430, time: 0:00:49
 Epoch: 151, lr: 1.0e-02, train_loss: 0.5910, train_acc: 0.7922 test_loss: 0.9726, test_acc: 0.7341, best: 0.7430, time: 0:00:49
 Epoch: 152, lr: 1.0e-02, train_loss: 0.5669, train_acc: 0.7990 test_loss: 0.9501, test_acc: 0.7376, best: 0.7430, time: 0:00:50
 Epoch: 153, lr: 1.0e-02, train_loss: 0.5807, train_acc: 0.7944 test_loss: 0.8800, test_acc: 0.7491, best: 0.7491, time: 0:00:50
 Epoch: 154, lr: 1.0e-02, train_loss: 0.5777, train_acc: 0.7970 test_loss: 0.8900, test_acc: 0.7480, best: 0.7491, time: 0:00:49
 Epoch: 155, lr: 1.0e-02, train_loss: 0.5705, train_acc: 0.7960 test_loss: 1.0634, test_acc: 0.7209, best: 0.7491, time: 0:00:50
 Epoch: 156, lr: 1.0e-02, train_loss: 0.5887, train_acc: 0.7942 test_loss: 0.9802, test_acc: 0.7259, best: 0.7491, time: 0:00:49
 Epoch: 157, lr: 1.0e-02, train_loss: 0.5757, train_acc: 0.8004 test_loss: 0.9463, test_acc: 0.7298, best: 0.7491, time: 0:00:48
 Epoch: 158, lr: 1.0e-02, train_loss: 0.5730, train_acc: 0.7974 test_loss: 0.9440, test_acc: 0.7322, best: 0.7491, time: 0:00:50
 Epoch: 159, lr: 1.0e-02, train_loss: 0.5695, train_acc: 0.8018 test_loss: 0.9205, test_acc: 0.7492, best: 0.7492, time: 0:00:50
 Epoch: 160, lr: 1.0e-02, train_loss: 0.5650, train_acc: 0.7986 test_loss: 0.9417, test_acc: 0.7295, best: 0.7492, time: 0:00:49
 Epoch: 161, lr: 1.0e-02, train_loss: 0.5604, train_acc: 0.8102 test_loss: 0.8810, test_acc: 0.7494, best: 0.7494, time: 0:00:50
 Epoch: 162, lr: 1.0e-02, train_loss: 0.5486, train_acc: 0.8064 test_loss: 0.9328, test_acc: 0.7469, best: 0.7494, time: 0:00:49
 Epoch: 163, lr: 1.0e-02, train_loss: 0.5355, train_acc: 0.8068 test_loss: 0.9260, test_acc: 0.7474, best: 0.7494, time: 0:00:49
 Epoch: 164, lr: 1.0e-02, train_loss: 0.5581, train_acc: 0.8078 test_loss: 1.0480, test_acc: 0.7209, best: 0.7494, time: 0:00:48
 Epoch: 165, lr: 1.0e-02, train_loss: 0.5756, train_acc: 0.8004 test_loss: 0.9825, test_acc: 0.7310, best: 0.7494, time: 0:00:49
 Epoch: 166, lr: 1.0e-02, train_loss: 0.5465, train_acc: 0.8100 test_loss: 1.0307, test_acc: 0.7208, best: 0.7494, time: 0:00:49
 Epoch: 167, lr: 1.0e-02, train_loss: 0.5406, train_acc: 0.8044 test_loss: 0.9305, test_acc: 0.7446, best: 0.7494, time: 0:00:49
 Epoch: 168, lr: 1.0e-02, train_loss: 0.5525, train_acc: 0.8046 test_loss: 0.9583, test_acc: 0.7316, best: 0.7494, time: 0:00:49
 Epoch: 169, lr: 1.0e-02, train_loss: 0.5267, train_acc: 0.8182 test_loss: 1.0258, test_acc: 0.7280, best: 0.7494, time: 0:00:49
 Epoch: 170, lr: 1.0e-02, train_loss: 0.5449, train_acc: 0.8122 test_loss: 1.0287, test_acc: 0.7398, best: 0.7494, time: 0:00:49
 Epoch: 171, lr: 1.0e-02, train_loss: 0.5392, train_acc: 0.8102 test_loss: 0.9154, test_acc: 0.7479, best: 0.7494, time: 0:00:48
 Epoch: 172, lr: 1.0e-02, train_loss: 0.5257, train_acc: 0.8212 test_loss: 0.9143, test_acc: 0.7565, best: 0.7565, time: 0:00:50
 Epoch: 173, lr: 1.0e-02, train_loss: 0.5397, train_acc: 0.8120 test_loss: 0.9293, test_acc: 0.7471, best: 0.7565, time: 0:00:48
 Epoch: 174, lr: 1.0e-02, train_loss: 0.5387, train_acc: 0.8130 test_loss: 1.0807, test_acc: 0.7271, best: 0.7565, time: 0:00:49
 Epoch: 175, lr: 1.0e-02, train_loss: 0.5268, train_acc: 0.8190 test_loss: 1.0956, test_acc: 0.7166, best: 0.7565, time: 0:00:49
 Epoch: 176, lr: 1.0e-02, train_loss: 0.5241, train_acc: 0.8144 test_loss: 0.9728, test_acc: 0.7414, best: 0.7565, time: 0:00:49
 Epoch: 177, lr: 1.0e-02, train_loss: 0.5324, train_acc: 0.8106 test_loss: 1.0034, test_acc: 0.7444, best: 0.7565, time: 0:00:49
 Epoch: 178, lr: 1.0e-02, train_loss: 0.5232, train_acc: 0.8122 test_loss: 0.9737, test_acc: 0.7376, best: 0.7565, time: 0:00:49
 Epoch: 179, lr: 1.0e-02, train_loss: 0.5151, train_acc: 0.8116 test_loss: 1.0048, test_acc: 0.7464, best: 0.7565, time: 0:00:49
 Epoch: 180, lr: 2.0e-03, train_loss: 0.4388, train_acc: 0.8514 test_loss: 0.8576, test_acc: 0.7726, best: 0.7726, time: 0:00:49
 Epoch: 181, lr: 2.0e-03, train_loss: 0.4274, train_acc: 0.8502 test_loss: 0.9419, test_acc: 0.7578, best: 0.7726, time: 0:00:49
 Epoch: 182, lr: 2.0e-03, train_loss: 0.4224, train_acc: 0.8574 test_loss: 0.8823, test_acc: 0.7704, best: 0.7726, time: 0:00:49
 Epoch: 183, lr: 2.0e-03, train_loss: 0.4225, train_acc: 0.8558 test_loss: 0.9110, test_acc: 0.7658, best: 0.7726, time: 0:00:49
 Epoch: 184, lr: 2.0e-03, train_loss: 0.3886, train_acc: 0.8628 test_loss: 0.9624, test_acc: 0.7561, best: 0.7726, time: 0:00:49
 Epoch: 185, lr: 2.0e-03, train_loss: 0.4062, train_acc: 0.8584 test_loss: 0.8897, test_acc: 0.7649, best: 0.7726, time: 0:00:48
 Epoch: 186, lr: 2.0e-03, train_loss: 0.4034, train_acc: 0.8572 test_loss: 0.9011, test_acc: 0.7685, best: 0.7726, time: 0:00:49
 Epoch: 187, lr: 2.0e-03, train_loss: 0.3932, train_acc: 0.8612 test_loss: 0.9201, test_acc: 0.7628, best: 0.7726, time: 0:00:48
 Epoch: 188, lr: 2.0e-03, train_loss: 0.3784, train_acc: 0.8682 test_loss: 0.9318, test_acc: 0.7656, best: 0.7726, time: 0:00:49
 Epoch: 189, lr: 2.0e-03, train_loss: 0.3744, train_acc: 0.8694 test_loss: 0.9189, test_acc: 0.7640, best: 0.7726, time: 0:00:48
 Epoch: 190, lr: 2.0e-03, train_loss: 0.3860, train_acc: 0.8666 test_loss: 0.9181, test_acc: 0.7709, best: 0.7726, time: 0:00:49
 Epoch: 191, lr: 2.0e-03, train_loss: 0.3658, train_acc: 0.8764 test_loss: 0.9417, test_acc: 0.7579, best: 0.7726, time: 0:00:49
 Epoch: 192, lr: 2.0e-03, train_loss: 0.3776, train_acc: 0.8718 test_loss: 0.9253, test_acc: 0.7712, best: 0.7726, time: 0:00:48
 Epoch: 193, lr: 2.0e-03, train_loss: 0.3737, train_acc: 0.8686 test_loss: 0.9460, test_acc: 0.7655, best: 0.7726, time: 0:00:49
 Epoch: 194, lr: 2.0e-03, train_loss: 0.3881, train_acc: 0.8680 test_loss: 0.9745, test_acc: 0.7648, best: 0.7726, time: 0:00:48
 Epoch: 195, lr: 2.0e-03, train_loss: 0.3772, train_acc: 0.8686 test_loss: 0.9414, test_acc: 0.7589, best: 0.7726, time: 0:00:49
 Epoch: 196, lr: 2.0e-03, train_loss: 0.3834, train_acc: 0.8674 test_loss: 0.9953, test_acc: 0.7616, best: 0.7726, time: 0:00:49
 Epoch: 197, lr: 2.0e-03, train_loss: 0.3825, train_acc: 0.8726 test_loss: 0.9600, test_acc: 0.7591, best: 0.7726, time: 0:00:49
 Epoch: 198, lr: 2.0e-03, train_loss: 0.3623, train_acc: 0.8780 test_loss: 0.9129, test_acc: 0.7686, best: 0.7726, time: 0:00:49
 Epoch: 199, lr: 2.0e-03, train_loss: 0.3723, train_acc: 0.8698 test_loss: 0.9431, test_acc: 0.7654, best: 0.7726, time: 0:00:49
 Epoch: 200, lr: 2.0e-03, train_loss: 0.3781, train_acc: 0.8618 test_loss: 0.9414, test_acc: 0.7675, best: 0.7726, time: 0:00:49
 Epoch: 201, lr: 2.0e-03, train_loss: 0.3406, train_acc: 0.8812 test_loss: 0.9700, test_acc: 0.7658, best: 0.7726, time: 0:00:49
 Epoch: 202, lr: 2.0e-03, train_loss: 0.3658, train_acc: 0.8712 test_loss: 0.9222, test_acc: 0.7686, best: 0.7726, time: 0:00:49
 Epoch: 203, lr: 2.0e-03, train_loss: 0.3542, train_acc: 0.8750 test_loss: 0.9447, test_acc: 0.7690, best: 0.7726, time: 0:00:49
 Epoch: 204, lr: 2.0e-03, train_loss: 0.3841, train_acc: 0.8664 test_loss: 0.8969, test_acc: 0.7686, best: 0.7726, time: 0:00:49
 Epoch: 205, lr: 2.0e-03, train_loss: 0.3625, train_acc: 0.8718 test_loss: 0.9231, test_acc: 0.7691, best: 0.7726, time: 0:00:49
 Epoch: 206, lr: 2.0e-03, train_loss: 0.3478, train_acc: 0.8770 test_loss: 1.0664, test_acc: 0.7501, best: 0.7726, time: 0:00:50
 Epoch: 207, lr: 2.0e-03, train_loss: 0.3493, train_acc: 0.8824 test_loss: 0.9472, test_acc: 0.7634, best: 0.7726, time: 0:00:49
 Epoch: 208, lr: 2.0e-03, train_loss: 0.3422, train_acc: 0.8764 test_loss: 0.9437, test_acc: 0.7630, best: 0.7726, time: 0:00:48
 Epoch: 209, lr: 2.0e-03, train_loss: 0.3594, train_acc: 0.8752 test_loss: 0.9797, test_acc: 0.7642, best: 0.7726, time: 0:00:49
 Epoch: 210, lr: 2.0e-03, train_loss: 0.3711, train_acc: 0.8674 test_loss: 0.9511, test_acc: 0.7635, best: 0.7726, time: 0:00:49
 Epoch: 211, lr: 2.0e-03, train_loss: 0.3704, train_acc: 0.8736 test_loss: 0.9618, test_acc: 0.7666, best: 0.7726, time: 0:00:48
 Epoch: 212, lr: 2.0e-03, train_loss: 0.3500, train_acc: 0.8760 test_loss: 0.9379, test_acc: 0.7751, best: 0.7751, time: 0:00:49
 Epoch: 213, lr: 2.0e-03, train_loss: 0.3411, train_acc: 0.8842 test_loss: 0.9945, test_acc: 0.7646, best: 0.7751, time: 0:00:49
 Epoch: 214, lr: 2.0e-03, train_loss: 0.3470, train_acc: 0.8816 test_loss: 0.9456, test_acc: 0.7702, best: 0.7751, time: 0:00:48
 Epoch: 215, lr: 2.0e-03, train_loss: 0.3525, train_acc: 0.8768 test_loss: 0.9660, test_acc: 0.7632, best: 0.7751, time: 0:00:48
 Epoch: 216, lr: 2.0e-03, train_loss: 0.3385, train_acc: 0.8840 test_loss: 0.9158, test_acc: 0.7695, best: 0.7751, time: 0:00:49
 Epoch: 217, lr: 2.0e-03, train_loss: 0.3428, train_acc: 0.8762 test_loss: 0.9702, test_acc: 0.7695, best: 0.7751, time: 0:00:48
 Epoch: 218, lr: 2.0e-03, train_loss: 0.3541, train_acc: 0.8706 test_loss: 0.9320, test_acc: 0.7739, best: 0.7751, time: 0:00:48
 Epoch: 219, lr: 2.0e-03, train_loss: 0.3412, train_acc: 0.8808 test_loss: 0.9463, test_acc: 0.7698, best: 0.7751, time: 0:00:48
 Epoch: 220, lr: 2.0e-03, train_loss: 0.3369, train_acc: 0.8826 test_loss: 0.9875, test_acc: 0.7651, best: 0.7751, time: 0:00:49
 Epoch: 221, lr: 2.0e-03, train_loss: 0.3321, train_acc: 0.8852 test_loss: 0.9837, test_acc: 0.7664, best: 0.7751, time: 0:00:49
 Epoch: 222, lr: 2.0e-03, train_loss: 0.3329, train_acc: 0.8888 test_loss: 0.9854, test_acc: 0.7675, best: 0.7751, time: 0:00:48
 Epoch: 223, lr: 2.0e-03, train_loss: 0.3403, train_acc: 0.8836 test_loss: 0.9871, test_acc: 0.7681, best: 0.7751, time: 0:00:49
 Epoch: 224, lr: 2.0e-03, train_loss: 0.3498, train_acc: 0.8764 test_loss: 0.9666, test_acc: 0.7746, best: 0.7751, time: 0:00:49
 Epoch: 225, lr: 2.0e-03, train_loss: 0.3545, train_acc: 0.8770 test_loss: 0.9097, test_acc: 0.7734, best: 0.7751, time: 0:00:49
 Epoch: 226, lr: 2.0e-03, train_loss: 0.3335, train_acc: 0.8868 test_loss: 0.9594, test_acc: 0.7738, best: 0.7751, time: 0:00:48
 Epoch: 227, lr: 2.0e-03, train_loss: 0.3382, train_acc: 0.8858 test_loss: 0.9281, test_acc: 0.7718, best: 0.7751, time: 0:00:49
 Epoch: 228, lr: 2.0e-03, train_loss: 0.3172, train_acc: 0.8866 test_loss: 0.9902, test_acc: 0.7648, best: 0.7751, time: 0:00:49
 Epoch: 229, lr: 2.0e-03, train_loss: 0.3426, train_acc: 0.8824 test_loss: 0.9480, test_acc: 0.7726, best: 0.7751, time: 0:00:49
 Epoch: 230, lr: 2.0e-03, train_loss: 0.3353, train_acc: 0.8848 test_loss: 0.9722, test_acc: 0.7704, best: 0.7751, time: 0:00:49
 Epoch: 231, lr: 2.0e-03, train_loss: 0.3237, train_acc: 0.8866 test_loss: 0.9489, test_acc: 0.7695, best: 0.7751, time: 0:00:49
 Epoch: 232, lr: 2.0e-03, train_loss: 0.3338, train_acc: 0.8854 test_loss: 0.9394, test_acc: 0.7758, best: 0.7758, time: 0:00:48
 Epoch: 233, lr: 2.0e-03, train_loss: 0.3204, train_acc: 0.8896 test_loss: 1.0436, test_acc: 0.7580, best: 0.7758, time: 0:00:49
 Epoch: 234, lr: 2.0e-03, train_loss: 0.3203, train_acc: 0.8838 test_loss: 0.9843, test_acc: 0.7666, best: 0.7758, time: 0:00:48
 Epoch: 235, lr: 2.0e-03, train_loss: 0.3466, train_acc: 0.8826 test_loss: 0.9530, test_acc: 0.7739, best: 0.7758, time: 0:00:49
 Epoch: 236, lr: 2.0e-03, train_loss: 0.3412, train_acc: 0.8828 test_loss: 0.9923, test_acc: 0.7676, best: 0.7758, time: 0:00:49
 Epoch: 237, lr: 2.0e-03, train_loss: 0.3423, train_acc: 0.8800 test_loss: 0.9942, test_acc: 0.7724, best: 0.7758, time: 0:00:49
 Epoch: 238, lr: 2.0e-03, train_loss: 0.3193, train_acc: 0.8856 test_loss: 0.9846, test_acc: 0.7636, best: 0.7758, time: 0:00:48
 Epoch: 239, lr: 2.0e-03, train_loss: 0.3285, train_acc: 0.8892 test_loss: 1.0172, test_acc: 0.7631, best: 0.7758, time: 0:00:49
 Epoch: 240, lr: 4.0e-04, train_loss: 0.3284, train_acc: 0.8842 test_loss: 0.9709, test_acc: 0.7696, best: 0.7758, time: 0:00:49
 Epoch: 241, lr: 4.0e-04, train_loss: 0.3121, train_acc: 0.8980 test_loss: 0.9673, test_acc: 0.7642, best: 0.7758, time: 0:00:48
 Epoch: 242, lr: 4.0e-04, train_loss: 0.3164, train_acc: 0.8916 test_loss: 0.9423, test_acc: 0.7706, best: 0.7758, time: 0:00:49
 Epoch: 243, lr: 4.0e-04, train_loss: 0.3160, train_acc: 0.8906 test_loss: 0.9958, test_acc: 0.7680, best: 0.7758, time: 0:00:48
 Epoch: 244, lr: 4.0e-04, train_loss: 0.2965, train_acc: 0.8962 test_loss: 0.9632, test_acc: 0.7638, best: 0.7758, time: 0:00:49
 Epoch: 245, lr: 4.0e-04, train_loss: 0.3151, train_acc: 0.8918 test_loss: 0.9765, test_acc: 0.7662, best: 0.7758, time: 0:00:49
 Epoch: 246, lr: 4.0e-04, train_loss: 0.2991, train_acc: 0.8960 test_loss: 0.9619, test_acc: 0.7690, best: 0.7758, time: 0:00:48
 Epoch: 247, lr: 4.0e-04, train_loss: 0.3196, train_acc: 0.8918 test_loss: 0.9645, test_acc: 0.7651, best: 0.7758, time: 0:00:49
 Epoch: 248, lr: 4.0e-04, train_loss: 0.3210, train_acc: 0.8882 test_loss: 0.9627, test_acc: 0.7675, best: 0.7758, time: 0:00:49
 Epoch: 249, lr: 4.0e-04, train_loss: 0.3030, train_acc: 0.8958 test_loss: 0.9876, test_acc: 0.7666, best: 0.7758, time: 0:00:50
 Epoch: 250, lr: 4.0e-04, train_loss: 0.3160, train_acc: 0.8862 test_loss: 1.0089, test_acc: 0.7670, best: 0.7758, time: 0:00:48
 Epoch: 251, lr: 4.0e-04, train_loss: 0.3051, train_acc: 0.8988 test_loss: 1.0105, test_acc: 0.7685, best: 0.7758, time: 0:00:49
 Epoch: 252, lr: 4.0e-04, train_loss: 0.3013, train_acc: 0.8994 test_loss: 0.9845, test_acc: 0.7720, best: 0.7758, time: 0:00:48
 Epoch: 253, lr: 4.0e-04, train_loss: 0.2987, train_acc: 0.8958 test_loss: 0.9647, test_acc: 0.7699, best: 0.7758, time: 0:00:49
 Epoch: 254, lr: 4.0e-04, train_loss: 0.2917, train_acc: 0.9000 test_loss: 1.0202, test_acc: 0.7684, best: 0.7758, time: 0:00:49
 Epoch: 255, lr: 4.0e-04, train_loss: 0.3175, train_acc: 0.8898 test_loss: 0.9961, test_acc: 0.7674, best: 0.7758, time: 0:00:49
 Epoch: 256, lr: 4.0e-04, train_loss: 0.3035, train_acc: 0.8948 test_loss: 0.9886, test_acc: 0.7724, best: 0.7758, time: 0:00:49
 Epoch: 257, lr: 4.0e-04, train_loss: 0.3121, train_acc: 0.8922 test_loss: 0.9830, test_acc: 0.7651, best: 0.7758, time: 0:00:49
 Epoch: 258, lr: 4.0e-04, train_loss: 0.3106, train_acc: 0.8958 test_loss: 0.9873, test_acc: 0.7722, best: 0.7758, time: 0:00:49
 Epoch: 259, lr: 4.0e-04, train_loss: 0.3176, train_acc: 0.8928 test_loss: 0.9814, test_acc: 0.7692, best: 0.7758, time: 0:00:49
 Epoch: 260, lr: 4.0e-04, train_loss: 0.3028, train_acc: 0.9000 test_loss: 0.9781, test_acc: 0.7722, best: 0.7758, time: 0:00:48
 Epoch: 261, lr: 4.0e-04, train_loss: 0.3029, train_acc: 0.8952 test_loss: 0.9416, test_acc: 0.7716, best: 0.7758, time: 0:00:49
 Epoch: 262, lr: 4.0e-04, train_loss: 0.2871, train_acc: 0.9034 test_loss: 0.9842, test_acc: 0.7699, best: 0.7758, time: 0:00:49
 Epoch: 263, lr: 4.0e-04, train_loss: 0.3033, train_acc: 0.8936 test_loss: 0.9722, test_acc: 0.7702, best: 0.7758, time: 0:00:50
 Epoch: 264, lr: 4.0e-04, train_loss: 0.3046, train_acc: 0.8988 test_loss: 0.9555, test_acc: 0.7754, best: 0.7758, time: 0:00:49
 Epoch: 265, lr: 4.0e-04, train_loss: 0.2946, train_acc: 0.9000 test_loss: 1.0242, test_acc: 0.7732, best: 0.7758, time: 0:00:49
 Epoch: 266, lr: 4.0e-04, train_loss: 0.3117, train_acc: 0.8912 test_loss: 1.0067, test_acc: 0.7739, best: 0.7758, time: 0:00:49
 Epoch: 267, lr: 4.0e-04, train_loss: 0.2849, train_acc: 0.9048 test_loss: 0.9625, test_acc: 0.7761, best: 0.7761, time: 0:00:49
 Epoch: 268, lr: 4.0e-04, train_loss: 0.3181, train_acc: 0.8924 test_loss: 1.0097, test_acc: 0.7639, best: 0.7761, time: 0:00:48
 Epoch: 269, lr: 4.0e-04, train_loss: 0.2961, train_acc: 0.9022 test_loss: 0.9651, test_acc: 0.7728, best: 0.7761, time: 0:00:49
 Epoch: 270, lr: 8.0e-05, train_loss: 0.3000, train_acc: 0.8972 test_loss: 0.9997, test_acc: 0.7701, best: 0.7761, time: 0:00:49
 Epoch: 271, lr: 8.0e-05, train_loss: 0.3049, train_acc: 0.8926 test_loss: 1.0159, test_acc: 0.7708, best: 0.7761, time: 0:00:49
 Epoch: 272, lr: 8.0e-05, train_loss: 0.2984, train_acc: 0.8986 test_loss: 0.9609, test_acc: 0.7740, best: 0.7761, time: 0:00:49
 Epoch: 273, lr: 8.0e-05, train_loss: 0.3114, train_acc: 0.8906 test_loss: 0.9839, test_acc: 0.7750, best: 0.7761, time: 0:00:49
 Epoch: 274, lr: 8.0e-05, train_loss: 0.3016, train_acc: 0.8962 test_loss: 0.9644, test_acc: 0.7710, best: 0.7761, time: 0:00:49
 Epoch: 275, lr: 8.0e-05, train_loss: 0.2767, train_acc: 0.9032 test_loss: 0.9520, test_acc: 0.7751, best: 0.7761, time: 0:00:49
 Epoch: 276, lr: 8.0e-05, train_loss: 0.3025, train_acc: 0.8982 test_loss: 1.1091, test_acc: 0.7504, best: 0.7761, time: 0:00:49
 Epoch: 277, lr: 8.0e-05, train_loss: 0.2904, train_acc: 0.8990 test_loss: 0.9810, test_acc: 0.7755, best: 0.7761, time: 0:00:49
 Epoch: 278, lr: 8.0e-05, train_loss: 0.3051, train_acc: 0.8948 test_loss: 0.9834, test_acc: 0.7756, best: 0.7761, time: 0:00:48
 Epoch: 279, lr: 8.0e-05, train_loss: 0.3154, train_acc: 0.8900 test_loss: 0.9764, test_acc: 0.7760, best: 0.7761, time: 0:00:48
 Epoch: 280, lr: 8.0e-05, train_loss: 0.3072, train_acc: 0.8924 test_loss: 1.0150, test_acc: 0.7721, best: 0.7761, time: 0:00:49
 Epoch: 281, lr: 8.0e-05, train_loss: 0.2920, train_acc: 0.8996 test_loss: 0.9620, test_acc: 0.7740, best: 0.7761, time: 0:00:49
 Epoch: 282, lr: 8.0e-05, train_loss: 0.3051, train_acc: 0.8958 test_loss: 0.9675, test_acc: 0.7754, best: 0.7761, time: 0:00:49
 Epoch: 283, lr: 8.0e-05, train_loss: 0.2889, train_acc: 0.8984 test_loss: 0.9691, test_acc: 0.7729, best: 0.7761, time: 0:00:48
 Epoch: 284, lr: 8.0e-05, train_loss: 0.2751, train_acc: 0.9086 test_loss: 0.9883, test_acc: 0.7695, best: 0.7761, time: 0:00:48
 Epoch: 285, lr: 8.0e-05, train_loss: 0.3161, train_acc: 0.8924 test_loss: 0.9946, test_acc: 0.7750, best: 0.7761, time: 0:00:48
 Epoch: 286, lr: 8.0e-05, train_loss: 0.3024, train_acc: 0.8964 test_loss: 0.9684, test_acc: 0.7770, best: 0.7770, time: 0:00:50
 Epoch: 287, lr: 8.0e-05, train_loss: 0.3062, train_acc: 0.8950 test_loss: 0.9594, test_acc: 0.7732, best: 0.7770, time: 0:00:48
 Epoch: 288, lr: 8.0e-05, train_loss: 0.2993, train_acc: 0.8940 test_loss: 0.9593, test_acc: 0.7759, best: 0.7770, time: 0:00:48
 Epoch: 289, lr: 8.0e-05, train_loss: 0.2957, train_acc: 0.9006 test_loss: 0.9885, test_acc: 0.7741, best: 0.7770, time: 0:00:48
 Epoch: 290, lr: 8.0e-05, train_loss: 0.2929, train_acc: 0.8982 test_loss: 0.9992, test_acc: 0.7732, best: 0.7770, time: 0:00:49
 Epoch: 291, lr: 8.0e-05, train_loss: 0.2813, train_acc: 0.9020 test_loss: 0.9550, test_acc: 0.7748, best: 0.7770, time: 0:00:49
 Epoch: 292, lr: 8.0e-05, train_loss: 0.2710, train_acc: 0.9084 test_loss: 1.0420, test_acc: 0.7698, best: 0.7770, time: 0:00:49
 Epoch: 293, lr: 8.0e-05, train_loss: 0.2970, train_acc: 0.8992 test_loss: 0.9997, test_acc: 0.7706, best: 0.7770, time: 0:00:48
 Epoch: 294, lr: 8.0e-05, train_loss: 0.3003, train_acc: 0.8944 test_loss: 0.9920, test_acc: 0.7700, best: 0.7770, time: 0:00:49
 Epoch: 295, lr: 8.0e-05, train_loss: 0.3169, train_acc: 0.8910 test_loss: 0.9570, test_acc: 0.7705, best: 0.7770, time: 0:00:48
 Epoch: 296, lr: 8.0e-05, train_loss: 0.2955, train_acc: 0.9016 test_loss: 1.0079, test_acc: 0.7772, best: 0.7772, time: 0:00:49
 Epoch: 297, lr: 8.0e-05, train_loss: 0.3076, train_acc: 0.8974 test_loss: 0.9772, test_acc: 0.7720, best: 0.7772, time: 0:00:49
 Epoch: 298, lr: 8.0e-05, train_loss: 0.2757, train_acc: 0.9046 test_loss: 0.9655, test_acc: 0.7738, best: 0.7772, time: 0:00:50
 Epoch: 299, lr: 8.0e-05, train_loss: 0.2928, train_acc: 0.9004 test_loss: 0.9646, test_acc: 0.7729, best: 0.7772, time: 0:00:48
 Highest accuracy: 0.7772