
 Run on time: 2022-06-24 19:58:10.720321

 Arguments:
	 gpu                  : True
	 log                  : True
	 seed                 : 0
	 momentum             : 0.9
	 amsgrad              : True
	 dataset              : STL10
	 batch_size           : 8
	 architecture         : RESNET50_4222
	 im_size              : 224
	 relu_threshold       : 4.0
	 learning_rate        : 0.01
	 pretrained_backbone  : 
	 pretrained_ann       : 
	 weight_decay         : 0.0
	 test_only            : False
	 epochs               : 300
	 lr_interval          : [180, 240, 270]
	 lr_reduce            : 5
	 optimizer            : SGD
	 dropout              : 0.2
	 kernel_size          : 3
	 dont_save            : False
	 visualize            : False
	 devices              : 0,1
 DataParallel(
  (module): NetworkByName(
    (net): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(4, 4), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer4): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
      (fc): Linear(in_features=2048, out_features=10, bias=True)
    )
  )
)
 SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0
)
 Epoch: 1, lr: 1.0e-02, train_loss: 4.3061, train_acc: 0.1058 test_loss: 6.0622, test_acc: 0.1165, best: 0.1165, time: 0:01:54
 Epoch: 2, lr: 1.0e-02, train_loss: 2.5244, train_acc: 0.1366 test_loss: 4.8607, test_acc: 0.1944, best: 0.1944, time: 0:01:51
 Epoch: 3, lr: 1.0e-02, train_loss: 2.3413, train_acc: 0.1576 test_loss: 10.6252, test_acc: 0.2220, best: 0.2220, time: 0:01:50
 Epoch: 4, lr: 1.0e-02, train_loss: 2.2073, train_acc: 0.1866 test_loss: 4.0211, test_acc: 0.2437, best: 0.2437, time: 0:01:50
 Epoch: 5, lr: 1.0e-02, train_loss: 2.1189, train_acc: 0.2046 test_loss: 12.7295, test_acc: 0.2845, best: 0.2845, time: 0:01:50
 Epoch: 6, lr: 1.0e-02, train_loss: 2.0545, train_acc: 0.2242 test_loss: 5.6385, test_acc: 0.2881, best: 0.2881, time: 0:01:50
 Epoch: 7, lr: 1.0e-02, train_loss: 2.0094, train_acc: 0.2398 test_loss: 3.9743, test_acc: 0.2777, best: 0.2881, time: 0:01:49
 Epoch: 8, lr: 1.0e-02, train_loss: 1.9728, train_acc: 0.2454 test_loss: 5.1907, test_acc: 0.2979, best: 0.2979, time: 0:01:50
 Epoch: 9, lr: 1.0e-02, train_loss: 1.9414, train_acc: 0.2628 test_loss: 4.4885, test_acc: 0.3177, best: 0.3177, time: 0:01:50
 Epoch: 10, lr: 1.0e-02, train_loss: 1.9008, train_acc: 0.2674 test_loss: 6.0572, test_acc: 0.3264, best: 0.3264, time: 0:01:50
 Epoch: 11, lr: 1.0e-02, train_loss: 1.8901, train_acc: 0.2814 test_loss: 7.1953, test_acc: 0.3301, best: 0.3301, time: 0:01:50
 Epoch: 12, lr: 1.0e-02, train_loss: 1.8616, train_acc: 0.2802 test_loss: 5.9645, test_acc: 0.3272, best: 0.3301, time: 0:01:49
 Epoch: 13, lr: 1.0e-02, train_loss: 1.8596, train_acc: 0.2882 test_loss: 7.2259, test_acc: 0.3230, best: 0.3301, time: 0:01:49
 Epoch: 14, lr: 1.0e-02, train_loss: 1.8784, train_acc: 0.2830 test_loss: 4.9445, test_acc: 0.3305, best: 0.3305, time: 0:01:49
 Epoch: 15, lr: 1.0e-02, train_loss: 1.8331, train_acc: 0.2980 test_loss: 5.0353, test_acc: 0.3196, best: 0.3305, time: 0:01:48
 Epoch: 16, lr: 1.0e-02, train_loss: 1.7981, train_acc: 0.3062 test_loss: 16.0969, test_acc: 0.3295, best: 0.3305, time: 0:01:48
 Epoch: 17, lr: 1.0e-02, train_loss: 1.7804, train_acc: 0.3142 test_loss: 10.1411, test_acc: 0.3669, best: 0.3669, time: 0:01:49
 Epoch: 18, lr: 1.0e-02, train_loss: 1.7630, train_acc: 0.3236 test_loss: 20.7479, test_acc: 0.3386, best: 0.3669, time: 0:01:48
 Epoch: 19, lr: 1.0e-02, train_loss: 1.7718, train_acc: 0.3128 test_loss: 4.0891, test_acc: 0.3668, best: 0.3669, time: 0:01:48
 Epoch: 20, lr: 1.0e-02, train_loss: 1.7593, train_acc: 0.3166 test_loss: 6.1966, test_acc: 0.3633, best: 0.3669, time: 0:01:48
 Epoch: 21, lr: 1.0e-02, train_loss: 1.7422, train_acc: 0.3348 test_loss: 8.5219, test_acc: 0.3475, best: 0.3669, time: 0:01:48
 Epoch: 22, lr: 1.0e-02, train_loss: 1.7236, train_acc: 0.3456 test_loss: 4.2474, test_acc: 0.3578, best: 0.3669, time: 0:01:48
 Epoch: 23, lr: 1.0e-02, train_loss: 1.7035, train_acc: 0.3480 test_loss: 32.4268, test_acc: 0.3817, best: 0.3817, time: 0:01:49
 Epoch: 24, lr: 1.0e-02, train_loss: 1.6900, train_acc: 0.3604 test_loss: 12.4587, test_acc: 0.3841, best: 0.3841, time: 0:01:49
 Epoch: 25, lr: 1.0e-02, train_loss: 1.7480, train_acc: 0.3390 test_loss: 7.3637, test_acc: 0.3377, best: 0.3841, time: 0:01:48
 Epoch: 26, lr: 1.0e-02, train_loss: 1.8320, train_acc: 0.2982 test_loss: 3.9189, test_acc: 0.3200, best: 0.3841, time: 0:01:48
 Epoch: 27, lr: 1.0e-02, train_loss: 1.8085, train_acc: 0.3034 test_loss: 3.3375, test_acc: 0.3511, best: 0.3841, time: 0:01:48
 Epoch: 28, lr: 1.0e-02, train_loss: 1.7867, train_acc: 0.3178 test_loss: 7.0698, test_acc: 0.3777, best: 0.3841, time: 0:01:48
 Epoch: 29, lr: 1.0e-02, train_loss: 1.7250, train_acc: 0.3410 test_loss: 10.9259, test_acc: 0.3701, best: 0.3841, time: 0:01:48
 Epoch: 30, lr: 1.0e-02, train_loss: 1.7007, train_acc: 0.3552 test_loss: 18.1622, test_acc: 0.3899, best: 0.3899, time: 0:01:49
 Epoch: 31, lr: 1.0e-02, train_loss: 1.6819, train_acc: 0.3562 test_loss: 14.8649, test_acc: 0.3782, best: 0.3899, time: 0:01:48
 Epoch: 32, lr: 1.0e-02, train_loss: 1.6706, train_acc: 0.3700 test_loss: 19.9162, test_acc: 0.3856, best: 0.3899, time: 0:01:48
 Epoch: 33, lr: 1.0e-02, train_loss: 1.6484, train_acc: 0.3738 test_loss: 6.4840, test_acc: 0.4134, best: 0.4134, time: 0:01:49
 Epoch: 34, lr: 1.0e-02, train_loss: 1.6289, train_acc: 0.3812 test_loss: 15.0720, test_acc: 0.4031, best: 0.4134, time: 0:01:48
 Epoch: 35, lr: 1.0e-02, train_loss: 1.6079, train_acc: 0.3864 test_loss: 14.5620, test_acc: 0.4032, best: 0.4134, time: 0:01:48
 Epoch: 36, lr: 1.0e-02, train_loss: 1.6197, train_acc: 0.3882 test_loss: 69.4631, test_acc: 0.3424, best: 0.4134, time: 0:01:48
 Epoch: 37, lr: 1.0e-02, train_loss: 1.6171, train_acc: 0.3884 test_loss: 2.2915, test_acc: 0.4270, best: 0.4270, time: 0:01:49
 Epoch: 38, lr: 1.0e-02, train_loss: 1.6522, train_acc: 0.3704 test_loss: 2.4176, test_acc: 0.4135, best: 0.4270, time: 0:01:48
 Epoch: 39, lr: 1.0e-02, train_loss: 1.7640, train_acc: 0.3244 test_loss: 3.2425, test_acc: 0.3718, best: 0.4270, time: 0:01:48
 Epoch: 40, lr: 1.0e-02, train_loss: 1.7286, train_acc: 0.3386 test_loss: 2.0756, test_acc: 0.3495, best: 0.4270, time: 0:01:48
 Epoch: 41, lr: 1.0e-02, train_loss: 1.7012, train_acc: 0.3550 test_loss: 1.9789, test_acc: 0.4095, best: 0.4270, time: 0:01:48
 Epoch: 42, lr: 1.0e-02, train_loss: 1.6634, train_acc: 0.3620 test_loss: 4.4995, test_acc: 0.4002, best: 0.4270, time: 0:01:48
 Epoch: 43, lr: 1.0e-02, train_loss: 1.6831, train_acc: 0.3620 test_loss: 2.5683, test_acc: 0.3985, best: 0.4270, time: 0:01:48
 Epoch: 44, lr: 1.0e-02, train_loss: 1.6582, train_acc: 0.3790 test_loss: 2.6132, test_acc: 0.3929, best: 0.4270, time: 0:01:48
 Epoch: 45, lr: 1.0e-02, train_loss: 1.6312, train_acc: 0.3796 test_loss: 5.6799, test_acc: 0.3736, best: 0.4270, time: 0:01:48
 Epoch: 46, lr: 1.0e-02, train_loss: 1.6122, train_acc: 0.3922 test_loss: 2.8691, test_acc: 0.4031, best: 0.4270, time: 0:01:48
 Epoch: 47, lr: 1.0e-02, train_loss: 1.5992, train_acc: 0.4068 test_loss: 3.4001, test_acc: 0.4086, best: 0.4270, time: 0:01:48
 Epoch: 48, lr: 1.0e-02, train_loss: 1.5863, train_acc: 0.4034 test_loss: 2.7178, test_acc: 0.4245, best: 0.4270, time: 0:01:48
 Epoch: 49, lr: 1.0e-02, train_loss: 1.5913, train_acc: 0.3944 test_loss: 8.7171, test_acc: 0.4146, best: 0.4270, time: 0:01:48
 Epoch: 50, lr: 1.0e-02, train_loss: 1.5507, train_acc: 0.4118 test_loss: 11.2427, test_acc: 0.4321, best: 0.4321, time: 0:01:48
 Epoch: 51, lr: 1.0e-02, train_loss: 1.5787, train_acc: 0.3976 test_loss: 10.6807, test_acc: 0.3369, best: 0.4321, time: 0:01:48
 Epoch: 52, lr: 1.0e-02, train_loss: 1.5977, train_acc: 0.4006 test_loss: 5.7898, test_acc: 0.4254, best: 0.4321, time: 0:01:48
 Epoch: 53, lr: 1.0e-02, train_loss: 1.5606, train_acc: 0.4106 test_loss: 4.6180, test_acc: 0.4074, best: 0.4321, time: 0:01:48
 Epoch: 54, lr: 1.0e-02, train_loss: 1.5747, train_acc: 0.4066 test_loss: 1.9435, test_acc: 0.4160, best: 0.4321, time: 0:01:48
 Epoch: 55, lr: 1.0e-02, train_loss: 1.5544, train_acc: 0.4112 test_loss: 1.7613, test_acc: 0.4360, best: 0.4360, time: 0:01:49
 Epoch: 56, lr: 1.0e-02, train_loss: 1.5736, train_acc: 0.4064 test_loss: 1.6085, test_acc: 0.4595, best: 0.4595, time: 0:01:49
 Epoch: 57, lr: 1.0e-02, train_loss: 1.5559, train_acc: 0.4136 test_loss: 1.8347, test_acc: 0.4394, best: 0.4595, time: 0:01:48
 Epoch: 58, lr: 1.0e-02, train_loss: 1.5550, train_acc: 0.4124 test_loss: 1.8348, test_acc: 0.4442, best: 0.4595, time: 0:01:48
 Epoch: 59, lr: 1.0e-02, train_loss: 1.5305, train_acc: 0.4234 test_loss: 3.2029, test_acc: 0.3969, best: 0.4595, time: 0:01:48
 Epoch: 60, lr: 1.0e-02, train_loss: 1.5254, train_acc: 0.4124 test_loss: 2.0610, test_acc: 0.4522, best: 0.4595, time: 0:01:48
 Epoch: 61, lr: 1.0e-02, train_loss: 1.4967, train_acc: 0.4300 test_loss: 1.4950, test_acc: 0.4845, best: 0.4845, time: 0:01:49
 Epoch: 62, lr: 1.0e-02, train_loss: 1.4843, train_acc: 0.4482 test_loss: 2.0296, test_acc: 0.4839, best: 0.4845, time: 0:01:48
 Epoch: 63, lr: 1.0e-02, train_loss: 1.4805, train_acc: 0.4488 test_loss: 2.0200, test_acc: 0.4708, best: 0.4845, time: 0:01:48
 Epoch: 64, lr: 1.0e-02, train_loss: 1.4503, train_acc: 0.4668 test_loss: 2.1231, test_acc: 0.4776, best: 0.4845, time: 0:01:48
 Epoch: 65, lr: 1.0e-02, train_loss: 1.4585, train_acc: 0.4534 test_loss: 1.6319, test_acc: 0.4880, best: 0.4880, time: 0:01:49
 Epoch: 66, lr: 1.0e-02, train_loss: 1.4334, train_acc: 0.4682 test_loss: 2.9865, test_acc: 0.4641, best: 0.4880, time: 0:01:48
 Epoch: 67, lr: 1.0e-02, train_loss: 1.4266, train_acc: 0.4658 test_loss: 3.2774, test_acc: 0.4763, best: 0.4880, time: 0:01:48
 Epoch: 68, lr: 1.0e-02, train_loss: 1.4561, train_acc: 0.4544 test_loss: 2.6002, test_acc: 0.4689, best: 0.4880, time: 0:01:48
 Epoch: 69, lr: 1.0e-02, train_loss: 1.4346, train_acc: 0.4718 test_loss: 2.3998, test_acc: 0.4833, best: 0.4880, time: 0:01:48
 Epoch: 70, lr: 1.0e-02, train_loss: 1.4323, train_acc: 0.4596 test_loss: 3.8050, test_acc: 0.4750, best: 0.4880, time: 0:01:48
 Epoch: 71, lr: 1.0e-02, train_loss: 1.4317, train_acc: 0.4652 test_loss: 1.4333, test_acc: 0.5194, best: 0.5194, time: 0:01:48
 Epoch: 72, lr: 1.0e-02, train_loss: 1.4578, train_acc: 0.4556 test_loss: 2.3952, test_acc: 0.4671, best: 0.5194, time: 0:01:48
 Epoch: 73, lr: 1.0e-02, train_loss: 1.4130, train_acc: 0.4746 test_loss: 2.7544, test_acc: 0.4549, best: 0.5194, time: 0:01:48
 Epoch: 74, lr: 1.0e-02, train_loss: 1.4118, train_acc: 0.4788 test_loss: 8.4983, test_acc: 0.4258, best: 0.5194, time: 0:01:48
 Epoch: 75, lr: 1.0e-02, train_loss: 1.4141, train_acc: 0.4752 test_loss: 5.4143, test_acc: 0.4968, best: 0.5194, time: 0:01:48
 Epoch: 76, lr: 1.0e-02, train_loss: 1.3903, train_acc: 0.4856 test_loss: 6.8395, test_acc: 0.4836, best: 0.5194, time: 0:01:48
 Epoch: 77, lr: 1.0e-02, train_loss: 1.3885, train_acc: 0.4850 test_loss: 4.7399, test_acc: 0.4755, best: 0.5194, time: 0:01:48
 Epoch: 78, lr: 1.0e-02, train_loss: 1.3963, train_acc: 0.4870 test_loss: 6.5035, test_acc: 0.4121, best: 0.5194, time: 0:01:48
 Epoch: 79, lr: 1.0e-02, train_loss: 1.3472, train_acc: 0.4976 test_loss: 1.4236, test_acc: 0.5191, best: 0.5194, time: 0:01:48
 Epoch: 80, lr: 1.0e-02, train_loss: 1.3593, train_acc: 0.4964 test_loss: 3.1417, test_acc: 0.4639, best: 0.5194, time: 0:01:48
 Epoch: 81, lr: 1.0e-02, train_loss: 1.3660, train_acc: 0.4988 test_loss: 3.2142, test_acc: 0.4789, best: 0.5194, time: 0:01:48
 Epoch: 82, lr: 1.0e-02, train_loss: 1.3250, train_acc: 0.5016 test_loss: 2.2013, test_acc: 0.5242, best: 0.5242, time: 0:01:49
 Epoch: 83, lr: 1.0e-02, train_loss: 1.3180, train_acc: 0.5130 test_loss: 3.1176, test_acc: 0.4815, best: 0.5242, time: 0:01:48
 Epoch: 84, lr: 1.0e-02, train_loss: 1.3431, train_acc: 0.5066 test_loss: 1.6025, test_acc: 0.5045, best: 0.5242, time: 0:01:48
 Epoch: 85, lr: 1.0e-02, train_loss: 1.4307, train_acc: 0.4714 test_loss: 2.4937, test_acc: 0.4510, best: 0.5242, time: 0:01:48
 Epoch: 86, lr: 1.0e-02, train_loss: 1.4202, train_acc: 0.4814 test_loss: 4.4083, test_acc: 0.4215, best: 0.5242, time: 0:01:48
 Epoch: 87, lr: 1.0e-02, train_loss: 1.4309, train_acc: 0.4624 test_loss: 2.3875, test_acc: 0.4795, best: 0.5242, time: 0:01:48
 Epoch: 88, lr: 1.0e-02, train_loss: 1.3689, train_acc: 0.4934 test_loss: 1.5780, test_acc: 0.5229, best: 0.5242, time: 0:01:48
 Epoch: 89, lr: 1.0e-02, train_loss: 1.3571, train_acc: 0.4914 test_loss: 2.2994, test_acc: 0.5088, best: 0.5242, time: 0:01:48
 Epoch: 90, lr: 1.0e-02, train_loss: 1.3288, train_acc: 0.4996 test_loss: 2.3195, test_acc: 0.4974, best: 0.5242, time: 0:01:48
 Epoch: 91, lr: 1.0e-02, train_loss: 1.3566, train_acc: 0.5026 test_loss: 3.1134, test_acc: 0.4806, best: 0.5242, time: 0:01:48
 Epoch: 92, lr: 1.0e-02, train_loss: 1.3258, train_acc: 0.5128 test_loss: 1.9146, test_acc: 0.5111, best: 0.5242, time: 0:01:48
 Epoch: 93, lr: 1.0e-02, train_loss: 1.3121, train_acc: 0.5138 test_loss: 2.5966, test_acc: 0.4835, best: 0.5242, time: 0:01:48
 Epoch: 94, lr: 1.0e-02, train_loss: 1.2968, train_acc: 0.5240 test_loss: 9.4380, test_acc: 0.4129, best: 0.5242, time: 0:01:48
 Epoch: 95, lr: 1.0e-02, train_loss: 1.3002, train_acc: 0.5252 test_loss: 3.2913, test_acc: 0.5015, best: 0.5242, time: 0:01:48
 Epoch: 96, lr: 1.0e-02, train_loss: 1.2794, train_acc: 0.5288 test_loss: 3.7631, test_acc: 0.4500, best: 0.5242, time: 0:01:48
 Epoch: 97, lr: 1.0e-02, train_loss: 1.2535, train_acc: 0.5398 test_loss: 4.2387, test_acc: 0.4851, best: 0.5242, time: 0:01:48
 Epoch: 98, lr: 1.0e-02, train_loss: 1.2471, train_acc: 0.5380 test_loss: 2.7726, test_acc: 0.4960, best: 0.5242, time: 0:01:48
 Epoch: 99, lr: 1.0e-02, train_loss: 1.2239, train_acc: 0.5566 test_loss: 2.3494, test_acc: 0.4963, best: 0.5242, time: 0:01:48
 Epoch: 100, lr: 1.0e-02, train_loss: 1.2474, train_acc: 0.5478 test_loss: 4.2112, test_acc: 0.4854, best: 0.5242, time: 0:01:48
 Epoch: 101, lr: 1.0e-02, train_loss: 1.2498, train_acc: 0.5418 test_loss: 3.3117, test_acc: 0.4726, best: 0.5242, time: 0:01:48
 Epoch: 102, lr: 1.0e-02, train_loss: 1.2143, train_acc: 0.5600 test_loss: 2.3313, test_acc: 0.5357, best: 0.5357, time: 0:01:48
 Epoch: 103, lr: 1.0e-02, train_loss: 1.2193, train_acc: 0.5456 test_loss: 2.7373, test_acc: 0.5271, best: 0.5357, time: 0:01:48
 Epoch: 104, lr: 1.0e-02, train_loss: 1.2248, train_acc: 0.5536 test_loss: 2.7226, test_acc: 0.5431, best: 0.5431, time: 0:01:49
 Epoch: 105, lr: 1.0e-02, train_loss: 1.2320, train_acc: 0.5488 test_loss: 2.2699, test_acc: 0.5400, best: 0.5431, time: 0:01:48
 Epoch: 106, lr: 1.0e-02, train_loss: 1.2187, train_acc: 0.5552 test_loss: 5.6297, test_acc: 0.5109, best: 0.5431, time: 0:01:48
 Epoch: 107, lr: 1.0e-02, train_loss: 1.2419, train_acc: 0.5412 test_loss: 3.7506, test_acc: 0.5301, best: 0.5431, time: 0:01:48
 Epoch: 108, lr: 1.0e-02, train_loss: 1.2147, train_acc: 0.5532 test_loss: 4.0467, test_acc: 0.5178, best: 0.5431, time: 0:01:48
 Epoch: 109, lr: 1.0e-02, train_loss: 1.1835, train_acc: 0.5746 test_loss: 7.0713, test_acc: 0.4796, best: 0.5431, time: 0:01:48
 Epoch: 110, lr: 1.0e-02, train_loss: 1.1990, train_acc: 0.5660 test_loss: 17.7426, test_acc: 0.4295, best: 0.5431, time: 0:01:48
 Epoch: 111, lr: 1.0e-02, train_loss: 1.2006, train_acc: 0.5566 test_loss: 5.1051, test_acc: 0.5280, best: 0.5431, time: 0:01:48
 Epoch: 112, lr: 1.0e-02, train_loss: 1.1961, train_acc: 0.5664 test_loss: 4.6435, test_acc: 0.5004, best: 0.5431, time: 0:01:48
 Epoch: 113, lr: 1.0e-02, train_loss: 1.1772, train_acc: 0.5682 test_loss: 2.2537, test_acc: 0.5278, best: 0.5431, time: 0:01:48
 Epoch: 114, lr: 1.0e-02, train_loss: 1.1761, train_acc: 0.5664 test_loss: 4.3853, test_acc: 0.4659, best: 0.5431, time: 0:01:48
 Epoch: 115, lr: 1.0e-02, train_loss: 1.2415, train_acc: 0.5478 test_loss: 11.2737, test_acc: 0.4655, best: 0.5431, time: 0:01:48
 Epoch: 116, lr: 1.0e-02, train_loss: 1.1802, train_acc: 0.5682 test_loss: 2.4334, test_acc: 0.5205, best: 0.5431, time: 0:01:48
 Epoch: 117, lr: 1.0e-02, train_loss: 1.2041, train_acc: 0.5702 test_loss: 3.3296, test_acc: 0.5281, best: 0.5431, time: 0:01:48
 Epoch: 118, lr: 1.0e-02, train_loss: 1.1597, train_acc: 0.5796 test_loss: 4.0033, test_acc: 0.5224, best: 0.5431, time: 0:01:48
 Epoch: 119, lr: 1.0e-02, train_loss: 1.1415, train_acc: 0.5836 test_loss: 1.8395, test_acc: 0.5420, best: 0.5431, time: 0:01:48
 Epoch: 120, lr: 1.0e-02, train_loss: 1.1306, train_acc: 0.5886 test_loss: 8.9930, test_acc: 0.4340, best: 0.5431, time: 0:01:48
 Epoch: 121, lr: 1.0e-02, train_loss: 1.1489, train_acc: 0.5816 test_loss: 1.9089, test_acc: 0.5416, best: 0.5431, time: 0:01:48
 Epoch: 122, lr: 1.0e-02, train_loss: 1.1859, train_acc: 0.5698 test_loss: 1.5057, test_acc: 0.5585, best: 0.5585, time: 0:01:49
 Epoch: 123, lr: 1.0e-02, train_loss: 1.1662, train_acc: 0.5752 test_loss: 7.5588, test_acc: 0.4299, best: 0.5585, time: 0:01:48
 Epoch: 124, lr: 1.0e-02, train_loss: 1.1584, train_acc: 0.5794 test_loss: 1.9590, test_acc: 0.5262, best: 0.5585, time: 0:01:48
 Epoch: 125, lr: 1.0e-02, train_loss: 1.1371, train_acc: 0.5792 test_loss: 2.3137, test_acc: 0.5105, best: 0.5585, time: 0:01:48
 Epoch: 126, lr: 1.0e-02, train_loss: 1.1163, train_acc: 0.5808 test_loss: 3.5365, test_acc: 0.4803, best: 0.5585, time: 0:01:48
 Epoch: 127, lr: 1.0e-02, train_loss: 1.1034, train_acc: 0.5996 test_loss: 8.6509, test_acc: 0.4677, best: 0.5585, time: 0:01:48
 Epoch: 128, lr: 1.0e-02, train_loss: 1.0929, train_acc: 0.6102 test_loss: 2.7664, test_acc: 0.5389, best: 0.5585, time: 0:01:48
 Epoch: 129, lr: 1.0e-02, train_loss: 1.1038, train_acc: 0.5938 test_loss: 3.0242, test_acc: 0.5333, best: 0.5585, time: 0:01:48
 Epoch: 130, lr: 1.0e-02, train_loss: 1.0982, train_acc: 0.6012 test_loss: 3.1236, test_acc: 0.5104, best: 0.5585, time: 0:01:48
 Epoch: 131, lr: 1.0e-02, train_loss: 1.0853, train_acc: 0.6088 test_loss: 1.4007, test_acc: 0.5991, best: 0.5991, time: 0:01:49
 Epoch: 132, lr: 1.0e-02, train_loss: 1.1044, train_acc: 0.5900 test_loss: 2.9089, test_acc: 0.5563, best: 0.5991, time: 0:01:49
 Epoch: 133, lr: 1.0e-02, train_loss: 1.0967, train_acc: 0.6008 test_loss: 1.3109, test_acc: 0.6031, best: 0.6031, time: 0:01:49
 Epoch: 134, lr: 1.0e-02, train_loss: 1.1029, train_acc: 0.5968 test_loss: 2.8666, test_acc: 0.5711, best: 0.6031, time: 0:01:48
 Epoch: 135, lr: 1.0e-02, train_loss: 1.1111, train_acc: 0.5906 test_loss: 2.0179, test_acc: 0.6028, best: 0.6031, time: 0:01:48
 Epoch: 136, lr: 1.0e-02, train_loss: 1.0798, train_acc: 0.6106 test_loss: 3.2006, test_acc: 0.5676, best: 0.6031, time: 0:01:49
 Epoch: 137, lr: 1.0e-02, train_loss: 1.1095, train_acc: 0.5952 test_loss: 1.7160, test_acc: 0.5765, best: 0.6031, time: 0:01:49
 Epoch: 138, lr: 1.0e-02, train_loss: 1.1069, train_acc: 0.5996 test_loss: 2.9580, test_acc: 0.5906, best: 0.6031, time: 0:01:49
 Epoch: 139, lr: 1.0e-02, train_loss: 1.0945, train_acc: 0.6006 test_loss: 1.6679, test_acc: 0.5996, best: 0.6031, time: 0:01:49
 Epoch: 140, lr: 1.0e-02, train_loss: 1.0984, train_acc: 0.5974 test_loss: 1.5281, test_acc: 0.6014, best: 0.6031, time: 0:01:49
 Epoch: 141, lr: 1.0e-02, train_loss: 1.0562, train_acc: 0.6154 test_loss: 2.5108, test_acc: 0.5860, best: 0.6031, time: 0:01:49
 Epoch: 142, lr: 1.0e-02, train_loss: 1.0760, train_acc: 0.6052 test_loss: 1.5786, test_acc: 0.6055, best: 0.6055, time: 0:01:49
 Epoch: 143, lr: 1.0e-02, train_loss: 1.0677, train_acc: 0.6114 test_loss: 5.0475, test_acc: 0.5474, best: 0.6055, time: 0:01:49
 Epoch: 144, lr: 1.0e-02, train_loss: 1.0451, train_acc: 0.6228 test_loss: 3.8369, test_acc: 0.5713, best: 0.6055, time: 0:01:49
 Epoch: 145, lr: 1.0e-02, train_loss: 1.0428, train_acc: 0.6192 test_loss: 2.0703, test_acc: 0.6172, best: 0.6172, time: 0:01:49
 Epoch: 146, lr: 1.0e-02, train_loss: 1.0346, train_acc: 0.6202 test_loss: 8.0103, test_acc: 0.4978, best: 0.6172, time: 0:01:49
 Epoch: 147, lr: 1.0e-02, train_loss: 1.0352, train_acc: 0.6200 test_loss: 1.6769, test_acc: 0.5941, best: 0.6172, time: 0:01:49
 Epoch: 148, lr: 1.0e-02, train_loss: 1.0331, train_acc: 0.6292 test_loss: 4.2275, test_acc: 0.5331, best: 0.6172, time: 0:01:49
 Epoch: 149, lr: 1.0e-02, train_loss: 1.0577, train_acc: 0.6142 test_loss: 5.9839, test_acc: 0.5162, best: 0.6172, time: 0:01:49
 Epoch: 150, lr: 1.0e-02, train_loss: 1.0359, train_acc: 0.6264 test_loss: 7.3665, test_acc: 0.5074, best: 0.6172, time: 0:01:49
 Epoch: 151, lr: 1.0e-02, train_loss: 1.0365, train_acc: 0.6230 test_loss: 4.0706, test_acc: 0.5409, best: 0.6172, time: 0:01:48
 Epoch: 152, lr: 1.0e-02, train_loss: 1.0293, train_acc: 0.6268 test_loss: 4.0732, test_acc: 0.5764, best: 0.6172, time: 0:01:49
 Epoch: 153, lr: 1.0e-02, train_loss: 0.9906, train_acc: 0.6380 test_loss: 3.7335, test_acc: 0.5516, best: 0.6172, time: 0:01:49
 Epoch: 154, lr: 1.0e-02, train_loss: 1.0128, train_acc: 0.6270 test_loss: 4.7847, test_acc: 0.5390, best: 0.6172, time: 0:01:49
 Epoch: 155, lr: 1.0e-02, train_loss: 1.0101, train_acc: 0.6360 test_loss: 2.3566, test_acc: 0.5704, best: 0.6172, time: 0:01:49
 Epoch: 156, lr: 1.0e-02, train_loss: 1.0327, train_acc: 0.6232 test_loss: 12.1291, test_acc: 0.5085, best: 0.6172, time: 0:01:49
 Epoch: 157, lr: 1.0e-02, train_loss: 1.0346, train_acc: 0.6248 test_loss: 2.1389, test_acc: 0.5809, best: 0.6172, time: 0:01:49
 Epoch: 158, lr: 1.0e-02, train_loss: 1.0117, train_acc: 0.6376 test_loss: 4.0996, test_acc: 0.5714, best: 0.6172, time: 0:01:49
 Epoch: 159, lr: 1.0e-02, train_loss: 1.0040, train_acc: 0.6424 test_loss: 7.6758, test_acc: 0.4793, best: 0.6172, time: 0:01:49
 Epoch: 160, lr: 1.0e-02, train_loss: 0.9812, train_acc: 0.6506 test_loss: 4.1028, test_acc: 0.5219, best: 0.6172, time: 0:01:49
 Epoch: 161, lr: 1.0e-02, train_loss: 0.9808, train_acc: 0.6442 test_loss: 16.5927, test_acc: 0.4461, best: 0.6172, time: 0:01:49
 Epoch: 162, lr: 1.0e-02, train_loss: 0.9625, train_acc: 0.6414 test_loss: 4.5958, test_acc: 0.5341, best: 0.6172, time: 0:01:49
 Epoch: 163, lr: 1.0e-02, train_loss: 0.9592, train_acc: 0.6440 test_loss: 12.8639, test_acc: 0.4709, best: 0.6172, time: 0:01:49
 Epoch: 164, lr: 1.0e-02, train_loss: 0.9485, train_acc: 0.6604 test_loss: 5.7240, test_acc: 0.5201, best: 0.6172, time: 0:01:49
 Epoch: 165, lr: 1.0e-02, train_loss: 0.9809, train_acc: 0.6476 test_loss: 4.9199, test_acc: 0.5537, best: 0.6172, time: 0:01:49
 Epoch: 166, lr: 1.0e-02, train_loss: 0.9281, train_acc: 0.6646 test_loss: 10.2669, test_acc: 0.5035, best: 0.6172, time: 0:01:49
 Epoch: 167, lr: 1.0e-02, train_loss: 0.9582, train_acc: 0.6504 test_loss: 4.2958, test_acc: 0.5493, best: 0.6172, time: 0:01:49
 Epoch: 168, lr: 1.0e-02, train_loss: 0.9419, train_acc: 0.6588 test_loss: 11.5067, test_acc: 0.4995, best: 0.6172, time: 0:01:49
 Epoch: 169, lr: 1.0e-02, train_loss: 0.9432, train_acc: 0.6632 test_loss: 4.3168, test_acc: 0.5325, best: 0.6172, time: 0:01:49
 Epoch: 170, lr: 1.0e-02, train_loss: 0.9619, train_acc: 0.6554 test_loss: 4.5475, test_acc: 0.5429, best: 0.6172, time: 0:01:49
 Epoch: 171, lr: 1.0e-02, train_loss: 0.9321, train_acc: 0.6586 test_loss: 5.6459, test_acc: 0.5646, best: 0.6172, time: 0:01:49
 Epoch: 172, lr: 1.0e-02, train_loss: 0.9370, train_acc: 0.6602 test_loss: 5.1192, test_acc: 0.5644, best: 0.6172, time: 0:01:49
 Epoch: 173, lr: 1.0e-02, train_loss: 0.9182, train_acc: 0.6700 test_loss: 11.4970, test_acc: 0.5109, best: 0.6172, time: 0:01:49
 Epoch: 174, lr: 1.0e-02, train_loss: 0.9247, train_acc: 0.6702 test_loss: 8.2193, test_acc: 0.5116, best: 0.6172, time: 0:01:49
 Epoch: 175, lr: 1.0e-02, train_loss: 0.9040, train_acc: 0.6748 test_loss: 6.7053, test_acc: 0.5506, best: 0.6172, time: 0:01:49
 Epoch: 176, lr: 1.0e-02, train_loss: 0.9096, train_acc: 0.6710 test_loss: 12.5721, test_acc: 0.4991, best: 0.6172, time: 0:01:49
 Epoch: 177, lr: 1.0e-02, train_loss: 0.9323, train_acc: 0.6650 test_loss: 6.2690, test_acc: 0.5194, best: 0.6172, time: 0:01:49
 Epoch: 178, lr: 1.0e-02, train_loss: 0.8872, train_acc: 0.6826 test_loss: 3.6284, test_acc: 0.5550, best: 0.6172, time: 0:01:49
 Epoch: 179, lr: 1.0e-02, train_loss: 0.9497, train_acc: 0.6544 test_loss: 4.3010, test_acc: 0.5620, best: 0.6172, time: 0:01:49
 Epoch: 180, lr: 2.0e-03, train_loss: 0.8990, train_acc: 0.6846 test_loss: 5.0128, test_acc: 0.5764, best: 0.6172, time: 0:01:49
 Epoch: 181, lr: 2.0e-03, train_loss: 0.8331, train_acc: 0.6960 test_loss: 25.7344, test_acc: 0.4899, best: 0.6172, time: 0:01:49
 Epoch: 182, lr: 2.0e-03, train_loss: 0.8034, train_acc: 0.7142 test_loss: 4.4541, test_acc: 0.6016, best: 0.6172, time: 0:01:49
 Epoch: 183, lr: 2.0e-03, train_loss: 0.7989, train_acc: 0.7198 test_loss: 7.0129, test_acc: 0.5610, best: 0.6172, time: 0:01:49
 Epoch: 184, lr: 2.0e-03, train_loss: 0.8120, train_acc: 0.7104 test_loss: 5.7272, test_acc: 0.5787, best: 0.6172, time: 0:01:49
 Epoch: 185, lr: 2.0e-03, train_loss: 0.7772, train_acc: 0.7232 test_loss: 7.8177, test_acc: 0.5555, best: 0.6172, time: 0:01:49
 Epoch: 186, lr: 2.0e-03, train_loss: 0.7766, train_acc: 0.7280 test_loss: 7.1340, test_acc: 0.5687, best: 0.6172, time: 0:01:49
 Epoch: 187, lr: 2.0e-03, train_loss: 0.7651, train_acc: 0.7258 test_loss: 4.0377, test_acc: 0.6088, best: 0.6172, time: 0:01:49
 Epoch: 188, lr: 2.0e-03, train_loss: 0.7821, train_acc: 0.7214 test_loss: 8.1549, test_acc: 0.5627, best: 0.6172, time: 0:01:49
 Epoch: 189, lr: 2.0e-03, train_loss: 0.7826, train_acc: 0.7180 test_loss: 3.4678, test_acc: 0.6039, best: 0.6172, time: 0:01:49
 Epoch: 190, lr: 2.0e-03, train_loss: 0.7574, train_acc: 0.7248 test_loss: 3.8504, test_acc: 0.5999, best: 0.6172, time: 0:01:49
 Epoch: 191, lr: 2.0e-03, train_loss: 0.7741, train_acc: 0.7256 test_loss: 4.7503, test_acc: 0.5917, best: 0.6172, time: 0:01:49
 Epoch: 192, lr: 2.0e-03, train_loss: 0.7555, train_acc: 0.7302 test_loss: 6.6071, test_acc: 0.5710, best: 0.6172, time: 0:01:49
 Epoch: 193, lr: 2.0e-03, train_loss: 0.7232, train_acc: 0.7462 test_loss: 6.4701, test_acc: 0.5520, best: 0.6172, time: 0:01:49
 Epoch: 194, lr: 2.0e-03, train_loss: 0.7409, train_acc: 0.7298 test_loss: 2.8246, test_acc: 0.6111, best: 0.6172, time: 0:01:49
 Epoch: 195, lr: 2.0e-03, train_loss: 0.7343, train_acc: 0.7360 test_loss: 3.8516, test_acc: 0.5919, best: 0.6172, time: 0:01:49
 Epoch: 196, lr: 2.0e-03, train_loss: 0.7249, train_acc: 0.7372 test_loss: 8.7019, test_acc: 0.5336, best: 0.6172, time: 0:01:49
 Epoch: 197, lr: 2.0e-03, train_loss: 0.7391, train_acc: 0.7350 test_loss: 4.1096, test_acc: 0.6108, best: 0.6172, time: 0:01:49
 Epoch: 198, lr: 2.0e-03, train_loss: 0.7417, train_acc: 0.7266 test_loss: 6.9961, test_acc: 0.5636, best: 0.6172, time: 0:01:49
 Epoch: 199, lr: 2.0e-03, train_loss: 0.7191, train_acc: 0.7498 test_loss: 5.3547, test_acc: 0.6054, best: 0.6172, time: 0:01:49
 Epoch: 200, lr: 2.0e-03, train_loss: 0.7465, train_acc: 0.7350 test_loss: 6.2298, test_acc: 0.5765, best: 0.6172, time: 0:01:48
 Epoch: 201, lr: 2.0e-03, train_loss: 0.7071, train_acc: 0.7496 test_loss: 2.9872, test_acc: 0.6200, best: 0.6200, time: 0:01:49
 Epoch: 202, lr: 2.0e-03, train_loss: 0.7509, train_acc: 0.7316 test_loss: 3.2641, test_acc: 0.6185, best: 0.6200, time: 0:01:49
 Epoch: 203, lr: 2.0e-03, train_loss: 0.7264, train_acc: 0.7526 test_loss: 9.9468, test_acc: 0.5674, best: 0.6200, time: 0:01:49
 Epoch: 204, lr: 2.0e-03, train_loss: 0.7067, train_acc: 0.7508 test_loss: 2.2945, test_acc: 0.6350, best: 0.6350, time: 0:01:49
 Epoch: 205, lr: 2.0e-03, train_loss: 0.7200, train_acc: 0.7428 test_loss: 7.1527, test_acc: 0.5483, best: 0.6350, time: 0:01:49
 Epoch: 206, lr: 2.0e-03, train_loss: 0.7039, train_acc: 0.7542 test_loss: 3.2813, test_acc: 0.6009, best: 0.6350, time: 0:01:49
 Epoch: 207, lr: 2.0e-03, train_loss: 0.6988, train_acc: 0.7550 test_loss: 3.0936, test_acc: 0.6079, best: 0.6350, time: 0:01:49
 Epoch: 208, lr: 2.0e-03, train_loss: 0.7095, train_acc: 0.7486 test_loss: 3.7158, test_acc: 0.5884, best: 0.6350, time: 0:01:49
 Epoch: 209, lr: 2.0e-03, train_loss: 0.7034, train_acc: 0.7490 test_loss: 8.5481, test_acc: 0.5334, best: 0.6350, time: 0:01:49
 Epoch: 210, lr: 2.0e-03, train_loss: 0.7232, train_acc: 0.7476 test_loss: 5.4147, test_acc: 0.5624, best: 0.6350, time: 0:01:49
 Epoch: 211, lr: 2.0e-03, train_loss: 0.7018, train_acc: 0.7572 test_loss: 4.2416, test_acc: 0.5824, best: 0.6350, time: 0:01:49
 Epoch: 212, lr: 2.0e-03, train_loss: 0.6969, train_acc: 0.7536 test_loss: 8.9338, test_acc: 0.5657, best: 0.6350, time: 0:01:49
 Epoch: 213, lr: 2.0e-03, train_loss: 0.6803, train_acc: 0.7632 test_loss: 7.6610, test_acc: 0.5360, best: 0.6350, time: 0:01:49
 Epoch: 214, lr: 2.0e-03, train_loss: 0.6955, train_acc: 0.7538 test_loss: 2.9970, test_acc: 0.6062, best: 0.6350, time: 0:01:49
 Epoch: 215, lr: 2.0e-03, train_loss: 0.6834, train_acc: 0.7584 test_loss: 3.2875, test_acc: 0.6100, best: 0.6350, time: 0:01:49
 Epoch: 216, lr: 2.0e-03, train_loss: 0.6909, train_acc: 0.7544 test_loss: 2.6847, test_acc: 0.6078, best: 0.6350, time: 0:01:49
 Epoch: 217, lr: 2.0e-03, train_loss: 0.6961, train_acc: 0.7560 test_loss: 4.2917, test_acc: 0.5863, best: 0.6350, time: 0:01:49
 Epoch: 218, lr: 2.0e-03, train_loss: 0.6817, train_acc: 0.7582 test_loss: 9.8801, test_acc: 0.5345, best: 0.6350, time: 0:01:49
 Epoch: 219, lr: 2.0e-03, train_loss: 0.6995, train_acc: 0.7502 test_loss: 6.3835, test_acc: 0.5750, best: 0.6350, time: 0:01:49
 Epoch: 220, lr: 2.0e-03, train_loss: 0.6709, train_acc: 0.7670 test_loss: 9.9477, test_acc: 0.5569, best: 0.6350, time: 0:01:49
 Epoch: 221, lr: 2.0e-03, train_loss: 0.6729, train_acc: 0.7602 test_loss: 4.6255, test_acc: 0.6005, best: 0.6350, time: 0:01:49
 Epoch: 222, lr: 2.0e-03, train_loss: 0.6733, train_acc: 0.7650 test_loss: 10.9849, test_acc: 0.5479, best: 0.6350, time: 0:01:49
 Epoch: 223, lr: 2.0e-03, train_loss: 0.6907, train_acc: 0.7554 test_loss: 8.8977, test_acc: 0.5590, best: 0.6350, time: 0:01:49
 Epoch: 224, lr: 2.0e-03, train_loss: 0.6837, train_acc: 0.7620 test_loss: 4.4120, test_acc: 0.6108, best: 0.6350, time: 0:01:49
 Epoch: 225, lr: 2.0e-03, train_loss: 0.6422, train_acc: 0.7722 test_loss: 7.0015, test_acc: 0.5661, best: 0.6350, time: 0:01:49
 Epoch: 226, lr: 2.0e-03, train_loss: 0.6729, train_acc: 0.7632 test_loss: 8.7210, test_acc: 0.5700, best: 0.6350, time: 0:01:49
 Epoch: 227, lr: 2.0e-03, train_loss: 0.6920, train_acc: 0.7594 test_loss: 8.3459, test_acc: 0.5670, best: 0.6350, time: 0:01:49
 Epoch: 228, lr: 2.0e-03, train_loss: 0.6633, train_acc: 0.7620 test_loss: 9.5993, test_acc: 0.5647, best: 0.6350, time: 0:01:49
 Epoch: 229, lr: 2.0e-03, train_loss: 0.6456, train_acc: 0.7688 test_loss: 11.0211, test_acc: 0.5669, best: 0.6350, time: 0:01:49
 Epoch: 230, lr: 2.0e-03, train_loss: 0.6583, train_acc: 0.7708 test_loss: 4.9396, test_acc: 0.5966, best: 0.6350, time: 0:01:49
 Epoch: 231, lr: 2.0e-03, train_loss: 0.6297, train_acc: 0.7848 test_loss: 5.0524, test_acc: 0.5799, best: 0.6350, time: 0:01:49
 Epoch: 232, lr: 2.0e-03, train_loss: 0.6314, train_acc: 0.7808 test_loss: 7.7259, test_acc: 0.5516, best: 0.6350, time: 0:01:49
 Epoch: 233, lr: 2.0e-03, train_loss: 0.6694, train_acc: 0.7594 test_loss: 11.1459, test_acc: 0.5316, best: 0.6350, time: 0:01:49
 Epoch: 234, lr: 2.0e-03, train_loss: 0.6725, train_acc: 0.7618 test_loss: 8.8566, test_acc: 0.5417, best: 0.6350, time: 0:01:48
 Epoch: 235, lr: 2.0e-03, train_loss: 0.6563, train_acc: 0.7708 test_loss: 5.9448, test_acc: 0.5867, best: 0.6350, time: 0:01:49
 Epoch: 236, lr: 2.0e-03, train_loss: 0.6415, train_acc: 0.7752 test_loss: 11.4732, test_acc: 0.5385, best: 0.6350, time: 0:01:49
 Epoch: 237, lr: 2.0e-03, train_loss: 0.6434, train_acc: 0.7758 test_loss: 4.7208, test_acc: 0.5875, best: 0.6350, time: 0:01:49
 Epoch: 238, lr: 2.0e-03, train_loss: 0.6294, train_acc: 0.7792 test_loss: 10.1623, test_acc: 0.5537, best: 0.6350, time: 0:01:49
 Epoch: 239, lr: 2.0e-03, train_loss: 0.6491, train_acc: 0.7752 test_loss: 5.2729, test_acc: 0.5753, best: 0.6350, time: 0:01:48
 Epoch: 240, lr: 4.0e-04, train_loss: 0.6371, train_acc: 0.7814 test_loss: 6.8806, test_acc: 0.5750, best: 0.6350, time: 0:01:49
 Epoch: 241, lr: 4.0e-04, train_loss: 0.6170, train_acc: 0.7878 test_loss: 6.0794, test_acc: 0.5683, best: 0.6350, time: 0:01:49
 Epoch: 242, lr: 4.0e-04, train_loss: 0.6201, train_acc: 0.7818 test_loss: 4.2761, test_acc: 0.5994, best: 0.6350, time: 0:01:49
 Epoch: 243, lr: 4.0e-04, train_loss: 0.6049, train_acc: 0.7918 test_loss: 5.4445, test_acc: 0.5884, best: 0.6350, time: 0:01:49
 Epoch: 244, lr: 4.0e-04, train_loss: 0.6161, train_acc: 0.7864 test_loss: 8.9238, test_acc: 0.5697, best: 0.6350, time: 0:01:49
 Epoch: 245, lr: 4.0e-04, train_loss: 0.6052, train_acc: 0.7836 test_loss: 5.4254, test_acc: 0.5856, best: 0.6350, time: 0:01:48
 Epoch: 246, lr: 4.0e-04, train_loss: 0.6377, train_acc: 0.7830 test_loss: 9.3808, test_acc: 0.5491, best: 0.6350, time: 0:01:49
 Epoch: 247, lr: 4.0e-04, train_loss: 0.6008, train_acc: 0.7938 test_loss: 6.5713, test_acc: 0.5697, best: 0.6350, time: 0:01:49
 Epoch: 248, lr: 4.0e-04, train_loss: 0.6066, train_acc: 0.7884 test_loss: 4.1147, test_acc: 0.6030, best: 0.6350, time: 0:01:49
 Epoch: 249, lr: 4.0e-04, train_loss: 0.5951, train_acc: 0.7920 test_loss: 6.5776, test_acc: 0.5849, best: 0.6350, time: 0:01:49
 Epoch: 250, lr: 4.0e-04, train_loss: 0.6231, train_acc: 0.7806 test_loss: 9.1687, test_acc: 0.5385, best: 0.6350, time: 0:01:49
 Epoch: 251, lr: 4.0e-04, train_loss: 0.6123, train_acc: 0.7870 test_loss: 12.7597, test_acc: 0.5219, best: 0.6350, time: 0:01:49
 Epoch: 252, lr: 4.0e-04, train_loss: 0.6085, train_acc: 0.7862 test_loss: 2.3046, test_acc: 0.6325, best: 0.6350, time: 0:01:49
 Epoch: 253, lr: 4.0e-04, train_loss: 0.6120, train_acc: 0.7822 test_loss: 6.7044, test_acc: 0.5736, best: 0.6350, time: 0:01:49
 Epoch: 254, lr: 4.0e-04, train_loss: 0.5979, train_acc: 0.7834 test_loss: 6.9766, test_acc: 0.5699, best: 0.6350, time: 0:01:49
 Epoch: 255, lr: 4.0e-04, train_loss: 0.6125, train_acc: 0.7910 test_loss: 5.2278, test_acc: 0.5955, best: 0.6350, time: 0:01:49
 Epoch: 256, lr: 4.0e-04, train_loss: 0.5892, train_acc: 0.7900 test_loss: 3.6551, test_acc: 0.6086, best: 0.6350, time: 0:01:49
 Epoch: 257, lr: 4.0e-04, train_loss: 0.5816, train_acc: 0.7948 test_loss: 5.0112, test_acc: 0.5879, best: 0.6350, time: 0:01:49
 Epoch: 258, lr: 4.0e-04, train_loss: 0.6097, train_acc: 0.7910 test_loss: 8.8153, test_acc: 0.5585, best: 0.6350, time: 0:01:49
 Epoch: 259, lr: 4.0e-04, train_loss: 0.5869, train_acc: 0.7936 test_loss: 7.9306, test_acc: 0.5786, best: 0.6350, time: 0:01:49
 Epoch: 260, lr: 4.0e-04, train_loss: 0.5962, train_acc: 0.7926 test_loss: 3.2799, test_acc: 0.6262, best: 0.6350, time: 0:01:49
 Epoch: 261, lr: 4.0e-04, train_loss: 0.6050, train_acc: 0.7904 test_loss: 6.5376, test_acc: 0.5903, best: 0.6350, time: 0:01:49
 Epoch: 262, lr: 4.0e-04, train_loss: 0.6012, train_acc: 0.7972 test_loss: 5.0009, test_acc: 0.5989, best: 0.6350, time: 0:01:49
 Epoch: 263, lr: 4.0e-04, train_loss: 0.5808, train_acc: 0.8004 test_loss: 8.6130, test_acc: 0.5673, best: 0.6350, time: 0:01:49
 Epoch: 264, lr: 4.0e-04, train_loss: 0.6017, train_acc: 0.7892 test_loss: 7.8791, test_acc: 0.5686, best: 0.6350, time: 0:01:49
 Epoch: 265, lr: 4.0e-04, train_loss: 0.5934, train_acc: 0.7930 test_loss: 5.1883, test_acc: 0.5975, best: 0.6350, time: 0:01:49
 Epoch: 266, lr: 4.0e-04, train_loss: 0.5778, train_acc: 0.8006 test_loss: 6.9231, test_acc: 0.5783, best: 0.6350, time: 0:01:48
 Epoch: 267, lr: 4.0e-04, train_loss: 0.5995, train_acc: 0.7916 test_loss: 5.2531, test_acc: 0.5911, best: 0.6350, time: 0:01:48
 Epoch: 268, lr: 4.0e-04, train_loss: 0.6018, train_acc: 0.7870 test_loss: 8.2157, test_acc: 0.5713, best: 0.6350, time: 0:01:49
 Epoch: 269, lr: 4.0e-04, train_loss: 0.5823, train_acc: 0.7948 test_loss: 3.8443, test_acc: 0.6148, best: 0.6350, time: 0:01:49
 Epoch: 270, lr: 8.0e-05, train_loss: 0.5874, train_acc: 0.7942 test_loss: 3.8682, test_acc: 0.6168, best: 0.6350, time: 0:01:49
 Epoch: 271, lr: 8.0e-05, train_loss: 0.6005, train_acc: 0.7898 test_loss: 6.3186, test_acc: 0.5992, best: 0.6350, time: 0:01:49
 Epoch: 272, lr: 8.0e-05, train_loss: 0.5792, train_acc: 0.7934 test_loss: 5.3499, test_acc: 0.5919, best: 0.6350, time: 0:01:49
 Epoch: 273, lr: 8.0e-05, train_loss: 0.5783, train_acc: 0.7966 test_loss: 7.4162, test_acc: 0.5777, best: 0.6350, time: 0:01:49
 Epoch: 274, lr: 8.0e-05, train_loss: 0.5620, train_acc: 0.8036 test_loss: 2.9387, test_acc: 0.6260, best: 0.6350, time: 0:01:49
 Epoch: 275, lr: 8.0e-05, train_loss: 0.5980, train_acc: 0.7864 test_loss: 5.0416, test_acc: 0.5933, best: 0.6350, time: 0:01:49
 Epoch: 276, lr: 8.0e-05, train_loss: 0.5871, train_acc: 0.7958 test_loss: 9.6418, test_acc: 0.5579, best: 0.6350, time: 0:01:49
 Epoch: 277, lr: 8.0e-05, train_loss: 0.5801, train_acc: 0.7976 test_loss: 9.2864, test_acc: 0.5664, best: 0.6350, time: 0:01:49
 Epoch: 278, lr: 8.0e-05, train_loss: 0.5864, train_acc: 0.7984 test_loss: 5.3509, test_acc: 0.5994, best: 0.6350, time: 0:01:49
 Epoch: 279, lr: 8.0e-05, train_loss: 0.5940, train_acc: 0.7908 test_loss: 4.9439, test_acc: 0.5946, best: 0.6350, time: 0:01:49
 Epoch: 280, lr: 8.0e-05, train_loss: 0.5876, train_acc: 0.7928 test_loss: 5.4261, test_acc: 0.6065, best: 0.6350, time: 0:01:49
 Epoch: 281, lr: 8.0e-05, train_loss: 0.5825, train_acc: 0.8002 test_loss: 4.5360, test_acc: 0.6085, best: 0.6350, time: 0:01:49
 Epoch: 282, lr: 8.0e-05, train_loss: 0.5599, train_acc: 0.8022 test_loss: 8.3757, test_acc: 0.5806, best: 0.6350, time: 0:01:49
 Epoch: 283, lr: 8.0e-05, train_loss: 0.5819, train_acc: 0.7974 test_loss: 3.5486, test_acc: 0.6176, best: 0.6350, time: 0:01:49
 Epoch: 284, lr: 8.0e-05, train_loss: 0.5933, train_acc: 0.7938 test_loss: 2.5920, test_acc: 0.6349, best: 0.6350, time: 0:01:49
 Epoch: 285, lr: 8.0e-05, train_loss: 0.5673, train_acc: 0.7954 test_loss: 7.5361, test_acc: 0.5780, best: 0.6350, time: 0:01:49
 Epoch: 286, lr: 8.0e-05, train_loss: 0.5724, train_acc: 0.7998 test_loss: 4.4327, test_acc: 0.5938, best: 0.6350, time: 0:01:48
 Epoch: 287, lr: 8.0e-05, train_loss: 0.5687, train_acc: 0.8010 test_loss: 8.3100, test_acc: 0.5765, best: 0.6350, time: 0:01:48
 Epoch: 288, lr: 8.0e-05, train_loss: 0.5907, train_acc: 0.7970 test_loss: 3.7254, test_acc: 0.6178, best: 0.6350, time: 0:01:49
 Epoch: 289, lr: 8.0e-05, train_loss: 0.5839, train_acc: 0.8008 test_loss: 7.5216, test_acc: 0.5669, best: 0.6350, time: 0:01:49
 Epoch: 290, lr: 8.0e-05, train_loss: 0.5710, train_acc: 0.8018 test_loss: 8.6175, test_acc: 0.5619, best: 0.6350, time: 0:01:49
 Epoch: 291, lr: 8.0e-05, train_loss: 0.5822, train_acc: 0.7998 test_loss: 5.8836, test_acc: 0.5921, best: 0.6350, time: 0:01:49
 Epoch: 292, lr: 8.0e-05, train_loss: 0.5806, train_acc: 0.8060 test_loss: 18.1240, test_acc: 0.5226, best: 0.6350, time: 0:01:48
 Epoch: 293, lr: 8.0e-05, train_loss: 0.5820, train_acc: 0.7960 test_loss: 4.9175, test_acc: 0.5984, best: 0.6350, time: 0:01:49
 Epoch: 294, lr: 8.0e-05, train_loss: 0.5580, train_acc: 0.8052 test_loss: 6.9363, test_acc: 0.5815, best: 0.6350, time: 0:01:49
 Epoch: 295, lr: 8.0e-05, train_loss: 0.5820, train_acc: 0.7956 test_loss: 3.3933, test_acc: 0.6264, best: 0.6350, time: 0:01:49
 Epoch: 296, lr: 8.0e-05, train_loss: 0.5802, train_acc: 0.7984 test_loss: 10.5174, test_acc: 0.5643, best: 0.6350, time: 0:01:49
 Epoch: 297, lr: 8.0e-05, train_loss: 0.5957, train_acc: 0.7924 test_loss: 1.8425, test_acc: 0.6536, best: 0.6536, time: 0:01:49
 Epoch: 298, lr: 8.0e-05, train_loss: 0.5790, train_acc: 0.7944 test_loss: 8.0727, test_acc: 0.5743, best: 0.6536, time: 0:01:49
 Epoch: 299, lr: 8.0e-05, train_loss: 0.5971, train_acc: 0.7966 test_loss: 7.5768, test_acc: 0.5799, best: 0.6536, time: 0:01:49
 Highest accuracy: 0.6536