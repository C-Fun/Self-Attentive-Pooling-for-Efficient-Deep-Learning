
 Run on time: 2022-06-24 20:02:31.449901

 Arguments:
	 gpu                  : True
	 log                  : True
	 seed                 : 0
	 momentum             : 0.9
	 amsgrad              : True
	 dataset              : STL10
	 batch_size           : 8
	 architecture         : RESNET50_GAUSSIAN_POOL_2222
	 im_size              : 224
	 relu_threshold       : 4.0
	 learning_rate        : 0.01
	 pretrained_backbone  : 
	 pretrained_ann       : 
	 weight_decay         : 0.0
	 test_only            : False
	 epochs               : 300
	 lr_interval          : [180, 240, 270]
	 lr_reduce            : 5
	 optimizer            : SGD
	 dropout              : 0.2
	 kernel_size          : 3
	 dont_save            : False
	 visualize            : False
	 devices              : 0,1
 DataParallel(
  (module): NetworkByName(
    (net): ResNet_v2(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (pooling): GaussianPooling2d(
            kernel_size=2, stride=2, padding=0
            (ToHidden): Sequential(
              (0): AdaptiveAvgPool2d(output_size=(1, 1))
              (1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
              (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (3): ReLU()
            )
            (ToMean): Sequential(
              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (ToSigma): Sequential(
              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Sigmoid()
            )
            (activation): Softplus(beta=1, threshold=20)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (pooling): GaussianPooling2d(
            kernel_size=2, stride=2, padding=0
            (ToHidden): Sequential(
              (0): AdaptiveAvgPool2d(output_size=(1, 1))
              (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
              (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (3): ReLU()
            )
            (ToMean): Sequential(
              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (ToSigma): Sequential(
              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Sigmoid()
            )
            (activation): Softplus(beta=1, threshold=20)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (pooling): GaussianPooling2d(
            kernel_size=2, stride=2, padding=0
            (ToHidden): Sequential(
              (0): AdaptiveAvgPool2d(output_size=(1, 1))
              (1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))
              (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (3): ReLU()
            )
            (ToMean): Sequential(
              (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (ToSigma): Sequential(
              (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Sigmoid()
            )
            (activation): Softplus(beta=1, threshold=20)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer4): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (pooling): GaussianPooling2d(
            kernel_size=2, stride=2, padding=0
            (ToHidden): Sequential(
              (0): AdaptiveAvgPool2d(output_size=(1, 1))
              (1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1))
              (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (3): ReLU()
            )
            (ToMean): Sequential(
              (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (ToSigma): Sequential(
              (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Sigmoid()
            )
            (activation): Softplus(beta=1, threshold=20)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
      (fc): Linear(in_features=2048, out_features=10, bias=True)
    )
  )
)
 SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0
)
 Epoch: 1, lr: 1.0e-02, train_loss: 3.7635, train_acc: 0.1324 test_loss: 33.4176, test_acc: 0.1890, best: 0.1890, time: 0:02:22
 Epoch: 2, lr: 1.0e-02, train_loss: 2.3753, train_acc: 0.1774 test_loss: 1.9955, test_acc: 0.2404, best: 0.2404, time: 0:02:24
 Epoch: 3, lr: 1.0e-02, train_loss: 2.2811, train_acc: 0.1962 test_loss: 141.4965, test_acc: 0.2011, best: 0.2404, time: 0:02:24
 Epoch: 4, lr: 1.0e-02, train_loss: 2.2089, train_acc: 0.2030 test_loss: 2.7398, test_acc: 0.2660, best: 0.2660, time: 0:02:25
 Epoch: 5, lr: 1.0e-02, train_loss: 2.1227, train_acc: 0.2296 test_loss: 9.1075, test_acc: 0.2496, best: 0.2660, time: 0:02:23
 Epoch: 6, lr: 1.0e-02, train_loss: 2.0211, train_acc: 0.2546 test_loss: 2.7728, test_acc: 0.3204, best: 0.3204, time: 0:02:25
 Epoch: 7, lr: 1.0e-02, train_loss: 1.9787, train_acc: 0.2652 test_loss: 12.7365, test_acc: 0.3332, best: 0.3332, time: 0:02:25
 Epoch: 8, lr: 1.0e-02, train_loss: 1.9087, train_acc: 0.2882 test_loss: 9.4346, test_acc: 0.3184, best: 0.3332, time: 0:02:23
 Epoch: 9, lr: 1.0e-02, train_loss: 1.8595, train_acc: 0.3096 test_loss: 16.8056, test_acc: 0.3316, best: 0.3332, time: 0:02:23
 Epoch: 10, lr: 1.0e-02, train_loss: 1.8577, train_acc: 0.3114 test_loss: 4.3072, test_acc: 0.3929, best: 0.3929, time: 0:02:23
 Epoch: 11, lr: 1.0e-02, train_loss: 1.8433, train_acc: 0.3164 test_loss: 6.3248, test_acc: 0.3777, best: 0.3929, time: 0:02:23
 Epoch: 12, lr: 1.0e-02, train_loss: 1.7826, train_acc: 0.3316 test_loss: 2.8896, test_acc: 0.3941, best: 0.3941, time: 0:02:24
 Epoch: 13, lr: 1.0e-02, train_loss: 1.7606, train_acc: 0.3508 test_loss: 26.5160, test_acc: 0.4035, best: 0.4035, time: 0:02:24
 Epoch: 14, lr: 1.0e-02, train_loss: 1.7089, train_acc: 0.3570 test_loss: 46.8103, test_acc: 0.3807, best: 0.4035, time: 0:02:23
 Epoch: 15, lr: 1.0e-02, train_loss: 1.6829, train_acc: 0.3722 test_loss: 2.7279, test_acc: 0.4275, best: 0.4275, time: 0:02:23
 Epoch: 16, lr: 1.0e-02, train_loss: 1.6628, train_acc: 0.3774 test_loss: 84.7400, test_acc: 0.4199, best: 0.4275, time: 0:02:23
 Epoch: 17, lr: 1.0e-02, train_loss: 1.6574, train_acc: 0.3774 test_loss: 26.6075, test_acc: 0.4176, best: 0.4275, time: 0:02:22
 Epoch: 18, lr: 1.0e-02, train_loss: 1.5839, train_acc: 0.4070 test_loss: 38.9067, test_acc: 0.4375, best: 0.4375, time: 0:02:23
 Epoch: 19, lr: 1.0e-02, train_loss: 1.5703, train_acc: 0.4172 test_loss: 332.7160, test_acc: 0.3881, best: 0.4375, time: 0:02:21
 Epoch: 20, lr: 1.0e-02, train_loss: 1.5581, train_acc: 0.4202 test_loss: 97.4562, test_acc: 0.4705, best: 0.4705, time: 0:02:22
 Epoch: 21, lr: 1.0e-02, train_loss: 1.5143, train_acc: 0.4430 test_loss: 101.4554, test_acc: 0.4716, best: 0.4716, time: 0:02:22
 Epoch: 22, lr: 1.0e-02, train_loss: 1.4926, train_acc: 0.4408 test_loss: 47.1978, test_acc: 0.4666, best: 0.4716, time: 0:02:21
 Epoch: 23, lr: 1.0e-02, train_loss: 1.4589, train_acc: 0.4620 test_loss: 28.2590, test_acc: 0.5115, best: 0.5115, time: 0:02:21
 Epoch: 24, lr: 1.0e-02, train_loss: 1.4593, train_acc: 0.4470 test_loss: 323.9452, test_acc: 0.4562, best: 0.5115, time: 0:02:21
 Epoch: 25, lr: 1.0e-02, train_loss: 1.4207, train_acc: 0.4802 test_loss: 114.8830, test_acc: 0.4710, best: 0.5115, time: 0:02:21
 Epoch: 26, lr: 1.0e-02, train_loss: 1.4105, train_acc: 0.4758 test_loss: 27.1928, test_acc: 0.5192, best: 0.5192, time: 0:02:22
 Epoch: 27, lr: 1.0e-02, train_loss: 1.3873, train_acc: 0.4910 test_loss: 97.6122, test_acc: 0.5051, best: 0.5192, time: 0:02:17
 Epoch: 28, lr: 1.0e-02, train_loss: 1.3715, train_acc: 0.4918 test_loss: 160.6094, test_acc: 0.5151, best: 0.5192, time: 0:02:17
 Epoch: 29, lr: 1.0e-02, train_loss: 1.3684, train_acc: 0.5010 test_loss: 4.6632, test_acc: 0.5436, best: 0.5436, time: 0:02:18
 Epoch: 30, lr: 1.0e-02, train_loss: 1.3405, train_acc: 0.5170 test_loss: 16.9834, test_acc: 0.5306, best: 0.5436, time: 0:02:17
 Epoch: 31, lr: 1.0e-02, train_loss: 1.3042, train_acc: 0.5268 test_loss: 328.2769, test_acc: 0.5051, best: 0.5436, time: 0:02:17
 Epoch: 32, lr: 1.0e-02, train_loss: 1.2800, train_acc: 0.5322 test_loss: 79.6077, test_acc: 0.5464, best: 0.5464, time: 0:02:19
 Epoch: 33, lr: 1.0e-02, train_loss: 1.2590, train_acc: 0.5396 test_loss: 9.2920, test_acc: 0.5537, best: 0.5537, time: 0:02:18
 Epoch: 34, lr: 1.0e-02, train_loss: 1.2314, train_acc: 0.5454 test_loss: 90.8347, test_acc: 0.5317, best: 0.5537, time: 0:02:17
 Epoch: 35, lr: 1.0e-02, train_loss: 1.2376, train_acc: 0.5466 test_loss: 601.9083, test_acc: 0.4930, best: 0.5537, time: 0:02:17
 Epoch: 36, lr: 1.0e-02, train_loss: 1.2156, train_acc: 0.5546 test_loss: 132.7475, test_acc: 0.5410, best: 0.5537, time: 0:02:17
 Epoch: 37, lr: 1.0e-02, train_loss: 1.2119, train_acc: 0.5606 test_loss: 5.6001, test_acc: 0.5815, best: 0.5815, time: 0:02:18
 Epoch: 38, lr: 1.0e-02, train_loss: 1.1704, train_acc: 0.5694 test_loss: 140.5591, test_acc: 0.5357, best: 0.5815, time: 0:02:17
 Epoch: 39, lr: 1.0e-02, train_loss: 1.1552, train_acc: 0.5838 test_loss: 699.2704, test_acc: 0.4968, best: 0.5815, time: 0:02:17
 Epoch: 40, lr: 1.0e-02, train_loss: 1.1292, train_acc: 0.5886 test_loss: 132.0832, test_acc: 0.5754, best: 0.5815, time: 0:02:17
 Epoch: 41, lr: 1.0e-02, train_loss: 1.1255, train_acc: 0.5970 test_loss: 6.0651, test_acc: 0.5645, best: 0.5815, time: 0:02:17
 Epoch: 42, lr: 1.0e-02, train_loss: 1.1567, train_acc: 0.5748 test_loss: 71.4797, test_acc: 0.5715, best: 0.5815, time: 0:02:17
 Epoch: 43, lr: 1.0e-02, train_loss: 1.1294, train_acc: 0.5904 test_loss: 12.5051, test_acc: 0.5944, best: 0.5944, time: 0:02:18
 Epoch: 44, lr: 1.0e-02, train_loss: 1.0875, train_acc: 0.6084 test_loss: 63.4709, test_acc: 0.5847, best: 0.5944, time: 0:02:17
 Epoch: 45, lr: 1.0e-02, train_loss: 1.0815, train_acc: 0.6098 test_loss: 66.6267, test_acc: 0.5866, best: 0.5944, time: 0:02:17
 Epoch: 46, lr: 1.0e-02, train_loss: 1.0636, train_acc: 0.6266 test_loss: 147.2368, test_acc: 0.6002, best: 0.6002, time: 0:02:18
 Epoch: 47, lr: 1.0e-02, train_loss: 1.0264, train_acc: 0.6342 test_loss: 169.0753, test_acc: 0.5853, best: 0.6002, time: 0:02:17
 Epoch: 48, lr: 1.0e-02, train_loss: 1.0347, train_acc: 0.6306 test_loss: 4.7239, test_acc: 0.6064, best: 0.6064, time: 0:02:17
 Epoch: 49, lr: 1.0e-02, train_loss: 1.0312, train_acc: 0.6348 test_loss: 119.0735, test_acc: 0.6120, best: 0.6120, time: 0:02:18
 Epoch: 50, lr: 1.0e-02, train_loss: 1.0221, train_acc: 0.6324 test_loss: 5.8009, test_acc: 0.5896, best: 0.6120, time: 0:02:17
 Epoch: 51, lr: 1.0e-02, train_loss: 0.9829, train_acc: 0.6424 test_loss: 24.6667, test_acc: 0.6302, best: 0.6302, time: 0:02:18
 Epoch: 52, lr: 1.0e-02, train_loss: 0.9819, train_acc: 0.6492 test_loss: 27.9742, test_acc: 0.6432, best: 0.6432, time: 0:02:17
 Epoch: 53, lr: 1.0e-02, train_loss: 0.9738, train_acc: 0.6514 test_loss: 323.6928, test_acc: 0.5781, best: 0.6432, time: 0:02:17
 Epoch: 54, lr: 1.0e-02, train_loss: 0.9346, train_acc: 0.6684 test_loss: 348.4979, test_acc: 0.6105, best: 0.6432, time: 0:02:17
 Epoch: 55, lr: 1.0e-02, train_loss: 0.9371, train_acc: 0.6590 test_loss: 301.6506, test_acc: 0.6054, best: 0.6432, time: 0:02:17
 Epoch: 56, lr: 1.0e-02, train_loss: 0.9512, train_acc: 0.6600 test_loss: 509.4862, test_acc: 0.5955, best: 0.6432, time: 0:02:17
 Epoch: 57, lr: 1.0e-02, train_loss: 0.9359, train_acc: 0.6640 test_loss: 56.8345, test_acc: 0.6301, best: 0.6432, time: 0:02:17
 Epoch: 58, lr: 1.0e-02, train_loss: 0.9071, train_acc: 0.6806 test_loss: 66.1458, test_acc: 0.6004, best: 0.6432, time: 0:02:17
 Epoch: 59, lr: 1.0e-02, train_loss: 0.9045, train_acc: 0.6778 test_loss: 2.8774, test_acc: 0.6461, best: 0.6461, time: 0:02:18
 Epoch: 60, lr: 1.0e-02, train_loss: 0.8839, train_acc: 0.6802 test_loss: 15.6382, test_acc: 0.6259, best: 0.6461, time: 0:02:17
 Epoch: 61, lr: 1.0e-02, train_loss: 0.9110, train_acc: 0.6742 test_loss: 12.2446, test_acc: 0.6382, best: 0.6461, time: 0:02:17
 Epoch: 62, lr: 1.0e-02, train_loss: 0.8609, train_acc: 0.6876 test_loss: 15.8078, test_acc: 0.5989, best: 0.6461, time: 0:02:17
 Epoch: 63, lr: 1.0e-02, train_loss: 0.8669, train_acc: 0.6926 test_loss: 1.4621, test_acc: 0.6754, best: 0.6754, time: 0:02:18
 Epoch: 64, lr: 1.0e-02, train_loss: 0.8553, train_acc: 0.6948 test_loss: 26.1609, test_acc: 0.6069, best: 0.6754, time: 0:02:17
 Epoch: 65, lr: 1.0e-02, train_loss: 0.8645, train_acc: 0.6862 test_loss: 19.8688, test_acc: 0.6429, best: 0.6754, time: 0:02:17
 Epoch: 66, lr: 1.0e-02, train_loss: 0.8205, train_acc: 0.7054 test_loss: 15.9938, test_acc: 0.6519, best: 0.6754, time: 0:02:17
 Epoch: 67, lr: 1.0e-02, train_loss: 0.8298, train_acc: 0.6990 test_loss: 12.5482, test_acc: 0.6404, best: 0.6754, time: 0:02:17
 Epoch: 68, lr: 1.0e-02, train_loss: 0.8054, train_acc: 0.7248 test_loss: 136.4297, test_acc: 0.6081, best: 0.6754, time: 0:02:17
 Epoch: 69, lr: 1.0e-02, train_loss: 0.7839, train_acc: 0.7232 test_loss: 46.7927, test_acc: 0.6469, best: 0.6754, time: 0:02:17
 Epoch: 70, lr: 1.0e-02, train_loss: 0.7897, train_acc: 0.7250 test_loss: 116.2110, test_acc: 0.6170, best: 0.6754, time: 0:02:17
 Epoch: 71, lr: 1.0e-02, train_loss: 0.7657, train_acc: 0.7334 test_loss: 35.2934, test_acc: 0.6764, best: 0.6764, time: 0:02:18
 Epoch: 72, lr: 1.0e-02, train_loss: 0.7682, train_acc: 0.7276 test_loss: 53.0300, test_acc: 0.6378, best: 0.6764, time: 0:02:18
 Epoch: 73, lr: 1.0e-02, train_loss: 0.7640, train_acc: 0.7224 test_loss: 368.4097, test_acc: 0.6005, best: 0.6764, time: 0:02:17
 Epoch: 74, lr: 1.0e-02, train_loss: 0.7718, train_acc: 0.7232 test_loss: 6.3407, test_acc: 0.6379, best: 0.6764, time: 0:02:17
 Epoch: 75, lr: 1.0e-02, train_loss: 0.8141, train_acc: 0.7134 test_loss: 34.2938, test_acc: 0.6395, best: 0.6764, time: 0:02:17
 Epoch: 76, lr: 1.0e-02, train_loss: 0.7511, train_acc: 0.7346 test_loss: 5.0405, test_acc: 0.6830, best: 0.6830, time: 0:02:18
 Epoch: 77, lr: 1.0e-02, train_loss: 0.7300, train_acc: 0.7496 test_loss: 80.5077, test_acc: 0.6546, best: 0.6830, time: 0:02:17
 Epoch: 78, lr: 1.0e-02, train_loss: 0.7122, train_acc: 0.7488 test_loss: 78.6294, test_acc: 0.6482, best: 0.6830, time: 0:02:17
 Epoch: 79, lr: 1.0e-02, train_loss: 0.7502, train_acc: 0.7356 test_loss: 149.1563, test_acc: 0.6381, best: 0.6830, time: 0:02:17
 Epoch: 80, lr: 1.0e-02, train_loss: 0.7281, train_acc: 0.7438 test_loss: 127.9693, test_acc: 0.6759, best: 0.6830, time: 0:02:17
 Epoch: 81, lr: 1.0e-02, train_loss: 0.7047, train_acc: 0.7580 test_loss: 3179.9573, test_acc: 0.6039, best: 0.6830, time: 0:02:17
 Epoch: 82, lr: 1.0e-02, train_loss: 0.6887, train_acc: 0.7564 test_loss: 222.4156, test_acc: 0.6679, best: 0.6830, time: 0:02:16
 Epoch: 83, lr: 1.0e-02, train_loss: 0.6672, train_acc: 0.7572 test_loss: 292.5991, test_acc: 0.6270, best: 0.6830, time: 0:02:17
 Epoch: 84, lr: 1.0e-02, train_loss: 0.6922, train_acc: 0.7548 test_loss: 434.3522, test_acc: 0.6348, best: 0.6830, time: 0:02:17
 Epoch: 85, lr: 1.0e-02, train_loss: 0.6679, train_acc: 0.7686 test_loss: 162.8287, test_acc: 0.6560, best: 0.6830, time: 0:02:17
 Epoch: 86, lr: 1.0e-02, train_loss: 0.6781, train_acc: 0.7586 test_loss: 96.5881, test_acc: 0.6562, best: 0.6830, time: 0:02:17
 Epoch: 87, lr: 1.0e-02, train_loss: 0.6368, train_acc: 0.7780 test_loss: 58.1773, test_acc: 0.6904, best: 0.6904, time: 0:02:18
 Epoch: 88, lr: 1.0e-02, train_loss: 0.6561, train_acc: 0.7674 test_loss: 118.1657, test_acc: 0.6764, best: 0.6904, time: 0:02:17
 Epoch: 89, lr: 1.0e-02, train_loss: 0.6517, train_acc: 0.7682 test_loss: 39.9995, test_acc: 0.6696, best: 0.6904, time: 0:02:17
 Epoch: 90, lr: 1.0e-02, train_loss: 0.6478, train_acc: 0.7708 test_loss: 127.4923, test_acc: 0.6418, best: 0.6904, time: 0:02:17
 Epoch: 91, lr: 1.0e-02, train_loss: 0.5989, train_acc: 0.7908 test_loss: 1270.1058, test_acc: 0.6010, best: 0.6904, time: 0:02:17
 Epoch: 92, lr: 1.0e-02, train_loss: 0.6070, train_acc: 0.7850 test_loss: 620.6975, test_acc: 0.6024, best: 0.6904, time: 0:02:17
 Epoch: 93, lr: 1.0e-02, train_loss: 0.6042, train_acc: 0.7812 test_loss: 42.3326, test_acc: 0.6744, best: 0.6904, time: 0:02:17
 Epoch: 94, lr: 1.0e-02, train_loss: 0.5922, train_acc: 0.7904 test_loss: 649.1054, test_acc: 0.6041, best: 0.6904, time: 0:02:18
 Epoch: 95, lr: 1.0e-02, train_loss: 0.6107, train_acc: 0.7904 test_loss: 115.7067, test_acc: 0.6494, best: 0.6904, time: 0:02:21
 Epoch: 96, lr: 1.0e-02, train_loss: 0.5777, train_acc: 0.7952 test_loss: 186.7499, test_acc: 0.6252, best: 0.6904, time: 0:02:20
 Epoch: 97, lr: 1.0e-02, train_loss: 0.5866, train_acc: 0.7970 test_loss: 70.9550, test_acc: 0.6603, best: 0.6904, time: 0:02:18
 Epoch: 98, lr: 1.0e-02, train_loss: 0.5603, train_acc: 0.8038 test_loss: 36.2225, test_acc: 0.6614, best: 0.6904, time: 0:02:19
 Epoch: 99, lr: 1.0e-02, train_loss: 0.6026, train_acc: 0.7920 test_loss: 9.5429, test_acc: 0.6855, best: 0.6904, time: 0:02:18
 Epoch: 100, lr: 1.0e-02, train_loss: 0.5594, train_acc: 0.8002 test_loss: 134.9296, test_acc: 0.6238, best: 0.6904, time: 0:02:20
 Epoch: 101, lr: 1.0e-02, train_loss: 0.5581, train_acc: 0.8014 test_loss: 3.8052, test_acc: 0.7064, best: 0.7064, time: 0:02:19
 Epoch: 102, lr: 1.0e-02, train_loss: 0.5585, train_acc: 0.8024 test_loss: 25.2307, test_acc: 0.6923, best: 0.7064, time: 0:02:21
 Epoch: 103, lr: 1.0e-02, train_loss: 0.5719, train_acc: 0.8092 test_loss: 14.4407, test_acc: 0.7206, best: 0.7206, time: 0:02:22
 Epoch: 104, lr: 1.0e-02, train_loss: 0.5309, train_acc: 0.8156 test_loss: 53.1535, test_acc: 0.7065, best: 0.7206, time: 0:02:21
 Epoch: 105, lr: 1.0e-02, train_loss: 0.5437, train_acc: 0.8134 test_loss: 25.7516, test_acc: 0.7107, best: 0.7206, time: 0:02:21
 Epoch: 106, lr: 1.0e-02, train_loss: 0.5335, train_acc: 0.8094 test_loss: 60.1104, test_acc: 0.6653, best: 0.7206, time: 0:02:21
 Epoch: 107, lr: 1.0e-02, train_loss: 0.5248, train_acc: 0.8206 test_loss: 294.4670, test_acc: 0.6254, best: 0.7206, time: 0:02:21
 Epoch: 108, lr: 1.0e-02, train_loss: 0.5073, train_acc: 0.8240 test_loss: 101.3903, test_acc: 0.6448, best: 0.7206, time: 0:02:22
 Epoch: 109, lr: 1.0e-02, train_loss: 0.5231, train_acc: 0.8212 test_loss: 3.6599, test_acc: 0.7160, best: 0.7206, time: 0:02:22
 Epoch: 110, lr: 1.0e-02, train_loss: 0.5198, train_acc: 0.8136 test_loss: 80.7243, test_acc: 0.6326, best: 0.7206, time: 0:02:22
 Epoch: 111, lr: 1.0e-02, train_loss: 0.5094, train_acc: 0.8196 test_loss: 11.4009, test_acc: 0.6984, best: 0.7206, time: 0:02:22
 Epoch: 112, lr: 1.0e-02, train_loss: 0.5055, train_acc: 0.8242 test_loss: 26.3566, test_acc: 0.6946, best: 0.7206, time: 0:02:22
 Epoch: 113, lr: 1.0e-02, train_loss: 0.5105, train_acc: 0.8194 test_loss: 33.7936, test_acc: 0.6907, best: 0.7206, time: 0:02:22
 Epoch: 114, lr: 1.0e-02, train_loss: 0.4809, train_acc: 0.8340 test_loss: 99.5358, test_acc: 0.6750, best: 0.7206, time: 0:02:22
 Epoch: 115, lr: 1.0e-02, train_loss: 0.4921, train_acc: 0.8308 test_loss: 186.8201, test_acc: 0.6679, best: 0.7206, time: 0:02:22
 Epoch: 116, lr: 1.0e-02, train_loss: 0.4812, train_acc: 0.8336 test_loss: 511.7238, test_acc: 0.6703, best: 0.7206, time: 0:02:22
 Epoch: 117, lr: 1.0e-02, train_loss: 0.4773, train_acc: 0.8312 test_loss: 2823.4751, test_acc: 0.5756, best: 0.7206, time: 0:02:22
 Epoch: 118, lr: 1.0e-02, train_loss: 0.4959, train_acc: 0.8280 test_loss: 119.4990, test_acc: 0.6937, best: 0.7206, time: 0:02:22
 Epoch: 119, lr: 1.0e-02, train_loss: 0.4636, train_acc: 0.8388 test_loss: 477.2185, test_acc: 0.6525, best: 0.7206, time: 0:02:22
 Epoch: 120, lr: 1.0e-02, train_loss: 0.4770, train_acc: 0.8340 test_loss: 8.4034, test_acc: 0.7184, best: 0.7206, time: 0:02:22
 Epoch: 121, lr: 1.0e-02, train_loss: 0.4649, train_acc: 0.8362 test_loss: 4.7630, test_acc: 0.7006, best: 0.7206, time: 0:02:22
 Epoch: 122, lr: 1.0e-02, train_loss: 0.4540, train_acc: 0.8368 test_loss: 31.0701, test_acc: 0.6961, best: 0.7206, time: 0:02:22
 Epoch: 123, lr: 1.0e-02, train_loss: 0.4589, train_acc: 0.8414 test_loss: 1631.0998, test_acc: 0.5885, best: 0.7206, time: 0:02:22
 Epoch: 124, lr: 1.0e-02, train_loss: 0.4576, train_acc: 0.8440 test_loss: 702.0096, test_acc: 0.6579, best: 0.7206, time: 0:02:22
 Epoch: 125, lr: 1.0e-02, train_loss: 0.4317, train_acc: 0.8476 test_loss: 296.8154, test_acc: 0.6691, best: 0.7206, time: 0:02:22
 Epoch: 126, lr: 1.0e-02, train_loss: 0.4577, train_acc: 0.8434 test_loss: 735.2248, test_acc: 0.6666, best: 0.7206, time: 0:02:22
 Epoch: 127, lr: 1.0e-02, train_loss: 0.4494, train_acc: 0.8436 test_loss: 101.4152, test_acc: 0.6771, best: 0.7206, time: 0:02:22
 Epoch: 128, lr: 1.0e-02, train_loss: 0.4397, train_acc: 0.8516 test_loss: 37.1773, test_acc: 0.6820, best: 0.7206, time: 0:02:22
 Epoch: 129, lr: 1.0e-02, train_loss: 0.4377, train_acc: 0.8496 test_loss: 81.3135, test_acc: 0.6770, best: 0.7206, time: 0:02:22
 Epoch: 130, lr: 1.0e-02, train_loss: 0.4617, train_acc: 0.8388 test_loss: 50.4453, test_acc: 0.6883, best: 0.7206, time: 0:02:22
 Epoch: 131, lr: 1.0e-02, train_loss: 0.4548, train_acc: 0.8456 test_loss: 9.7777, test_acc: 0.7070, best: 0.7206, time: 0:02:22
 Epoch: 132, lr: 1.0e-02, train_loss: 0.4250, train_acc: 0.8514 test_loss: 97.2053, test_acc: 0.6871, best: 0.7206, time: 0:02:23
 Epoch: 133, lr: 1.0e-02, train_loss: 0.4437, train_acc: 0.8518 test_loss: 34.7920, test_acc: 0.7017, best: 0.7206, time: 0:02:22
 Epoch: 134, lr: 1.0e-02, train_loss: 0.4144, train_acc: 0.8578 test_loss: 94.8222, test_acc: 0.6805, best: 0.7206, time: 0:02:22
 Epoch: 135, lr: 1.0e-02, train_loss: 0.4370, train_acc: 0.8424 test_loss: 35.6630, test_acc: 0.6924, best: 0.7206, time: 0:02:22
 Epoch: 136, lr: 1.0e-02, train_loss: 0.4348, train_acc: 0.8524 test_loss: 306.9916, test_acc: 0.6558, best: 0.7206, time: 0:02:22
 Epoch: 137, lr: 1.0e-02, train_loss: 0.4321, train_acc: 0.8480 test_loss: 117.7265, test_acc: 0.6910, best: 0.7206, time: 0:02:22
 Epoch: 138, lr: 1.0e-02, train_loss: 0.4069, train_acc: 0.8604 test_loss: 474.3258, test_acc: 0.6536, best: 0.7206, time: 0:02:23
 Epoch: 139, lr: 1.0e-02, train_loss: 0.4051, train_acc: 0.8574 test_loss: 32.8028, test_acc: 0.7145, best: 0.7206, time: 0:02:22
 Epoch: 140, lr: 1.0e-02, train_loss: 0.4006, train_acc: 0.8664 test_loss: 80.6754, test_acc: 0.6986, best: 0.7206, time: 0:02:22
 Epoch: 141, lr: 1.0e-02, train_loss: 0.4245, train_acc: 0.8590 test_loss: 492.0766, test_acc: 0.6311, best: 0.7206, time: 0:02:23
 Epoch: 142, lr: 1.0e-02, train_loss: 0.4100, train_acc: 0.8628 test_loss: 139.6181, test_acc: 0.6665, best: 0.7206, time: 0:02:22
 Epoch: 143, lr: 1.0e-02, train_loss: 0.3819, train_acc: 0.8682 test_loss: 17.7460, test_acc: 0.7216, best: 0.7216, time: 0:02:23
 Epoch: 144, lr: 1.0e-02, train_loss: 0.4008, train_acc: 0.8604 test_loss: 23.4777, test_acc: 0.7220, best: 0.7220, time: 0:02:23
 Epoch: 145, lr: 1.0e-02, train_loss: 0.3639, train_acc: 0.8754 test_loss: 82.7483, test_acc: 0.6841, best: 0.7220, time: 0:02:22
 Epoch: 146, lr: 1.0e-02, train_loss: 0.3911, train_acc: 0.8672 test_loss: 168.6682, test_acc: 0.7117, best: 0.7220, time: 0:02:22
 Epoch: 147, lr: 1.0e-02, train_loss: 0.3900, train_acc: 0.8672 test_loss: 52.4550, test_acc: 0.7071, best: 0.7220, time: 0:02:23
 Epoch: 148, lr: 1.0e-02, train_loss: 0.3784, train_acc: 0.8678 test_loss: 115.9579, test_acc: 0.7076, best: 0.7220, time: 0:02:22
 Epoch: 149, lr: 1.0e-02, train_loss: 0.3877, train_acc: 0.8654 test_loss: 412.0036, test_acc: 0.6614, best: 0.7220, time: 0:02:23
 Epoch: 150, lr: 1.0e-02, train_loss: 0.3904, train_acc: 0.8660 test_loss: 1209.4670, test_acc: 0.6398, best: 0.7220, time: 0:02:23
 Epoch: 151, lr: 1.0e-02, train_loss: 0.3739, train_acc: 0.8700 test_loss: 504.9469, test_acc: 0.6424, best: 0.7220, time: 0:02:23
 Epoch: 152, lr: 1.0e-02, train_loss: 0.3600, train_acc: 0.8714 test_loss: 120.1224, test_acc: 0.6699, best: 0.7220, time: 0:02:22
 Epoch: 153, lr: 1.0e-02, train_loss: 0.3534, train_acc: 0.8812 test_loss: 111.2494, test_acc: 0.6890, best: 0.7220, time: 0:02:22
 Epoch: 154, lr: 1.0e-02, train_loss: 0.3586, train_acc: 0.8794 test_loss: 721.1586, test_acc: 0.6402, best: 0.7220, time: 0:02:22
 Epoch: 155, lr: 1.0e-02, train_loss: 0.3699, train_acc: 0.8784 test_loss: 610.7632, test_acc: 0.6551, best: 0.7220, time: 0:02:23
 Epoch: 156, lr: 1.0e-02, train_loss: 0.3526, train_acc: 0.8742 test_loss: 1449.7926, test_acc: 0.6090, best: 0.7220, time: 0:02:22
 Epoch: 157, lr: 1.0e-02, train_loss: 0.3672, train_acc: 0.8794 test_loss: 265.4599, test_acc: 0.6666, best: 0.7220, time: 0:02:22
 Epoch: 158, lr: 1.0e-02, train_loss: 0.3325, train_acc: 0.8836 test_loss: 38.7032, test_acc: 0.6924, best: 0.7220, time: 0:02:22
 Epoch: 159, lr: 1.0e-02, train_loss: 0.3368, train_acc: 0.8810 test_loss: 63.3793, test_acc: 0.7013, best: 0.7220, time: 0:02:23
 Epoch: 160, lr: 1.0e-02, train_loss: 0.3528, train_acc: 0.8784 test_loss: 120.4307, test_acc: 0.6794, best: 0.7220, time: 0:02:23
 Epoch: 161, lr: 1.0e-02, train_loss: 0.3681, train_acc: 0.8738 test_loss: 11.2702, test_acc: 0.7009, best: 0.7220, time: 0:02:22
 Epoch: 162, lr: 1.0e-02, train_loss: 0.3449, train_acc: 0.8812 test_loss: 363.8702, test_acc: 0.6428, best: 0.7220, time: 0:02:22
 Epoch: 163, lr: 1.0e-02, train_loss: 0.3201, train_acc: 0.8892 test_loss: 77.6094, test_acc: 0.6903, best: 0.7220, time: 0:02:23
 Epoch: 164, lr: 1.0e-02, train_loss: 0.3167, train_acc: 0.8900 test_loss: 213.5675, test_acc: 0.6480, best: 0.7220, time: 0:02:23
 Epoch: 165, lr: 1.0e-02, train_loss: 0.3530, train_acc: 0.8802 test_loss: 20.7416, test_acc: 0.7057, best: 0.7220, time: 0:02:22
 Epoch: 166, lr: 1.0e-02, train_loss: 0.3416, train_acc: 0.8812 test_loss: 30.3731, test_acc: 0.6701, best: 0.7220, time: 0:02:22
 Epoch: 167, lr: 1.0e-02, train_loss: 0.3569, train_acc: 0.8764 test_loss: 47.7240, test_acc: 0.6746, best: 0.7220, time: 0:02:22
 Epoch: 168, lr: 1.0e-02, train_loss: 0.3487, train_acc: 0.8794 test_loss: 9.7267, test_acc: 0.7075, best: 0.7220, time: 0:02:22
 Epoch: 169, lr: 1.0e-02, train_loss: 0.3426, train_acc: 0.8792 test_loss: 41.1360, test_acc: 0.6813, best: 0.7220, time: 0:02:22
 Epoch: 170, lr: 1.0e-02, train_loss: 0.3317, train_acc: 0.8858 test_loss: 163.6398, test_acc: 0.6607, best: 0.7220, time: 0:02:22
 Epoch: 171, lr: 1.0e-02, train_loss: 0.3271, train_acc: 0.8876 test_loss: 1784.4466, test_acc: 0.5755, best: 0.7220, time: 0:02:22
 Epoch: 172, lr: 1.0e-02, train_loss: 0.3577, train_acc: 0.8780 test_loss: 525.5612, test_acc: 0.6587, best: 0.7220, time: 0:02:22
 Epoch: 173, lr: 1.0e-02, train_loss: 0.3462, train_acc: 0.8834 test_loss: 1412.3127, test_acc: 0.6255, best: 0.7220, time: 0:02:22
 Epoch: 174, lr: 1.0e-02, train_loss: 0.3427, train_acc: 0.8814 test_loss: 574.9405, test_acc: 0.6580, best: 0.7220, time: 0:02:22
 Epoch: 175, lr: 1.0e-02, train_loss: 0.3271, train_acc: 0.8868 test_loss: 395.4977, test_acc: 0.7086, best: 0.7220, time: 0:02:22
 Epoch: 176, lr: 1.0e-02, train_loss: 0.3215, train_acc: 0.8908 test_loss: 66.5072, test_acc: 0.7252, best: 0.7252, time: 0:02:23
 Epoch: 177, lr: 1.0e-02, train_loss: 0.3217, train_acc: 0.8894 test_loss: 91.9534, test_acc: 0.7104, best: 0.7252, time: 0:02:22
 Epoch: 178, lr: 1.0e-02, train_loss: 0.3247, train_acc: 0.8854 test_loss: 40.6485, test_acc: 0.6854, best: 0.7252, time: 0:02:22
 Epoch: 179, lr: 1.0e-02, train_loss: 0.3171, train_acc: 0.8922 test_loss: 25.7449, test_acc: 0.6985, best: 0.7252, time: 0:02:22
 Epoch: 180, lr: 2.0e-03, train_loss: 0.2738, train_acc: 0.9030 test_loss: 206.4919, test_acc: 0.6455, best: 0.7252, time: 0:02:22
 Epoch: 181, lr: 2.0e-03, train_loss: 0.2454, train_acc: 0.9196 test_loss: 71.4963, test_acc: 0.6800, best: 0.7252, time: 0:02:23
 Epoch: 182, lr: 2.0e-03, train_loss: 0.2477, train_acc: 0.9130 test_loss: 43.9786, test_acc: 0.7053, best: 0.7252, time: 0:02:22
 Epoch: 183, lr: 2.0e-03, train_loss: 0.2377, train_acc: 0.9170 test_loss: 128.8928, test_acc: 0.6606, best: 0.7252, time: 0:02:22
 Epoch: 184, lr: 2.0e-03, train_loss: 0.2173, train_acc: 0.9278 test_loss: 41.3476, test_acc: 0.7011, best: 0.7252, time: 0:02:22
 Epoch: 185, lr: 2.0e-03, train_loss: 0.2206, train_acc: 0.9252 test_loss: 165.5283, test_acc: 0.6730, best: 0.7252, time: 0:02:23
 Epoch: 186, lr: 2.0e-03, train_loss: 0.2306, train_acc: 0.9192 test_loss: 13.6979, test_acc: 0.7485, best: 0.7485, time: 0:02:23
 Epoch: 187, lr: 2.0e-03, train_loss: 0.2219, train_acc: 0.9246 test_loss: 36.8462, test_acc: 0.7103, best: 0.7485, time: 0:02:23
 Epoch: 188, lr: 2.0e-03, train_loss: 0.2318, train_acc: 0.9262 test_loss: 41.8758, test_acc: 0.7156, best: 0.7485, time: 0:02:23
 Epoch: 189, lr: 2.0e-03, train_loss: 0.2048, train_acc: 0.9296 test_loss: 20.7511, test_acc: 0.7185, best: 0.7485, time: 0:02:23
 Epoch: 190, lr: 2.0e-03, train_loss: 0.2333, train_acc: 0.9220 test_loss: 47.8246, test_acc: 0.7169, best: 0.7485, time: 0:02:23
 Epoch: 191, lr: 2.0e-03, train_loss: 0.2042, train_acc: 0.9304 test_loss: 106.9103, test_acc: 0.6787, best: 0.7485, time: 0:02:23
 Epoch: 192, lr: 2.0e-03, train_loss: 0.2246, train_acc: 0.9290 test_loss: 152.2049, test_acc: 0.6744, best: 0.7485, time: 0:02:22
 Epoch: 193, lr: 2.0e-03, train_loss: 0.2304, train_acc: 0.9212 test_loss: 96.7912, test_acc: 0.6891, best: 0.7485, time: 0:02:22
 Epoch: 194, lr: 2.0e-03, train_loss: 0.1970, train_acc: 0.9302 test_loss: 17.2995, test_acc: 0.7488, best: 0.7488, time: 0:02:23
 Epoch: 195, lr: 2.0e-03, train_loss: 0.2127, train_acc: 0.9276 test_loss: 55.6303, test_acc: 0.7051, best: 0.7488, time: 0:02:23
 Epoch: 196, lr: 2.0e-03, train_loss: 0.2090, train_acc: 0.9290 test_loss: 120.8138, test_acc: 0.6883, best: 0.7488, time: 0:02:22
 Epoch: 197, lr: 2.0e-03, train_loss: 0.2170, train_acc: 0.9244 test_loss: 157.9724, test_acc: 0.6724, best: 0.7488, time: 0:02:22
 Epoch: 198, lr: 2.0e-03, train_loss: 0.2088, train_acc: 0.9306 test_loss: 147.2604, test_acc: 0.6935, best: 0.7488, time: 0:02:22
 Epoch: 199, lr: 2.0e-03, train_loss: 0.1873, train_acc: 0.9358 test_loss: 141.9041, test_acc: 0.7006, best: 0.7488, time: 0:02:22
 Epoch: 200, lr: 2.0e-03, train_loss: 0.2066, train_acc: 0.9294 test_loss: 90.7666, test_acc: 0.6973, best: 0.7488, time: 0:02:22
 Epoch: 201, lr: 2.0e-03, train_loss: 0.2021, train_acc: 0.9332 test_loss: 87.8421, test_acc: 0.7044, best: 0.7488, time: 0:02:23
 Epoch: 202, lr: 2.0e-03, train_loss: 0.1907, train_acc: 0.9344 test_loss: 126.5557, test_acc: 0.6863, best: 0.7488, time: 0:02:23
 Epoch: 203, lr: 2.0e-03, train_loss: 0.1966, train_acc: 0.9318 test_loss: 491.2403, test_acc: 0.6401, best: 0.7488, time: 0:02:23
 Epoch: 204, lr: 2.0e-03, train_loss: 0.2046, train_acc: 0.9310 test_loss: 301.6278, test_acc: 0.6927, best: 0.7488, time: 0:02:23
 Epoch: 205, lr: 2.0e-03, train_loss: 0.2041, train_acc: 0.9296 test_loss: 111.6856, test_acc: 0.6853, best: 0.7488, time: 0:02:22
 Epoch: 206, lr: 2.0e-03, train_loss: 0.1883, train_acc: 0.9348 test_loss: 286.1275, test_acc: 0.6633, best: 0.7488, time: 0:02:23
 Epoch: 207, lr: 2.0e-03, train_loss: 0.1874, train_acc: 0.9372 test_loss: 30.6999, test_acc: 0.7351, best: 0.7488, time: 0:02:22
 Epoch: 208, lr: 2.0e-03, train_loss: 0.2186, train_acc: 0.9270 test_loss: 206.5048, test_acc: 0.6398, best: 0.7488, time: 0:02:22
 Epoch: 209, lr: 2.0e-03, train_loss: 0.1812, train_acc: 0.9364 test_loss: 259.6473, test_acc: 0.6577, best: 0.7488, time: 0:02:22
 Epoch: 210, lr: 2.0e-03, train_loss: 0.1903, train_acc: 0.9368 test_loss: 49.5201, test_acc: 0.7126, best: 0.7488, time: 0:02:22
 Epoch: 211, lr: 2.0e-03, train_loss: 0.1865, train_acc: 0.9348 test_loss: 39.3238, test_acc: 0.7260, best: 0.7488, time: 0:02:22
 Epoch: 212, lr: 2.0e-03, train_loss: 0.1996, train_acc: 0.9338 test_loss: 112.8431, test_acc: 0.6723, best: 0.7488, time: 0:02:22
 Epoch: 213, lr: 2.0e-03, train_loss: 0.1883, train_acc: 0.9340 test_loss: 1024.8333, test_acc: 0.6266, best: 0.7488, time: 0:02:22
 Epoch: 214, lr: 2.0e-03, train_loss: 0.1857, train_acc: 0.9364 test_loss: 123.1532, test_acc: 0.6971, best: 0.7488, time: 0:02:22
 Epoch: 215, lr: 2.0e-03, train_loss: 0.1835, train_acc: 0.9394 test_loss: 233.9320, test_acc: 0.7190, best: 0.7488, time: 0:02:22
 Epoch: 216, lr: 2.0e-03, train_loss: 0.1902, train_acc: 0.9352 test_loss: 990.9287, test_acc: 0.5975, best: 0.7488, time: 0:02:22
 Epoch: 217, lr: 2.0e-03, train_loss: 0.1875, train_acc: 0.9364 test_loss: 88.0526, test_acc: 0.7057, best: 0.7488, time: 0:02:22
 Epoch: 218, lr: 2.0e-03, train_loss: 0.2124, train_acc: 0.9286 test_loss: 30.3730, test_acc: 0.7519, best: 0.7519, time: 0:02:23
 Epoch: 219, lr: 2.0e-03, train_loss: 0.2006, train_acc: 0.9318 test_loss: 91.2848, test_acc: 0.7180, best: 0.7519, time: 0:02:22
 Epoch: 220, lr: 2.0e-03, train_loss: 0.2034, train_acc: 0.9270 test_loss: 154.0956, test_acc: 0.6985, best: 0.7519, time: 0:02:22
 Epoch: 221, lr: 2.0e-03, train_loss: 0.1775, train_acc: 0.9394 test_loss: 65.2344, test_acc: 0.7129, best: 0.7519, time: 0:02:22
 Epoch: 222, lr: 2.0e-03, train_loss: 0.1775, train_acc: 0.9396 test_loss: 95.4210, test_acc: 0.7229, best: 0.7519, time: 0:02:22
 Epoch: 223, lr: 2.0e-03, train_loss: 0.1830, train_acc: 0.9390 test_loss: 500.3195, test_acc: 0.6274, best: 0.7519, time: 0:02:22
 Epoch: 224, lr: 2.0e-03, train_loss: 0.1758, train_acc: 0.9424 test_loss: 137.7770, test_acc: 0.7054, best: 0.7519, time: 0:02:22
 Epoch: 225, lr: 2.0e-03, train_loss: 0.1748, train_acc: 0.9422 test_loss: 50.3983, test_acc: 0.7381, best: 0.7519, time: 0:02:22
 Epoch: 226, lr: 2.0e-03, train_loss: 0.1881, train_acc: 0.9352 test_loss: 180.1703, test_acc: 0.6994, best: 0.7519, time: 0:02:22
 Epoch: 227, lr: 2.0e-03, train_loss: 0.1749, train_acc: 0.9422 test_loss: 34.9183, test_acc: 0.7362, best: 0.7519, time: 0:02:22
 Epoch: 228, lr: 2.0e-03, train_loss: 0.1927, train_acc: 0.9362 test_loss: 1634.8463, test_acc: 0.6184, best: 0.7519, time: 0:02:22
 Epoch: 229, lr: 2.0e-03, train_loss: 0.1835, train_acc: 0.9370 test_loss: 224.8744, test_acc: 0.6790, best: 0.7519, time: 0:02:22
 Epoch: 230, lr: 2.0e-03, train_loss: 0.1774, train_acc: 0.9408 test_loss: 296.5669, test_acc: 0.6890, best: 0.7519, time: 0:02:23
 Epoch: 231, lr: 2.0e-03, train_loss: 0.1906, train_acc: 0.9322 test_loss: 373.6141, test_acc: 0.6522, best: 0.7519, time: 0:02:23
 Epoch: 232, lr: 2.0e-03, train_loss: 0.1856, train_acc: 0.9384 test_loss: 153.0084, test_acc: 0.6939, best: 0.7519, time: 0:02:22
 Epoch: 233, lr: 2.0e-03, train_loss: 0.1822, train_acc: 0.9366 test_loss: 149.8139, test_acc: 0.6787, best: 0.7519, time: 0:02:23
 Epoch: 234, lr: 2.0e-03, train_loss: 0.1800, train_acc: 0.9410 test_loss: 410.2265, test_acc: 0.6416, best: 0.7519, time: 0:02:23
 Epoch: 235, lr: 2.0e-03, train_loss: 0.1873, train_acc: 0.9370 test_loss: 312.0488, test_acc: 0.6897, best: 0.7519, time: 0:02:22
 Epoch: 236, lr: 2.0e-03, train_loss: 0.1752, train_acc: 0.9418 test_loss: 113.7797, test_acc: 0.6983, best: 0.7519, time: 0:02:22
 Epoch: 237, lr: 2.0e-03, train_loss: 0.1691, train_acc: 0.9440 test_loss: 280.6172, test_acc: 0.6759, best: 0.7519, time: 0:02:22
 Epoch: 238, lr: 2.0e-03, train_loss: 0.1898, train_acc: 0.9354 test_loss: 168.2225, test_acc: 0.6886, best: 0.7519, time: 0:02:22
 Epoch: 239, lr: 2.0e-03, train_loss: 0.1783, train_acc: 0.9386 test_loss: 91.3215, test_acc: 0.7034, best: 0.7519, time: 0:02:22
 Epoch: 240, lr: 4.0e-04, train_loss: 0.1833, train_acc: 0.9380 test_loss: 296.8399, test_acc: 0.6745, best: 0.7519, time: 0:02:22
 Epoch: 241, lr: 4.0e-04, train_loss: 0.1751, train_acc: 0.9426 test_loss: 59.3371, test_acc: 0.7215, best: 0.7519, time: 0:02:23
 Epoch: 242, lr: 4.0e-04, train_loss: 0.1692, train_acc: 0.9458 test_loss: 327.0523, test_acc: 0.6673, best: 0.7519, time: 0:02:22
 Epoch: 243, lr: 4.0e-04, train_loss: 0.1792, train_acc: 0.9424 test_loss: 78.4194, test_acc: 0.7130, best: 0.7519, time: 0:02:22
 Epoch: 244, lr: 4.0e-04, train_loss: 0.1619, train_acc: 0.9454 test_loss: 308.2968, test_acc: 0.6675, best: 0.7519, time: 0:02:23
 Epoch: 245, lr: 4.0e-04, train_loss: 0.1669, train_acc: 0.9446 test_loss: 96.9874, test_acc: 0.7055, best: 0.7519, time: 0:02:22
 Epoch: 246, lr: 4.0e-04, train_loss: 0.1627, train_acc: 0.9460 test_loss: 60.6349, test_acc: 0.7231, best: 0.7519, time: 0:02:22
 Epoch: 247, lr: 4.0e-04, train_loss: 0.1624, train_acc: 0.9450 test_loss: 60.3559, test_acc: 0.7325, best: 0.7519, time: 0:02:22
 Epoch: 248, lr: 4.0e-04, train_loss: 0.1735, train_acc: 0.9440 test_loss: 214.6857, test_acc: 0.6755, best: 0.7519, time: 0:02:23
 Epoch: 249, lr: 4.0e-04, train_loss: 0.1748, train_acc: 0.9426 test_loss: 80.1958, test_acc: 0.7356, best: 0.7519, time: 0:02:23
 Epoch: 250, lr: 4.0e-04, train_loss: 0.1506, train_acc: 0.9446 test_loss: 72.3781, test_acc: 0.7276, best: 0.7519, time: 0:02:23
 Epoch: 251, lr: 4.0e-04, train_loss: 0.1650, train_acc: 0.9470 test_loss: 908.9093, test_acc: 0.6198, best: 0.7519, time: 0:02:23
 Epoch: 252, lr: 4.0e-04, train_loss: 0.1565, train_acc: 0.9478 test_loss: 176.1692, test_acc: 0.7059, best: 0.7519, time: 0:02:23
 Epoch: 253, lr: 4.0e-04, train_loss: 0.1713, train_acc: 0.9396 test_loss: 106.8684, test_acc: 0.7216, best: 0.7519, time: 0:02:23
 Epoch: 254, lr: 4.0e-04, train_loss: 0.1554, train_acc: 0.9492 test_loss: 205.5562, test_acc: 0.7063, best: 0.7519, time: 0:02:23
 Epoch: 255, lr: 4.0e-04, train_loss: 0.1564, train_acc: 0.9482 test_loss: 149.4516, test_acc: 0.6970, best: 0.7519, time: 0:02:23
 Epoch: 256, lr: 4.0e-04, train_loss: 0.1659, train_acc: 0.9452 test_loss: 127.9796, test_acc: 0.6863, best: 0.7519, time: 0:02:23
 Epoch: 257, lr: 4.0e-04, train_loss: 0.1772, train_acc: 0.9396 test_loss: 144.5185, test_acc: 0.6813, best: 0.7519, time: 0:02:23
 Epoch: 258, lr: 4.0e-04, train_loss: 0.1519, train_acc: 0.9492 test_loss: 318.5218, test_acc: 0.6558, best: 0.7519, time: 0:02:23
 Epoch: 259, lr: 4.0e-04, train_loss: 0.1572, train_acc: 0.9462 test_loss: 401.3600, test_acc: 0.6726, best: 0.7519, time: 0:02:23
 Epoch: 260, lr: 4.0e-04, train_loss: 0.1736, train_acc: 0.9444 test_loss: 160.9412, test_acc: 0.6831, best: 0.7519, time: 0:02:23
 Epoch: 261, lr: 4.0e-04, train_loss: 0.1624, train_acc: 0.9458 test_loss: 94.4697, test_acc: 0.7000, best: 0.7519, time: 0:02:23
 Epoch: 262, lr: 4.0e-04, train_loss: 0.1636, train_acc: 0.9434 test_loss: 544.9577, test_acc: 0.6759, best: 0.7519, time: 0:02:23
 Epoch: 263, lr: 4.0e-04, train_loss: 0.1598, train_acc: 0.9462 test_loss: 33.9927, test_acc: 0.7501, best: 0.7519, time: 0:02:23
 Epoch: 264, lr: 4.0e-04, train_loss: 0.1677, train_acc: 0.9490 test_loss: 289.9799, test_acc: 0.6614, best: 0.7519, time: 0:02:23
 Epoch: 265, lr: 4.0e-04, train_loss: 0.1722, train_acc: 0.9452 test_loss: 98.5946, test_acc: 0.7075, best: 0.7519, time: 0:02:23
 Epoch: 266, lr: 4.0e-04, train_loss: 0.1832, train_acc: 0.9366 test_loss: 187.2080, test_acc: 0.7019, best: 0.7519, time: 0:02:23
 Epoch: 267, lr: 4.0e-04, train_loss: 0.1506, train_acc: 0.9486 test_loss: 39.0026, test_acc: 0.7350, best: 0.7519, time: 0:02:23
 Epoch: 268, lr: 4.0e-04, train_loss: 0.1712, train_acc: 0.9410 test_loss: 251.3015, test_acc: 0.6669, best: 0.7519, time: 0:02:22
 Epoch: 269, lr: 4.0e-04, train_loss: 0.1555, train_acc: 0.9448 test_loss: 84.4007, test_acc: 0.7065, best: 0.7519, time: 0:02:23
 Epoch: 270, lr: 8.0e-05, train_loss: 0.1574, train_acc: 0.9456 test_loss: 132.4203, test_acc: 0.6907, best: 0.7519, time: 0:02:23
 Epoch: 271, lr: 8.0e-05, train_loss: 0.1591, train_acc: 0.9474 test_loss: 125.1139, test_acc: 0.7127, best: 0.7519, time: 0:02:23
 Epoch: 272, lr: 8.0e-05, train_loss: 0.1621, train_acc: 0.9450 test_loss: 64.4723, test_acc: 0.7235, best: 0.7519, time: 0:02:23
 Epoch: 273, lr: 8.0e-05, train_loss: 0.1591, train_acc: 0.9468 test_loss: 602.4126, test_acc: 0.6419, best: 0.7519, time: 0:02:23
 Epoch: 274, lr: 8.0e-05, train_loss: 0.1614, train_acc: 0.9436 test_loss: 50.4555, test_acc: 0.7356, best: 0.7519, time: 0:02:23
 Epoch: 275, lr: 8.0e-05, train_loss: 0.1520, train_acc: 0.9472 test_loss: 618.1662, test_acc: 0.6329, best: 0.7519, time: 0:02:22
 Epoch: 276, lr: 8.0e-05, train_loss: 0.1634, train_acc: 0.9452 test_loss: 126.1759, test_acc: 0.6959, best: 0.7519, time: 0:02:20
 Epoch: 277, lr: 8.0e-05, train_loss: 0.1606, train_acc: 0.9456 test_loss: 379.8669, test_acc: 0.6464, best: 0.7519, time: 0:02:20
 Epoch: 278, lr: 8.0e-05, train_loss: 0.1644, train_acc: 0.9456 test_loss: 173.0360, test_acc: 0.6977, best: 0.7519, time: 0:02:19
 Epoch: 279, lr: 8.0e-05, train_loss: 0.1688, train_acc: 0.9428 test_loss: 251.3423, test_acc: 0.6883, best: 0.7519, time: 0:02:20
 Epoch: 280, lr: 8.0e-05, train_loss: 0.1746, train_acc: 0.9386 test_loss: 611.9944, test_acc: 0.6198, best: 0.7519, time: 0:02:20
 Epoch: 281, lr: 8.0e-05, train_loss: 0.1576, train_acc: 0.9456 test_loss: 133.4871, test_acc: 0.7045, best: 0.7519, time: 0:02:19
 Epoch: 282, lr: 8.0e-05, train_loss: 0.1637, train_acc: 0.9444 test_loss: 33.9904, test_acc: 0.7405, best: 0.7519, time: 0:02:19
 Epoch: 283, lr: 8.0e-05, train_loss: 0.1645, train_acc: 0.9474 test_loss: 134.0380, test_acc: 0.7135, best: 0.7519, time: 0:02:15
 Epoch: 284, lr: 8.0e-05, train_loss: 0.1562, train_acc: 0.9442 test_loss: 79.6707, test_acc: 0.7130, best: 0.7519, time: 0:02:11
 Epoch: 285, lr: 8.0e-05, train_loss: 0.1671, train_acc: 0.9430 test_loss: 77.6567, test_acc: 0.7167, best: 0.7519, time: 0:02:12
 Epoch: 286, lr: 8.0e-05, train_loss: 0.1490, train_acc: 0.9488 test_loss: 43.8497, test_acc: 0.7400, best: 0.7519, time: 0:02:11
 Epoch: 287, lr: 8.0e-05, train_loss: 0.1613, train_acc: 0.9464 test_loss: 250.9151, test_acc: 0.6699, best: 0.7519, time: 0:02:12
 Epoch: 288, lr: 8.0e-05, train_loss: 0.1542, train_acc: 0.9498 test_loss: 72.6163, test_acc: 0.7236, best: 0.7519, time: 0:02:12
 Epoch: 289, lr: 8.0e-05, train_loss: 0.1737, train_acc: 0.9418 test_loss: 162.2967, test_acc: 0.6829, best: 0.7519, time: 0:02:12
 Epoch: 290, lr: 8.0e-05, train_loss: 0.1567, train_acc: 0.9444 test_loss: 35.9261, test_acc: 0.7416, best: 0.7519, time: 0:02:12
 Epoch: 291, lr: 8.0e-05, train_loss: 0.1564, train_acc: 0.9472 test_loss: 236.4167, test_acc: 0.6689, best: 0.7519, time: 0:02:11
 Epoch: 292, lr: 8.0e-05, train_loss: 0.1638, train_acc: 0.9470 test_loss: 72.4967, test_acc: 0.7296, best: 0.7519, time: 0:02:11
 Epoch: 293, lr: 8.0e-05, train_loss: 0.1466, train_acc: 0.9504 test_loss: 89.0025, test_acc: 0.7232, best: 0.7519, time: 0:02:11
 Epoch: 294, lr: 8.0e-05, train_loss: 0.1566, train_acc: 0.9508 test_loss: 476.0554, test_acc: 0.6585, best: 0.7519, time: 0:02:11
 Epoch: 295, lr: 8.0e-05, train_loss: 0.1622, train_acc: 0.9442 test_loss: 442.8409, test_acc: 0.6615, best: 0.7519, time: 0:02:11
 Epoch: 296, lr: 8.0e-05, train_loss: 0.1637, train_acc: 0.9430 test_loss: 49.7730, test_acc: 0.7146, best: 0.7519, time: 0:02:11
 Epoch: 297, lr: 8.0e-05, train_loss: 0.1571, train_acc: 0.9468 test_loss: 206.7324, test_acc: 0.7075, best: 0.7519, time: 0:02:12
 Epoch: 298, lr: 8.0e-05, train_loss: 0.1598, train_acc: 0.9478 test_loss: 571.4491, test_acc: 0.6461, best: 0.7519, time: 0:02:11
 Epoch: 299, lr: 8.0e-05, train_loss: 0.1689, train_acc: 0.9440 test_loss: 37.0593, test_acc: 0.7428, best: 0.7519, time: 0:02:11
 Highest accuracy: 0.7519