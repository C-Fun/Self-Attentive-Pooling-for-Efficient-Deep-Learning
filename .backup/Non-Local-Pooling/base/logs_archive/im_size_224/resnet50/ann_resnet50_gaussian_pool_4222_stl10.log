
 Run on time: 2022-06-24 19:59:39.657570

 Arguments:
	 gpu                  : True
	 log                  : True
	 seed                 : 0
	 momentum             : 0.9
	 amsgrad              : True
	 dataset              : STL10
	 batch_size           : 8
	 architecture         : RESNET50_gaussian_pool_4222
	 im_size              : 224
	 relu_threshold       : 4.0
	 learning_rate        : 0.01
	 pretrained_backbone  : 
	 pretrained_ann       : 
	 weight_decay         : 0.0
	 test_only            : False
	 epochs               : 300
	 lr_interval          : [180, 240, 270]
	 lr_reduce            : 5
	 optimizer            : SGD
	 dropout              : 0.2
	 kernel_size          : 3
	 dont_save            : False
	 visualize            : False
	 devices              : 0,1
 DataParallel(
  (module): NetworkByName(
    (net): ResNet_v2(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (pooling): GaussianPooling2d(
            kernel_size=4, stride=4, padding=0
            (ToHidden): Sequential(
              (0): AdaptiveAvgPool2d(output_size=(1, 1))
              (1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
              (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (3): ReLU()
            )
            (ToMean): Sequential(
              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (ToSigma): Sequential(
              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Sigmoid()
            )
            (activation): Softplus(beta=1, threshold=20)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (pooling): GaussianPooling2d(
            kernel_size=2, stride=2, padding=0
            (ToHidden): Sequential(
              (0): AdaptiveAvgPool2d(output_size=(1, 1))
              (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
              (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (3): ReLU()
            )
            (ToMean): Sequential(
              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (ToSigma): Sequential(
              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Sigmoid()
            )
            (activation): Softplus(beta=1, threshold=20)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (pooling): GaussianPooling2d(
            kernel_size=2, stride=2, padding=0
            (ToHidden): Sequential(
              (0): AdaptiveAvgPool2d(output_size=(1, 1))
              (1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))
              (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (3): ReLU()
            )
            (ToMean): Sequential(
              (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (ToSigma): Sequential(
              (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Sigmoid()
            )
            (activation): Softplus(beta=1, threshold=20)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer4): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (pooling): GaussianPooling2d(
            kernel_size=2, stride=2, padding=0
            (ToHidden): Sequential(
              (0): AdaptiveAvgPool2d(output_size=(1, 1))
              (1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1))
              (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (3): ReLU()
            )
            (ToMean): Sequential(
              (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (ToSigma): Sequential(
              (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Sigmoid()
            )
            (activation): Softplus(beta=1, threshold=20)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
      (fc): Linear(in_features=2048, out_features=10, bias=True)
    )
  )
)
 SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0
)
 Epoch: 1, lr: 1.0e-02, train_loss: 12.0649, train_acc: 0.1144 test_loss: 22.1789, test_acc: 0.1391, best: 0.1391, time: 0:02:19
 Epoch: 2, lr: 1.0e-02, train_loss: 3.7718, train_acc: 0.1118 test_loss: 17.1347, test_acc: 0.1375, best: 0.1391, time: 0:02:16
 Epoch: 3, lr: 1.0e-02, train_loss: 2.6802, train_acc: 0.1276 test_loss: 3.3707, test_acc: 0.1314, best: 0.1391, time: 0:02:16
 Epoch: 4, lr: 1.0e-02, train_loss: 2.3797, train_acc: 0.1468 test_loss: 17.3815, test_acc: 0.1184, best: 0.1391, time: 0:02:16
 Epoch: 5, lr: 1.0e-02, train_loss: 2.2702, train_acc: 0.1682 test_loss: 8.3482, test_acc: 0.1441, best: 0.1441, time: 0:02:17
 Epoch: 6, lr: 1.0e-02, train_loss: 2.2291, train_acc: 0.1784 test_loss: 4.1697, test_acc: 0.1960, best: 0.1960, time: 0:02:17
 Epoch: 7, lr: 1.0e-02, train_loss: 2.2107, train_acc: 0.1796 test_loss: 2.5813, test_acc: 0.2099, best: 0.2099, time: 0:02:17
 Epoch: 8, lr: 1.0e-02, train_loss: 2.2001, train_acc: 0.1826 test_loss: 2.2638, test_acc: 0.2001, best: 0.2099, time: 0:02:15
 Epoch: 9, lr: 1.0e-02, train_loss: 2.1729, train_acc: 0.1886 test_loss: 2.1931, test_acc: 0.2354, best: 0.2354, time: 0:02:16
 Epoch: 10, lr: 1.0e-02, train_loss: 2.2119, train_acc: 0.1784 test_loss: 2.4892, test_acc: 0.1969, best: 0.2354, time: 0:02:16
 Epoch: 11, lr: 1.0e-02, train_loss: 2.2284, train_acc: 0.1702 test_loss: 2.4153, test_acc: 0.2135, best: 0.2354, time: 0:02:15
 Epoch: 12, lr: 1.0e-02, train_loss: 2.1987, train_acc: 0.1802 test_loss: 9.5411, test_acc: 0.1989, best: 0.2354, time: 0:02:15
 Epoch: 13, lr: 1.0e-02, train_loss: 2.1802, train_acc: 0.1846 test_loss: 2.6384, test_acc: 0.2333, best: 0.2354, time: 0:02:15
 Epoch: 14, lr: 1.0e-02, train_loss: 2.1421, train_acc: 0.1986 test_loss: 2.1916, test_acc: 0.2502, best: 0.2502, time: 0:02:16
 Epoch: 15, lr: 1.0e-02, train_loss: 2.1555, train_acc: 0.1926 test_loss: 6.0087, test_acc: 0.2067, best: 0.2502, time: 0:02:14
 Epoch: 16, lr: 1.0e-02, train_loss: 2.1272, train_acc: 0.2060 test_loss: 3.6808, test_acc: 0.2278, best: 0.2502, time: 0:02:14
 Epoch: 17, lr: 1.0e-02, train_loss: 2.1205, train_acc: 0.2034 test_loss: 3.4980, test_acc: 0.2355, best: 0.2502, time: 0:02:14
 Epoch: 18, lr: 1.0e-02, train_loss: 2.0882, train_acc: 0.2166 test_loss: 2.8245, test_acc: 0.2516, best: 0.2516, time: 0:02:15
 Epoch: 19, lr: 1.0e-02, train_loss: 2.1039, train_acc: 0.2152 test_loss: 2.1723, test_acc: 0.2298, best: 0.2516, time: 0:02:14
 Epoch: 20, lr: 1.0e-02, train_loss: 2.0689, train_acc: 0.2250 test_loss: 7.9844, test_acc: 0.2290, best: 0.2516, time: 0:02:14
 Epoch: 21, lr: 1.0e-02, train_loss: 2.0709, train_acc: 0.2226 test_loss: 5.1979, test_acc: 0.2421, best: 0.2516, time: 0:02:14
 Epoch: 22, lr: 1.0e-02, train_loss: 2.0554, train_acc: 0.2240 test_loss: 2.1730, test_acc: 0.2644, best: 0.2644, time: 0:02:15
 Epoch: 23, lr: 1.0e-02, train_loss: 2.0462, train_acc: 0.2266 test_loss: 4.8083, test_acc: 0.2467, best: 0.2644, time: 0:02:14
 Epoch: 24, lr: 1.0e-02, train_loss: 2.0274, train_acc: 0.2388 test_loss: 3.9944, test_acc: 0.2690, best: 0.2690, time: 0:02:15
 Epoch: 25, lr: 1.0e-02, train_loss: 2.0219, train_acc: 0.2376 test_loss: 5.1096, test_acc: 0.2694, best: 0.2694, time: 0:02:14
 Epoch: 26, lr: 1.0e-02, train_loss: 2.0067, train_acc: 0.2374 test_loss: 4.5975, test_acc: 0.2582, best: 0.2694, time: 0:02:14
 Epoch: 27, lr: 1.0e-02, train_loss: 1.9926, train_acc: 0.2430 test_loss: 4.2457, test_acc: 0.3045, best: 0.3045, time: 0:02:14
 Epoch: 28, lr: 1.0e-02, train_loss: 1.9870, train_acc: 0.2392 test_loss: 7.4077, test_acc: 0.2680, best: 0.3045, time: 0:02:14
 Epoch: 29, lr: 1.0e-02, train_loss: 1.9985, train_acc: 0.2346 test_loss: 5.6035, test_acc: 0.2672, best: 0.3045, time: 0:02:14
 Epoch: 30, lr: 1.0e-02, train_loss: 2.0070, train_acc: 0.2380 test_loss: 11.3068, test_acc: 0.2744, best: 0.3045, time: 0:02:14
 Epoch: 31, lr: 1.0e-02, train_loss: 1.9835, train_acc: 0.2412 test_loss: 72.2718, test_acc: 0.2587, best: 0.3045, time: 0:02:13
 Epoch: 32, lr: 1.0e-02, train_loss: 1.9592, train_acc: 0.2618 test_loss: 13.1681, test_acc: 0.3139, best: 0.3139, time: 0:02:14
 Epoch: 33, lr: 1.0e-02, train_loss: 1.9351, train_acc: 0.2532 test_loss: 13.6941, test_acc: 0.3074, best: 0.3139, time: 0:02:14
 Epoch: 34, lr: 1.0e-02, train_loss: 1.9420, train_acc: 0.2542 test_loss: 5.2661, test_acc: 0.2929, best: 0.3139, time: 0:02:14
 Epoch: 35, lr: 1.0e-02, train_loss: 1.9449, train_acc: 0.2592 test_loss: 4.4291, test_acc: 0.3217, best: 0.3217, time: 0:02:14
 Epoch: 36, lr: 1.0e-02, train_loss: 1.9325, train_acc: 0.2604 test_loss: 3.1758, test_acc: 0.3060, best: 0.3217, time: 0:02:14
 Epoch: 37, lr: 1.0e-02, train_loss: 1.9204, train_acc: 0.2674 test_loss: 19.9865, test_acc: 0.2869, best: 0.3217, time: 0:02:13
 Epoch: 38, lr: 1.0e-02, train_loss: 1.8859, train_acc: 0.2736 test_loss: 6.1214, test_acc: 0.3289, best: 0.3289, time: 0:02:14
 Epoch: 39, lr: 1.0e-02, train_loss: 1.8903, train_acc: 0.2650 test_loss: 5.4383, test_acc: 0.2869, best: 0.3289, time: 0:02:14
 Epoch: 40, lr: 1.0e-02, train_loss: 1.8576, train_acc: 0.2800 test_loss: 12.2903, test_acc: 0.2552, best: 0.3289, time: 0:02:13
 Epoch: 41, lr: 1.0e-02, train_loss: 1.8709, train_acc: 0.2740 test_loss: 6.9013, test_acc: 0.2799, best: 0.3289, time: 0:02:14
 Epoch: 42, lr: 1.0e-02, train_loss: 1.8637, train_acc: 0.2826 test_loss: 6.8071, test_acc: 0.2604, best: 0.3289, time: 0:02:14
 Epoch: 43, lr: 1.0e-02, train_loss: 1.8860, train_acc: 0.2766 test_loss: 2.7741, test_acc: 0.2866, best: 0.3289, time: 0:02:14
 Epoch: 44, lr: 1.0e-02, train_loss: 1.9002, train_acc: 0.2652 test_loss: 2.3638, test_acc: 0.3106, best: 0.3289, time: 0:02:13
 Epoch: 45, lr: 1.0e-02, train_loss: 1.8947, train_acc: 0.2746 test_loss: 1.8260, test_acc: 0.3074, best: 0.3289, time: 0:02:14
 Epoch: 46, lr: 1.0e-02, train_loss: 1.9216, train_acc: 0.2612 test_loss: 1.9917, test_acc: 0.3048, best: 0.3289, time: 0:02:14
 Epoch: 47, lr: 1.0e-02, train_loss: 1.8780, train_acc: 0.2774 test_loss: 1.8679, test_acc: 0.3155, best: 0.3289, time: 0:02:14
 Epoch: 48, lr: 1.0e-02, train_loss: 1.9117, train_acc: 0.2720 test_loss: 2.3181, test_acc: 0.2960, best: 0.3289, time: 0:02:13
 Epoch: 49, lr: 1.0e-02, train_loss: 1.8772, train_acc: 0.2742 test_loss: 2.7197, test_acc: 0.3145, best: 0.3289, time: 0:02:14
 Epoch: 50, lr: 1.0e-02, train_loss: 1.8865, train_acc: 0.2830 test_loss: 3.0295, test_acc: 0.2954, best: 0.3289, time: 0:02:14
 Epoch: 51, lr: 1.0e-02, train_loss: 1.8543, train_acc: 0.2840 test_loss: 3.0755, test_acc: 0.2891, best: 0.3289, time: 0:02:14
 Epoch: 52, lr: 1.0e-02, train_loss: 1.8829, train_acc: 0.2822 test_loss: 3.5969, test_acc: 0.3004, best: 0.3289, time: 0:02:14
 Epoch: 53, lr: 1.0e-02, train_loss: 1.9382, train_acc: 0.2610 test_loss: 2.4468, test_acc: 0.2899, best: 0.3289, time: 0:02:13
 Epoch: 54, lr: 1.0e-02, train_loss: 1.9263, train_acc: 0.2554 test_loss: 1.9089, test_acc: 0.3458, best: 0.3458, time: 0:02:15
 Epoch: 55, lr: 1.0e-02, train_loss: 1.8915, train_acc: 0.2736 test_loss: 2.4641, test_acc: 0.3152, best: 0.3458, time: 0:02:14
 Epoch: 56, lr: 1.0e-02, train_loss: 1.8393, train_acc: 0.2904 test_loss: 2.6339, test_acc: 0.3247, best: 0.3458, time: 0:02:14
 Epoch: 57, lr: 1.0e-02, train_loss: 1.8294, train_acc: 0.2904 test_loss: 2.1603, test_acc: 0.3404, best: 0.3458, time: 0:02:14
 Epoch: 58, lr: 1.0e-02, train_loss: 1.8132, train_acc: 0.2984 test_loss: 5.4413, test_acc: 0.3209, best: 0.3458, time: 0:02:14
 Epoch: 59, lr: 1.0e-02, train_loss: 1.8054, train_acc: 0.2984 test_loss: 8.0423, test_acc: 0.3182, best: 0.3458, time: 0:02:14
 Epoch: 60, lr: 1.0e-02, train_loss: 1.7746, train_acc: 0.3136 test_loss: 7.0622, test_acc: 0.2950, best: 0.3458, time: 0:02:13
 Epoch: 61, lr: 1.0e-02, train_loss: 1.7833, train_acc: 0.3108 test_loss: 3.1043, test_acc: 0.3481, best: 0.3481, time: 0:02:14
 Epoch: 62, lr: 1.0e-02, train_loss: 1.7813, train_acc: 0.3192 test_loss: 2.8007, test_acc: 0.3564, best: 0.3564, time: 0:02:14
 Epoch: 63, lr: 1.0e-02, train_loss: 1.7471, train_acc: 0.3274 test_loss: 12.6701, test_acc: 0.3681, best: 0.3681, time: 0:02:14
 Epoch: 64, lr: 1.0e-02, train_loss: 1.7693, train_acc: 0.3108 test_loss: 2.9937, test_acc: 0.3781, best: 0.3781, time: 0:02:14
 Epoch: 65, lr: 1.0e-02, train_loss: 1.8254, train_acc: 0.2938 test_loss: 8.8330, test_acc: 0.2856, best: 0.3781, time: 0:02:13
 Epoch: 66, lr: 1.0e-02, train_loss: 1.7998, train_acc: 0.3126 test_loss: 2.6610, test_acc: 0.3695, best: 0.3781, time: 0:02:14
 Epoch: 67, lr: 1.0e-02, train_loss: 1.7757, train_acc: 0.3206 test_loss: 2.7544, test_acc: 0.3509, best: 0.3781, time: 0:02:14
 Epoch: 68, lr: 1.0e-02, train_loss: 1.7788, train_acc: 0.3226 test_loss: 4.7781, test_acc: 0.3695, best: 0.3781, time: 0:02:14
 Epoch: 69, lr: 1.0e-02, train_loss: 1.7496, train_acc: 0.3276 test_loss: 1.8852, test_acc: 0.3606, best: 0.3781, time: 0:02:13
 Epoch: 70, lr: 1.0e-02, train_loss: 1.7270, train_acc: 0.3280 test_loss: 10.1122, test_acc: 0.3536, best: 0.3781, time: 0:02:14
 Epoch: 71, lr: 1.0e-02, train_loss: 1.7429, train_acc: 0.3348 test_loss: 9.9216, test_acc: 0.3681, best: 0.3781, time: 0:02:13
 Epoch: 72, lr: 1.0e-02, train_loss: 1.7616, train_acc: 0.3240 test_loss: 3.4712, test_acc: 0.3956, best: 0.3956, time: 0:02:14
 Epoch: 73, lr: 1.0e-02, train_loss: 1.7498, train_acc: 0.3336 test_loss: 3.7688, test_acc: 0.4066, best: 0.4066, time: 0:02:14
 Epoch: 74, lr: 1.0e-02, train_loss: 1.7342, train_acc: 0.3364 test_loss: 2.2906, test_acc: 0.4054, best: 0.4066, time: 0:02:14
 Epoch: 75, lr: 1.0e-02, train_loss: 1.7394, train_acc: 0.3282 test_loss: 3.5750, test_acc: 0.4041, best: 0.4066, time: 0:02:13
 Epoch: 76, lr: 1.0e-02, train_loss: 1.7314, train_acc: 0.3354 test_loss: 2.6585, test_acc: 0.3900, best: 0.4066, time: 0:02:14
 Epoch: 77, lr: 1.0e-02, train_loss: 1.6994, train_acc: 0.3538 test_loss: 3.5196, test_acc: 0.4050, best: 0.4066, time: 0:02:13
 Epoch: 78, lr: 1.0e-02, train_loss: 1.7163, train_acc: 0.3422 test_loss: 26.9722, test_acc: 0.3556, best: 0.4066, time: 0:02:13
 Epoch: 79, lr: 1.0e-02, train_loss: 1.7231, train_acc: 0.3386 test_loss: 27.0224, test_acc: 0.3643, best: 0.4066, time: 0:02:14
 Epoch: 80, lr: 1.0e-02, train_loss: 1.7155, train_acc: 0.3456 test_loss: 9.8774, test_acc: 0.3551, best: 0.4066, time: 0:02:13
 Epoch: 81, lr: 1.0e-02, train_loss: 1.7195, train_acc: 0.3454 test_loss: 4.6344, test_acc: 0.3704, best: 0.4066, time: 0:02:13
 Epoch: 82, lr: 1.0e-02, train_loss: 1.7083, train_acc: 0.3486 test_loss: 2.4311, test_acc: 0.3771, best: 0.4066, time: 0:02:14
 Epoch: 83, lr: 1.0e-02, train_loss: 1.7066, train_acc: 0.3538 test_loss: 10.7797, test_acc: 0.3589, best: 0.4066, time: 0:02:14
 Epoch: 84, lr: 1.0e-02, train_loss: 1.7345, train_acc: 0.3364 test_loss: 3.7425, test_acc: 0.3640, best: 0.4066, time: 0:02:14
 Epoch: 85, lr: 1.0e-02, train_loss: 1.6941, train_acc: 0.3552 test_loss: 2.6015, test_acc: 0.3937, best: 0.4066, time: 0:02:14
 Epoch: 86, lr: 1.0e-02, train_loss: 1.7165, train_acc: 0.3446 test_loss: 1.9925, test_acc: 0.4326, best: 0.4326, time: 0:02:14
 Epoch: 87, lr: 1.0e-02, train_loss: 1.6889, train_acc: 0.3596 test_loss: 1.6319, test_acc: 0.4269, best: 0.4326, time: 0:02:13
 Epoch: 88, lr: 1.0e-02, train_loss: 1.6766, train_acc: 0.3518 test_loss: 5.4012, test_acc: 0.3990, best: 0.4326, time: 0:02:14
 Epoch: 89, lr: 1.0e-02, train_loss: 1.6875, train_acc: 0.3598 test_loss: 3.3964, test_acc: 0.4269, best: 0.4326, time: 0:02:14
 Epoch: 90, lr: 1.0e-02, train_loss: 1.6689, train_acc: 0.3570 test_loss: 2.5498, test_acc: 0.4185, best: 0.4326, time: 0:02:14
 Epoch: 91, lr: 1.0e-02, train_loss: 1.6501, train_acc: 0.3684 test_loss: 6.4229, test_acc: 0.4276, best: 0.4326, time: 0:02:13
 Epoch: 92, lr: 1.0e-02, train_loss: 1.6661, train_acc: 0.3654 test_loss: 2.8339, test_acc: 0.4199, best: 0.4326, time: 0:02:14
 Epoch: 93, lr: 1.0e-02, train_loss: 1.6366, train_acc: 0.3722 test_loss: 6.5875, test_acc: 0.4213, best: 0.4326, time: 0:02:14
 Epoch: 94, lr: 1.0e-02, train_loss: 1.6744, train_acc: 0.3624 test_loss: 41.3633, test_acc: 0.3726, best: 0.4326, time: 0:02:14
 Epoch: 95, lr: 1.0e-02, train_loss: 1.6673, train_acc: 0.3628 test_loss: 2.1662, test_acc: 0.4449, best: 0.4449, time: 0:02:14
 Epoch: 96, lr: 1.0e-02, train_loss: 1.6436, train_acc: 0.3714 test_loss: 4.8688, test_acc: 0.4040, best: 0.4449, time: 0:02:14
 Epoch: 97, lr: 1.0e-02, train_loss: 1.6427, train_acc: 0.3828 test_loss: 3.6978, test_acc: 0.4174, best: 0.4449, time: 0:02:14
 Epoch: 98, lr: 1.0e-02, train_loss: 1.6398, train_acc: 0.3800 test_loss: 3.8896, test_acc: 0.4089, best: 0.4449, time: 0:02:14
 Epoch: 99, lr: 1.0e-02, train_loss: 1.6556, train_acc: 0.3720 test_loss: 19.3256, test_acc: 0.3942, best: 0.4449, time: 0:02:14
 Epoch: 100, lr: 1.0e-02, train_loss: 1.6280, train_acc: 0.3780 test_loss: 2.0492, test_acc: 0.4406, best: 0.4449, time: 0:02:14
 Epoch: 101, lr: 1.0e-02, train_loss: 1.6472, train_acc: 0.3860 test_loss: 2.1669, test_acc: 0.4414, best: 0.4449, time: 0:02:14
 Epoch: 102, lr: 1.0e-02, train_loss: 1.6325, train_acc: 0.3820 test_loss: 2.8293, test_acc: 0.4499, best: 0.4499, time: 0:02:14
 Epoch: 103, lr: 1.0e-02, train_loss: 1.6176, train_acc: 0.3752 test_loss: 2.1131, test_acc: 0.4471, best: 0.4499, time: 0:02:14
 Epoch: 104, lr: 1.0e-02, train_loss: 1.6114, train_acc: 0.3852 test_loss: 2.5028, test_acc: 0.4550, best: 0.4550, time: 0:02:14
 Epoch: 105, lr: 1.0e-02, train_loss: 1.7930, train_acc: 0.3222 test_loss: 2.4716, test_acc: 0.3882, best: 0.4550, time: 0:02:14
 Epoch: 106, lr: 1.0e-02, train_loss: 1.7159, train_acc: 0.3458 test_loss: 3.8166, test_acc: 0.4256, best: 0.4550, time: 0:02:14
 Epoch: 107, lr: 1.0e-02, train_loss: 1.6882, train_acc: 0.3668 test_loss: 5.5833, test_acc: 0.4274, best: 0.4550, time: 0:02:14
 Epoch: 108, lr: 1.0e-02, train_loss: 1.6720, train_acc: 0.3750 test_loss: 16.1323, test_acc: 0.4115, best: 0.4550, time: 0:02:14
 Epoch: 109, lr: 1.0e-02, train_loss: 1.7475, train_acc: 0.3360 test_loss: 2.1612, test_acc: 0.3951, best: 0.4550, time: 0:02:14
 Epoch: 110, lr: 1.0e-02, train_loss: 1.7324, train_acc: 0.3444 test_loss: 18.9423, test_acc: 0.2371, best: 0.4550, time: 0:02:14
 Epoch: 111, lr: 1.0e-02, train_loss: 1.9552, train_acc: 0.2682 test_loss: 44.5189, test_acc: 0.3779, best: 0.4550, time: 0:02:14
 Epoch: 112, lr: 1.0e-02, train_loss: 1.8481, train_acc: 0.3048 test_loss: 19.2659, test_acc: 0.4069, best: 0.4550, time: 0:02:14
 Epoch: 113, lr: 1.0e-02, train_loss: 1.7637, train_acc: 0.3316 test_loss: 43.8906, test_acc: 0.3598, best: 0.4550, time: 0:02:14
 Epoch: 114, lr: 1.0e-02, train_loss: 1.7381, train_acc: 0.3420 test_loss: 22.1347, test_acc: 0.3817, best: 0.4550, time: 0:02:14
 Epoch: 115, lr: 1.0e-02, train_loss: 1.7150, train_acc: 0.3462 test_loss: 18.0206, test_acc: 0.4045, best: 0.4550, time: 0:02:14
 Epoch: 116, lr: 1.0e-02, train_loss: 1.7182, train_acc: 0.3548 test_loss: 8.1350, test_acc: 0.4037, best: 0.4550, time: 0:02:14
 Epoch: 117, lr: 1.0e-02, train_loss: 1.7041, train_acc: 0.3520 test_loss: 7.1673, test_acc: 0.4076, best: 0.4550, time: 0:02:14
 Epoch: 118, lr: 1.0e-02, train_loss: 1.6911, train_acc: 0.3664 test_loss: 25.0953, test_acc: 0.3992, best: 0.4550, time: 0:02:14
 Epoch: 119, lr: 1.0e-02, train_loss: 1.6749, train_acc: 0.3664 test_loss: 47.0755, test_acc: 0.4047, best: 0.4550, time: 0:02:14
 Epoch: 120, lr: 1.0e-02, train_loss: 1.6517, train_acc: 0.3676 test_loss: 29.1351, test_acc: 0.3752, best: 0.4550, time: 0:02:14
 Epoch: 121, lr: 1.0e-02, train_loss: 1.6566, train_acc: 0.3672 test_loss: 10.7822, test_acc: 0.4119, best: 0.4550, time: 0:02:14
 Epoch: 122, lr: 1.0e-02, train_loss: 1.6624, train_acc: 0.3694 test_loss: 3.3087, test_acc: 0.4200, best: 0.4550, time: 0:02:14
 Epoch: 123, lr: 1.0e-02, train_loss: 1.6584, train_acc: 0.3722 test_loss: 14.8418, test_acc: 0.3885, best: 0.4550, time: 0:02:14
 Epoch: 124, lr: 1.0e-02, train_loss: 1.6482, train_acc: 0.3836 test_loss: 26.9196, test_acc: 0.4004, best: 0.4550, time: 0:02:14
 Epoch: 125, lr: 1.0e-02, train_loss: 1.6274, train_acc: 0.3892 test_loss: 259.1398, test_acc: 0.3789, best: 0.4550, time: 0:02:14
 Epoch: 126, lr: 1.0e-02, train_loss: 1.6223, train_acc: 0.3860 test_loss: 6.9031, test_acc: 0.4501, best: 0.4550, time: 0:02:14
 Epoch: 127, lr: 1.0e-02, train_loss: 1.6062, train_acc: 0.3960 test_loss: 37.6304, test_acc: 0.4271, best: 0.4550, time: 0:02:14
 Epoch: 128, lr: 1.0e-02, train_loss: 1.6033, train_acc: 0.3962 test_loss: 97.8672, test_acc: 0.4109, best: 0.4550, time: 0:02:14
 Epoch: 129, lr: 1.0e-02, train_loss: 1.6314, train_acc: 0.3860 test_loss: 9.0791, test_acc: 0.4205, best: 0.4550, time: 0:02:14
 Epoch: 130, lr: 1.0e-02, train_loss: 1.7185, train_acc: 0.3472 test_loss: 72.2545, test_acc: 0.4032, best: 0.4550, time: 0:02:14
 Epoch: 131, lr: 1.0e-02, train_loss: 1.6932, train_acc: 0.3616 test_loss: 16.6260, test_acc: 0.4206, best: 0.4550, time: 0:02:14
 Epoch: 132, lr: 1.0e-02, train_loss: 1.7199, train_acc: 0.3498 test_loss: 11.2778, test_acc: 0.4123, best: 0.4550, time: 0:02:14
 Epoch: 133, lr: 1.0e-02, train_loss: 1.7279, train_acc: 0.3516 test_loss: 3.2026, test_acc: 0.3982, best: 0.4550, time: 0:02:14
 Epoch: 134, lr: 1.0e-02, train_loss: 1.7594, train_acc: 0.3390 test_loss: 4.5631, test_acc: 0.4146, best: 0.4550, time: 0:02:14
 Epoch: 135, lr: 1.0e-02, train_loss: 1.7495, train_acc: 0.3294 test_loss: 3.0525, test_acc: 0.4161, best: 0.4550, time: 0:02:14
 Epoch: 136, lr: 1.0e-02, train_loss: 1.7548, train_acc: 0.3348 test_loss: 28.6892, test_acc: 0.3952, best: 0.4550, time: 0:02:14
 Epoch: 137, lr: 1.0e-02, train_loss: 1.7288, train_acc: 0.3418 test_loss: 17.6859, test_acc: 0.4143, best: 0.4550, time: 0:02:14
 Epoch: 138, lr: 1.0e-02, train_loss: 1.7070, train_acc: 0.3484 test_loss: 54.5244, test_acc: 0.3949, best: 0.4550, time: 0:02:14
 Epoch: 139, lr: 1.0e-02, train_loss: 1.6985, train_acc: 0.3554 test_loss: 5.1309, test_acc: 0.4300, best: 0.4550, time: 0:02:14
 Epoch: 140, lr: 1.0e-02, train_loss: 1.6578, train_acc: 0.3726 test_loss: 9.1841, test_acc: 0.4475, best: 0.4550, time: 0:02:14
 Epoch: 141, lr: 1.0e-02, train_loss: 1.6660, train_acc: 0.3748 test_loss: 28.5151, test_acc: 0.4088, best: 0.4550, time: 0:02:14
 Epoch: 142, lr: 1.0e-02, train_loss: 1.6648, train_acc: 0.3676 test_loss: 26.9743, test_acc: 0.4085, best: 0.4550, time: 0:02:14
 Epoch: 143, lr: 1.0e-02, train_loss: 1.7067, train_acc: 0.3586 test_loss: 24.8885, test_acc: 0.3429, best: 0.4550, time: 0:02:14
 Epoch: 144, lr: 1.0e-02, train_loss: 1.8752, train_acc: 0.3020 test_loss: 13.4604, test_acc: 0.3932, best: 0.4550, time: 0:02:14
 Epoch: 145, lr: 1.0e-02, train_loss: 1.8090, train_acc: 0.3296 test_loss: 16.1325, test_acc: 0.3959, best: 0.4550, time: 0:02:14
 Epoch: 146, lr: 1.0e-02, train_loss: 1.7558, train_acc: 0.3388 test_loss: 4.0040, test_acc: 0.4345, best: 0.4550, time: 0:02:14
 Epoch: 147, lr: 1.0e-02, train_loss: 1.7215, train_acc: 0.3540 test_loss: 8.7112, test_acc: 0.4251, best: 0.4550, time: 0:02:14
 Epoch: 148, lr: 1.0e-02, train_loss: 1.7062, train_acc: 0.3584 test_loss: 4.6274, test_acc: 0.4250, best: 0.4550, time: 0:02:14
 Epoch: 149, lr: 1.0e-02, train_loss: 1.7211, train_acc: 0.3508 test_loss: 48.8938, test_acc: 0.3917, best: 0.4550, time: 0:02:14
 Epoch: 150, lr: 1.0e-02, train_loss: 1.7935, train_acc: 0.3264 test_loss: 1.5801, test_acc: 0.4200, best: 0.4550, time: 0:02:14
 Epoch: 151, lr: 1.0e-02, train_loss: 1.7805, train_acc: 0.3318 test_loss: 1.8194, test_acc: 0.4156, best: 0.4550, time: 0:02:14
 Epoch: 152, lr: 1.0e-02, train_loss: 1.7675, train_acc: 0.3338 test_loss: 3.6980, test_acc: 0.4140, best: 0.4550, time: 0:02:14
 Epoch: 153, lr: 1.0e-02, train_loss: 1.7921, train_acc: 0.3294 test_loss: 2.1512, test_acc: 0.4135, best: 0.4550, time: 0:02:14
 Epoch: 154, lr: 1.0e-02, train_loss: 1.7688, train_acc: 0.3374 test_loss: 11.7576, test_acc: 0.4078, best: 0.4550, time: 0:02:14
 Epoch: 155, lr: 1.0e-02, train_loss: 1.7249, train_acc: 0.3556 test_loss: 7.5489, test_acc: 0.4256, best: 0.4550, time: 0:02:14
 Epoch: 156, lr: 1.0e-02, train_loss: 1.7209, train_acc: 0.3582 test_loss: 20.8380, test_acc: 0.4166, best: 0.4550, time: 0:02:14
 Epoch: 157, lr: 1.0e-02, train_loss: 1.7096, train_acc: 0.3636 test_loss: 17.4095, test_acc: 0.4466, best: 0.4550, time: 0:02:14
 Epoch: 158, lr: 1.0e-02, train_loss: 1.6797, train_acc: 0.3690 test_loss: 6.2524, test_acc: 0.4474, best: 0.4550, time: 0:02:14
 Epoch: 159, lr: 1.0e-02, train_loss: 1.6652, train_acc: 0.3720 test_loss: 21.2014, test_acc: 0.4482, best: 0.4550, time: 0:02:15
 Epoch: 160, lr: 1.0e-02, train_loss: 1.6599, train_acc: 0.3710 test_loss: 5.2135, test_acc: 0.4559, best: 0.4559, time: 0:02:15
 Epoch: 161, lr: 1.0e-02, train_loss: 1.6514, train_acc: 0.3808 test_loss: 3.8126, test_acc: 0.4689, best: 0.4689, time: 0:02:43
 Epoch: 162, lr: 1.0e-02, train_loss: 1.6447, train_acc: 0.3846 test_loss: 8.6839, test_acc: 0.4652, best: 0.4689, time: 0:02:14
 Epoch: 163, lr: 1.0e-02, train_loss: 1.6178, train_acc: 0.3864 test_loss: 40.4597, test_acc: 0.4461, best: 0.4689, time: 0:02:14
 Epoch: 164, lr: 1.0e-02, train_loss: 1.6138, train_acc: 0.4038 test_loss: 1.6692, test_acc: 0.4640, best: 0.4689, time: 0:02:14
 Epoch: 165, lr: 1.0e-02, train_loss: 1.6277, train_acc: 0.3908 test_loss: 2.8637, test_acc: 0.4825, best: 0.4825, time: 0:02:15
 Epoch: 166, lr: 1.0e-02, train_loss: 1.6051, train_acc: 0.4010 test_loss: 50.2729, test_acc: 0.4621, best: 0.4825, time: 0:02:14
 Epoch: 167, lr: 1.0e-02, train_loss: 1.6205, train_acc: 0.3940 test_loss: 3.2192, test_acc: 0.4768, best: 0.4825, time: 0:02:14
 Epoch: 168, lr: 1.0e-02, train_loss: 1.6120, train_acc: 0.3942 test_loss: 2.9065, test_acc: 0.4714, best: 0.4825, time: 0:02:14
 Epoch: 169, lr: 1.0e-02, train_loss: 1.5968, train_acc: 0.4050 test_loss: 2.4217, test_acc: 0.4750, best: 0.4825, time: 0:02:14
 Epoch: 170, lr: 1.0e-02, train_loss: 1.5978, train_acc: 0.4078 test_loss: 10.1409, test_acc: 0.4627, best: 0.4825, time: 0:02:14
 Epoch: 171, lr: 1.0e-02, train_loss: 1.5680, train_acc: 0.4126 test_loss: 54.9804, test_acc: 0.4860, best: 0.4860, time: 0:02:14
 Epoch: 172, lr: 1.0e-02, train_loss: 1.5738, train_acc: 0.4130 test_loss: 6.0948, test_acc: 0.4938, best: 0.4938, time: 0:02:15
 Epoch: 173, lr: 1.0e-02, train_loss: 1.6387, train_acc: 0.3912 test_loss: 13.6309, test_acc: 0.4489, best: 0.4938, time: 0:02:14
 Epoch: 174, lr: 1.0e-02, train_loss: 1.6381, train_acc: 0.3910 test_loss: 350.3204, test_acc: 0.4047, best: 0.4938, time: 0:02:14
 Epoch: 175, lr: 1.0e-02, train_loss: 1.6335, train_acc: 0.3876 test_loss: 151.7430, test_acc: 0.4258, best: 0.4938, time: 0:02:14
 Epoch: 176, lr: 1.0e-02, train_loss: 1.5922, train_acc: 0.3978 test_loss: 26.0842, test_acc: 0.4765, best: 0.4938, time: 0:02:14
 Epoch: 177, lr: 1.0e-02, train_loss: 1.5773, train_acc: 0.4182 test_loss: 44.1796, test_acc: 0.4429, best: 0.4938, time: 0:02:14
 Epoch: 178, lr: 1.0e-02, train_loss: 1.6738, train_acc: 0.3810 test_loss: 1.9903, test_acc: 0.4600, best: 0.4938, time: 0:02:14
 Epoch: 179, lr: 1.0e-02, train_loss: 1.6218, train_acc: 0.3944 test_loss: 21.9107, test_acc: 0.4582, best: 0.4938, time: 0:02:14
 Epoch: 180, lr: 2.0e-03, train_loss: 1.5633, train_acc: 0.4096 test_loss: 30.4049, test_acc: 0.4622, best: 0.4938, time: 0:02:14
 Epoch: 181, lr: 2.0e-03, train_loss: 1.5300, train_acc: 0.4402 test_loss: 441.8988, test_acc: 0.4556, best: 0.4938, time: 0:02:14
 Epoch: 182, lr: 2.0e-03, train_loss: 1.5210, train_acc: 0.4366 test_loss: 54.5675, test_acc: 0.4635, best: 0.4938, time: 0:02:14
 Epoch: 183, lr: 2.0e-03, train_loss: 1.5287, train_acc: 0.4284 test_loss: 24.0707, test_acc: 0.4901, best: 0.4938, time: 0:02:14
 Epoch: 184, lr: 2.0e-03, train_loss: 1.5120, train_acc: 0.4494 test_loss: 65.2674, test_acc: 0.4579, best: 0.4938, time: 0:02:14
 Epoch: 185, lr: 2.0e-03, train_loss: 1.5196, train_acc: 0.4326 test_loss: 71.1331, test_acc: 0.4600, best: 0.4938, time: 0:02:14
 Epoch: 186, lr: 2.0e-03, train_loss: 1.5137, train_acc: 0.4448 test_loss: 14.5807, test_acc: 0.4786, best: 0.4938, time: 0:02:14
 Epoch: 187, lr: 2.0e-03, train_loss: 1.5080, train_acc: 0.4412 test_loss: 245.2403, test_acc: 0.4595, best: 0.4938, time: 0:02:14
 Epoch: 188, lr: 2.0e-03, train_loss: 1.4994, train_acc: 0.4418 test_loss: 4.2034, test_acc: 0.4953, best: 0.4953, time: 0:02:15
 Epoch: 189, lr: 2.0e-03, train_loss: 1.4930, train_acc: 0.4400 test_loss: 14.6522, test_acc: 0.4993, best: 0.4993, time: 0:02:14
 Epoch: 190, lr: 2.0e-03, train_loss: 1.5016, train_acc: 0.4364 test_loss: 36.1788, test_acc: 0.4851, best: 0.4993, time: 0:02:14
 Epoch: 191, lr: 2.0e-03, train_loss: 1.4895, train_acc: 0.4458 test_loss: 24.8116, test_acc: 0.4845, best: 0.4993, time: 0:02:14
 Epoch: 192, lr: 2.0e-03, train_loss: 1.4910, train_acc: 0.4466 test_loss: 46.5411, test_acc: 0.4793, best: 0.4993, time: 0:02:14
 Epoch: 193, lr: 2.0e-03, train_loss: 1.4732, train_acc: 0.4574 test_loss: 160.7501, test_acc: 0.4724, best: 0.4993, time: 0:02:14
 Epoch: 194, lr: 2.0e-03, train_loss: 1.4894, train_acc: 0.4392 test_loss: 90.2482, test_acc: 0.4672, best: 0.4993, time: 0:02:14
 Epoch: 195, lr: 2.0e-03, train_loss: 1.4749, train_acc: 0.4488 test_loss: 192.8758, test_acc: 0.4855, best: 0.4993, time: 0:02:14
 Epoch: 196, lr: 2.0e-03, train_loss: 1.4687, train_acc: 0.4506 test_loss: 110.2859, test_acc: 0.4795, best: 0.4993, time: 0:02:14
 Epoch: 197, lr: 2.0e-03, train_loss: 1.4726, train_acc: 0.4518 test_loss: 60.5866, test_acc: 0.4904, best: 0.4993, time: 0:02:14
 Epoch: 198, lr: 2.0e-03, train_loss: 1.4825, train_acc: 0.4420 test_loss: 150.8438, test_acc: 0.4745, best: 0.4993, time: 0:02:14
 Epoch: 199, lr: 2.0e-03, train_loss: 1.4719, train_acc: 0.4442 test_loss: 129.3170, test_acc: 0.4661, best: 0.4993, time: 0:02:14
 Epoch: 200, lr: 2.0e-03, train_loss: 1.4571, train_acc: 0.4526 test_loss: 69.5970, test_acc: 0.4758, best: 0.4993, time: 0:02:14
 Epoch: 201, lr: 2.0e-03, train_loss: 1.4563, train_acc: 0.4602 test_loss: 247.2015, test_acc: 0.4894, best: 0.4993, time: 0:02:14
 Epoch: 202, lr: 2.0e-03, train_loss: 1.4591, train_acc: 0.4598 test_loss: 95.1702, test_acc: 0.4898, best: 0.4993, time: 0:02:14
 Epoch: 203, lr: 2.0e-03, train_loss: 1.4691, train_acc: 0.4472 test_loss: 110.7151, test_acc: 0.4988, best: 0.4993, time: 0:02:14
 Epoch: 204, lr: 2.0e-03, train_loss: 1.4727, train_acc: 0.4554 test_loss: 103.0506, test_acc: 0.4748, best: 0.4993, time: 0:02:14
 Epoch: 205, lr: 2.0e-03, train_loss: 1.4891, train_acc: 0.4404 test_loss: 40.5441, test_acc: 0.4913, best: 0.4993, time: 0:02:14
 Epoch: 206, lr: 2.0e-03, train_loss: 1.4539, train_acc: 0.4662 test_loss: 12.0206, test_acc: 0.4959, best: 0.4993, time: 0:02:14
 Epoch: 207, lr: 2.0e-03, train_loss: 1.4506, train_acc: 0.4648 test_loss: 11.1050, test_acc: 0.5125, best: 0.5125, time: 0:02:15
 Epoch: 208, lr: 2.0e-03, train_loss: 1.4616, train_acc: 0.4554 test_loss: 8.5267, test_acc: 0.5198, best: 0.5198, time: 0:02:15
 Epoch: 209, lr: 2.0e-03, train_loss: 1.4475, train_acc: 0.4646 test_loss: 185.4142, test_acc: 0.5080, best: 0.5198, time: 0:02:14
 Epoch: 210, lr: 2.0e-03, train_loss: 1.5215, train_acc: 0.4334 test_loss: 118.9146, test_acc: 0.4818, best: 0.5198, time: 0:02:14
 Epoch: 211, lr: 2.0e-03, train_loss: 1.5007, train_acc: 0.4410 test_loss: 2764.1794, test_acc: 0.4595, best: 0.5198, time: 0:02:14
 Epoch: 212, lr: 2.0e-03, train_loss: 1.4641, train_acc: 0.4546 test_loss: 1050.1413, test_acc: 0.4865, best: 0.5198, time: 0:02:14
 Epoch: 213, lr: 2.0e-03, train_loss: 1.5041, train_acc: 0.4434 test_loss: 1035.7962, test_acc: 0.4749, best: 0.5198, time: 0:02:14
 Epoch: 214, lr: 2.0e-03, train_loss: 1.4770, train_acc: 0.4446 test_loss: 392.7667, test_acc: 0.4731, best: 0.5198, time: 0:02:14
 Epoch: 215, lr: 2.0e-03, train_loss: 1.4564, train_acc: 0.4566 test_loss: 117.1554, test_acc: 0.4948, best: 0.5198, time: 0:02:14
 Epoch: 216, lr: 2.0e-03, train_loss: 1.4857, train_acc: 0.4488 test_loss: 306.4176, test_acc: 0.4950, best: 0.5198, time: 0:02:14
 Epoch: 217, lr: 2.0e-03, train_loss: 1.4696, train_acc: 0.4528 test_loss: 658.7439, test_acc: 0.4936, best: 0.5198, time: 0:02:14
 Epoch: 218, lr: 2.0e-03, train_loss: 1.4789, train_acc: 0.4512 test_loss: 598.7972, test_acc: 0.4968, best: 0.5198, time: 0:02:14
 Epoch: 219, lr: 2.0e-03, train_loss: 1.4574, train_acc: 0.4644 test_loss: 218.4227, test_acc: 0.4798, best: 0.5198, time: 0:02:14
 Epoch: 220, lr: 2.0e-03, train_loss: 1.4558, train_acc: 0.4572 test_loss: 1548.4617, test_acc: 0.4873, best: 0.5198, time: 0:02:14
 Epoch: 221, lr: 2.0e-03, train_loss: 1.4548, train_acc: 0.4602 test_loss: 601.3967, test_acc: 0.4884, best: 0.5198, time: 0:02:14
 Epoch: 222, lr: 2.0e-03, train_loss: 1.4548, train_acc: 0.4600 test_loss: 752.2944, test_acc: 0.4850, best: 0.5198, time: 0:02:14
 Epoch: 223, lr: 2.0e-03, train_loss: 1.4624, train_acc: 0.4712 test_loss: 767.8552, test_acc: 0.4900, best: 0.5198, time: 0:02:14
 Epoch: 224, lr: 2.0e-03, train_loss: 1.4521, train_acc: 0.4730 test_loss: 146.5157, test_acc: 0.5099, best: 0.5198, time: 0:02:14
 Epoch: 225, lr: 2.0e-03, train_loss: 1.4479, train_acc: 0.4706 test_loss: 906.6661, test_acc: 0.4904, best: 0.5198, time: 0:02:14
 Epoch: 226, lr: 2.0e-03, train_loss: 1.4441, train_acc: 0.4558 test_loss: 339.9943, test_acc: 0.4875, best: 0.5198, time: 0:02:14
 Epoch: 227, lr: 2.0e-03, train_loss: 1.4379, train_acc: 0.4696 test_loss: 1112.4058, test_acc: 0.4874, best: 0.5198, time: 0:02:14
 Epoch: 228, lr: 2.0e-03, train_loss: 1.4354, train_acc: 0.4614 test_loss: 379.2573, test_acc: 0.4996, best: 0.5198, time: 0:02:14
 Epoch: 229, lr: 2.0e-03, train_loss: 1.4586, train_acc: 0.4638 test_loss: 68.6922, test_acc: 0.5128, best: 0.5198, time: 0:02:14
 Epoch: 230, lr: 2.0e-03, train_loss: 1.4530, train_acc: 0.4582 test_loss: 413.5712, test_acc: 0.5054, best: 0.5198, time: 0:02:14
 Epoch: 231, lr: 2.0e-03, train_loss: 1.4328, train_acc: 0.4666 test_loss: 555.8969, test_acc: 0.5018, best: 0.5198, time: 0:02:14
 Epoch: 232, lr: 2.0e-03, train_loss: 1.4379, train_acc: 0.4688 test_loss: 1080.3387, test_acc: 0.5094, best: 0.5198, time: 0:02:14
 Epoch: 233, lr: 2.0e-03, train_loss: 1.4264, train_acc: 0.4684 test_loss: 935.0444, test_acc: 0.5070, best: 0.5198, time: 0:02:14
 Epoch: 234, lr: 2.0e-03, train_loss: 1.4478, train_acc: 0.4670 test_loss: 272.2980, test_acc: 0.5102, best: 0.5198, time: 0:02:14
 Epoch: 235, lr: 2.0e-03, train_loss: 1.4356, train_acc: 0.4686 test_loss: 1644.1813, test_acc: 0.4873, best: 0.5198, time: 0:02:14
 Epoch: 236, lr: 2.0e-03, train_loss: 1.4345, train_acc: 0.4688 test_loss: 676.1846, test_acc: 0.4979, best: 0.5198, time: 0:02:14
 Epoch: 237, lr: 2.0e-03, train_loss: 1.4305, train_acc: 0.4698 test_loss: 1087.3115, test_acc: 0.4808, best: 0.5198, time: 0:02:14
 Epoch: 238, lr: 2.0e-03, train_loss: 1.4272, train_acc: 0.4674 test_loss: 329.9951, test_acc: 0.5069, best: 0.5198, time: 0:02:14
 Epoch: 239, lr: 2.0e-03, train_loss: 1.4158, train_acc: 0.4798 test_loss: 4192.0565, test_acc: 0.4826, best: 0.5198, time: 0:02:14
 Epoch: 240, lr: 4.0e-04, train_loss: 1.4172, train_acc: 0.4734 test_loss: 156.4546, test_acc: 0.5064, best: 0.5198, time: 0:02:14
 Epoch: 241, lr: 4.0e-04, train_loss: 1.4077, train_acc: 0.4812 test_loss: 401.9386, test_acc: 0.5019, best: 0.5198, time: 0:02:14
 Epoch: 242, lr: 4.0e-04, train_loss: 1.3962, train_acc: 0.4780 test_loss: 1115.9373, test_acc: 0.5086, best: 0.5198, time: 0:02:13
 Epoch: 243, lr: 4.0e-04, train_loss: 1.4022, train_acc: 0.4820 test_loss: 1756.3474, test_acc: 0.4860, best: 0.5198, time: 0:02:12
 Epoch: 244, lr: 4.0e-04, train_loss: 1.4006, train_acc: 0.4820 test_loss: 284.7736, test_acc: 0.5059, best: 0.5198, time: 0:02:12
 Epoch: 245, lr: 4.0e-04, train_loss: 1.4066, train_acc: 0.4800 test_loss: 2261.5329, test_acc: 0.4935, best: 0.5198, time: 0:02:12
 Epoch: 246, lr: 4.0e-04, train_loss: 1.3842, train_acc: 0.4882 test_loss: 600.8309, test_acc: 0.5152, best: 0.5198, time: 0:02:12
 Epoch: 247, lr: 4.0e-04, train_loss: 1.3799, train_acc: 0.4866 test_loss: 537.9535, test_acc: 0.5174, best: 0.5198, time: 0:02:12
 Epoch: 248, lr: 4.0e-04, train_loss: 1.3976, train_acc: 0.4826 test_loss: 287.6352, test_acc: 0.5138, best: 0.5198, time: 0:02:12
 Epoch: 249, lr: 4.0e-04, train_loss: 1.3995, train_acc: 0.4854 test_loss: 125.7954, test_acc: 0.5329, best: 0.5329, time: 0:02:12
 Epoch: 250, lr: 4.0e-04, train_loss: 1.4005, train_acc: 0.4796 test_loss: 902.3774, test_acc: 0.5035, best: 0.5329, time: 0:02:12
 Epoch: 251, lr: 4.0e-04, train_loss: 1.3730, train_acc: 0.4918 test_loss: 290.0402, test_acc: 0.5218, best: 0.5329, time: 0:02:12
 Epoch: 252, lr: 4.0e-04, train_loss: 1.3879, train_acc: 0.4868 test_loss: 90.1007, test_acc: 0.5145, best: 0.5329, time: 0:02:12
 Epoch: 253, lr: 4.0e-04, train_loss: 1.3900, train_acc: 0.4864 test_loss: 2255.3906, test_acc: 0.4854, best: 0.5329, time: 0:02:12
 Epoch: 254, lr: 4.0e-04, train_loss: 1.3779, train_acc: 0.4872 test_loss: 3478.8383, test_acc: 0.4830, best: 0.5329, time: 0:02:12
 Epoch: 255, lr: 4.0e-04, train_loss: 1.3699, train_acc: 0.4890 test_loss: 7356.2804, test_acc: 0.4808, best: 0.5329, time: 0:02:12
 Epoch: 256, lr: 4.0e-04, train_loss: 1.3828, train_acc: 0.4858 test_loss: 1722.4467, test_acc: 0.5066, best: 0.5329, time: 0:02:12
 Epoch: 257, lr: 4.0e-04, train_loss: 1.3959, train_acc: 0.4872 test_loss: 3285.8020, test_acc: 0.4909, best: 0.5329, time: 0:02:12
 Epoch: 258, lr: 4.0e-04, train_loss: 1.3828, train_acc: 0.4810 test_loss: 1301.9585, test_acc: 0.5154, best: 0.5329, time: 0:02:12
 Epoch: 259, lr: 4.0e-04, train_loss: 1.3831, train_acc: 0.4906 test_loss: 38203.8989, test_acc: 0.4690, best: 0.5329, time: 0:02:12
 Epoch: 260, lr: 4.0e-04, train_loss: 1.3917, train_acc: 0.4934 test_loss: 98.0722, test_acc: 0.5344, best: 0.5344, time: 0:02:12
 Epoch: 261, lr: 4.0e-04, train_loss: 1.3912, train_acc: 0.4818 test_loss: 882.5430, test_acc: 0.5092, best: 0.5344, time: 0:02:12
 Epoch: 262, lr: 4.0e-04, train_loss: 1.4018, train_acc: 0.4820 test_loss: 20111.3586, test_acc: 0.4676, best: 0.5344, time: 0:02:12
 Epoch: 263, lr: 4.0e-04, train_loss: 1.3757, train_acc: 0.4878 test_loss: 235.9075, test_acc: 0.5194, best: 0.5344, time: 0:02:12
 Epoch: 264, lr: 4.0e-04, train_loss: 1.3760, train_acc: 0.4944 test_loss: 466.8825, test_acc: 0.5158, best: 0.5344, time: 0:02:12
 Epoch: 265, lr: 4.0e-04, train_loss: 1.3669, train_acc: 0.4984 test_loss: 2982.1158, test_acc: 0.5099, best: 0.5344, time: 0:02:12
 Epoch: 266, lr: 4.0e-04, train_loss: 1.3972, train_acc: 0.4820 test_loss: 2810.0859, test_acc: 0.4855, best: 0.5344, time: 0:02:12
 Epoch: 267, lr: 4.0e-04, train_loss: 1.3584, train_acc: 0.4900 test_loss: 490.7357, test_acc: 0.5159, best: 0.5344, time: 0:02:12
 Epoch: 268, lr: 4.0e-04, train_loss: 1.3877, train_acc: 0.4852 test_loss: 1761.9638, test_acc: 0.4864, best: 0.5344, time: 0:02:12
 Epoch: 269, lr: 4.0e-04, train_loss: 1.3840, train_acc: 0.4848 test_loss: 2017.6301, test_acc: 0.5040, best: 0.5344, time: 0:02:12
 Epoch: 270, lr: 8.0e-05, train_loss: 1.3552, train_acc: 0.4874 test_loss: 295.6855, test_acc: 0.5236, best: 0.5344, time: 0:02:12
 Epoch: 271, lr: 8.0e-05, train_loss: 1.3798, train_acc: 0.4892 test_loss: 496.0400, test_acc: 0.5122, best: 0.5344, time: 0:02:12
 Epoch: 272, lr: 8.0e-05, train_loss: 1.3601, train_acc: 0.4974 test_loss: 1677.3080, test_acc: 0.5139, best: 0.5344, time: 0:02:12
 Epoch: 273, lr: 8.0e-05, train_loss: 1.3665, train_acc: 0.4964 test_loss: 3963.1762, test_acc: 0.4795, best: 0.5344, time: 0:02:12
 Epoch: 274, lr: 8.0e-05, train_loss: 1.3599, train_acc: 0.4968 test_loss: 149.6706, test_acc: 0.5189, best: 0.5344, time: 0:02:12
 Epoch: 275, lr: 8.0e-05, train_loss: 1.3581, train_acc: 0.4938 test_loss: 273.6921, test_acc: 0.5218, best: 0.5344, time: 0:02:12
 Epoch: 276, lr: 8.0e-05, train_loss: 1.3532, train_acc: 0.5064 test_loss: 1871.2370, test_acc: 0.5028, best: 0.5344, time: 0:02:12
 Epoch: 277, lr: 8.0e-05, train_loss: 1.3798, train_acc: 0.4936 test_loss: 201.9349, test_acc: 0.5130, best: 0.5344, time: 0:02:12
 Epoch: 278, lr: 8.0e-05, train_loss: 1.3407, train_acc: 0.5042 test_loss: 8865.8023, test_acc: 0.4869, best: 0.5344, time: 0:02:12
 Epoch: 279, lr: 8.0e-05, train_loss: 1.3646, train_acc: 0.4938 test_loss: 1760.7410, test_acc: 0.5014, best: 0.5344, time: 0:02:12
 Epoch: 280, lr: 8.0e-05, train_loss: 1.3708, train_acc: 0.4944 test_loss: 512.0501, test_acc: 0.5148, best: 0.5344, time: 0:02:12
 Epoch: 281, lr: 8.0e-05, train_loss: 1.3643, train_acc: 0.4916 test_loss: 453.7392, test_acc: 0.5166, best: 0.5344, time: 0:02:12
 Epoch: 282, lr: 8.0e-05, train_loss: 1.3587, train_acc: 0.4994 test_loss: 149.4656, test_acc: 0.5190, best: 0.5344, time: 0:02:12
 Epoch: 283, lr: 8.0e-05, train_loss: 1.3696, train_acc: 0.4926 test_loss: 449.4898, test_acc: 0.5050, best: 0.5344, time: 0:02:12
 Epoch: 284, lr: 8.0e-05, train_loss: 1.3756, train_acc: 0.4998 test_loss: 492.7882, test_acc: 0.5108, best: 0.5344, time: 0:02:12
 Epoch: 285, lr: 8.0e-05, train_loss: 1.3668, train_acc: 0.5026 test_loss: 9312.5453, test_acc: 0.4928, best: 0.5344, time: 0:02:12
 Epoch: 286, lr: 8.0e-05, train_loss: 1.3673, train_acc: 0.4960 test_loss: 681.1509, test_acc: 0.5170, best: 0.5344, time: 0:02:12
 Epoch: 287, lr: 8.0e-05, train_loss: 1.3666, train_acc: 0.4934 test_loss: 730.5679, test_acc: 0.5059, best: 0.5344, time: 0:02:12
 Epoch: 288, lr: 8.0e-05, train_loss: 1.3499, train_acc: 0.5018 test_loss: 304.9932, test_acc: 0.5206, best: 0.5344, time: 0:02:12
 Epoch: 289, lr: 8.0e-05, train_loss: 1.3744, train_acc: 0.4886 test_loss: 224.1183, test_acc: 0.5145, best: 0.5344, time: 0:02:12
 Epoch: 290, lr: 8.0e-05, train_loss: 1.3594, train_acc: 0.5040 test_loss: 771.4777, test_acc: 0.5175, best: 0.5344, time: 0:02:12
 Epoch: 291, lr: 8.0e-05, train_loss: 1.3593, train_acc: 0.4964 test_loss: 1093.9763, test_acc: 0.4969, best: 0.5344, time: 0:02:12
 Epoch: 292, lr: 8.0e-05, train_loss: 1.3747, train_acc: 0.4894 test_loss: 427.8276, test_acc: 0.5006, best: 0.5344, time: 0:02:12
 Epoch: 293, lr: 8.0e-05, train_loss: 1.3709, train_acc: 0.4900 test_loss: 2442.3773, test_acc: 0.4976, best: 0.5344, time: 0:02:12
 Epoch: 294, lr: 8.0e-05, train_loss: 1.3723, train_acc: 0.4922 test_loss: 1212.6986, test_acc: 0.5130, best: 0.5344, time: 0:02:12
 Epoch: 295, lr: 8.0e-05, train_loss: 1.3876, train_acc: 0.4886 test_loss: 39476.0008, test_acc: 0.4510, best: 0.5344, time: 0:02:12
 Epoch: 296, lr: 8.0e-05, train_loss: 1.3877, train_acc: 0.4832 test_loss: 812.3706, test_acc: 0.5138, best: 0.5344, time: 0:02:12
 Epoch: 297, lr: 8.0e-05, train_loss: 1.3607, train_acc: 0.5010 test_loss: 62.0493, test_acc: 0.5239, best: 0.5344, time: 0:02:12
 Epoch: 298, lr: 8.0e-05, train_loss: 1.3584, train_acc: 0.4974 test_loss: 3245.7018, test_acc: 0.4901, best: 0.5344, time: 0:02:12
 Epoch: 299, lr: 8.0e-05, train_loss: 1.3625, train_acc: 0.4970 test_loss: 1033.6168, test_acc: 0.5009, best: 0.5344, time: 0:02:12
 Highest accuracy: 0.5344