
 Run on time: 2022-05-26 03:28:52.298757

 Arguments:
	 gpu                  : True
	 log                  : True
	 seed                 : 0
	 momentum             : 0.9
	 amsgrad              : True
	 dataset              : STL10
	 batch_size           : 8
	 architecture         : RESNET50_GAUSSIAN_POOL
	 im_size              : 224
	 relu_threshold       : 4.0
	 learning_rate        : 0.01
	 pretrained_backbone  : 
	 pretrained_ann       : 
	 weight_decay         : 0.0
	 test_only            : False
	 epochs               : 300
	 lr_interval          : [180, 240, 270]
	 lr_reduce            : 5
	 optimizer            : SGD
	 dropout              : 0.2
	 kernel_size          : 3
	 dont_save            : False
	 visualize            : False
	 devices              : 0
 DataParallel(
  (module): Network(
    (net): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (pooling): GaussianPooling2d(
            kernel_size=2, stride=2, padding=0
            (ToHidden): Sequential(
              (0): AdaptiveAvgPool2d(output_size=(1, 1))
              (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
              (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (3): ReLU()
            )
            (ToMean): Sequential(
              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (ToSigma): Sequential(
              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Sigmoid()
            )
            (activation): Softplus(beta=1, threshold=20)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (pooling): GaussianPooling2d(
            kernel_size=2, stride=2, padding=0
            (ToHidden): Sequential(
              (0): AdaptiveAvgPool2d(output_size=(1, 1))
              (1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))
              (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (3): ReLU()
            )
            (ToMean): Sequential(
              (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (ToSigma): Sequential(
              (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Sigmoid()
            )
            (activation): Softplus(beta=1, threshold=20)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer4): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (pooling): GaussianPooling2d(
            kernel_size=2, stride=2, padding=0
            (ToHidden): Sequential(
              (0): AdaptiveAvgPool2d(output_size=(1, 1))
              (1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1))
              (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (3): ReLU()
            )
            (ToMean): Sequential(
              (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (ToSigma): Sequential(
              (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Sigmoid()
            )
            (activation): Softplus(beta=1, threshold=20)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
      (fc): Linear(in_features=2048, out_features=10, bias=True)
    )
  )
)
 SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0
)
 Epoch: 1, lr: 1.0e-02, train_loss: 3.2585, train_acc: 0.1344 test_loss: 329.8939, test_acc: 0.2184, best: 0.2184, time: 0:03:03
 Epoch: 2, lr: 1.0e-02, train_loss: 2.0772, train_acc: 0.2120 test_loss: 18.5659, test_acc: 0.2349, best: 0.2349, time: 0:03:03
 Epoch: 3, lr: 1.0e-02, train_loss: 1.9627, train_acc: 0.2374 test_loss: 36.5100, test_acc: 0.2994, best: 0.2994, time: 0:03:02
 Epoch: 4, lr: 1.0e-02, train_loss: 1.9012, train_acc: 0.2646 test_loss: 146.2908, test_acc: 0.3066, best: 0.3066, time: 0:03:02
 Epoch: 5, lr: 1.0e-02, train_loss: 1.8280, train_acc: 0.2890 test_loss: 1383.5501, test_acc: 0.3076, best: 0.3076, time: 0:03:02
 Epoch: 6, lr: 1.0e-02, train_loss: 1.7870, train_acc: 0.3094 test_loss: 688.3208, test_acc: 0.3713, best: 0.3713, time: 0:03:02
 Epoch: 7, lr: 1.0e-02, train_loss: 1.7456, train_acc: 0.3294 test_loss: 221.7181, test_acc: 0.3996, best: 0.3996, time: 0:03:02
 Epoch: 8, lr: 1.0e-02, train_loss: 1.6879, train_acc: 0.3522 test_loss: 9.7990, test_acc: 0.3889, best: 0.3996, time: 0:03:02
 Epoch: 9, lr: 1.0e-02, train_loss: 1.6502, train_acc: 0.3730 test_loss: 96.1554, test_acc: 0.3962, best: 0.3996, time: 0:03:01
 Epoch: 10, lr: 1.0e-02, train_loss: 1.5945, train_acc: 0.3972 test_loss: 51.4693, test_acc: 0.4233, best: 0.4233, time: 0:03:02
 Epoch: 11, lr: 1.0e-02, train_loss: 1.5325, train_acc: 0.4258 test_loss: 12.0353, test_acc: 0.4520, best: 0.4520, time: 0:03:02
 Epoch: 12, lr: 1.0e-02, train_loss: 1.5047, train_acc: 0.4392 test_loss: 2049.4674, test_acc: 0.4606, best: 0.4606, time: 0:03:02
 Epoch: 13, lr: 1.0e-02, train_loss: 1.4588, train_acc: 0.4502 test_loss: 3652.0803, test_acc: 0.4280, best: 0.4606, time: 0:03:01
 Epoch: 14, lr: 1.0e-02, train_loss: 1.4238, train_acc: 0.4640 test_loss: 310.6974, test_acc: 0.4986, best: 0.4986, time: 0:03:02
 Epoch: 15, lr: 1.0e-02, train_loss: 1.3935, train_acc: 0.4850 test_loss: 1245.6484, test_acc: 0.5148, best: 0.5148, time: 0:03:02
 Epoch: 16, lr: 1.0e-02, train_loss: 1.3783, train_acc: 0.4906 test_loss: 1806.7997, test_acc: 0.5215, best: 0.5215, time: 0:03:02
 Epoch: 17, lr: 1.0e-02, train_loss: 1.3750, train_acc: 0.4932 test_loss: 332.7474, test_acc: 0.5275, best: 0.5275, time: 0:03:02
 Epoch: 18, lr: 1.0e-02, train_loss: 1.3592, train_acc: 0.5026 test_loss: 190.3679, test_acc: 0.5496, best: 0.5496, time: 0:03:02
 Epoch: 19, lr: 1.0e-02, train_loss: 1.3321, train_acc: 0.5148 test_loss: 18684.8598, test_acc: 0.4715, best: 0.5496, time: 0:03:01
 Epoch: 20, lr: 1.0e-02, train_loss: 1.2756, train_acc: 0.5270 test_loss: 884.6423, test_acc: 0.4873, best: 0.5496, time: 0:03:01
 Epoch: 21, lr: 1.0e-02, train_loss: 1.2478, train_acc: 0.5452 test_loss: 5734.8747, test_acc: 0.4984, best: 0.5496, time: 0:03:02
 Epoch: 22, lr: 1.0e-02, train_loss: 1.2167, train_acc: 0.5548 test_loss: 256.2481, test_acc: 0.5714, best: 0.5714, time: 0:03:02
 Epoch: 23, lr: 1.0e-02, train_loss: 1.1861, train_acc: 0.5664 test_loss: 268.9641, test_acc: 0.5716, best: 0.5716, time: 0:03:02
 Epoch: 24, lr: 1.0e-02, train_loss: 1.1757, train_acc: 0.5700 test_loss: 25.1452, test_acc: 0.5773, best: 0.5773, time: 0:03:02
 Epoch: 25, lr: 1.0e-02, train_loss: 1.1420, train_acc: 0.5888 test_loss: 113.0801, test_acc: 0.6099, best: 0.6099, time: 0:03:02
 Epoch: 26, lr: 1.0e-02, train_loss: 1.1212, train_acc: 0.5986 test_loss: 2199.3238, test_acc: 0.5839, best: 0.6099, time: 0:03:01
 Epoch: 27, lr: 1.0e-02, train_loss: 1.1277, train_acc: 0.5928 test_loss: 777.7559, test_acc: 0.5777, best: 0.6099, time: 0:03:01
 Epoch: 28, lr: 1.0e-02, train_loss: 1.0794, train_acc: 0.6110 test_loss: 108.0271, test_acc: 0.5775, best: 0.6099, time: 0:03:01
 Epoch: 29, lr: 1.0e-02, train_loss: 1.0574, train_acc: 0.6184 test_loss: 3594.6903, test_acc: 0.5416, best: 0.6099, time: 0:03:02
 Epoch: 30, lr: 1.0e-02, train_loss: 1.0578, train_acc: 0.6184 test_loss: 423.6278, test_acc: 0.5801, best: 0.6099, time: 0:03:02
 Epoch: 31, lr: 1.0e-02, train_loss: 1.0281, train_acc: 0.6238 test_loss: 212.3081, test_acc: 0.5923, best: 0.6099, time: 0:03:01
 Epoch: 32, lr: 1.0e-02, train_loss: 0.9848, train_acc: 0.6524 test_loss: 46.6053, test_acc: 0.6408, best: 0.6408, time: 0:03:02
 Epoch: 33, lr: 1.0e-02, train_loss: 1.0001, train_acc: 0.6436 test_loss: 62.1268, test_acc: 0.6485, best: 0.6485, time: 0:03:02
 Epoch: 34, lr: 1.0e-02, train_loss: 0.9472, train_acc: 0.6622 test_loss: 64.6430, test_acc: 0.6554, best: 0.6554, time: 0:03:02
 Epoch: 35, lr: 1.0e-02, train_loss: 0.9490, train_acc: 0.6564 test_loss: 14.7407, test_acc: 0.6319, best: 0.6554, time: 0:03:02
 Epoch: 36, lr: 1.0e-02, train_loss: 0.9343, train_acc: 0.6624 test_loss: 234.1772, test_acc: 0.5900, best: 0.6554, time: 0:03:02
 Epoch: 37, lr: 1.0e-02, train_loss: 0.9161, train_acc: 0.6684 test_loss: 323.2567, test_acc: 0.6094, best: 0.6554, time: 0:03:02
 Epoch: 38, lr: 1.0e-02, train_loss: 0.9281, train_acc: 0.6572 test_loss: 25.4622, test_acc: 0.6341, best: 0.6554, time: 0:03:01
 Epoch: 39, lr: 1.0e-02, train_loss: 0.8963, train_acc: 0.6786 test_loss: 275.8529, test_acc: 0.6349, best: 0.6554, time: 0:03:01
 Epoch: 40, lr: 1.0e-02, train_loss: 0.8884, train_acc: 0.6836 test_loss: 337.8062, test_acc: 0.6579, best: 0.6579, time: 0:03:02
 Epoch: 41, lr: 1.0e-02, train_loss: 0.8672, train_acc: 0.6840 test_loss: 35.1857, test_acc: 0.6867, best: 0.6867, time: 0:03:02
 Epoch: 42, lr: 1.0e-02, train_loss: 0.8409, train_acc: 0.7014 test_loss: 1119.8628, test_acc: 0.6170, best: 0.6867, time: 0:03:02
 Epoch: 43, lr: 1.0e-02, train_loss: 0.8575, train_acc: 0.6912 test_loss: 140.7089, test_acc: 0.6468, best: 0.6867, time: 0:03:02
 Epoch: 44, lr: 1.0e-02, train_loss: 0.8314, train_acc: 0.7096 test_loss: 18.7138, test_acc: 0.6759, best: 0.6867, time: 0:03:02
 Epoch: 45, lr: 1.0e-02, train_loss: 0.7955, train_acc: 0.7154 test_loss: 204.5686, test_acc: 0.6308, best: 0.6867, time: 0:03:02
 Epoch: 46, lr: 1.0e-02, train_loss: 0.7858, train_acc: 0.7182 test_loss: 11.3987, test_acc: 0.6365, best: 0.6867, time: 0:03:02
 Epoch: 47, lr: 1.0e-02, train_loss: 0.7675, train_acc: 0.7162 test_loss: 65.2564, test_acc: 0.6281, best: 0.6867, time: 0:03:01
 Epoch: 48, lr: 1.0e-02, train_loss: 0.7968, train_acc: 0.7236 test_loss: 123.5615, test_acc: 0.6236, best: 0.6867, time: 0:03:02
 Epoch: 49, lr: 1.0e-02, train_loss: 0.7815, train_acc: 0.7274 test_loss: 5.5544, test_acc: 0.6787, best: 0.6867, time: 0:03:02
 Epoch: 50, lr: 1.0e-02, train_loss: 0.7445, train_acc: 0.7410 test_loss: 7.0226, test_acc: 0.6771, best: 0.6867, time: 0:03:02
 Epoch: 51, lr: 1.0e-02, train_loss: 0.7252, train_acc: 0.7480 test_loss: 43.1930, test_acc: 0.6971, best: 0.6971, time: 0:03:02
 Epoch: 52, lr: 1.0e-02, train_loss: 0.7330, train_acc: 0.7362 test_loss: 2146.3589, test_acc: 0.6964, best: 0.6971, time: 0:03:01
 Epoch: 53, lr: 1.0e-02, train_loss: 0.7239, train_acc: 0.7390 test_loss: 120.2427, test_acc: 0.7184, best: 0.7184, time: 0:03:02
 Epoch: 54, lr: 1.0e-02, train_loss: 0.7001, train_acc: 0.7570 test_loss: 58.3828, test_acc: 0.6703, best: 0.7184, time: 0:03:02
 Epoch: 55, lr: 1.0e-02, train_loss: 0.6858, train_acc: 0.7600 test_loss: 10.7201, test_acc: 0.6911, best: 0.7184, time: 0:03:01
 Epoch: 56, lr: 1.0e-02, train_loss: 0.6557, train_acc: 0.7594 test_loss: 1.6445, test_acc: 0.7191, best: 0.7191, time: 0:03:02
 Epoch: 57, lr: 1.0e-02, train_loss: 0.6796, train_acc: 0.7510 test_loss: 2.0590, test_acc: 0.7070, best: 0.7191, time: 0:03:02
 Epoch: 58, lr: 1.0e-02, train_loss: 0.6806, train_acc: 0.7636 test_loss: 2.8522, test_acc: 0.7077, best: 0.7191, time: 0:03:02
 Epoch: 59, lr: 1.0e-02, train_loss: 0.6593, train_acc: 0.7618 test_loss: 20.8214, test_acc: 0.6961, best: 0.7191, time: 0:03:01
 Epoch: 60, lr: 1.0e-02, train_loss: 0.6503, train_acc: 0.7614 test_loss: 15.9699, test_acc: 0.6606, best: 0.7191, time: 0:03:01
 Epoch: 61, lr: 1.0e-02, train_loss: 0.7164, train_acc: 0.7434 test_loss: 1.1105, test_acc: 0.7039, best: 0.7191, time: 0:03:01
 Epoch: 62, lr: 1.0e-02, train_loss: 0.6633, train_acc: 0.7644 test_loss: 0.7941, test_acc: 0.7369, best: 0.7369, time: 0:03:02
 Epoch: 63, lr: 1.0e-02, train_loss: 0.6606, train_acc: 0.7680 test_loss: 0.8708, test_acc: 0.7271, best: 0.7369, time: 0:03:02
 Epoch: 64, lr: 1.0e-02, train_loss: 0.6227, train_acc: 0.7728 test_loss: 0.8214, test_acc: 0.7405, best: 0.7405, time: 0:03:02
 Epoch: 65, lr: 1.0e-02, train_loss: 0.6242, train_acc: 0.7736 test_loss: 0.7691, test_acc: 0.7516, best: 0.7516, time: 0:03:02
 Epoch: 66, lr: 1.0e-02, train_loss: 0.6027, train_acc: 0.7830 test_loss: 0.7598, test_acc: 0.7518, best: 0.7518, time: 0:03:02
 Epoch: 67, lr: 1.0e-02, train_loss: 0.5838, train_acc: 0.7918 test_loss: 0.7842, test_acc: 0.7345, best: 0.7518, time: 0:03:01
 Epoch: 68, lr: 1.0e-02, train_loss: 0.5738, train_acc: 0.7978 test_loss: 0.7371, test_acc: 0.7574, best: 0.7574, time: 0:03:02
 Epoch: 69, lr: 1.0e-02, train_loss: 0.5886, train_acc: 0.7954 test_loss: 0.7496, test_acc: 0.7545, best: 0.7574, time: 0:03:01
 Epoch: 70, lr: 1.0e-02, train_loss: 0.5524, train_acc: 0.8018 test_loss: 0.7871, test_acc: 0.7474, best: 0.7574, time: 0:03:02
 Epoch: 71, lr: 1.0e-02, train_loss: 0.5726, train_acc: 0.7960 test_loss: 0.9164, test_acc: 0.7512, best: 0.7574, time: 0:03:02
 Epoch: 72, lr: 1.0e-02, train_loss: 0.5502, train_acc: 0.8066 test_loss: 0.7574, test_acc: 0.7685, best: 0.7685, time: 0:03:02
 Epoch: 73, lr: 1.0e-02, train_loss: 0.5382, train_acc: 0.8108 test_loss: 0.7968, test_acc: 0.7492, best: 0.7685, time: 0:03:02
 Epoch: 74, lr: 1.0e-02, train_loss: 0.5253, train_acc: 0.8192 test_loss: 0.7318, test_acc: 0.7656, best: 0.7685, time: 0:03:01
 Epoch: 75, lr: 1.0e-02, train_loss: 0.5203, train_acc: 0.8158 test_loss: 0.7736, test_acc: 0.7570, best: 0.7685, time: 0:03:01
 Epoch: 76, lr: 1.0e-02, train_loss: 0.4939, train_acc: 0.8290 test_loss: 0.7418, test_acc: 0.7574, best: 0.7685, time: 0:03:02
 Epoch: 77, lr: 1.0e-02, train_loss: 0.4837, train_acc: 0.8280 test_loss: 0.7802, test_acc: 0.7681, best: 0.7685, time: 0:03:02
 Epoch: 78, lr: 1.0e-02, train_loss: 0.4741, train_acc: 0.8350 test_loss: 0.8371, test_acc: 0.7619, best: 0.7685, time: 0:03:02
 Epoch: 79, lr: 1.0e-02, train_loss: 0.4894, train_acc: 0.8292 test_loss: 0.7131, test_acc: 0.7674, best: 0.7685, time: 0:03:01
 Epoch: 80, lr: 1.0e-02, train_loss: 0.4535, train_acc: 0.8362 test_loss: 0.7917, test_acc: 0.7629, best: 0.7685, time: 0:03:02
 Epoch: 81, lr: 1.0e-02, train_loss: 0.4776, train_acc: 0.8290 test_loss: 0.7516, test_acc: 0.7536, best: 0.7685, time: 0:03:01
 Epoch: 82, lr: 1.0e-02, train_loss: 0.4570, train_acc: 0.8380 test_loss: 0.6882, test_acc: 0.7785, best: 0.7785, time: 0:03:02
 Epoch: 83, lr: 1.0e-02, train_loss: 0.4445, train_acc: 0.8426 test_loss: 0.7923, test_acc: 0.7552, best: 0.7785, time: 0:03:01
 Epoch: 84, lr: 1.0e-02, train_loss: 0.4468, train_acc: 0.8408 test_loss: 0.7028, test_acc: 0.7704, best: 0.7785, time: 0:03:02
 Epoch: 85, lr: 1.0e-02, train_loss: 0.4525, train_acc: 0.8390 test_loss: 0.7157, test_acc: 0.7738, best: 0.7785, time: 0:03:01
 Epoch: 86, lr: 1.0e-02, train_loss: 0.4235, train_acc: 0.8522 test_loss: 0.7651, test_acc: 0.7749, best: 0.7785, time: 0:03:02
 Epoch: 87, lr: 1.0e-02, train_loss: 0.4153, train_acc: 0.8544 test_loss: 1.0704, test_acc: 0.7548, best: 0.7785, time: 0:03:02
 Epoch: 88, lr: 1.0e-02, train_loss: 0.4132, train_acc: 0.8564 test_loss: 0.8524, test_acc: 0.7598, best: 0.7785, time: 0:03:01
 Epoch: 89, lr: 1.0e-02, train_loss: 0.4268, train_acc: 0.8490 test_loss: 0.7723, test_acc: 0.7766, best: 0.7785, time: 0:03:01
 Epoch: 90, lr: 1.0e-02, train_loss: 0.4193, train_acc: 0.8542 test_loss: 0.7015, test_acc: 0.7817, best: 0.7817, time: 0:03:02
 Epoch: 91, lr: 1.0e-02, train_loss: 0.4019, train_acc: 0.8612 test_loss: 0.8835, test_acc: 0.7729, best: 0.7817, time: 0:03:02
 Epoch: 92, lr: 1.0e-02, train_loss: 0.4105, train_acc: 0.8588 test_loss: 1.7875, test_acc: 0.7389, best: 0.7817, time: 0:03:01
 Epoch: 93, lr: 1.0e-02, train_loss: 0.4126, train_acc: 0.8544 test_loss: 1.2233, test_acc: 0.7495, best: 0.7817, time: 0:03:01
 Epoch: 94, lr: 1.0e-02, train_loss: 0.3860, train_acc: 0.8654 test_loss: 0.8324, test_acc: 0.7696, best: 0.7817, time: 0:03:01
 Epoch: 95, lr: 1.0e-02, train_loss: 0.4086, train_acc: 0.8578 test_loss: 0.6925, test_acc: 0.7808, best: 0.7817, time: 0:03:02
 Epoch: 96, lr: 1.0e-02, train_loss: 0.3878, train_acc: 0.8676 test_loss: 0.7041, test_acc: 0.7811, best: 0.7817, time: 0:03:01
 Epoch: 97, lr: 1.0e-02, train_loss: 0.3757, train_acc: 0.8638 test_loss: 0.7633, test_acc: 0.7729, best: 0.7817, time: 0:03:02
 Epoch: 98, lr: 1.0e-02, train_loss: 0.3517, train_acc: 0.8788 test_loss: 0.7232, test_acc: 0.7810, best: 0.7817, time: 0:03:02
 Epoch: 99, lr: 1.0e-02, train_loss: 0.3448, train_acc: 0.8764 test_loss: 0.7708, test_acc: 0.7799, best: 0.7817, time: 0:03:01
 Epoch: 100, lr: 1.0e-02, train_loss: 0.3545, train_acc: 0.8760 test_loss: 0.7376, test_acc: 0.7808, best: 0.7817, time: 0:03:01
 Epoch: 101, lr: 1.0e-02, train_loss: 0.3456, train_acc: 0.8808 test_loss: 0.7805, test_acc: 0.7861, best: 0.7861, time: 0:03:02
 Epoch: 102, lr: 1.0e-02, train_loss: 0.3567, train_acc: 0.8766 test_loss: 0.7447, test_acc: 0.7722, best: 0.7861, time: 0:03:02
 Epoch: 103, lr: 1.0e-02, train_loss: 0.3464, train_acc: 0.8824 test_loss: 0.7132, test_acc: 0.7843, best: 0.7861, time: 0:03:01
 Epoch: 104, lr: 1.0e-02, train_loss: 0.3316, train_acc: 0.8848 test_loss: 0.7664, test_acc: 0.7824, best: 0.7861, time: 0:03:01
 Epoch: 105, lr: 1.0e-02, train_loss: 0.3380, train_acc: 0.8838 test_loss: 0.8322, test_acc: 0.7774, best: 0.7861, time: 0:03:01
 Epoch: 106, lr: 1.0e-02, train_loss: 0.3400, train_acc: 0.8900 test_loss: 1.1199, test_acc: 0.7700, best: 0.7861, time: 0:03:01
 Epoch: 107, lr: 1.0e-02, train_loss: 0.3234, train_acc: 0.8876 test_loss: 0.6899, test_acc: 0.7980, best: 0.7980, time: 0:03:02
 Epoch: 108, lr: 1.0e-02, train_loss: 0.3165, train_acc: 0.8894 test_loss: 0.7771, test_acc: 0.7812, best: 0.7980, time: 0:03:01
 Epoch: 109, lr: 1.0e-02, train_loss: 0.3255, train_acc: 0.8852 test_loss: 0.7626, test_acc: 0.7923, best: 0.7980, time: 0:03:02
 Epoch: 110, lr: 1.0e-02, train_loss: 0.3483, train_acc: 0.8808 test_loss: 0.7169, test_acc: 0.8004, best: 0.8004, time: 0:03:02
 Epoch: 111, lr: 1.0e-02, train_loss: 0.3301, train_acc: 0.8876 test_loss: 0.7835, test_acc: 0.7788, best: 0.8004, time: 0:03:01
 Epoch: 112, lr: 1.0e-02, train_loss: 0.3058, train_acc: 0.8978 test_loss: 0.8561, test_acc: 0.7660, best: 0.8004, time: 0:03:01
 Epoch: 113, lr: 1.0e-02, train_loss: 0.3341, train_acc: 0.8836 test_loss: 0.8271, test_acc: 0.7778, best: 0.8004, time: 0:03:02
 Epoch: 114, lr: 1.0e-02, train_loss: 0.3301, train_acc: 0.8860 test_loss: 0.8021, test_acc: 0.7708, best: 0.8004, time: 0:03:01
 Epoch: 115, lr: 1.0e-02, train_loss: 0.3078, train_acc: 0.8918 test_loss: 0.7390, test_acc: 0.7980, best: 0.8004, time: 0:03:01
 Epoch: 116, lr: 1.0e-02, train_loss: 0.3090, train_acc: 0.8902 test_loss: 0.9942, test_acc: 0.7844, best: 0.8004, time: 0:03:02
 Epoch: 117, lr: 1.0e-02, train_loss: 0.3015, train_acc: 0.8948 test_loss: 0.9419, test_acc: 0.7802, best: 0.8004, time: 0:03:01
 Epoch: 118, lr: 1.0e-02, train_loss: 0.2945, train_acc: 0.8956 test_loss: 1.3252, test_acc: 0.7628, best: 0.8004, time: 0:03:01
 Epoch: 119, lr: 1.0e-02, train_loss: 0.3044, train_acc: 0.8920 test_loss: 1.9087, test_acc: 0.7341, best: 0.8004, time: 0:03:01
 Epoch: 120, lr: 1.0e-02, train_loss: 0.2980, train_acc: 0.8978 test_loss: 1.0808, test_acc: 0.7545, best: 0.8004, time: 0:03:02
 Epoch: 121, lr: 1.0e-02, train_loss: 0.2945, train_acc: 0.8962 test_loss: 1.0441, test_acc: 0.7741, best: 0.8004, time: 0:03:01
 Epoch: 122, lr: 1.0e-02, train_loss: 0.2980, train_acc: 0.8978 test_loss: 0.7941, test_acc: 0.7874, best: 0.8004, time: 0:03:01
 Epoch: 123, lr: 1.0e-02, train_loss: 0.2829, train_acc: 0.8990 test_loss: 0.7684, test_acc: 0.7864, best: 0.8004, time: 0:03:02
 Epoch: 124, lr: 1.0e-02, train_loss: 0.2902, train_acc: 0.8980 test_loss: 0.8971, test_acc: 0.7734, best: 0.8004, time: 0:03:01
 Epoch: 125, lr: 1.0e-02, train_loss: 0.2832, train_acc: 0.9016 test_loss: 3.5588, test_acc: 0.7662, best: 0.8004, time: 0:03:02
 Epoch: 126, lr: 1.0e-02, train_loss: 0.2737, train_acc: 0.9056 test_loss: 0.7977, test_acc: 0.7921, best: 0.8004, time: 0:03:02
 Epoch: 127, lr: 1.0e-02, train_loss: 0.2744, train_acc: 0.9044 test_loss: 1.8704, test_acc: 0.7721, best: 0.8004, time: 0:03:02
 Epoch: 128, lr: 1.0e-02, train_loss: 0.3075, train_acc: 0.8954 test_loss: 2.2147, test_acc: 0.7796, best: 0.8004, time: 0:03:01
 Epoch: 129, lr: 1.0e-02, train_loss: 0.2638, train_acc: 0.9112 test_loss: 0.9675, test_acc: 0.7917, best: 0.8004, time: 0:03:01
 Epoch: 130, lr: 1.0e-02, train_loss: 0.2722, train_acc: 0.9044 test_loss: 1.0334, test_acc: 0.7827, best: 0.8004, time: 0:03:02
 Epoch: 131, lr: 1.0e-02, train_loss: 0.2511, train_acc: 0.9164 test_loss: 0.8119, test_acc: 0.7911, best: 0.8004, time: 0:03:02
 Epoch: 132, lr: 1.0e-02, train_loss: 0.2644, train_acc: 0.9056 test_loss: 0.7825, test_acc: 0.7901, best: 0.8004, time: 0:03:01
 Epoch: 133, lr: 1.0e-02, train_loss: 0.2898, train_acc: 0.9016 test_loss: 0.7764, test_acc: 0.7985, best: 0.8004, time: 0:03:01
 Epoch: 134, lr: 1.0e-02, train_loss: 0.2735, train_acc: 0.9078 test_loss: 0.7394, test_acc: 0.7959, best: 0.8004, time: 0:03:02
 Epoch: 135, lr: 1.0e-02, train_loss: 0.2736, train_acc: 0.9052 test_loss: 0.8410, test_acc: 0.7919, best: 0.8004, time: 0:03:01
 Epoch: 136, lr: 1.0e-02, train_loss: 0.2458, train_acc: 0.9172 test_loss: 0.8035, test_acc: 0.7885, best: 0.8004, time: 0:03:01
 Epoch: 137, lr: 1.0e-02, train_loss: 0.2543, train_acc: 0.9098 test_loss: 0.8041, test_acc: 0.8011, best: 0.8011, time: 0:03:02
 Epoch: 138, lr: 1.0e-02, train_loss: 0.2607, train_acc: 0.9114 test_loss: 0.9007, test_acc: 0.7881, best: 0.8011, time: 0:03:02
 Epoch: 139, lr: 1.0e-02, train_loss: 0.2644, train_acc: 0.9120 test_loss: 0.8569, test_acc: 0.7947, best: 0.8011, time: 0:03:01
 Epoch: 140, lr: 1.0e-02, train_loss: 0.2555, train_acc: 0.9098 test_loss: 0.7474, test_acc: 0.8051, best: 0.8051, time: 0:03:02
 Epoch: 141, lr: 1.0e-02, train_loss: 0.2443, train_acc: 0.9140 test_loss: 0.7526, test_acc: 0.7936, best: 0.8051, time: 0:03:02
 Epoch: 142, lr: 1.0e-02, train_loss: 0.2454, train_acc: 0.9160 test_loss: 0.7768, test_acc: 0.8019, best: 0.8051, time: 0:03:02
 Epoch: 143, lr: 1.0e-02, train_loss: 0.2423, train_acc: 0.9150 test_loss: 0.8939, test_acc: 0.7865, best: 0.8051, time: 0:03:02
 Epoch: 144, lr: 1.0e-02, train_loss: 0.2494, train_acc: 0.9224 test_loss: 0.7227, test_acc: 0.8074, best: 0.8074, time: 0:03:02
 Epoch: 145, lr: 1.0e-02, train_loss: 0.2433, train_acc: 0.9184 test_loss: 0.7715, test_acc: 0.8050, best: 0.8074, time: 0:03:01
 Epoch: 146, lr: 1.0e-02, train_loss: 0.2309, train_acc: 0.9230 test_loss: 0.8927, test_acc: 0.7930, best: 0.8074, time: 0:03:02
 Epoch: 147, lr: 1.0e-02, train_loss: 0.2180, train_acc: 0.9262 test_loss: 0.8876, test_acc: 0.7766, best: 0.8074, time: 0:03:01
 Epoch: 148, lr: 1.0e-02, train_loss: 0.2464, train_acc: 0.9176 test_loss: 0.8179, test_acc: 0.7949, best: 0.8074, time: 0:03:02
 Epoch: 149, lr: 1.0e-02, train_loss: 0.2305, train_acc: 0.9194 test_loss: 0.9733, test_acc: 0.7765, best: 0.8074, time: 0:03:01
 Epoch: 150, lr: 1.0e-02, train_loss: 0.2351, train_acc: 0.9186 test_loss: 0.8205, test_acc: 0.7945, best: 0.8074, time: 0:03:02
 Epoch: 151, lr: 1.0e-02, train_loss: 0.2329, train_acc: 0.9200 test_loss: 0.8160, test_acc: 0.7911, best: 0.8074, time: 0:03:02
 Epoch: 152, lr: 1.0e-02, train_loss: 0.2160, train_acc: 0.9244 test_loss: 0.8036, test_acc: 0.8017, best: 0.8074, time: 0:03:02
 Epoch: 153, lr: 1.0e-02, train_loss: 0.2275, train_acc: 0.9212 test_loss: 0.7944, test_acc: 0.8011, best: 0.8074, time: 0:03:02
 Epoch: 154, lr: 1.0e-02, train_loss: 0.2323, train_acc: 0.9210 test_loss: 0.8197, test_acc: 0.7970, best: 0.8074, time: 0:03:02
 Epoch: 155, lr: 1.0e-02, train_loss: 0.2403, train_acc: 0.9218 test_loss: 0.9929, test_acc: 0.7739, best: 0.8074, time: 0:03:02
 Epoch: 156, lr: 1.0e-02, train_loss: 0.2158, train_acc: 0.9310 test_loss: 0.9942, test_acc: 0.7778, best: 0.8074, time: 0:03:01
 Epoch: 157, lr: 1.0e-02, train_loss: 0.2409, train_acc: 0.9156 test_loss: 0.9178, test_acc: 0.7907, best: 0.8074, time: 0:03:01
 Epoch: 158, lr: 1.0e-02, train_loss: 0.2259, train_acc: 0.9202 test_loss: 0.7875, test_acc: 0.8006, best: 0.8074, time: 0:03:02
 Epoch: 159, lr: 1.0e-02, train_loss: 0.2103, train_acc: 0.9236 test_loss: 0.8217, test_acc: 0.7946, best: 0.8074, time: 0:03:01
 Epoch: 160, lr: 1.0e-02, train_loss: 0.2171, train_acc: 0.9264 test_loss: 0.7745, test_acc: 0.8050, best: 0.8074, time: 0:03:01
 Epoch: 161, lr: 1.0e-02, train_loss: 0.2252, train_acc: 0.9220 test_loss: 0.7573, test_acc: 0.7970, best: 0.8074, time: 0:03:01
 Epoch: 162, lr: 1.0e-02, train_loss: 0.1907, train_acc: 0.9366 test_loss: 0.9508, test_acc: 0.8007, best: 0.8074, time: 0:03:02
 Epoch: 163, lr: 1.0e-02, train_loss: 0.2216, train_acc: 0.9264 test_loss: 1.1110, test_acc: 0.7879, best: 0.8074, time: 0:03:01
 Epoch: 164, lr: 1.0e-02, train_loss: 0.2061, train_acc: 0.9282 test_loss: 0.8659, test_acc: 0.7905, best: 0.8074, time: 0:03:01
 Epoch: 165, lr: 1.0e-02, train_loss: 0.2236, train_acc: 0.9248 test_loss: 0.7908, test_acc: 0.8039, best: 0.8074, time: 0:03:02
 Epoch: 166, lr: 1.0e-02, train_loss: 0.2166, train_acc: 0.9276 test_loss: 0.7623, test_acc: 0.8037, best: 0.8074, time: 0:03:01
 Epoch: 167, lr: 1.0e-02, train_loss: 0.2160, train_acc: 0.9218 test_loss: 0.8224, test_acc: 0.7986, best: 0.8074, time: 0:03:01
 Epoch: 168, lr: 1.0e-02, train_loss: 0.2200, train_acc: 0.9212 test_loss: 0.8169, test_acc: 0.7981, best: 0.8074, time: 0:03:02
 Epoch: 169, lr: 1.0e-02, train_loss: 0.2191, train_acc: 0.9278 test_loss: 0.8790, test_acc: 0.7899, best: 0.8074, time: 0:03:01
 Epoch: 170, lr: 1.0e-02, train_loss: 0.1988, train_acc: 0.9292 test_loss: 0.9472, test_acc: 0.7910, best: 0.8074, time: 0:03:01
 Epoch: 171, lr: 1.0e-02, train_loss: 0.1870, train_acc: 0.9362 test_loss: 1.1257, test_acc: 0.7695, best: 0.8074, time: 0:03:01
 Epoch: 172, lr: 1.0e-02, train_loss: 0.1815, train_acc: 0.9380 test_loss: 0.9665, test_acc: 0.7860, best: 0.8074, time: 0:03:02
 Epoch: 173, lr: 1.0e-02, train_loss: 0.2038, train_acc: 0.9296 test_loss: 0.8863, test_acc: 0.7956, best: 0.8074, time: 0:03:01
 Epoch: 174, lr: 1.0e-02, train_loss: 0.2092, train_acc: 0.9236 test_loss: 1.0251, test_acc: 0.7906, best: 0.8074, time: 0:03:01
 Epoch: 175, lr: 1.0e-02, train_loss: 0.2117, train_acc: 0.9260 test_loss: 1.2760, test_acc: 0.7917, best: 0.8074, time: 0:03:02
 Epoch: 176, lr: 1.0e-02, train_loss: 0.2118, train_acc: 0.9238 test_loss: 17.1054, test_acc: 0.7340, best: 0.8074, time: 0:03:01
 Epoch: 177, lr: 1.0e-02, train_loss: 0.2231, train_acc: 0.9212 test_loss: 2.2522, test_acc: 0.7672, best: 0.8074, time: 0:03:01
 Epoch: 178, lr: 1.0e-02, train_loss: 0.2146, train_acc: 0.9294 test_loss: 0.8292, test_acc: 0.8110, best: 0.8110, time: 0:03:02
 Epoch: 179, lr: 1.0e-02, train_loss: 0.2026, train_acc: 0.9296 test_loss: 0.8468, test_acc: 0.7980, best: 0.8110, time: 0:03:02
 Epoch: 180, lr: 2.0e-03, train_loss: 0.1754, train_acc: 0.9382 test_loss: 0.7291, test_acc: 0.8220, best: 0.8220, time: 0:03:02
 Epoch: 181, lr: 2.0e-03, train_loss: 0.1521, train_acc: 0.9472 test_loss: 0.8152, test_acc: 0.8214, best: 0.8220, time: 0:03:01
 Epoch: 182, lr: 2.0e-03, train_loss: 0.1462, train_acc: 0.9502 test_loss: 0.7058, test_acc: 0.8275, best: 0.8275, time: 0:03:02
 Epoch: 183, lr: 2.0e-03, train_loss: 0.1438, train_acc: 0.9510 test_loss: 0.7849, test_acc: 0.8260, best: 0.8275, time: 0:03:01
 Epoch: 184, lr: 2.0e-03, train_loss: 0.1266, train_acc: 0.9592 test_loss: 0.7570, test_acc: 0.8287, best: 0.8287, time: 0:03:02
 Epoch: 185, lr: 2.0e-03, train_loss: 0.1336, train_acc: 0.9550 test_loss: 0.7528, test_acc: 0.8279, best: 0.8287, time: 0:03:01
 Epoch: 186, lr: 2.0e-03, train_loss: 0.1430, train_acc: 0.9530 test_loss: 0.9328, test_acc: 0.8203, best: 0.8287, time: 0:03:02
 Epoch: 187, lr: 2.0e-03, train_loss: 0.1380, train_acc: 0.9538 test_loss: 0.7343, test_acc: 0.8255, best: 0.8287, time: 0:03:01
 Epoch: 188, lr: 2.0e-03, train_loss: 0.1401, train_acc: 0.9520 test_loss: 0.7538, test_acc: 0.8301, best: 0.8301, time: 0:03:02
 Epoch: 189, lr: 2.0e-03, train_loss: 0.1388, train_acc: 0.9494 test_loss: 0.9758, test_acc: 0.8224, best: 0.8301, time: 0:03:02
 Epoch: 190, lr: 2.0e-03, train_loss: 0.1332, train_acc: 0.9552 test_loss: 0.8488, test_acc: 0.8251, best: 0.8301, time: 0:03:01
 Epoch: 191, lr: 2.0e-03, train_loss: 0.1306, train_acc: 0.9544 test_loss: 0.7325, test_acc: 0.8241, best: 0.8301, time: 0:03:01
 Epoch: 192, lr: 2.0e-03, train_loss: 0.1083, train_acc: 0.9634 test_loss: 0.7726, test_acc: 0.8285, best: 0.8301, time: 0:03:01
 Epoch: 193, lr: 2.0e-03, train_loss: 0.1233, train_acc: 0.9586 test_loss: 0.7531, test_acc: 0.8289, best: 0.8301, time: 0:03:01
 Epoch: 194, lr: 2.0e-03, train_loss: 0.1185, train_acc: 0.9602 test_loss: 0.7846, test_acc: 0.8277, best: 0.8301, time: 0:03:01
 Epoch: 195, lr: 2.0e-03, train_loss: 0.1158, train_acc: 0.9616 test_loss: 0.8319, test_acc: 0.8225, best: 0.8301, time: 0:03:01
 Epoch: 196, lr: 2.0e-03, train_loss: 0.1178, train_acc: 0.9616 test_loss: 0.8824, test_acc: 0.8201, best: 0.8301, time: 0:03:02
 Epoch: 197, lr: 2.0e-03, train_loss: 0.1302, train_acc: 0.9552 test_loss: 0.7277, test_acc: 0.8260, best: 0.8301, time: 0:03:01
 Epoch: 198, lr: 2.0e-03, train_loss: 0.1221, train_acc: 0.9588 test_loss: 0.8194, test_acc: 0.8237, best: 0.8301, time: 0:03:01
 Epoch: 199, lr: 2.0e-03, train_loss: 0.1064, train_acc: 0.9630 test_loss: 0.9066, test_acc: 0.8215, best: 0.8301, time: 0:03:02
 Epoch: 200, lr: 2.0e-03, train_loss: 0.1318, train_acc: 0.9552 test_loss: 0.8045, test_acc: 0.8193, best: 0.8301, time: 0:03:01
 Epoch: 201, lr: 2.0e-03, train_loss: 0.1127, train_acc: 0.9596 test_loss: 0.7598, test_acc: 0.8264, best: 0.8301, time: 0:03:01
 Epoch: 202, lr: 2.0e-03, train_loss: 0.1192, train_acc: 0.9612 test_loss: 1.3913, test_acc: 0.8019, best: 0.8301, time: 0:03:02
 Epoch: 203, lr: 2.0e-03, train_loss: 0.1115, train_acc: 0.9628 test_loss: 0.8074, test_acc: 0.8271, best: 0.8301, time: 0:03:01
 Epoch: 204, lr: 2.0e-03, train_loss: 0.1174, train_acc: 0.9578 test_loss: 0.8781, test_acc: 0.8161, best: 0.8301, time: 0:03:01
 Epoch: 205, lr: 2.0e-03, train_loss: 0.1144, train_acc: 0.9614 test_loss: 0.7849, test_acc: 0.8260, best: 0.8301, time: 0:03:01
 Epoch: 206, lr: 2.0e-03, train_loss: 0.1142, train_acc: 0.9620 test_loss: 0.9223, test_acc: 0.8183, best: 0.8301, time: 0:03:01
 Epoch: 207, lr: 2.0e-03, train_loss: 0.1150, train_acc: 0.9602 test_loss: 0.8217, test_acc: 0.8211, best: 0.8301, time: 0:03:01
 Epoch: 208, lr: 2.0e-03, train_loss: 0.1069, train_acc: 0.9628 test_loss: 0.8860, test_acc: 0.8164, best: 0.8301, time: 0:03:01
 Epoch: 209, lr: 2.0e-03, train_loss: 0.0961, train_acc: 0.9676 test_loss: 0.8430, test_acc: 0.8191, best: 0.8301, time: 0:03:02
 Epoch: 210, lr: 2.0e-03, train_loss: 0.1103, train_acc: 0.9658 test_loss: 1.0709, test_acc: 0.8109, best: 0.8301, time: 0:03:01
 Epoch: 211, lr: 2.0e-03, train_loss: 0.1128, train_acc: 0.9640 test_loss: 0.8708, test_acc: 0.8151, best: 0.8301, time: 0:03:01
 Epoch: 212, lr: 2.0e-03, train_loss: 0.1098, train_acc: 0.9606 test_loss: 0.8660, test_acc: 0.8159, best: 0.8301, time: 0:03:01
 Epoch: 213, lr: 2.0e-03, train_loss: 0.1135, train_acc: 0.9606 test_loss: 1.0971, test_acc: 0.8080, best: 0.8301, time: 0:03:01
 Epoch: 214, lr: 2.0e-03, train_loss: 0.0986, train_acc: 0.9652 test_loss: 0.9291, test_acc: 0.8127, best: 0.8301, time: 0:03:01
 Epoch: 215, lr: 2.0e-03, train_loss: 0.1096, train_acc: 0.9604 test_loss: 0.9319, test_acc: 0.8126, best: 0.8301, time: 0:03:02
 Epoch: 216, lr: 2.0e-03, train_loss: 0.1107, train_acc: 0.9616 test_loss: 0.8308, test_acc: 0.8246, best: 0.8301, time: 0:03:01
 Epoch: 217, lr: 2.0e-03, train_loss: 0.1008, train_acc: 0.9648 test_loss: 0.8297, test_acc: 0.8205, best: 0.8301, time: 0:03:01
 Epoch: 218, lr: 2.0e-03, train_loss: 0.1135, train_acc: 0.9600 test_loss: 0.7729, test_acc: 0.8310, best: 0.8310, time: 0:03:02
 Epoch: 219, lr: 2.0e-03, train_loss: 0.1004, train_acc: 0.9654 test_loss: 1.0991, test_acc: 0.8035, best: 0.8310, time: 0:03:01
 Epoch: 220, lr: 2.0e-03, train_loss: 0.1140, train_acc: 0.9618 test_loss: 0.8781, test_acc: 0.8155, best: 0.8310, time: 0:03:01
 Epoch: 221, lr: 2.0e-03, train_loss: 0.1132, train_acc: 0.9584 test_loss: 0.8092, test_acc: 0.8241, best: 0.8310, time: 0:03:01
 Epoch: 222, lr: 2.0e-03, train_loss: 0.0936, train_acc: 0.9666 test_loss: 1.4950, test_acc: 0.8023, best: 0.8310, time: 0:03:01
 Epoch: 223, lr: 2.0e-03, train_loss: 0.1019, train_acc: 0.9670 test_loss: 1.0069, test_acc: 0.8133, best: 0.8310, time: 0:03:01
 Epoch: 224, lr: 2.0e-03, train_loss: 0.1112, train_acc: 0.9628 test_loss: 0.8782, test_acc: 0.8120, best: 0.8310, time: 0:03:01
 Epoch: 225, lr: 2.0e-03, train_loss: 0.1079, train_acc: 0.9638 test_loss: 0.9195, test_acc: 0.8200, best: 0.8310, time: 0:03:01
 Epoch: 226, lr: 2.0e-03, train_loss: 0.1195, train_acc: 0.9584 test_loss: 0.9749, test_acc: 0.8103, best: 0.8310, time: 0:03:01
 Epoch: 227, lr: 2.0e-03, train_loss: 0.1151, train_acc: 0.9598 test_loss: 1.3661, test_acc: 0.7967, best: 0.8310, time: 0:03:01
 Epoch: 228, lr: 2.0e-03, train_loss: 0.0994, train_acc: 0.9656 test_loss: 0.8630, test_acc: 0.8205, best: 0.8310, time: 0:03:01
 Epoch: 229, lr: 2.0e-03, train_loss: 0.1116, train_acc: 0.9632 test_loss: 0.9358, test_acc: 0.8107, best: 0.8310, time: 0:03:02
 Epoch: 230, lr: 2.0e-03, train_loss: 0.1007, train_acc: 0.9648 test_loss: 0.8381, test_acc: 0.8127, best: 0.8310, time: 0:03:01
 Epoch: 231, lr: 2.0e-03, train_loss: 0.1119, train_acc: 0.9592 test_loss: 0.9488, test_acc: 0.8111, best: 0.8310, time: 0:03:02
 Epoch: 232, lr: 2.0e-03, train_loss: 0.1016, train_acc: 0.9634 test_loss: 1.1700, test_acc: 0.7991, best: 0.8310, time: 0:03:01
 Epoch: 233, lr: 2.0e-03, train_loss: 0.1034, train_acc: 0.9646 test_loss: 0.8448, test_acc: 0.8141, best: 0.8310, time: 0:03:01
 Epoch: 234, lr: 2.0e-03, train_loss: 0.1049, train_acc: 0.9636 test_loss: 1.1231, test_acc: 0.8049, best: 0.8310, time: 0:03:01
 Epoch: 235, lr: 2.0e-03, train_loss: 0.0937, train_acc: 0.9664 test_loss: 1.3544, test_acc: 0.7984, best: 0.8310, time: 0:03:01
 Epoch: 236, lr: 2.0e-03, train_loss: 0.0978, train_acc: 0.9664 test_loss: 1.2570, test_acc: 0.8057, best: 0.8310, time: 0:03:01
 Epoch: 237, lr: 2.0e-03, train_loss: 0.0967, train_acc: 0.9672 test_loss: 1.4711, test_acc: 0.8051, best: 0.8310, time: 0:03:01
 Epoch: 238, lr: 2.0e-03, train_loss: 0.1048, train_acc: 0.9630 test_loss: 1.0572, test_acc: 0.8044, best: 0.8310, time: 0:03:02
 Epoch: 239, lr: 2.0e-03, train_loss: 0.1072, train_acc: 0.9610 test_loss: 0.8119, test_acc: 0.8231, best: 0.8310, time: 0:03:01
 Epoch: 240, lr: 4.0e-04, train_loss: 0.1096, train_acc: 0.9630 test_loss: 1.0218, test_acc: 0.8087, best: 0.8310, time: 0:03:01
 Epoch: 241, lr: 4.0e-04, train_loss: 0.0927, train_acc: 0.9728 test_loss: 0.8303, test_acc: 0.8203, best: 0.8310, time: 0:03:02
 Epoch: 242, lr: 4.0e-04, train_loss: 0.1000, train_acc: 0.9652 test_loss: 0.8333, test_acc: 0.8177, best: 0.8310, time: 0:03:01
 Epoch: 243, lr: 4.0e-04, train_loss: 0.0957, train_acc: 0.9660 test_loss: 1.3174, test_acc: 0.8064, best: 0.8310, time: 0:03:01
 Epoch: 244, lr: 4.0e-04, train_loss: 0.1012, train_acc: 0.9652 test_loss: 1.1843, test_acc: 0.8040, best: 0.8310, time: 0:03:02
 Epoch: 245, lr: 4.0e-04, train_loss: 0.1015, train_acc: 0.9676 test_loss: 0.9774, test_acc: 0.8159, best: 0.8310, time: 0:03:01
 Epoch: 246, lr: 4.0e-04, train_loss: 0.1033, train_acc: 0.9644 test_loss: 0.9721, test_acc: 0.8184, best: 0.8310, time: 0:03:01
 Epoch: 247, lr: 4.0e-04, train_loss: 0.0818, train_acc: 0.9702 test_loss: 0.9789, test_acc: 0.8151, best: 0.8310, time: 0:03:02
 Epoch: 248, lr: 4.0e-04, train_loss: 0.0992, train_acc: 0.9670 test_loss: 0.8016, test_acc: 0.8271, best: 0.8310, time: 0:03:01
 Epoch: 249, lr: 4.0e-04, train_loss: 0.1063, train_acc: 0.9652 test_loss: 1.0256, test_acc: 0.8104, best: 0.8310, time: 0:03:01
 Epoch: 250, lr: 4.0e-04, train_loss: 0.0975, train_acc: 0.9670 test_loss: 0.8791, test_acc: 0.8180, best: 0.8310, time: 0:03:01
 Epoch: 251, lr: 4.0e-04, train_loss: 0.0926, train_acc: 0.9682 test_loss: 0.8374, test_acc: 0.8210, best: 0.8310, time: 0:03:01
 Epoch: 252, lr: 4.0e-04, train_loss: 0.0918, train_acc: 0.9684 test_loss: 1.0305, test_acc: 0.8174, best: 0.8310, time: 0:03:02
 Epoch: 253, lr: 4.0e-04, train_loss: 0.1018, train_acc: 0.9672 test_loss: 0.8848, test_acc: 0.8184, best: 0.8310, time: 0:03:01
 Epoch: 254, lr: 4.0e-04, train_loss: 0.1077, train_acc: 0.9658 test_loss: 1.0771, test_acc: 0.8157, best: 0.8310, time: 0:03:02
 Epoch: 255, lr: 4.0e-04, train_loss: 0.0925, train_acc: 0.9686 test_loss: 0.9294, test_acc: 0.8194, best: 0.8310, time: 0:03:01
 Epoch: 256, lr: 4.0e-04, train_loss: 0.0923, train_acc: 0.9696 test_loss: 1.0854, test_acc: 0.8121, best: 0.8310, time: 0:03:01
 Epoch: 257, lr: 4.0e-04, train_loss: 0.0923, train_acc: 0.9694 test_loss: 1.0532, test_acc: 0.8140, best: 0.8310, time: 0:03:01
 Epoch: 258, lr: 4.0e-04, train_loss: 0.0927, train_acc: 0.9690 test_loss: 0.9989, test_acc: 0.8130, best: 0.8310, time: 0:03:01
 Epoch: 259, lr: 4.0e-04, train_loss: 0.0896, train_acc: 0.9692 test_loss: 0.8407, test_acc: 0.8203, best: 0.8310, time: 0:03:01
 Epoch: 260, lr: 4.0e-04, train_loss: 0.0822, train_acc: 0.9720 test_loss: 0.9894, test_acc: 0.8173, best: 0.8310, time: 0:03:01
 Epoch: 261, lr: 4.0e-04, train_loss: 0.0923, train_acc: 0.9678 test_loss: 1.5766, test_acc: 0.7959, best: 0.8310, time: 0:03:01
 Epoch: 262, lr: 4.0e-04, train_loss: 0.0901, train_acc: 0.9686 test_loss: 1.6129, test_acc: 0.8009, best: 0.8310, time: 0:03:02
 Epoch: 263, lr: 4.0e-04, train_loss: 0.0876, train_acc: 0.9684 test_loss: 0.9858, test_acc: 0.8170, best: 0.8310, time: 0:03:02
 Epoch: 264, lr: 4.0e-04, train_loss: 0.1081, train_acc: 0.9620 test_loss: 1.2285, test_acc: 0.8053, best: 0.8310, time: 0:03:01
 Epoch: 265, lr: 4.0e-04, train_loss: 0.0913, train_acc: 0.9676 test_loss: 1.3635, test_acc: 0.8030, best: 0.8310, time: 0:03:01
 Epoch: 266, lr: 4.0e-04, train_loss: 0.0906, train_acc: 0.9694 test_loss: 0.9191, test_acc: 0.8184, best: 0.8310, time: 0:03:01
 Epoch: 267, lr: 4.0e-04, train_loss: 0.0942, train_acc: 0.9692 test_loss: 0.7877, test_acc: 0.8294, best: 0.8310, time: 0:03:02
 Epoch: 268, lr: 4.0e-04, train_loss: 0.0905, train_acc: 0.9712 test_loss: 0.8911, test_acc: 0.8224, best: 0.8310, time: 0:03:01
 Epoch: 269, lr: 4.0e-04, train_loss: 0.0935, train_acc: 0.9660 test_loss: 1.0388, test_acc: 0.8101, best: 0.8310, time: 0:03:01
 Epoch: 270, lr: 8.0e-05, train_loss: 0.0807, train_acc: 0.9722 test_loss: 1.1969, test_acc: 0.8085, best: 0.8310, time: 0:03:02
 Epoch: 271, lr: 8.0e-05, train_loss: 0.0816, train_acc: 0.9742 test_loss: 0.8558, test_acc: 0.8241, best: 0.8310, time: 0:03:00
 Epoch: 272, lr: 8.0e-05, train_loss: 0.0860, train_acc: 0.9722 test_loss: 0.9298, test_acc: 0.8183, best: 0.8310, time: 0:03:01
 Epoch: 273, lr: 8.0e-05, train_loss: 0.0993, train_acc: 0.9658 test_loss: 1.1285, test_acc: 0.8120, best: 0.8310, time: 0:03:01
 Epoch: 274, lr: 8.0e-05, train_loss: 0.0794, train_acc: 0.9722 test_loss: 1.0669, test_acc: 0.8121, best: 0.8310, time: 0:03:01
 Epoch: 275, lr: 8.0e-05, train_loss: 0.0949, train_acc: 0.9696 test_loss: 0.8774, test_acc: 0.8217, best: 0.8310, time: 0:03:01
 Epoch: 276, lr: 8.0e-05, train_loss: 0.0930, train_acc: 0.9674 test_loss: 0.8846, test_acc: 0.8206, best: 0.8310, time: 0:03:01
 Epoch: 277, lr: 8.0e-05, train_loss: 0.1088, train_acc: 0.9662 test_loss: 1.1381, test_acc: 0.8094, best: 0.8310, time: 0:03:01
 Epoch: 278, lr: 8.0e-05, train_loss: 0.0878, train_acc: 0.9712 test_loss: 1.1415, test_acc: 0.8079, best: 0.8310, time: 0:03:01
 Epoch: 279, lr: 8.0e-05, train_loss: 0.0892, train_acc: 0.9692 test_loss: 1.0484, test_acc: 0.8153, best: 0.8310, time: 0:03:01
 Epoch: 280, lr: 8.0e-05, train_loss: 0.0834, train_acc: 0.9740 test_loss: 0.9882, test_acc: 0.8203, best: 0.8310, time: 0:03:01
 Epoch: 281, lr: 8.0e-05, train_loss: 0.0938, train_acc: 0.9672 test_loss: 0.7549, test_acc: 0.8324, best: 0.8324, time: 0:03:02
 Epoch: 282, lr: 8.0e-05, train_loss: 0.0908, train_acc: 0.9696 test_loss: 1.3618, test_acc: 0.8055, best: 0.8324, time: 0:03:01
 Epoch: 283, lr: 8.0e-05, train_loss: 0.0929, train_acc: 0.9696 test_loss: 1.4174, test_acc: 0.8087, best: 0.8324, time: 0:03:01
 Epoch: 284, lr: 8.0e-05, train_loss: 0.0929, train_acc: 0.9674 test_loss: 0.9662, test_acc: 0.8184, best: 0.8324, time: 0:03:01
 Epoch: 285, lr: 8.0e-05, train_loss: 0.0981, train_acc: 0.9646 test_loss: 1.3538, test_acc: 0.8073, best: 0.8324, time: 0:03:01
 Epoch: 286, lr: 8.0e-05, train_loss: 0.0916, train_acc: 0.9688 test_loss: 0.9645, test_acc: 0.8205, best: 0.8324, time: 0:03:01
 Epoch: 287, lr: 8.0e-05, train_loss: 0.0938, train_acc: 0.9680 test_loss: 0.8933, test_acc: 0.8213, best: 0.8324, time: 0:03:01
 Epoch: 288, lr: 8.0e-05, train_loss: 0.1006, train_acc: 0.9652 test_loss: 0.8688, test_acc: 0.8233, best: 0.8324, time: 0:03:00
 Epoch: 289, lr: 8.0e-05, train_loss: 0.0816, train_acc: 0.9730 test_loss: 1.1071, test_acc: 0.8155, best: 0.8324, time: 0:03:01
 Epoch: 290, lr: 8.0e-05, train_loss: 0.0802, train_acc: 0.9738 test_loss: 0.9893, test_acc: 0.8086, best: 0.8324, time: 0:03:01
 Epoch: 291, lr: 8.0e-05, train_loss: 0.1009, train_acc: 0.9654 test_loss: 0.8141, test_acc: 0.8239, best: 0.8324, time: 0:03:01
 Epoch: 292, lr: 8.0e-05, train_loss: 0.0925, train_acc: 0.9690 test_loss: 1.0958, test_acc: 0.8163, best: 0.8324, time: 0:03:01
 Epoch: 293, lr: 8.0e-05, train_loss: 0.0939, train_acc: 0.9676 test_loss: 1.3448, test_acc: 0.8061, best: 0.8324, time: 0:03:01
 Epoch: 294, lr: 8.0e-05, train_loss: 0.0910, train_acc: 0.9686 test_loss: 0.8984, test_acc: 0.8199, best: 0.8324, time: 0:03:01
 Epoch: 295, lr: 8.0e-05, train_loss: 0.0830, train_acc: 0.9750 test_loss: 0.8251, test_acc: 0.8241, best: 0.8324, time: 0:03:01
 Epoch: 296, lr: 8.0e-05, train_loss: 0.0849, train_acc: 0.9708 test_loss: 0.8763, test_acc: 0.8227, best: 0.8324, time: 0:03:01
 Epoch: 297, lr: 8.0e-05, train_loss: 0.0906, train_acc: 0.9666 test_loss: 1.0439, test_acc: 0.8174, best: 0.8324, time: 0:03:01
 Epoch: 298, lr: 8.0e-05, train_loss: 0.0916, train_acc: 0.9692 test_loss: 1.0029, test_acc: 0.8144, best: 0.8324, time: 0:03:02
 Epoch: 299, lr: 8.0e-05, train_loss: 0.0896, train_acc: 0.9710 test_loss: 0.8720, test_acc: 0.8240, best: 0.8324, time: 0:03:01
 Highest accuracy: 0.8324