
 Run on time: 2022-06-24 20:04:25.158982

 Arguments:
	 gpu                  : True
	 log                  : True
	 seed                 : 0
	 momentum             : 0.9
	 amsgrad              : True
	 dataset              : STL10
	 batch_size           : 8
	 architecture         : RESNET50_LIP_2222
	 im_size              : 224
	 relu_threshold       : 4.0
	 learning_rate        : 0.01
	 pretrained_backbone  : 
	 pretrained_ann       : 
	 weight_decay         : 0.0
	 test_only            : False
	 epochs               : 300
	 lr_interval          : [180, 240, 270]
	 lr_reduce            : 5
	 optimizer            : SGD
	 dropout              : 0.2
	 kernel_size          : 3
	 dont_save            : False
	 visualize            : False
	 devices              : 0,1
 DataParallel(
  (module): NetworkByName(
    (net): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (pool): Pool2d(
          (logit): Sequential(
            (pool_weight): LIP_BASE(
              (logit): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Sigmoid()
              )
            )
          )
          (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (pool): Pool2d(
          (logit): Sequential(
            (pool_weight): LIP_BASE(
              (logit): Sequential(
                (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
                (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Sigmoid()
              )
            )
          )
          (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (pool): Pool2d(
          (logit): Sequential(
            (pool_weight): LIP_BASE(
              (logit): Sequential(
                (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
                (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Sigmoid()
              )
            )
          )
          (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        )
      )
      (layer4): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (pool): Pool2d(
          (logit): Sequential(
            (pool_weight): LIP_BASE(
              (logit): Sequential(
                (0): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1))
                (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Sigmoid()
              )
            )
          )
          (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        )
      )
      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
      (fc): Linear(in_features=2048, out_features=10, bias=True)
    )
  )
)
 SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0
)
 Epoch: 1, lr: 1.0e-02, train_loss: 2.6625, train_acc: 0.1708 test_loss: 2.2755, test_acc: 0.2382, best: 0.2382, time: 0:02:18
 Epoch: 2, lr: 1.0e-02, train_loss: 2.0220, train_acc: 0.2286 test_loss: 1.8332, test_acc: 0.2906, best: 0.2906, time: 0:02:15
 Epoch: 3, lr: 1.0e-02, train_loss: 1.9011, train_acc: 0.2708 test_loss: 1.7839, test_acc: 0.3225, best: 0.3225, time: 0:02:15
 Epoch: 4, lr: 1.0e-02, train_loss: 1.8329, train_acc: 0.2944 test_loss: 1.6563, test_acc: 0.3601, best: 0.3601, time: 0:02:15
 Epoch: 5, lr: 1.0e-02, train_loss: 1.7966, train_acc: 0.3114 test_loss: 1.6954, test_acc: 0.3541, best: 0.3601, time: 0:02:14
 Epoch: 6, lr: 1.0e-02, train_loss: 1.7272, train_acc: 0.3420 test_loss: 1.7722, test_acc: 0.3441, best: 0.3601, time: 0:02:14
 Epoch: 7, lr: 1.0e-02, train_loss: 1.7031, train_acc: 0.3626 test_loss: 1.6138, test_acc: 0.3895, best: 0.3895, time: 0:02:15
 Epoch: 8, lr: 1.0e-02, train_loss: 1.6603, train_acc: 0.3786 test_loss: 1.5191, test_acc: 0.4285, best: 0.4285, time: 0:02:15
 Epoch: 9, lr: 1.0e-02, train_loss: 1.6082, train_acc: 0.4034 test_loss: 1.6994, test_acc: 0.4074, best: 0.4285, time: 0:02:15
 Epoch: 10, lr: 1.0e-02, train_loss: 1.5640, train_acc: 0.4122 test_loss: 1.5735, test_acc: 0.4155, best: 0.4285, time: 0:02:14
 Epoch: 11, lr: 1.0e-02, train_loss: 1.5352, train_acc: 0.4300 test_loss: 1.4916, test_acc: 0.4526, best: 0.4526, time: 0:02:15
 Epoch: 12, lr: 1.0e-02, train_loss: 1.5086, train_acc: 0.4396 test_loss: 1.4625, test_acc: 0.4740, best: 0.4740, time: 0:02:15
 Epoch: 13, lr: 1.0e-02, train_loss: 1.4387, train_acc: 0.4652 test_loss: 1.5842, test_acc: 0.4592, best: 0.4740, time: 0:02:14
 Epoch: 14, lr: 1.0e-02, train_loss: 1.4495, train_acc: 0.4718 test_loss: 1.3556, test_acc: 0.5066, best: 0.5066, time: 0:02:16
 Epoch: 15, lr: 1.0e-02, train_loss: 1.3993, train_acc: 0.4836 test_loss: 1.3696, test_acc: 0.4983, best: 0.5066, time: 0:02:14
 Epoch: 16, lr: 1.0e-02, train_loss: 1.3814, train_acc: 0.4990 test_loss: 1.3118, test_acc: 0.5272, best: 0.5272, time: 0:02:15
 Epoch: 17, lr: 1.0e-02, train_loss: 1.3605, train_acc: 0.4966 test_loss: 1.4388, test_acc: 0.5164, best: 0.5272, time: 0:02:14
 Epoch: 18, lr: 1.0e-02, train_loss: 1.3380, train_acc: 0.5086 test_loss: 1.2891, test_acc: 0.5477, best: 0.5477, time: 0:02:15
 Epoch: 19, lr: 1.0e-02, train_loss: 1.3043, train_acc: 0.5168 test_loss: 1.2418, test_acc: 0.5456, best: 0.5477, time: 0:02:14
 Epoch: 20, lr: 1.0e-02, train_loss: 1.2570, train_acc: 0.5440 test_loss: 1.3333, test_acc: 0.5329, best: 0.5477, time: 0:02:14
 Epoch: 21, lr: 1.0e-02, train_loss: 1.2545, train_acc: 0.5480 test_loss: 1.2850, test_acc: 0.5436, best: 0.5477, time: 0:02:14
 Epoch: 22, lr: 1.0e-02, train_loss: 1.2017, train_acc: 0.5662 test_loss: 1.2245, test_acc: 0.5779, best: 0.5779, time: 0:02:15
 Epoch: 23, lr: 1.0e-02, train_loss: 1.1822, train_acc: 0.5738 test_loss: 1.2735, test_acc: 0.5587, best: 0.5779, time: 0:02:14
 Epoch: 24, lr: 1.0e-02, train_loss: 1.1600, train_acc: 0.5744 test_loss: 1.1970, test_acc: 0.5807, best: 0.5807, time: 0:02:15
 Epoch: 25, lr: 1.0e-02, train_loss: 1.1516, train_acc: 0.5824 test_loss: 1.2898, test_acc: 0.5720, best: 0.5807, time: 0:02:14
 Epoch: 26, lr: 1.0e-02, train_loss: 1.1252, train_acc: 0.5880 test_loss: 1.1125, test_acc: 0.6106, best: 0.6106, time: 0:02:15
 Epoch: 27, lr: 1.0e-02, train_loss: 1.0881, train_acc: 0.5974 test_loss: 1.2304, test_acc: 0.5914, best: 0.6106, time: 0:02:13
 Epoch: 28, lr: 1.0e-02, train_loss: 1.0851, train_acc: 0.6074 test_loss: 1.1110, test_acc: 0.6188, best: 0.6188, time: 0:02:11
 Epoch: 29, lr: 1.0e-02, train_loss: 1.0645, train_acc: 0.6224 test_loss: 1.0867, test_acc: 0.6240, best: 0.6240, time: 0:02:11
 Epoch: 30, lr: 1.0e-02, train_loss: 1.0582, train_acc: 0.6220 test_loss: 1.2301, test_acc: 0.6028, best: 0.6240, time: 0:02:11
 Epoch: 31, lr: 1.0e-02, train_loss: 1.0306, train_acc: 0.6258 test_loss: 1.0784, test_acc: 0.6376, best: 0.6376, time: 0:02:11
 Epoch: 32, lr: 1.0e-02, train_loss: 1.0145, train_acc: 0.6350 test_loss: 1.1184, test_acc: 0.6318, best: 0.6376, time: 0:02:10
 Epoch: 33, lr: 1.0e-02, train_loss: 0.9750, train_acc: 0.6448 test_loss: 1.5019, test_acc: 0.5464, best: 0.6376, time: 0:02:10
 Epoch: 34, lr: 1.0e-02, train_loss: 0.9961, train_acc: 0.6404 test_loss: 0.9941, test_acc: 0.6587, best: 0.6587, time: 0:02:11
 Epoch: 35, lr: 1.0e-02, train_loss: 0.9709, train_acc: 0.6516 test_loss: 1.1170, test_acc: 0.6401, best: 0.6587, time: 0:02:10
 Epoch: 36, lr: 1.0e-02, train_loss: 0.9595, train_acc: 0.6524 test_loss: 1.0505, test_acc: 0.6490, best: 0.6587, time: 0:02:10
 Epoch: 37, lr: 1.0e-02, train_loss: 0.9356, train_acc: 0.6696 test_loss: 0.9842, test_acc: 0.6663, best: 0.6663, time: 0:02:10
 Epoch: 38, lr: 1.0e-02, train_loss: 0.9277, train_acc: 0.6698 test_loss: 0.9759, test_acc: 0.6623, best: 0.6663, time: 0:02:10
 Epoch: 39, lr: 1.0e-02, train_loss: 0.9094, train_acc: 0.6712 test_loss: 0.9452, test_acc: 0.6773, best: 0.6773, time: 0:02:10
 Epoch: 40, lr: 1.0e-02, train_loss: 0.8976, train_acc: 0.6838 test_loss: 1.0317, test_acc: 0.6689, best: 0.6773, time: 0:02:09
 Epoch: 41, lr: 1.0e-02, train_loss: 0.8870, train_acc: 0.6828 test_loss: 1.0118, test_acc: 0.6669, best: 0.6773, time: 0:02:09
 Epoch: 42, lr: 1.0e-02, train_loss: 0.8643, train_acc: 0.6970 test_loss: 0.9718, test_acc: 0.6710, best: 0.6773, time: 0:02:09
 Epoch: 43, lr: 1.0e-02, train_loss: 0.8621, train_acc: 0.6976 test_loss: 1.0695, test_acc: 0.6549, best: 0.6773, time: 0:02:10
 Epoch: 44, lr: 1.0e-02, train_loss: 0.8528, train_acc: 0.6940 test_loss: 0.9559, test_acc: 0.6865, best: 0.6865, time: 0:02:10
 Epoch: 45, lr: 1.0e-02, train_loss: 0.8320, train_acc: 0.7046 test_loss: 0.9391, test_acc: 0.6997, best: 0.6997, time: 0:02:10
 Epoch: 46, lr: 1.0e-02, train_loss: 0.8219, train_acc: 0.7006 test_loss: 1.0192, test_acc: 0.6532, best: 0.6997, time: 0:02:09
 Epoch: 47, lr: 1.0e-02, train_loss: 0.7981, train_acc: 0.7106 test_loss: 1.0349, test_acc: 0.6664, best: 0.6997, time: 0:02:10
 Epoch: 48, lr: 1.0e-02, train_loss: 0.8023, train_acc: 0.7126 test_loss: 1.0540, test_acc: 0.6603, best: 0.6997, time: 0:02:10
 Epoch: 49, lr: 1.0e-02, train_loss: 0.7977, train_acc: 0.7156 test_loss: 1.0786, test_acc: 0.6490, best: 0.6997, time: 0:02:10
 Epoch: 50, lr: 1.0e-02, train_loss: 0.7694, train_acc: 0.7306 test_loss: 0.9798, test_acc: 0.6867, best: 0.6997, time: 0:02:09
 Epoch: 51, lr: 1.0e-02, train_loss: 0.7907, train_acc: 0.7248 test_loss: 0.9797, test_acc: 0.6927, best: 0.6997, time: 0:02:09
 Epoch: 52, lr: 1.0e-02, train_loss: 0.7388, train_acc: 0.7372 test_loss: 1.1817, test_acc: 0.6587, best: 0.6997, time: 0:02:10
 Epoch: 53, lr: 1.0e-02, train_loss: 0.7489, train_acc: 0.7350 test_loss: 0.9939, test_acc: 0.6827, best: 0.6997, time: 0:02:09
 Epoch: 54, lr: 1.0e-02, train_loss: 0.7383, train_acc: 0.7384 test_loss: 1.0292, test_acc: 0.6886, best: 0.6997, time: 0:02:09
 Epoch: 55, lr: 1.0e-02, train_loss: 0.7010, train_acc: 0.7484 test_loss: 0.9932, test_acc: 0.6920, best: 0.6997, time: 0:02:09
 Epoch: 56, lr: 1.0e-02, train_loss: 0.7295, train_acc: 0.7378 test_loss: 0.9565, test_acc: 0.6986, best: 0.6997, time: 0:02:10
 Epoch: 57, lr: 1.0e-02, train_loss: 0.6959, train_acc: 0.7460 test_loss: 0.9725, test_acc: 0.6970, best: 0.6997, time: 0:02:10
 Epoch: 58, lr: 1.0e-02, train_loss: 0.6912, train_acc: 0.7592 test_loss: 0.8791, test_acc: 0.7175, best: 0.7175, time: 0:02:10
 Epoch: 59, lr: 1.0e-02, train_loss: 0.6935, train_acc: 0.7530 test_loss: 0.8963, test_acc: 0.7153, best: 0.7175, time: 0:02:09
 Epoch: 60, lr: 1.0e-02, train_loss: 0.6794, train_acc: 0.7598 test_loss: 1.1973, test_acc: 0.6843, best: 0.7175, time: 0:02:10
 Epoch: 61, lr: 1.0e-02, train_loss: 0.6829, train_acc: 0.7576 test_loss: 1.0228, test_acc: 0.6757, best: 0.7175, time: 0:02:09
 Epoch: 62, lr: 1.0e-02, train_loss: 0.6528, train_acc: 0.7658 test_loss: 1.0523, test_acc: 0.6800, best: 0.7175, time: 0:02:10
 Epoch: 63, lr: 1.0e-02, train_loss: 0.6481, train_acc: 0.7678 test_loss: 0.9508, test_acc: 0.7081, best: 0.7175, time: 0:02:10
 Epoch: 64, lr: 1.0e-02, train_loss: 0.6331, train_acc: 0.7786 test_loss: 0.9807, test_acc: 0.7019, best: 0.7175, time: 0:02:10
 Epoch: 65, lr: 1.0e-02, train_loss: 0.6286, train_acc: 0.7808 test_loss: 1.0790, test_acc: 0.6814, best: 0.7175, time: 0:02:09
 Epoch: 66, lr: 1.0e-02, train_loss: 0.6140, train_acc: 0.7778 test_loss: 0.9442, test_acc: 0.7105, best: 0.7175, time: 0:02:09
 Epoch: 67, lr: 1.0e-02, train_loss: 0.6488, train_acc: 0.7766 test_loss: 0.9858, test_acc: 0.6944, best: 0.7175, time: 0:02:09
 Epoch: 68, lr: 1.0e-02, train_loss: 0.6212, train_acc: 0.7856 test_loss: 0.9884, test_acc: 0.7070, best: 0.7175, time: 0:02:09
 Epoch: 69, lr: 1.0e-02, train_loss: 0.5957, train_acc: 0.7836 test_loss: 0.9183, test_acc: 0.7242, best: 0.7242, time: 0:02:10
 Epoch: 70, lr: 1.0e-02, train_loss: 0.6003, train_acc: 0.7866 test_loss: 0.8531, test_acc: 0.7372, best: 0.7372, time: 0:02:10
 Epoch: 71, lr: 1.0e-02, train_loss: 0.6022, train_acc: 0.7878 test_loss: 0.9508, test_acc: 0.7084, best: 0.7372, time: 0:02:10
 Epoch: 72, lr: 1.0e-02, train_loss: 0.5744, train_acc: 0.8006 test_loss: 0.9533, test_acc: 0.7269, best: 0.7372, time: 0:02:09
 Epoch: 73, lr: 1.0e-02, train_loss: 0.5722, train_acc: 0.7992 test_loss: 0.9229, test_acc: 0.7188, best: 0.7372, time: 0:02:09
 Epoch: 74, lr: 1.0e-02, train_loss: 0.5602, train_acc: 0.8018 test_loss: 0.9163, test_acc: 0.7200, best: 0.7372, time: 0:02:09
 Epoch: 75, lr: 1.0e-02, train_loss: 0.5749, train_acc: 0.8020 test_loss: 0.9362, test_acc: 0.7258, best: 0.7372, time: 0:02:10
 Epoch: 76, lr: 1.0e-02, train_loss: 0.5413, train_acc: 0.8148 test_loss: 0.9170, test_acc: 0.7345, best: 0.7372, time: 0:02:10
 Epoch: 77, lr: 1.0e-02, train_loss: 0.5479, train_acc: 0.8042 test_loss: 0.8659, test_acc: 0.7230, best: 0.7372, time: 0:02:09
 Epoch: 78, lr: 1.0e-02, train_loss: 0.5260, train_acc: 0.8124 test_loss: 0.9244, test_acc: 0.7276, best: 0.7372, time: 0:02:10
 Epoch: 79, lr: 1.0e-02, train_loss: 0.5275, train_acc: 0.8144 test_loss: 0.9601, test_acc: 0.7104, best: 0.7372, time: 0:02:10
 Epoch: 80, lr: 1.0e-02, train_loss: 0.5315, train_acc: 0.8154 test_loss: 0.8659, test_acc: 0.7370, best: 0.7372, time: 0:02:09
 Epoch: 81, lr: 1.0e-02, train_loss: 0.5128, train_acc: 0.8256 test_loss: 0.8806, test_acc: 0.7461, best: 0.7461, time: 0:02:10
 Epoch: 82, lr: 1.0e-02, train_loss: 0.5152, train_acc: 0.8184 test_loss: 1.0595, test_acc: 0.7033, best: 0.7461, time: 0:02:09
 Epoch: 83, lr: 1.0e-02, train_loss: 0.5265, train_acc: 0.8146 test_loss: 0.9230, test_acc: 0.7398, best: 0.7461, time: 0:02:09
 Epoch: 84, lr: 1.0e-02, train_loss: 0.5158, train_acc: 0.8178 test_loss: 0.9456, test_acc: 0.7330, best: 0.7461, time: 0:02:09
 Epoch: 85, lr: 1.0e-02, train_loss: 0.4876, train_acc: 0.8328 test_loss: 0.8464, test_acc: 0.7514, best: 0.7514, time: 0:02:10
 Epoch: 86, lr: 1.0e-02, train_loss: 0.4919, train_acc: 0.8288 test_loss: 0.9639, test_acc: 0.7224, best: 0.7514, time: 0:02:09
 Epoch: 87, lr: 1.0e-02, train_loss: 0.4657, train_acc: 0.8350 test_loss: 0.9187, test_acc: 0.7409, best: 0.7514, time: 0:02:10
 Epoch: 88, lr: 1.0e-02, train_loss: 0.4763, train_acc: 0.8346 test_loss: 0.9521, test_acc: 0.7442, best: 0.7514, time: 0:02:09
 Epoch: 89, lr: 1.0e-02, train_loss: 0.4705, train_acc: 0.8360 test_loss: 0.9741, test_acc: 0.7331, best: 0.7514, time: 0:02:09
 Epoch: 90, lr: 1.0e-02, train_loss: 0.4613, train_acc: 0.8368 test_loss: 0.9352, test_acc: 0.7364, best: 0.7514, time: 0:02:09
 Epoch: 91, lr: 1.0e-02, train_loss: 0.4617, train_acc: 0.8400 test_loss: 0.9164, test_acc: 0.7456, best: 0.7514, time: 0:02:09
 Epoch: 92, lr: 1.0e-02, train_loss: 0.4713, train_acc: 0.8362 test_loss: 1.0929, test_acc: 0.7077, best: 0.7514, time: 0:02:09
 Epoch: 93, lr: 1.0e-02, train_loss: 0.4490, train_acc: 0.8456 test_loss: 1.0808, test_acc: 0.7228, best: 0.7514, time: 0:02:09
 Epoch: 94, lr: 1.0e-02, train_loss: 0.4672, train_acc: 0.8386 test_loss: 1.0298, test_acc: 0.7320, best: 0.7514, time: 0:02:09
 Epoch: 95, lr: 1.0e-02, train_loss: 0.4377, train_acc: 0.8462 test_loss: 0.9508, test_acc: 0.7348, best: 0.7514, time: 0:02:09
 Epoch: 96, lr: 1.0e-02, train_loss: 0.4555, train_acc: 0.8438 test_loss: 0.9694, test_acc: 0.7231, best: 0.7514, time: 0:02:10
 Epoch: 97, lr: 1.0e-02, train_loss: 0.4394, train_acc: 0.8500 test_loss: 0.9979, test_acc: 0.7154, best: 0.7514, time: 0:02:09
 Epoch: 98, lr: 1.0e-02, train_loss: 0.4221, train_acc: 0.8564 test_loss: 1.0335, test_acc: 0.7201, best: 0.7514, time: 0:02:10
 Epoch: 99, lr: 1.0e-02, train_loss: 0.4333, train_acc: 0.8516 test_loss: 0.8984, test_acc: 0.7562, best: 0.7562, time: 0:02:11
 Epoch: 100, lr: 1.0e-02, train_loss: 0.4277, train_acc: 0.8526 test_loss: 0.9362, test_acc: 0.7466, best: 0.7562, time: 0:02:12
 Epoch: 101, lr: 1.0e-02, train_loss: 0.4161, train_acc: 0.8584 test_loss: 0.9065, test_acc: 0.7501, best: 0.7562, time: 0:02:12
 Epoch: 102, lr: 1.0e-02, train_loss: 0.4470, train_acc: 0.8434 test_loss: 1.0940, test_acc: 0.6934, best: 0.7562, time: 0:02:10
 Epoch: 103, lr: 1.0e-02, train_loss: 0.3979, train_acc: 0.8610 test_loss: 0.8962, test_acc: 0.7540, best: 0.7562, time: 0:02:12
 Epoch: 104, lr: 1.0e-02, train_loss: 0.4295, train_acc: 0.8568 test_loss: 0.8880, test_acc: 0.7515, best: 0.7562, time: 0:02:10
 Epoch: 105, lr: 1.0e-02, train_loss: 0.3999, train_acc: 0.8658 test_loss: 0.9156, test_acc: 0.7489, best: 0.7562, time: 0:02:12
 Epoch: 106, lr: 1.0e-02, train_loss: 0.4106, train_acc: 0.8622 test_loss: 0.9108, test_acc: 0.7490, best: 0.7562, time: 0:02:11
 Epoch: 107, lr: 1.0e-02, train_loss: 0.3882, train_acc: 0.8684 test_loss: 0.9007, test_acc: 0.7610, best: 0.7610, time: 0:02:13
 Epoch: 108, lr: 1.0e-02, train_loss: 0.3784, train_acc: 0.8724 test_loss: 0.9991, test_acc: 0.7446, best: 0.7610, time: 0:02:13
 Epoch: 109, lr: 1.0e-02, train_loss: 0.3905, train_acc: 0.8672 test_loss: 0.8865, test_acc: 0.7509, best: 0.7610, time: 0:02:12
 Epoch: 110, lr: 1.0e-02, train_loss: 0.3793, train_acc: 0.8654 test_loss: 1.0146, test_acc: 0.7330, best: 0.7610, time: 0:02:13
 Epoch: 111, lr: 1.0e-02, train_loss: 0.3729, train_acc: 0.8718 test_loss: 0.9179, test_acc: 0.7616, best: 0.7616, time: 0:02:14
 Epoch: 112, lr: 1.0e-02, train_loss: 0.3968, train_acc: 0.8604 test_loss: 1.0618, test_acc: 0.7208, best: 0.7616, time: 0:02:13
 Epoch: 113, lr: 1.0e-02, train_loss: 0.3622, train_acc: 0.8758 test_loss: 1.0255, test_acc: 0.7338, best: 0.7616, time: 0:02:13
 Epoch: 114, lr: 1.0e-02, train_loss: 0.3696, train_acc: 0.8726 test_loss: 1.1774, test_acc: 0.7211, best: 0.7616, time: 0:02:14
 Epoch: 115, lr: 1.0e-02, train_loss: 0.3816, train_acc: 0.8690 test_loss: 0.9849, test_acc: 0.7412, best: 0.7616, time: 0:02:14
 Epoch: 116, lr: 1.0e-02, train_loss: 0.3605, train_acc: 0.8722 test_loss: 1.0173, test_acc: 0.7384, best: 0.7616, time: 0:02:14
 Epoch: 117, lr: 1.0e-02, train_loss: 0.3514, train_acc: 0.8748 test_loss: 1.0599, test_acc: 0.7330, best: 0.7616, time: 0:02:14
 Epoch: 118, lr: 1.0e-02, train_loss: 0.3679, train_acc: 0.8758 test_loss: 1.1298, test_acc: 0.7314, best: 0.7616, time: 0:02:14
 Epoch: 119, lr: 1.0e-02, train_loss: 0.3472, train_acc: 0.8854 test_loss: 0.9738, test_acc: 0.7475, best: 0.7616, time: 0:02:14
 Epoch: 120, lr: 1.0e-02, train_loss: 0.3503, train_acc: 0.8820 test_loss: 1.1677, test_acc: 0.7246, best: 0.7616, time: 0:02:14
 Epoch: 121, lr: 1.0e-02, train_loss: 0.3487, train_acc: 0.8756 test_loss: 0.9271, test_acc: 0.7492, best: 0.7616, time: 0:02:14
 Epoch: 122, lr: 1.0e-02, train_loss: 0.3485, train_acc: 0.8788 test_loss: 1.0880, test_acc: 0.7339, best: 0.7616, time: 0:02:14
 Epoch: 123, lr: 1.0e-02, train_loss: 0.3486, train_acc: 0.8822 test_loss: 0.8975, test_acc: 0.7649, best: 0.7649, time: 0:02:14
 Epoch: 124, lr: 1.0e-02, train_loss: 0.3505, train_acc: 0.8798 test_loss: 0.9526, test_acc: 0.7470, best: 0.7649, time: 0:02:14
 Epoch: 125, lr: 1.0e-02, train_loss: 0.3423, train_acc: 0.8852 test_loss: 1.1541, test_acc: 0.7195, best: 0.7649, time: 0:02:14
 Epoch: 126, lr: 1.0e-02, train_loss: 0.3330, train_acc: 0.8860 test_loss: 1.0565, test_acc: 0.7440, best: 0.7649, time: 0:02:14
 Epoch: 127, lr: 1.0e-02, train_loss: 0.3389, train_acc: 0.8852 test_loss: 0.9817, test_acc: 0.7526, best: 0.7649, time: 0:02:14
 Epoch: 128, lr: 1.0e-02, train_loss: 0.3270, train_acc: 0.8860 test_loss: 0.9599, test_acc: 0.7571, best: 0.7649, time: 0:02:14
 Epoch: 129, lr: 1.0e-02, train_loss: 0.3264, train_acc: 0.8870 test_loss: 0.8528, test_acc: 0.7711, best: 0.7711, time: 0:02:15
 Epoch: 130, lr: 1.0e-02, train_loss: 0.3188, train_acc: 0.8884 test_loss: 0.8817, test_acc: 0.7800, best: 0.7800, time: 0:02:15
 Epoch: 131, lr: 1.0e-02, train_loss: 0.3235, train_acc: 0.8884 test_loss: 1.1528, test_acc: 0.7498, best: 0.7800, time: 0:02:14
 Epoch: 132, lr: 1.0e-02, train_loss: 0.3289, train_acc: 0.8848 test_loss: 0.8577, test_acc: 0.7639, best: 0.7800, time: 0:02:14
 Epoch: 133, lr: 1.0e-02, train_loss: 0.3169, train_acc: 0.8894 test_loss: 0.9767, test_acc: 0.7526, best: 0.7800, time: 0:02:14
 Epoch: 134, lr: 1.0e-02, train_loss: 0.3374, train_acc: 0.8858 test_loss: 0.9523, test_acc: 0.7579, best: 0.7800, time: 0:02:14
 Epoch: 135, lr: 1.0e-02, train_loss: 0.3199, train_acc: 0.8872 test_loss: 1.0472, test_acc: 0.7468, best: 0.7800, time: 0:02:14
 Epoch: 136, lr: 1.0e-02, train_loss: 0.3089, train_acc: 0.8950 test_loss: 0.9829, test_acc: 0.7675, best: 0.7800, time: 0:02:14
 Epoch: 137, lr: 1.0e-02, train_loss: 0.3132, train_acc: 0.8916 test_loss: 0.9750, test_acc: 0.7635, best: 0.7800, time: 0:02:14
 Epoch: 138, lr: 1.0e-02, train_loss: 0.3118, train_acc: 0.8936 test_loss: 0.9863, test_acc: 0.7585, best: 0.7800, time: 0:02:14
 Epoch: 139, lr: 1.0e-02, train_loss: 0.3258, train_acc: 0.8876 test_loss: 0.8246, test_acc: 0.7823, best: 0.7823, time: 0:02:15
 Epoch: 140, lr: 1.0e-02, train_loss: 0.2934, train_acc: 0.9006 test_loss: 0.8968, test_acc: 0.7690, best: 0.7823, time: 0:02:14
 Epoch: 141, lr: 1.0e-02, train_loss: 0.3175, train_acc: 0.8922 test_loss: 0.9856, test_acc: 0.7606, best: 0.7823, time: 0:02:14
 Epoch: 142, lr: 1.0e-02, train_loss: 0.3020, train_acc: 0.8938 test_loss: 1.0034, test_acc: 0.7474, best: 0.7823, time: 0:02:14
 Epoch: 143, lr: 1.0e-02, train_loss: 0.2687, train_acc: 0.9058 test_loss: 0.9015, test_acc: 0.7678, best: 0.7823, time: 0:02:14
 Epoch: 144, lr: 1.0e-02, train_loss: 0.2991, train_acc: 0.8970 test_loss: 0.9463, test_acc: 0.7615, best: 0.7823, time: 0:02:14
 Epoch: 145, lr: 1.0e-02, train_loss: 0.2990, train_acc: 0.8978 test_loss: 0.9997, test_acc: 0.7651, best: 0.7823, time: 0:02:14
 Epoch: 146, lr: 1.0e-02, train_loss: 0.2849, train_acc: 0.9030 test_loss: 0.9164, test_acc: 0.7699, best: 0.7823, time: 0:02:14
 Epoch: 147, lr: 1.0e-02, train_loss: 0.2961, train_acc: 0.9014 test_loss: 0.9887, test_acc: 0.7482, best: 0.7823, time: 0:02:14
 Epoch: 148, lr: 1.0e-02, train_loss: 0.3037, train_acc: 0.8930 test_loss: 1.0106, test_acc: 0.7596, best: 0.7823, time: 0:02:14
 Epoch: 149, lr: 1.0e-02, train_loss: 0.3087, train_acc: 0.8980 test_loss: 0.9665, test_acc: 0.7564, best: 0.7823, time: 0:02:14
 Epoch: 150, lr: 1.0e-02, train_loss: 0.2839, train_acc: 0.9008 test_loss: 1.0610, test_acc: 0.7492, best: 0.7823, time: 0:02:14
 Epoch: 151, lr: 1.0e-02, train_loss: 0.2854, train_acc: 0.9008 test_loss: 0.9992, test_acc: 0.7540, best: 0.7823, time: 0:02:14
 Epoch: 152, lr: 1.0e-02, train_loss: 0.2990, train_acc: 0.8974 test_loss: 0.9311, test_acc: 0.7592, best: 0.7823, time: 0:02:14
 Epoch: 153, lr: 1.0e-02, train_loss: 0.2648, train_acc: 0.9094 test_loss: 0.9786, test_acc: 0.7635, best: 0.7823, time: 0:02:14
 Epoch: 154, lr: 1.0e-02, train_loss: 0.3000, train_acc: 0.8988 test_loss: 1.0452, test_acc: 0.7549, best: 0.7823, time: 0:02:14
 Epoch: 155, lr: 1.0e-02, train_loss: 0.2761, train_acc: 0.9004 test_loss: 1.0524, test_acc: 0.7564, best: 0.7823, time: 0:02:14
 Epoch: 156, lr: 1.0e-02, train_loss: 0.2794, train_acc: 0.9048 test_loss: 0.9994, test_acc: 0.7602, best: 0.7823, time: 0:02:14
 Epoch: 157, lr: 1.0e-02, train_loss: 0.2630, train_acc: 0.9092 test_loss: 0.9478, test_acc: 0.7720, best: 0.7823, time: 0:02:14
 Epoch: 158, lr: 1.0e-02, train_loss: 0.2714, train_acc: 0.9062 test_loss: 0.9741, test_acc: 0.7718, best: 0.7823, time: 0:02:14
 Epoch: 159, lr: 1.0e-02, train_loss: 0.2794, train_acc: 0.9094 test_loss: 1.0388, test_acc: 0.7654, best: 0.7823, time: 0:02:14
 Epoch: 160, lr: 1.0e-02, train_loss: 0.2742, train_acc: 0.9084 test_loss: 1.0983, test_acc: 0.7435, best: 0.7823, time: 0:02:14
 Epoch: 161, lr: 1.0e-02, train_loss: 0.2704, train_acc: 0.9072 test_loss: 1.1055, test_acc: 0.7430, best: 0.7823, time: 0:02:14
 Epoch: 162, lr: 1.0e-02, train_loss: 0.2610, train_acc: 0.9104 test_loss: 0.9918, test_acc: 0.7629, best: 0.7823, time: 0:02:14
 Epoch: 163, lr: 1.0e-02, train_loss: 0.2686, train_acc: 0.9080 test_loss: 1.0081, test_acc: 0.7498, best: 0.7823, time: 0:02:14
 Epoch: 164, lr: 1.0e-02, train_loss: 0.2643, train_acc: 0.9080 test_loss: 1.0290, test_acc: 0.7609, best: 0.7823, time: 0:02:14
 Epoch: 165, lr: 1.0e-02, train_loss: 0.2682, train_acc: 0.9070 test_loss: 1.0094, test_acc: 0.7560, best: 0.7823, time: 0:02:14
 Epoch: 166, lr: 1.0e-02, train_loss: 0.2710, train_acc: 0.9070 test_loss: 0.9300, test_acc: 0.7730, best: 0.7823, time: 0:02:14
 Epoch: 167, lr: 1.0e-02, train_loss: 0.2596, train_acc: 0.9130 test_loss: 0.9457, test_acc: 0.7584, best: 0.7823, time: 0:02:14
 Epoch: 168, lr: 1.0e-02, train_loss: 0.2628, train_acc: 0.9076 test_loss: 0.9484, test_acc: 0.7722, best: 0.7823, time: 0:02:14
 Epoch: 169, lr: 1.0e-02, train_loss: 0.2721, train_acc: 0.9100 test_loss: 0.9559, test_acc: 0.7585, best: 0.7823, time: 0:02:14
 Epoch: 170, lr: 1.0e-02, train_loss: 0.2530, train_acc: 0.9104 test_loss: 1.0196, test_acc: 0.7575, best: 0.7823, time: 0:02:14
 Epoch: 171, lr: 1.0e-02, train_loss: 0.2644, train_acc: 0.9074 test_loss: 1.2096, test_acc: 0.7522, best: 0.7823, time: 0:02:14
 Epoch: 172, lr: 1.0e-02, train_loss: 0.2633, train_acc: 0.9098 test_loss: 0.9884, test_acc: 0.7554, best: 0.7823, time: 0:02:14
 Epoch: 173, lr: 1.0e-02, train_loss: 0.2360, train_acc: 0.9150 test_loss: 0.9905, test_acc: 0.7566, best: 0.7823, time: 0:02:14
 Epoch: 174, lr: 1.0e-02, train_loss: 0.2564, train_acc: 0.9124 test_loss: 0.9572, test_acc: 0.7716, best: 0.7823, time: 0:02:14
 Epoch: 175, lr: 1.0e-02, train_loss: 0.2482, train_acc: 0.9162 test_loss: 0.9674, test_acc: 0.7696, best: 0.7823, time: 0:02:14
 Epoch: 176, lr: 1.0e-02, train_loss: 0.2696, train_acc: 0.9034 test_loss: 0.9741, test_acc: 0.7578, best: 0.7823, time: 0:02:14
 Epoch: 177, lr: 1.0e-02, train_loss: 0.2391, train_acc: 0.9180 test_loss: 0.9342, test_acc: 0.7615, best: 0.7823, time: 0:02:14
 Epoch: 178, lr: 1.0e-02, train_loss: 0.2468, train_acc: 0.9124 test_loss: 1.1417, test_acc: 0.7511, best: 0.7823, time: 0:02:14
 Epoch: 179, lr: 1.0e-02, train_loss: 0.2700, train_acc: 0.9058 test_loss: 1.0690, test_acc: 0.7684, best: 0.7823, time: 0:02:13
 Epoch: 180, lr: 2.0e-03, train_loss: 0.2127, train_acc: 0.9288 test_loss: 0.9919, test_acc: 0.7765, best: 0.7823, time: 0:02:14
 Epoch: 181, lr: 2.0e-03, train_loss: 0.1936, train_acc: 0.9344 test_loss: 0.8755, test_acc: 0.7900, best: 0.7900, time: 0:02:14
 Epoch: 182, lr: 2.0e-03, train_loss: 0.1767, train_acc: 0.9394 test_loss: 0.8896, test_acc: 0.7866, best: 0.7900, time: 0:02:13
 Epoch: 183, lr: 2.0e-03, train_loss: 0.1755, train_acc: 0.9382 test_loss: 0.9360, test_acc: 0.7820, best: 0.7900, time: 0:02:13
 Epoch: 184, lr: 2.0e-03, train_loss: 0.1967, train_acc: 0.9348 test_loss: 0.9649, test_acc: 0.7791, best: 0.7900, time: 0:02:13
 Epoch: 185, lr: 2.0e-03, train_loss: 0.1703, train_acc: 0.9426 test_loss: 0.9032, test_acc: 0.7883, best: 0.7900, time: 0:02:13
 Epoch: 186, lr: 2.0e-03, train_loss: 0.1774, train_acc: 0.9386 test_loss: 0.9233, test_acc: 0.7830, best: 0.7900, time: 0:02:13
 Epoch: 187, lr: 2.0e-03, train_loss: 0.1624, train_acc: 0.9432 test_loss: 0.9486, test_acc: 0.7867, best: 0.7900, time: 0:02:13
 Epoch: 188, lr: 2.0e-03, train_loss: 0.1495, train_acc: 0.9534 test_loss: 0.9805, test_acc: 0.7836, best: 0.7900, time: 0:02:13
 Epoch: 189, lr: 2.0e-03, train_loss: 0.1754, train_acc: 0.9404 test_loss: 0.9587, test_acc: 0.7782, best: 0.7900, time: 0:02:13
 Epoch: 190, lr: 2.0e-03, train_loss: 0.1663, train_acc: 0.9418 test_loss: 0.9407, test_acc: 0.7815, best: 0.7900, time: 0:02:13
 Epoch: 191, lr: 2.0e-03, train_loss: 0.1603, train_acc: 0.9472 test_loss: 0.9467, test_acc: 0.7836, best: 0.7900, time: 0:02:13
 Epoch: 192, lr: 2.0e-03, train_loss: 0.1576, train_acc: 0.9466 test_loss: 0.9590, test_acc: 0.7754, best: 0.7900, time: 0:02:13
 Epoch: 193, lr: 2.0e-03, train_loss: 0.1612, train_acc: 0.9462 test_loss: 0.9700, test_acc: 0.7824, best: 0.7900, time: 0:02:13
 Epoch: 194, lr: 2.0e-03, train_loss: 0.1602, train_acc: 0.9460 test_loss: 0.9350, test_acc: 0.7799, best: 0.7900, time: 0:02:13
 Epoch: 195, lr: 2.0e-03, train_loss: 0.1634, train_acc: 0.9428 test_loss: 0.9326, test_acc: 0.7844, best: 0.7900, time: 0:02:13
 Epoch: 196, lr: 2.0e-03, train_loss: 0.1558, train_acc: 0.9476 test_loss: 0.9030, test_acc: 0.7890, best: 0.7900, time: 0:02:13
 Epoch: 197, lr: 2.0e-03, train_loss: 0.1640, train_acc: 0.9436 test_loss: 1.0301, test_acc: 0.7823, best: 0.7900, time: 0:02:13
 Epoch: 198, lr: 2.0e-03, train_loss: 0.1608, train_acc: 0.9480 test_loss: 1.0236, test_acc: 0.7819, best: 0.7900, time: 0:02:13
 Epoch: 199, lr: 2.0e-03, train_loss: 0.1572, train_acc: 0.9460 test_loss: 0.9530, test_acc: 0.7889, best: 0.7900, time: 0:02:13
 Epoch: 200, lr: 2.0e-03, train_loss: 0.1531, train_acc: 0.9478 test_loss: 0.9580, test_acc: 0.7843, best: 0.7900, time: 0:02:13
 Epoch: 201, lr: 2.0e-03, train_loss: 0.1572, train_acc: 0.9470 test_loss: 1.0329, test_acc: 0.7794, best: 0.7900, time: 0:02:13
 Epoch: 202, lr: 2.0e-03, train_loss: 0.1339, train_acc: 0.9544 test_loss: 0.9760, test_acc: 0.7800, best: 0.7900, time: 0:02:13
 Epoch: 203, lr: 2.0e-03, train_loss: 0.1494, train_acc: 0.9466 test_loss: 0.9732, test_acc: 0.7780, best: 0.7900, time: 0:02:13
 Epoch: 204, lr: 2.0e-03, train_loss: 0.1462, train_acc: 0.9514 test_loss: 1.0063, test_acc: 0.7768, best: 0.7900, time: 0:02:13
 Epoch: 205, lr: 2.0e-03, train_loss: 0.1483, train_acc: 0.9502 test_loss: 0.9300, test_acc: 0.7873, best: 0.7900, time: 0:02:13
 Epoch: 206, lr: 2.0e-03, train_loss: 0.1491, train_acc: 0.9458 test_loss: 0.9840, test_acc: 0.7781, best: 0.7900, time: 0:02:13
 Epoch: 207, lr: 2.0e-03, train_loss: 0.1352, train_acc: 0.9518 test_loss: 1.0092, test_acc: 0.7845, best: 0.7900, time: 0:02:13
 Epoch: 208, lr: 2.0e-03, train_loss: 0.1552, train_acc: 0.9448 test_loss: 1.0798, test_acc: 0.7771, best: 0.7900, time: 0:02:13
 Epoch: 209, lr: 2.0e-03, train_loss: 0.1390, train_acc: 0.9520 test_loss: 0.9399, test_acc: 0.7911, best: 0.7911, time: 0:02:14
 Epoch: 210, lr: 2.0e-03, train_loss: 0.1448, train_acc: 0.9492 test_loss: 1.0449, test_acc: 0.7819, best: 0.7911, time: 0:02:13
 Epoch: 211, lr: 2.0e-03, train_loss: 0.1354, train_acc: 0.9536 test_loss: 0.9218, test_acc: 0.7899, best: 0.7911, time: 0:02:13
 Epoch: 212, lr: 2.0e-03, train_loss: 0.1459, train_acc: 0.9490 test_loss: 0.9652, test_acc: 0.7886, best: 0.7911, time: 0:02:13
 Epoch: 213, lr: 2.0e-03, train_loss: 0.1355, train_acc: 0.9562 test_loss: 1.0059, test_acc: 0.7835, best: 0.7911, time: 0:02:13
 Epoch: 214, lr: 2.0e-03, train_loss: 0.1489, train_acc: 0.9472 test_loss: 0.9412, test_acc: 0.7893, best: 0.7911, time: 0:02:13
 Epoch: 215, lr: 2.0e-03, train_loss: 0.1506, train_acc: 0.9494 test_loss: 0.9422, test_acc: 0.7904, best: 0.7911, time: 0:02:13
 Epoch: 216, lr: 2.0e-03, train_loss: 0.1374, train_acc: 0.9524 test_loss: 0.9325, test_acc: 0.7919, best: 0.7919, time: 0:02:14
 Epoch: 217, lr: 2.0e-03, train_loss: 0.1474, train_acc: 0.9498 test_loss: 0.9782, test_acc: 0.7809, best: 0.7919, time: 0:02:13
 Epoch: 218, lr: 2.0e-03, train_loss: 0.1453, train_acc: 0.9502 test_loss: 0.9713, test_acc: 0.7863, best: 0.7919, time: 0:02:13
 Epoch: 219, lr: 2.0e-03, train_loss: 0.1424, train_acc: 0.9538 test_loss: 0.9790, test_acc: 0.7879, best: 0.7919, time: 0:02:13
 Epoch: 220, lr: 2.0e-03, train_loss: 0.1353, train_acc: 0.9536 test_loss: 0.9789, test_acc: 0.7891, best: 0.7919, time: 0:02:13
 Epoch: 221, lr: 2.0e-03, train_loss: 0.1507, train_acc: 0.9482 test_loss: 0.9189, test_acc: 0.7906, best: 0.7919, time: 0:02:13
 Epoch: 222, lr: 2.0e-03, train_loss: 0.1340, train_acc: 0.9530 test_loss: 0.9880, test_acc: 0.7876, best: 0.7919, time: 0:02:13
 Epoch: 223, lr: 2.0e-03, train_loss: 0.1435, train_acc: 0.9506 test_loss: 1.0361, test_acc: 0.7826, best: 0.7919, time: 0:02:13
 Epoch: 224, lr: 2.0e-03, train_loss: 0.1533, train_acc: 0.9464 test_loss: 1.0039, test_acc: 0.7794, best: 0.7919, time: 0:02:13
 Epoch: 225, lr: 2.0e-03, train_loss: 0.1411, train_acc: 0.9486 test_loss: 0.9959, test_acc: 0.7835, best: 0.7919, time: 0:02:13
 Epoch: 226, lr: 2.0e-03, train_loss: 0.1305, train_acc: 0.9558 test_loss: 1.0410, test_acc: 0.7792, best: 0.7919, time: 0:02:13
 Epoch: 227, lr: 2.0e-03, train_loss: 0.1509, train_acc: 0.9500 test_loss: 0.9959, test_acc: 0.7794, best: 0.7919, time: 0:02:13
 Epoch: 228, lr: 2.0e-03, train_loss: 0.1503, train_acc: 0.9500 test_loss: 0.9970, test_acc: 0.7817, best: 0.7919, time: 0:02:13
 Epoch: 229, lr: 2.0e-03, train_loss: 0.1401, train_acc: 0.9514 test_loss: 1.0190, test_acc: 0.7836, best: 0.7919, time: 0:02:13
 Epoch: 230, lr: 2.0e-03, train_loss: 0.1474, train_acc: 0.9492 test_loss: 1.0006, test_acc: 0.7913, best: 0.7919, time: 0:02:13
 Epoch: 231, lr: 2.0e-03, train_loss: 0.1371, train_acc: 0.9540 test_loss: 0.9890, test_acc: 0.7819, best: 0.7919, time: 0:02:13
 Epoch: 232, lr: 2.0e-03, train_loss: 0.1390, train_acc: 0.9504 test_loss: 1.0099, test_acc: 0.7810, best: 0.7919, time: 0:02:13
 Epoch: 233, lr: 2.0e-03, train_loss: 0.1373, train_acc: 0.9514 test_loss: 1.0531, test_acc: 0.7755, best: 0.7919, time: 0:02:13
 Epoch: 234, lr: 2.0e-03, train_loss: 0.1293, train_acc: 0.9550 test_loss: 0.9528, test_acc: 0.7930, best: 0.7930, time: 0:02:14
 Epoch: 235, lr: 2.0e-03, train_loss: 0.1347, train_acc: 0.9538 test_loss: 1.0288, test_acc: 0.7831, best: 0.7930, time: 0:02:13
 Epoch: 236, lr: 2.0e-03, train_loss: 0.1378, train_acc: 0.9544 test_loss: 0.9535, test_acc: 0.7917, best: 0.7930, time: 0:02:13
 Epoch: 237, lr: 2.0e-03, train_loss: 0.1398, train_acc: 0.9526 test_loss: 0.9697, test_acc: 0.7895, best: 0.7930, time: 0:02:13
 Epoch: 238, lr: 2.0e-03, train_loss: 0.1207, train_acc: 0.9556 test_loss: 1.0111, test_acc: 0.7845, best: 0.7930, time: 0:02:13
 Epoch: 239, lr: 2.0e-03, train_loss: 0.1313, train_acc: 0.9546 test_loss: 0.9634, test_acc: 0.7887, best: 0.7930, time: 0:02:13
 Epoch: 240, lr: 4.0e-04, train_loss: 0.1380, train_acc: 0.9524 test_loss: 0.9872, test_acc: 0.7886, best: 0.7930, time: 0:02:13
 Epoch: 241, lr: 4.0e-04, train_loss: 0.1277, train_acc: 0.9546 test_loss: 0.9681, test_acc: 0.7889, best: 0.7930, time: 0:02:13
 Epoch: 242, lr: 4.0e-04, train_loss: 0.1241, train_acc: 0.9560 test_loss: 1.0175, test_acc: 0.7826, best: 0.7930, time: 0:02:13
 Epoch: 243, lr: 4.0e-04, train_loss: 0.1289, train_acc: 0.9572 test_loss: 0.9657, test_acc: 0.7880, best: 0.7930, time: 0:02:13
 Epoch: 244, lr: 4.0e-04, train_loss: 0.1296, train_acc: 0.9564 test_loss: 0.9769, test_acc: 0.7890, best: 0.7930, time: 0:02:13
 Epoch: 245, lr: 4.0e-04, train_loss: 0.1296, train_acc: 0.9554 test_loss: 1.0004, test_acc: 0.7856, best: 0.7930, time: 0:02:13
 Epoch: 246, lr: 4.0e-04, train_loss: 0.1334, train_acc: 0.9556 test_loss: 0.9440, test_acc: 0.7930, best: 0.7930, time: 0:02:13
 Epoch: 247, lr: 4.0e-04, train_loss: 0.1359, train_acc: 0.9554 test_loss: 0.9396, test_acc: 0.7937, best: 0.7937, time: 0:02:14
 Epoch: 248, lr: 4.0e-04, train_loss: 0.1222, train_acc: 0.9584 test_loss: 0.9795, test_acc: 0.7904, best: 0.7937, time: 0:02:13
 Epoch: 249, lr: 4.0e-04, train_loss: 0.1302, train_acc: 0.9544 test_loss: 0.9483, test_acc: 0.7931, best: 0.7937, time: 0:02:13
 Epoch: 250, lr: 4.0e-04, train_loss: 0.1190, train_acc: 0.9596 test_loss: 0.9795, test_acc: 0.7911, best: 0.7937, time: 0:02:13
 Epoch: 251, lr: 4.0e-04, train_loss: 0.1246, train_acc: 0.9572 test_loss: 0.9367, test_acc: 0.7930, best: 0.7937, time: 0:02:13
 Epoch: 252, lr: 4.0e-04, train_loss: 0.1358, train_acc: 0.9532 test_loss: 0.9615, test_acc: 0.7927, best: 0.7937, time: 0:02:13
 Epoch: 253, lr: 4.0e-04, train_loss: 0.1255, train_acc: 0.9554 test_loss: 0.9612, test_acc: 0.7885, best: 0.7937, time: 0:02:13
 Epoch: 254, lr: 4.0e-04, train_loss: 0.1309, train_acc: 0.9556 test_loss: 0.9628, test_acc: 0.7915, best: 0.7937, time: 0:02:13
 Epoch: 255, lr: 4.0e-04, train_loss: 0.1310, train_acc: 0.9550 test_loss: 0.9732, test_acc: 0.7894, best: 0.7937, time: 0:02:13
 Epoch: 256, lr: 4.0e-04, train_loss: 0.1303, train_acc: 0.9548 test_loss: 0.9567, test_acc: 0.7916, best: 0.7937, time: 0:02:13
 Epoch: 257, lr: 4.0e-04, train_loss: 0.1236, train_acc: 0.9574 test_loss: 0.9810, test_acc: 0.7916, best: 0.7937, time: 0:02:13
 Epoch: 258, lr: 4.0e-04, train_loss: 0.1268, train_acc: 0.9540 test_loss: 1.0145, test_acc: 0.7866, best: 0.7937, time: 0:02:13
 Epoch: 259, lr: 4.0e-04, train_loss: 0.1241, train_acc: 0.9578 test_loss: 0.9553, test_acc: 0.7935, best: 0.7937, time: 0:02:13
 Epoch: 260, lr: 4.0e-04, train_loss: 0.1187, train_acc: 0.9608 test_loss: 0.9492, test_acc: 0.7914, best: 0.7937, time: 0:02:13
 Epoch: 261, lr: 4.0e-04, train_loss: 0.1279, train_acc: 0.9554 test_loss: 1.0033, test_acc: 0.7823, best: 0.7937, time: 0:02:13
 Epoch: 262, lr: 4.0e-04, train_loss: 0.1293, train_acc: 0.9570 test_loss: 0.9512, test_acc: 0.7947, best: 0.7947, time: 0:02:14
 Epoch: 263, lr: 4.0e-04, train_loss: 0.1212, train_acc: 0.9580 test_loss: 0.9827, test_acc: 0.7886, best: 0.7947, time: 0:02:13
 Epoch: 264, lr: 4.0e-04, train_loss: 0.1223, train_acc: 0.9562 test_loss: 1.0270, test_acc: 0.7881, best: 0.7947, time: 0:02:13
 Epoch: 265, lr: 4.0e-04, train_loss: 0.1091, train_acc: 0.9620 test_loss: 0.9932, test_acc: 0.7895, best: 0.7947, time: 0:02:13
 Epoch: 266, lr: 4.0e-04, train_loss: 0.1181, train_acc: 0.9568 test_loss: 0.9817, test_acc: 0.7903, best: 0.7947, time: 0:02:13
 Epoch: 267, lr: 4.0e-04, train_loss: 0.1212, train_acc: 0.9578 test_loss: 0.9925, test_acc: 0.7863, best: 0.7947, time: 0:02:13
 Epoch: 268, lr: 4.0e-04, train_loss: 0.1301, train_acc: 0.9584 test_loss: 0.9391, test_acc: 0.7943, best: 0.7947, time: 0:02:13
 Epoch: 269, lr: 4.0e-04, train_loss: 0.1168, train_acc: 0.9606 test_loss: 1.0335, test_acc: 0.7843, best: 0.7947, time: 0:02:13
 Epoch: 270, lr: 8.0e-05, train_loss: 0.1379, train_acc: 0.9532 test_loss: 0.9575, test_acc: 0.7914, best: 0.7947, time: 0:02:13
 Epoch: 271, lr: 8.0e-05, train_loss: 0.1214, train_acc: 0.9600 test_loss: 1.0144, test_acc: 0.7883, best: 0.7947, time: 0:02:13
 Epoch: 272, lr: 8.0e-05, train_loss: 0.1176, train_acc: 0.9600 test_loss: 0.9533, test_acc: 0.7907, best: 0.7947, time: 0:02:13
 Epoch: 273, lr: 8.0e-05, train_loss: 0.1142, train_acc: 0.9614 test_loss: 0.9708, test_acc: 0.7943, best: 0.7947, time: 0:02:13
 Epoch: 274, lr: 8.0e-05, train_loss: 0.1257, train_acc: 0.9566 test_loss: 1.0112, test_acc: 0.7866, best: 0.7947, time: 0:02:13
 Epoch: 275, lr: 8.0e-05, train_loss: 0.1212, train_acc: 0.9614 test_loss: 1.0077, test_acc: 0.7889, best: 0.7947, time: 0:02:13
 Epoch: 276, lr: 8.0e-05, train_loss: 0.1105, train_acc: 0.9630 test_loss: 0.9636, test_acc: 0.7926, best: 0.7947, time: 0:02:13
 Epoch: 277, lr: 8.0e-05, train_loss: 0.1214, train_acc: 0.9574 test_loss: 0.9777, test_acc: 0.7880, best: 0.7947, time: 0:02:13
 Epoch: 278, lr: 8.0e-05, train_loss: 0.1215, train_acc: 0.9588 test_loss: 0.9814, test_acc: 0.7890, best: 0.7947, time: 0:02:14
 Epoch: 279, lr: 8.0e-05, train_loss: 0.1276, train_acc: 0.9598 test_loss: 1.0531, test_acc: 0.7851, best: 0.7947, time: 0:02:13
 Epoch: 280, lr: 8.0e-05, train_loss: 0.1180, train_acc: 0.9606 test_loss: 0.9619, test_acc: 0.7919, best: 0.7947, time: 0:02:13
 Epoch: 281, lr: 8.0e-05, train_loss: 0.1265, train_acc: 0.9550 test_loss: 0.9817, test_acc: 0.7917, best: 0.7947, time: 0:02:13
 Epoch: 282, lr: 8.0e-05, train_loss: 0.1114, train_acc: 0.9602 test_loss: 0.9959, test_acc: 0.7877, best: 0.7947, time: 0:02:13
 Epoch: 283, lr: 8.0e-05, train_loss: 0.1252, train_acc: 0.9570 test_loss: 1.0118, test_acc: 0.7857, best: 0.7947, time: 0:02:13
 Epoch: 284, lr: 8.0e-05, train_loss: 0.1213, train_acc: 0.9594 test_loss: 1.0280, test_acc: 0.7875, best: 0.7947, time: 0:02:13
 Epoch: 285, lr: 8.0e-05, train_loss: 0.1189, train_acc: 0.9606 test_loss: 0.9536, test_acc: 0.7929, best: 0.7947, time: 0:02:13
 Epoch: 286, lr: 8.0e-05, train_loss: 0.1088, train_acc: 0.9630 test_loss: 0.9878, test_acc: 0.7893, best: 0.7947, time: 0:02:13
 Epoch: 287, lr: 8.0e-05, train_loss: 0.1219, train_acc: 0.9600 test_loss: 1.0174, test_acc: 0.7827, best: 0.7947, time: 0:02:13
 Epoch: 288, lr: 8.0e-05, train_loss: 0.1207, train_acc: 0.9596 test_loss: 0.9878, test_acc: 0.7883, best: 0.7947, time: 0:02:13
 Epoch: 289, lr: 8.0e-05, train_loss: 0.1238, train_acc: 0.9592 test_loss: 0.9957, test_acc: 0.7925, best: 0.7947, time: 0:02:13
 Epoch: 290, lr: 8.0e-05, train_loss: 0.1181, train_acc: 0.9618 test_loss: 0.9747, test_acc: 0.7911, best: 0.7947, time: 0:02:13
 Epoch: 291, lr: 8.0e-05, train_loss: 0.1231, train_acc: 0.9578 test_loss: 0.9879, test_acc: 0.7889, best: 0.7947, time: 0:02:14
 Epoch: 292, lr: 8.0e-05, train_loss: 0.1143, train_acc: 0.9608 test_loss: 1.0260, test_acc: 0.7865, best: 0.7947, time: 0:02:12
 Epoch: 293, lr: 8.0e-05, train_loss: 0.1067, train_acc: 0.9622 test_loss: 0.9872, test_acc: 0.7921, best: 0.7947, time: 0:02:11
 Epoch: 294, lr: 8.0e-05, train_loss: 0.1209, train_acc: 0.9602 test_loss: 0.9622, test_acc: 0.7880, best: 0.7947, time: 0:02:11
 Epoch: 295, lr: 8.0e-05, train_loss: 0.1211, train_acc: 0.9600 test_loss: 0.9759, test_acc: 0.7907, best: 0.7947, time: 0:02:10
 Epoch: 296, lr: 8.0e-05, train_loss: 0.1164, train_acc: 0.9630 test_loss: 0.9743, test_acc: 0.7931, best: 0.7947, time: 0:02:11
 Epoch: 297, lr: 8.0e-05, train_loss: 0.1333, train_acc: 0.9548 test_loss: 0.9406, test_acc: 0.7960, best: 0.7960, time: 0:02:11
 Epoch: 298, lr: 8.0e-05, train_loss: 0.1183, train_acc: 0.9592 test_loss: 0.9450, test_acc: 0.7951, best: 0.7960, time: 0:02:11
 Epoch: 299, lr: 8.0e-05, train_loss: 0.1175, train_acc: 0.9594 test_loss: 0.9709, test_acc: 0.7900, best: 0.7960, time: 0:02:10
 Highest accuracy: 0.7960