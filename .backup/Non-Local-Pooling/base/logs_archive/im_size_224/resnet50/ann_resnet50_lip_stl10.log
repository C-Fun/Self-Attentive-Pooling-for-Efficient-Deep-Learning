
 Run on time: 2022-06-24 20:03:39.030811

 Arguments:
	 gpu                  : True
	 log                  : True
	 seed                 : 0
	 momentum             : 0.9
	 amsgrad              : True
	 dataset              : STL10
	 batch_size           : 8
	 architecture         : RESNET50_LIP
	 im_size              : 224
	 relu_threshold       : 4.0
	 learning_rate        : 0.01
	 pretrained_backbone  : 
	 pretrained_ann       : 
	 weight_decay         : 0.0
	 test_only            : False
	 epochs               : 300
	 lr_interval          : [180, 240, 270]
	 lr_reduce            : 5
	 optimizer            : SGD
	 dropout              : 0.2
	 kernel_size          : 3
	 dont_save            : False
	 visualize            : False
	 devices              : 0,1
 DataParallel(
  (module): NetworkByName(
    (net): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (pool): Pool2d(
          (logit): Sequential(
            (pool_weight): LIP_BASE(
              (logit): Sequential(
                (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
                (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Sigmoid()
              )
            )
          )
          (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (pool): Pool2d(
          (logit): Sequential(
            (pool_weight): LIP_BASE(
              (logit): Sequential(
                (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
                (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Sigmoid()
              )
            )
          )
          (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        )
      )
      (layer4): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (pool): Pool2d(
          (logit): Sequential(
            (pool_weight): LIP_BASE(
              (logit): Sequential(
                (0): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1))
                (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Sigmoid()
              )
            )
          )
          (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        )
      )
      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
      (fc): Linear(in_features=2048, out_features=10, bias=True)
    )
  )
)
 SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0
)
 Epoch: 1, lr: 1.0e-02, train_loss: 2.6220, train_acc: 0.1548 test_loss: 2.4013, test_acc: 0.2384, best: 0.2384, time: 0:02:37
 Epoch: 2, lr: 1.0e-02, train_loss: 2.0623, train_acc: 0.2128 test_loss: 5.0465, test_acc: 0.2295, best: 0.2384, time: 0:02:35
 Epoch: 3, lr: 1.0e-02, train_loss: 1.9470, train_acc: 0.2502 test_loss: 4.8295, test_acc: 0.2721, best: 0.2721, time: 0:02:36
 Epoch: 4, lr: 1.0e-02, train_loss: 1.8661, train_acc: 0.2642 test_loss: 4.2506, test_acc: 0.2863, best: 0.2863, time: 0:02:35
 Epoch: 5, lr: 1.0e-02, train_loss: 1.8317, train_acc: 0.2806 test_loss: 10.5346, test_acc: 0.3300, best: 0.3300, time: 0:02:35
 Epoch: 6, lr: 1.0e-02, train_loss: 1.8389, train_acc: 0.2854 test_loss: 1.6285, test_acc: 0.3655, best: 0.3655, time: 0:02:36
 Epoch: 7, lr: 1.0e-02, train_loss: 1.7761, train_acc: 0.3150 test_loss: 8.5966, test_acc: 0.3256, best: 0.3655, time: 0:02:35
 Epoch: 8, lr: 1.0e-02, train_loss: 1.7385, train_acc: 0.3338 test_loss: 3.2875, test_acc: 0.3704, best: 0.3704, time: 0:02:35
 Epoch: 9, lr: 1.0e-02, train_loss: 1.6736, train_acc: 0.3696 test_loss: 3.8532, test_acc: 0.3794, best: 0.3794, time: 0:02:35
 Epoch: 10, lr: 1.0e-02, train_loss: 1.6300, train_acc: 0.3790 test_loss: 11.7926, test_acc: 0.3723, best: 0.3794, time: 0:02:35
 Epoch: 11, lr: 1.0e-02, train_loss: 1.6319, train_acc: 0.3754 test_loss: 2.4808, test_acc: 0.3546, best: 0.3794, time: 0:02:35
 Epoch: 12, lr: 1.0e-02, train_loss: 1.6063, train_acc: 0.3910 test_loss: 8.3364, test_acc: 0.4118, best: 0.4118, time: 0:02:35
 Epoch: 13, lr: 1.0e-02, train_loss: 1.5451, train_acc: 0.4124 test_loss: 7.8003, test_acc: 0.3015, best: 0.4118, time: 0:02:35
 Epoch: 14, lr: 1.0e-02, train_loss: 1.5301, train_acc: 0.4262 test_loss: 8.2123, test_acc: 0.4241, best: 0.4241, time: 0:02:35
 Epoch: 15, lr: 1.0e-02, train_loss: 1.5082, train_acc: 0.4324 test_loss: 10.5057, test_acc: 0.4124, best: 0.4241, time: 0:02:35
 Epoch: 16, lr: 1.0e-02, train_loss: 1.4828, train_acc: 0.4518 test_loss: 2.1289, test_acc: 0.4499, best: 0.4499, time: 0:02:36
 Epoch: 17, lr: 1.0e-02, train_loss: 1.4566, train_acc: 0.4614 test_loss: 4.1137, test_acc: 0.4296, best: 0.4499, time: 0:02:34
 Epoch: 18, lr: 1.0e-02, train_loss: 1.4339, train_acc: 0.4708 test_loss: 12.1962, test_acc: 0.4512, best: 0.4512, time: 0:02:35
 Epoch: 19, lr: 1.0e-02, train_loss: 1.3957, train_acc: 0.4862 test_loss: 2.9288, test_acc: 0.4634, best: 0.4634, time: 0:02:35
 Epoch: 20, lr: 1.0e-02, train_loss: 1.3910, train_acc: 0.4862 test_loss: 1.7580, test_acc: 0.4849, best: 0.4849, time: 0:02:35
 Epoch: 21, lr: 1.0e-02, train_loss: 1.3748, train_acc: 0.4916 test_loss: 8.3945, test_acc: 0.4740, best: 0.4849, time: 0:02:34
 Epoch: 22, lr: 1.0e-02, train_loss: 1.3396, train_acc: 0.5078 test_loss: 9.6048, test_acc: 0.4684, best: 0.4849, time: 0:02:34
 Epoch: 23, lr: 1.0e-02, train_loss: 1.3366, train_acc: 0.5042 test_loss: 5.4232, test_acc: 0.5041, best: 0.5041, time: 0:02:35
 Epoch: 24, lr: 1.0e-02, train_loss: 1.3089, train_acc: 0.5178 test_loss: 6.0254, test_acc: 0.4798, best: 0.5041, time: 0:02:33
 Epoch: 25, lr: 1.0e-02, train_loss: 1.3133, train_acc: 0.5196 test_loss: 4.0517, test_acc: 0.5084, best: 0.5084, time: 0:02:32
 Epoch: 26, lr: 1.0e-02, train_loss: 1.2766, train_acc: 0.5306 test_loss: 4.8396, test_acc: 0.5022, best: 0.5084, time: 0:02:32
 Epoch: 27, lr: 1.0e-02, train_loss: 1.2579, train_acc: 0.5456 test_loss: 6.5394, test_acc: 0.5085, best: 0.5085, time: 0:02:32
 Epoch: 28, lr: 1.0e-02, train_loss: 1.2333, train_acc: 0.5412 test_loss: 7.2330, test_acc: 0.4836, best: 0.5085, time: 0:02:31
 Epoch: 29, lr: 1.0e-02, train_loss: 1.2022, train_acc: 0.5584 test_loss: 5.0425, test_acc: 0.5367, best: 0.5367, time: 0:02:32
 Epoch: 30, lr: 1.0e-02, train_loss: 1.2112, train_acc: 0.5596 test_loss: 6.0105, test_acc: 0.5132, best: 0.5367, time: 0:02:32
 Epoch: 31, lr: 1.0e-02, train_loss: 1.1731, train_acc: 0.5724 test_loss: 2.9955, test_acc: 0.5290, best: 0.5367, time: 0:02:31
 Epoch: 32, lr: 1.0e-02, train_loss: 1.1713, train_acc: 0.5706 test_loss: 5.4474, test_acc: 0.5172, best: 0.5367, time: 0:02:32
 Epoch: 33, lr: 1.0e-02, train_loss: 1.1547, train_acc: 0.5758 test_loss: 3.9581, test_acc: 0.5481, best: 0.5481, time: 0:02:32
 Epoch: 34, lr: 1.0e-02, train_loss: 1.1433, train_acc: 0.5892 test_loss: 26.6911, test_acc: 0.5155, best: 0.5481, time: 0:02:31
 Epoch: 35, lr: 1.0e-02, train_loss: 1.1485, train_acc: 0.5812 test_loss: 1.5364, test_acc: 0.5591, best: 0.5591, time: 0:02:32
 Epoch: 36, lr: 1.0e-02, train_loss: 1.1529, train_acc: 0.5828 test_loss: 5.6245, test_acc: 0.5275, best: 0.5591, time: 0:02:31
 Epoch: 37, lr: 1.0e-02, train_loss: 1.1141, train_acc: 0.6048 test_loss: 1.8463, test_acc: 0.5787, best: 0.5787, time: 0:02:32
 Epoch: 38, lr: 1.0e-02, train_loss: 1.0983, train_acc: 0.5984 test_loss: 20.4272, test_acc: 0.5190, best: 0.5787, time: 0:02:31
 Epoch: 39, lr: 1.0e-02, train_loss: 1.0827, train_acc: 0.6098 test_loss: 4.5889, test_acc: 0.5469, best: 0.5787, time: 0:02:31
 Epoch: 40, lr: 1.0e-02, train_loss: 1.0739, train_acc: 0.6112 test_loss: 1.1867, test_acc: 0.6116, best: 0.6116, time: 0:02:32
 Epoch: 41, lr: 1.0e-02, train_loss: 1.1045, train_acc: 0.6016 test_loss: 1.6648, test_acc: 0.5720, best: 0.6116, time: 0:02:32
 Epoch: 42, lr: 1.0e-02, train_loss: 1.0970, train_acc: 0.6098 test_loss: 1.3592, test_acc: 0.5626, best: 0.6116, time: 0:02:32
 Epoch: 43, lr: 1.0e-02, train_loss: 1.0634, train_acc: 0.6156 test_loss: 1.9225, test_acc: 0.5407, best: 0.6116, time: 0:02:31
 Epoch: 44, lr: 1.0e-02, train_loss: 1.0237, train_acc: 0.6312 test_loss: 1.5575, test_acc: 0.6048, best: 0.6116, time: 0:02:31
 Epoch: 45, lr: 1.0e-02, train_loss: 1.0162, train_acc: 0.6356 test_loss: 3.4783, test_acc: 0.5636, best: 0.6116, time: 0:02:32
 Epoch: 46, lr: 1.0e-02, train_loss: 0.9873, train_acc: 0.6434 test_loss: 1.8988, test_acc: 0.5992, best: 0.6116, time: 0:02:32
 Epoch: 47, lr: 1.0e-02, train_loss: 0.9807, train_acc: 0.6476 test_loss: 2.0098, test_acc: 0.6078, best: 0.6116, time: 0:02:31
 Epoch: 48, lr: 1.0e-02, train_loss: 0.9535, train_acc: 0.6596 test_loss: 2.5285, test_acc: 0.5693, best: 0.6116, time: 0:02:31
 Epoch: 49, lr: 1.0e-02, train_loss: 0.9667, train_acc: 0.6442 test_loss: 2.9941, test_acc: 0.5800, best: 0.6116, time: 0:02:31
 Epoch: 50, lr: 1.0e-02, train_loss: 0.9484, train_acc: 0.6644 test_loss: 8.8155, test_acc: 0.5806, best: 0.6116, time: 0:02:31
 Epoch: 51, lr: 1.0e-02, train_loss: 0.9329, train_acc: 0.6624 test_loss: 33.0745, test_acc: 0.5715, best: 0.6116, time: 0:02:31
 Epoch: 52, lr: 1.0e-02, train_loss: 0.9234, train_acc: 0.6670 test_loss: 21.3518, test_acc: 0.5696, best: 0.6116, time: 0:02:32
 Epoch: 53, lr: 1.0e-02, train_loss: 0.9152, train_acc: 0.6776 test_loss: 23.2145, test_acc: 0.5535, best: 0.6116, time: 0:02:31
 Epoch: 54, lr: 1.0e-02, train_loss: 0.9161, train_acc: 0.6744 test_loss: 29.3504, test_acc: 0.5876, best: 0.6116, time: 0:02:31
 Epoch: 55, lr: 1.0e-02, train_loss: 0.9140, train_acc: 0.6766 test_loss: 2.8235, test_acc: 0.6474, best: 0.6474, time: 0:02:32
 Epoch: 56, lr: 1.0e-02, train_loss: 0.9114, train_acc: 0.6734 test_loss: 25.8642, test_acc: 0.5735, best: 0.6474, time: 0:02:31
 Epoch: 57, lr: 1.0e-02, train_loss: 0.8920, train_acc: 0.6894 test_loss: 6.3263, test_acc: 0.5547, best: 0.6474, time: 0:02:31
 Epoch: 58, lr: 1.0e-02, train_loss: 0.8795, train_acc: 0.6930 test_loss: 11.9858, test_acc: 0.5877, best: 0.6474, time: 0:02:31
 Epoch: 59, lr: 1.0e-02, train_loss: 0.8733, train_acc: 0.6886 test_loss: 2.7950, test_acc: 0.6171, best: 0.6474, time: 0:02:31
 Epoch: 60, lr: 1.0e-02, train_loss: 0.8649, train_acc: 0.6918 test_loss: 3.6503, test_acc: 0.6169, best: 0.6474, time: 0:02:31
 Epoch: 61, lr: 1.0e-02, train_loss: 0.8528, train_acc: 0.6968 test_loss: 2.2913, test_acc: 0.6104, best: 0.6474, time: 0:02:32
 Epoch: 62, lr: 1.0e-02, train_loss: 0.8134, train_acc: 0.7016 test_loss: 2.8056, test_acc: 0.6032, best: 0.6474, time: 0:02:31
 Epoch: 63, lr: 1.0e-02, train_loss: 0.8125, train_acc: 0.7052 test_loss: 4.0953, test_acc: 0.5949, best: 0.6474, time: 0:02:32
 Epoch: 64, lr: 1.0e-02, train_loss: 0.8209, train_acc: 0.7110 test_loss: 3.8759, test_acc: 0.6301, best: 0.6474, time: 0:02:32
 Epoch: 65, lr: 1.0e-02, train_loss: 0.7945, train_acc: 0.7198 test_loss: 3.9509, test_acc: 0.5965, best: 0.6474, time: 0:02:32
 Epoch: 66, lr: 1.0e-02, train_loss: 0.7940, train_acc: 0.7160 test_loss: 5.2640, test_acc: 0.6318, best: 0.6474, time: 0:02:31
 Epoch: 67, lr: 1.0e-02, train_loss: 0.7725, train_acc: 0.7238 test_loss: 4.1197, test_acc: 0.6114, best: 0.6474, time: 0:02:32
 Epoch: 68, lr: 1.0e-02, train_loss: 0.7745, train_acc: 0.7204 test_loss: 3.3930, test_acc: 0.6396, best: 0.6474, time: 0:02:32
 Epoch: 69, lr: 1.0e-02, train_loss: 0.7553, train_acc: 0.7308 test_loss: 4.4520, test_acc: 0.6452, best: 0.6474, time: 0:02:32
 Epoch: 70, lr: 1.0e-02, train_loss: 0.7443, train_acc: 0.7372 test_loss: 2.1328, test_acc: 0.6736, best: 0.6736, time: 0:02:32
 Epoch: 71, lr: 1.0e-02, train_loss: 0.7096, train_acc: 0.7446 test_loss: 2.0314, test_acc: 0.6784, best: 0.6784, time: 0:02:32
 Epoch: 72, lr: 1.0e-02, train_loss: 0.7527, train_acc: 0.7298 test_loss: 5.7225, test_acc: 0.6320, best: 0.6784, time: 0:02:31
 Epoch: 73, lr: 1.0e-02, train_loss: 0.7289, train_acc: 0.7480 test_loss: 5.1388, test_acc: 0.6419, best: 0.6784, time: 0:02:31
 Epoch: 74, lr: 1.0e-02, train_loss: 0.7178, train_acc: 0.7536 test_loss: 9.0191, test_acc: 0.6286, best: 0.6784, time: 0:02:31
 Epoch: 75, lr: 1.0e-02, train_loss: 0.7073, train_acc: 0.7496 test_loss: 8.4213, test_acc: 0.6654, best: 0.6784, time: 0:02:32
 Epoch: 76, lr: 1.0e-02, train_loss: 0.6743, train_acc: 0.7604 test_loss: 5.5029, test_acc: 0.6404, best: 0.6784, time: 0:02:32
 Epoch: 77, lr: 1.0e-02, train_loss: 0.6866, train_acc: 0.7562 test_loss: 7.9588, test_acc: 0.6160, best: 0.6784, time: 0:02:32
 Epoch: 78, lr: 1.0e-02, train_loss: 0.6691, train_acc: 0.7638 test_loss: 4.3953, test_acc: 0.6270, best: 0.6784, time: 0:02:32
 Epoch: 79, lr: 1.0e-02, train_loss: 0.6473, train_acc: 0.7748 test_loss: 8.3564, test_acc: 0.6144, best: 0.6784, time: 0:02:31
 Epoch: 80, lr: 1.0e-02, train_loss: 0.6834, train_acc: 0.7530 test_loss: 8.4378, test_acc: 0.6040, best: 0.6784, time: 0:02:32
 Epoch: 81, lr: 1.0e-02, train_loss: 0.6890, train_acc: 0.7616 test_loss: 2.0925, test_acc: 0.6770, best: 0.6784, time: 0:02:31
 Epoch: 82, lr: 1.0e-02, train_loss: 0.6559, train_acc: 0.7680 test_loss: 2.3650, test_acc: 0.6973, best: 0.6973, time: 0:02:32
 Epoch: 83, lr: 1.0e-02, train_loss: 0.6351, train_acc: 0.7730 test_loss: 4.0244, test_acc: 0.6389, best: 0.6973, time: 0:02:32
 Epoch: 84, lr: 1.0e-02, train_loss: 0.6226, train_acc: 0.7774 test_loss: 4.2345, test_acc: 0.6502, best: 0.6973, time: 0:02:32
 Epoch: 85, lr: 1.0e-02, train_loss: 0.6213, train_acc: 0.7816 test_loss: 5.6808, test_acc: 0.6554, best: 0.6973, time: 0:02:32
 Epoch: 86, lr: 1.0e-02, train_loss: 0.6162, train_acc: 0.7798 test_loss: 2.2772, test_acc: 0.7007, best: 0.7007, time: 0:02:34
 Epoch: 87, lr: 1.0e-02, train_loss: 0.6068, train_acc: 0.7878 test_loss: 2.2781, test_acc: 0.6889, best: 0.7007, time: 0:02:33
 Epoch: 88, lr: 1.0e-02, train_loss: 0.5997, train_acc: 0.7922 test_loss: 2.8319, test_acc: 0.7071, best: 0.7071, time: 0:02:32
 Epoch: 89, lr: 1.0e-02, train_loss: 0.6156, train_acc: 0.7836 test_loss: 4.3346, test_acc: 0.6651, best: 0.7071, time: 0:02:33
 Epoch: 90, lr: 1.0e-02, train_loss: 0.5832, train_acc: 0.7938 test_loss: 5.3905, test_acc: 0.6676, best: 0.7071, time: 0:02:32
 Epoch: 91, lr: 1.0e-02, train_loss: 0.5853, train_acc: 0.7958 test_loss: 3.2635, test_acc: 0.6854, best: 0.7071, time: 0:02:32
 Epoch: 92, lr: 1.0e-02, train_loss: 0.5971, train_acc: 0.7926 test_loss: 7.8704, test_acc: 0.6707, best: 0.7071, time: 0:02:34
 Epoch: 93, lr: 1.0e-02, train_loss: 0.5575, train_acc: 0.7988 test_loss: 3.4613, test_acc: 0.6807, best: 0.7071, time: 0:02:34
 Epoch: 94, lr: 1.0e-02, train_loss: 0.5597, train_acc: 0.7976 test_loss: 3.2640, test_acc: 0.7242, best: 0.7242, time: 0:02:34
 Epoch: 95, lr: 1.0e-02, train_loss: 0.5550, train_acc: 0.8032 test_loss: 3.7245, test_acc: 0.6893, best: 0.7242, time: 0:02:34
 Epoch: 96, lr: 1.0e-02, train_loss: 0.5481, train_acc: 0.8082 test_loss: 4.1613, test_acc: 0.6890, best: 0.7242, time: 0:02:34
 Epoch: 97, lr: 1.0e-02, train_loss: 0.5326, train_acc: 0.8118 test_loss: 2.6074, test_acc: 0.7166, best: 0.7242, time: 0:02:34
 Epoch: 98, lr: 1.0e-02, train_loss: 0.5085, train_acc: 0.8242 test_loss: 11.4723, test_acc: 0.6360, best: 0.7242, time: 0:02:34
 Epoch: 99, lr: 1.0e-02, train_loss: 0.5290, train_acc: 0.8132 test_loss: 3.6668, test_acc: 0.7131, best: 0.7242, time: 0:02:34
 Epoch: 100, lr: 1.0e-02, train_loss: 0.5246, train_acc: 0.8210 test_loss: 11.2455, test_acc: 0.6000, best: 0.7242, time: 0:02:34
 Epoch: 101, lr: 1.0e-02, train_loss: 0.5217, train_acc: 0.8162 test_loss: 3.4134, test_acc: 0.7021, best: 0.7242, time: 0:02:34
 Epoch: 102, lr: 1.0e-02, train_loss: 0.5150, train_acc: 0.8220 test_loss: 7.9655, test_acc: 0.6471, best: 0.7242, time: 0:02:34
 Epoch: 103, lr: 1.0e-02, train_loss: 0.5039, train_acc: 0.8230 test_loss: 2.9133, test_acc: 0.7053, best: 0.7242, time: 0:02:34
 Epoch: 104, lr: 1.0e-02, train_loss: 0.5199, train_acc: 0.8208 test_loss: 4.2665, test_acc: 0.6744, best: 0.7242, time: 0:02:34
 Epoch: 105, lr: 1.0e-02, train_loss: 0.5340, train_acc: 0.8124 test_loss: 7.4853, test_acc: 0.6630, best: 0.7242, time: 0:02:34
 Epoch: 106, lr: 1.0e-02, train_loss: 0.4820, train_acc: 0.8308 test_loss: 2.5053, test_acc: 0.6954, best: 0.7242, time: 0:02:34
 Epoch: 107, lr: 1.0e-02, train_loss: 0.4901, train_acc: 0.8266 test_loss: 3.1654, test_acc: 0.7124, best: 0.7242, time: 0:02:34
 Epoch: 108, lr: 1.0e-02, train_loss: 0.4812, train_acc: 0.8326 test_loss: 3.7722, test_acc: 0.7173, best: 0.7242, time: 0:02:34
 Epoch: 109, lr: 1.0e-02, train_loss: 0.4874, train_acc: 0.8282 test_loss: 4.9449, test_acc: 0.6857, best: 0.7242, time: 0:02:34
 Epoch: 110, lr: 1.0e-02, train_loss: 0.4733, train_acc: 0.8396 test_loss: 5.1300, test_acc: 0.7037, best: 0.7242, time: 0:02:34
 Epoch: 111, lr: 1.0e-02, train_loss: 0.4645, train_acc: 0.8408 test_loss: 5.9007, test_acc: 0.6917, best: 0.7242, time: 0:02:34
 Epoch: 112, lr: 1.0e-02, train_loss: 0.4623, train_acc: 0.8402 test_loss: 8.8740, test_acc: 0.6587, best: 0.7242, time: 0:02:34
 Epoch: 113, lr: 1.0e-02, train_loss: 0.4777, train_acc: 0.8354 test_loss: 17.9594, test_acc: 0.7089, best: 0.7242, time: 0:02:34
 Epoch: 114, lr: 1.0e-02, train_loss: 0.4642, train_acc: 0.8342 test_loss: 9.1168, test_acc: 0.6681, best: 0.7242, time: 0:02:34
 Epoch: 115, lr: 1.0e-02, train_loss: 0.4346, train_acc: 0.8488 test_loss: 6.7266, test_acc: 0.6770, best: 0.7242, time: 0:02:34
 Epoch: 116, lr: 1.0e-02, train_loss: 0.4475, train_acc: 0.8510 test_loss: 2.3993, test_acc: 0.7268, best: 0.7268, time: 0:02:35
 Epoch: 117, lr: 1.0e-02, train_loss: 0.4585, train_acc: 0.8382 test_loss: 2.8027, test_acc: 0.7073, best: 0.7268, time: 0:02:34
 Epoch: 118, lr: 1.0e-02, train_loss: 0.4354, train_acc: 0.8490 test_loss: 2.8035, test_acc: 0.7136, best: 0.7268, time: 0:02:34
 Epoch: 119, lr: 1.0e-02, train_loss: 0.4253, train_acc: 0.8492 test_loss: 7.1296, test_acc: 0.6899, best: 0.7268, time: 0:02:34
 Epoch: 120, lr: 1.0e-02, train_loss: 0.4423, train_acc: 0.8460 test_loss: 3.8236, test_acc: 0.7080, best: 0.7268, time: 0:02:34
 Epoch: 121, lr: 1.0e-02, train_loss: 0.4090, train_acc: 0.8616 test_loss: 3.0230, test_acc: 0.7181, best: 0.7268, time: 0:02:34
 Epoch: 122, lr: 1.0e-02, train_loss: 0.4240, train_acc: 0.8544 test_loss: 6.7826, test_acc: 0.7089, best: 0.7268, time: 0:02:34
 Epoch: 123, lr: 1.0e-02, train_loss: 0.4151, train_acc: 0.8542 test_loss: 1.6764, test_acc: 0.7374, best: 0.7374, time: 0:02:35
 Epoch: 124, lr: 1.0e-02, train_loss: 0.3823, train_acc: 0.8724 test_loss: 2.6917, test_acc: 0.7234, best: 0.7374, time: 0:02:34
 Epoch: 125, lr: 1.0e-02, train_loss: 0.3989, train_acc: 0.8610 test_loss: 2.2046, test_acc: 0.7384, best: 0.7384, time: 0:02:35
 Epoch: 126, lr: 1.0e-02, train_loss: 0.3933, train_acc: 0.8620 test_loss: 4.6584, test_acc: 0.7346, best: 0.7384, time: 0:02:34
 Epoch: 127, lr: 1.0e-02, train_loss: 0.4098, train_acc: 0.8556 test_loss: 12.3198, test_acc: 0.7003, best: 0.7384, time: 0:02:34
 Epoch: 128, lr: 1.0e-02, train_loss: 0.3858, train_acc: 0.8692 test_loss: 4.0223, test_acc: 0.7191, best: 0.7384, time: 0:02:34
 Epoch: 129, lr: 1.0e-02, train_loss: 0.3993, train_acc: 0.8630 test_loss: 5.3913, test_acc: 0.7248, best: 0.7384, time: 0:02:34
 Epoch: 130, lr: 1.0e-02, train_loss: 0.3776, train_acc: 0.8656 test_loss: 3.8872, test_acc: 0.6743, best: 0.7384, time: 0:02:34
 Epoch: 131, lr: 1.0e-02, train_loss: 0.3864, train_acc: 0.8638 test_loss: 2.7926, test_acc: 0.7301, best: 0.7384, time: 0:02:34
 Epoch: 132, lr: 1.0e-02, train_loss: 0.3712, train_acc: 0.8720 test_loss: 3.8485, test_acc: 0.7014, best: 0.7384, time: 0:02:34
 Epoch: 133, lr: 1.0e-02, train_loss: 0.3996, train_acc: 0.8628 test_loss: 8.1726, test_acc: 0.6941, best: 0.7384, time: 0:02:34
 Epoch: 134, lr: 1.0e-02, train_loss: 0.3731, train_acc: 0.8684 test_loss: 9.0328, test_acc: 0.7139, best: 0.7384, time: 0:02:34
 Epoch: 135, lr: 1.0e-02, train_loss: 0.3510, train_acc: 0.8762 test_loss: 4.0775, test_acc: 0.7133, best: 0.7384, time: 0:02:34
 Epoch: 136, lr: 1.0e-02, train_loss: 0.3663, train_acc: 0.8740 test_loss: 4.5321, test_acc: 0.6750, best: 0.7384, time: 0:02:34
 Epoch: 137, lr: 1.0e-02, train_loss: 0.3707, train_acc: 0.8750 test_loss: 24.8497, test_acc: 0.6548, best: 0.7384, time: 0:02:34
 Epoch: 138, lr: 1.0e-02, train_loss: 0.3693, train_acc: 0.8770 test_loss: 14.5666, test_acc: 0.6800, best: 0.7384, time: 0:02:34
 Epoch: 139, lr: 1.0e-02, train_loss: 0.3681, train_acc: 0.8730 test_loss: 9.4039, test_acc: 0.7169, best: 0.7384, time: 0:02:34
 Epoch: 140, lr: 1.0e-02, train_loss: 0.3742, train_acc: 0.8678 test_loss: 17.8040, test_acc: 0.6299, best: 0.7384, time: 0:02:34
 Epoch: 141, lr: 1.0e-02, train_loss: 0.3447, train_acc: 0.8812 test_loss: 5.0057, test_acc: 0.7067, best: 0.7384, time: 0:02:34
 Epoch: 142, lr: 1.0e-02, train_loss: 0.3403, train_acc: 0.8818 test_loss: 14.9185, test_acc: 0.6936, best: 0.7384, time: 0:02:34
 Epoch: 143, lr: 1.0e-02, train_loss: 0.3545, train_acc: 0.8776 test_loss: 22.1069, test_acc: 0.6815, best: 0.7384, time: 0:02:34
 Epoch: 144, lr: 1.0e-02, train_loss: 0.3495, train_acc: 0.8844 test_loss: 18.1148, test_acc: 0.6970, best: 0.7384, time: 0:02:34
 Epoch: 145, lr: 1.0e-02, train_loss: 0.3372, train_acc: 0.8828 test_loss: 6.7422, test_acc: 0.7346, best: 0.7384, time: 0:02:34
 Epoch: 146, lr: 1.0e-02, train_loss: 0.3578, train_acc: 0.8750 test_loss: 27.7961, test_acc: 0.7080, best: 0.7384, time: 0:02:34
 Epoch: 147, lr: 1.0e-02, train_loss: 0.3132, train_acc: 0.8924 test_loss: 12.0090, test_acc: 0.6860, best: 0.7384, time: 0:02:34
 Epoch: 148, lr: 1.0e-02, train_loss: 0.3353, train_acc: 0.8846 test_loss: 7.6170, test_acc: 0.7041, best: 0.7384, time: 0:02:34
 Epoch: 149, lr: 1.0e-02, train_loss: 0.3743, train_acc: 0.8684 test_loss: 0.8638, test_acc: 0.7722, best: 0.7722, time: 0:02:35
 Epoch: 150, lr: 1.0e-02, train_loss: 0.3436, train_acc: 0.8838 test_loss: 1.0142, test_acc: 0.7611, best: 0.7722, time: 0:02:34
 Epoch: 151, lr: 1.0e-02, train_loss: 0.3500, train_acc: 0.8760 test_loss: 0.9135, test_acc: 0.7766, best: 0.7766, time: 0:02:35
 Epoch: 152, lr: 1.0e-02, train_loss: 0.3213, train_acc: 0.8890 test_loss: 0.9895, test_acc: 0.7661, best: 0.7766, time: 0:02:34
 Epoch: 153, lr: 1.0e-02, train_loss: 0.3390, train_acc: 0.8832 test_loss: 0.8929, test_acc: 0.7750, best: 0.7766, time: 0:02:34
 Epoch: 154, lr: 1.0e-02, train_loss: 0.3449, train_acc: 0.8866 test_loss: 0.9122, test_acc: 0.7680, best: 0.7766, time: 0:02:34
 Epoch: 155, lr: 1.0e-02, train_loss: 0.3166, train_acc: 0.8926 test_loss: 0.9906, test_acc: 0.7559, best: 0.7766, time: 0:02:34
 Epoch: 156, lr: 1.0e-02, train_loss: 0.3398, train_acc: 0.8832 test_loss: 0.9983, test_acc: 0.7725, best: 0.7766, time: 0:02:34
 Epoch: 157, lr: 1.0e-02, train_loss: 0.3509, train_acc: 0.8786 test_loss: 0.9935, test_acc: 0.7756, best: 0.7766, time: 0:02:34
 Epoch: 158, lr: 1.0e-02, train_loss: 0.3192, train_acc: 0.8920 test_loss: 1.0500, test_acc: 0.7505, best: 0.7766, time: 0:02:34
 Epoch: 159, lr: 1.0e-02, train_loss: 0.3310, train_acc: 0.8872 test_loss: 1.6933, test_acc: 0.7652, best: 0.7766, time: 0:02:34
 Epoch: 160, lr: 1.0e-02, train_loss: 0.3160, train_acc: 0.8870 test_loss: 1.5180, test_acc: 0.7611, best: 0.7766, time: 0:02:34
 Epoch: 161, lr: 1.0e-02, train_loss: 0.3333, train_acc: 0.8840 test_loss: 1.0466, test_acc: 0.7672, best: 0.7766, time: 0:02:34
 Epoch: 162, lr: 1.0e-02, train_loss: 0.3179, train_acc: 0.8900 test_loss: 1.1796, test_acc: 0.7461, best: 0.7766, time: 0:02:34
 Epoch: 163, lr: 1.0e-02, train_loss: 0.2999, train_acc: 0.8988 test_loss: 1.2501, test_acc: 0.7684, best: 0.7766, time: 0:02:34
 Epoch: 164, lr: 1.0e-02, train_loss: 0.3053, train_acc: 0.8986 test_loss: 1.0500, test_acc: 0.7756, best: 0.7766, time: 0:02:34
 Epoch: 165, lr: 1.0e-02, train_loss: 0.2957, train_acc: 0.8988 test_loss: 1.1858, test_acc: 0.7444, best: 0.7766, time: 0:02:34
 Epoch: 166, lr: 1.0e-02, train_loss: 0.2956, train_acc: 0.9014 test_loss: 0.8456, test_acc: 0.7742, best: 0.7766, time: 0:02:34
 Epoch: 167, lr: 1.0e-02, train_loss: 0.3008, train_acc: 0.8968 test_loss: 2.8230, test_acc: 0.7185, best: 0.7766, time: 0:02:34
 Epoch: 168, lr: 1.0e-02, train_loss: 0.2878, train_acc: 0.8992 test_loss: 1.1639, test_acc: 0.7645, best: 0.7766, time: 0:02:34
 Epoch: 169, lr: 1.0e-02, train_loss: 0.2684, train_acc: 0.9048 test_loss: 1.2995, test_acc: 0.7434, best: 0.7766, time: 0:02:34
 Epoch: 170, lr: 1.0e-02, train_loss: 0.2988, train_acc: 0.9022 test_loss: 1.0496, test_acc: 0.7619, best: 0.7766, time: 0:02:34
 Epoch: 171, lr: 1.0e-02, train_loss: 0.2945, train_acc: 0.8990 test_loss: 1.4043, test_acc: 0.7381, best: 0.7766, time: 0:02:34
 Epoch: 172, lr: 1.0e-02, train_loss: 0.2903, train_acc: 0.8996 test_loss: 1.2192, test_acc: 0.7375, best: 0.7766, time: 0:02:34
 Epoch: 173, lr: 1.0e-02, train_loss: 0.2822, train_acc: 0.9042 test_loss: 1.1548, test_acc: 0.7675, best: 0.7766, time: 0:02:34
 Epoch: 174, lr: 1.0e-02, train_loss: 0.2739, train_acc: 0.9106 test_loss: 1.5768, test_acc: 0.7551, best: 0.7766, time: 0:02:34
 Epoch: 175, lr: 1.0e-02, train_loss: 0.2560, train_acc: 0.9096 test_loss: 1.4029, test_acc: 0.7372, best: 0.7766, time: 0:02:34
 Epoch: 176, lr: 1.0e-02, train_loss: 0.2855, train_acc: 0.9054 test_loss: 1.2017, test_acc: 0.7584, best: 0.7766, time: 0:02:34
 Epoch: 177, lr: 1.0e-02, train_loss: 0.2557, train_acc: 0.9138 test_loss: 0.9633, test_acc: 0.7795, best: 0.7795, time: 0:02:35
 Epoch: 178, lr: 1.0e-02, train_loss: 0.2666, train_acc: 0.9062 test_loss: 1.0712, test_acc: 0.7722, best: 0.7795, time: 0:02:35
 Epoch: 179, lr: 1.0e-02, train_loss: 0.2766, train_acc: 0.9016 test_loss: 1.4392, test_acc: 0.7532, best: 0.7795, time: 0:02:34
 Epoch: 180, lr: 2.0e-03, train_loss: 0.2050, train_acc: 0.9298 test_loss: 1.9191, test_acc: 0.7766, best: 0.7795, time: 0:02:34
 Epoch: 181, lr: 2.0e-03, train_loss: 0.2031, train_acc: 0.9280 test_loss: 1.4660, test_acc: 0.7840, best: 0.7840, time: 0:02:35
 Epoch: 182, lr: 2.0e-03, train_loss: 0.1794, train_acc: 0.9406 test_loss: 1.5096, test_acc: 0.7915, best: 0.7915, time: 0:02:35
 Epoch: 183, lr: 2.0e-03, train_loss: 0.1666, train_acc: 0.9434 test_loss: 1.1643, test_acc: 0.8024, best: 0.8024, time: 0:02:35
 Epoch: 184, lr: 2.0e-03, train_loss: 0.1729, train_acc: 0.9408 test_loss: 1.2440, test_acc: 0.7931, best: 0.8024, time: 0:02:34
 Epoch: 185, lr: 2.0e-03, train_loss: 0.1719, train_acc: 0.9400 test_loss: 2.0720, test_acc: 0.7824, best: 0.8024, time: 0:02:34
 Epoch: 186, lr: 2.0e-03, train_loss: 0.1577, train_acc: 0.9456 test_loss: 2.0148, test_acc: 0.7738, best: 0.8024, time: 0:02:34
 Epoch: 187, lr: 2.0e-03, train_loss: 0.1673, train_acc: 0.9444 test_loss: 2.1617, test_acc: 0.7601, best: 0.8024, time: 0:02:34
 Epoch: 188, lr: 2.0e-03, train_loss: 0.1614, train_acc: 0.9454 test_loss: 1.2274, test_acc: 0.8007, best: 0.8024, time: 0:02:34
 Epoch: 189, lr: 2.0e-03, train_loss: 0.1716, train_acc: 0.9410 test_loss: 1.5963, test_acc: 0.7835, best: 0.8024, time: 0:02:34
 Epoch: 190, lr: 2.0e-03, train_loss: 0.1774, train_acc: 0.9394 test_loss: 1.7333, test_acc: 0.7782, best: 0.8024, time: 0:02:34
 Epoch: 191, lr: 2.0e-03, train_loss: 0.1715, train_acc: 0.9416 test_loss: 1.4219, test_acc: 0.7955, best: 0.8024, time: 0:02:35
 Epoch: 192, lr: 2.0e-03, train_loss: 0.1693, train_acc: 0.9422 test_loss: 1.6074, test_acc: 0.7834, best: 0.8024, time: 0:02:34
 Epoch: 193, lr: 2.0e-03, train_loss: 0.1560, train_acc: 0.9460 test_loss: 1.8760, test_acc: 0.7886, best: 0.8024, time: 0:02:34
 Epoch: 194, lr: 2.0e-03, train_loss: 0.1606, train_acc: 0.9498 test_loss: 4.5460, test_acc: 0.7501, best: 0.8024, time: 0:02:34
 Epoch: 195, lr: 2.0e-03, train_loss: 0.1629, train_acc: 0.9452 test_loss: 1.4175, test_acc: 0.7903, best: 0.8024, time: 0:02:34
 Epoch: 196, lr: 2.0e-03, train_loss: 0.1569, train_acc: 0.9458 test_loss: 1.3605, test_acc: 0.7931, best: 0.8024, time: 0:02:35
 Epoch: 197, lr: 2.0e-03, train_loss: 0.1530, train_acc: 0.9442 test_loss: 1.0729, test_acc: 0.8006, best: 0.8024, time: 0:02:35
 Epoch: 198, lr: 2.0e-03, train_loss: 0.1674, train_acc: 0.9464 test_loss: 1.3956, test_acc: 0.7899, best: 0.8024, time: 0:02:34
 Epoch: 199, lr: 2.0e-03, train_loss: 0.1586, train_acc: 0.9472 test_loss: 1.1252, test_acc: 0.8000, best: 0.8024, time: 0:02:34
 Epoch: 200, lr: 2.0e-03, train_loss: 0.1601, train_acc: 0.9460 test_loss: 1.0822, test_acc: 0.7984, best: 0.8024, time: 0:02:34
 Epoch: 201, lr: 2.0e-03, train_loss: 0.1553, train_acc: 0.9488 test_loss: 1.1388, test_acc: 0.7981, best: 0.8024, time: 0:02:35
 Epoch: 202, lr: 2.0e-03, train_loss: 0.1649, train_acc: 0.9414 test_loss: 1.5069, test_acc: 0.7885, best: 0.8024, time: 0:02:34
 Epoch: 203, lr: 2.0e-03, train_loss: 0.1594, train_acc: 0.9434 test_loss: 3.7041, test_acc: 0.7555, best: 0.8024, time: 0:02:35
 Epoch: 204, lr: 2.0e-03, train_loss: 0.1490, train_acc: 0.9516 test_loss: 2.1809, test_acc: 0.7678, best: 0.8024, time: 0:02:34
 Epoch: 205, lr: 2.0e-03, train_loss: 0.1448, train_acc: 0.9518 test_loss: 1.1762, test_acc: 0.7941, best: 0.8024, time: 0:02:34
 Epoch: 206, lr: 2.0e-03, train_loss: 0.1494, train_acc: 0.9518 test_loss: 1.1256, test_acc: 0.7976, best: 0.8024, time: 0:02:34
 Epoch: 207, lr: 2.0e-03, train_loss: 0.1555, train_acc: 0.9484 test_loss: 1.3466, test_acc: 0.7961, best: 0.8024, time: 0:02:34
 Epoch: 208, lr: 2.0e-03, train_loss: 0.1501, train_acc: 0.9464 test_loss: 1.6180, test_acc: 0.7827, best: 0.8024, time: 0:02:34
 Epoch: 209, lr: 2.0e-03, train_loss: 0.1560, train_acc: 0.9474 test_loss: 1.8925, test_acc: 0.7725, best: 0.8024, time: 0:02:34
 Epoch: 210, lr: 2.0e-03, train_loss: 0.1479, train_acc: 0.9486 test_loss: 1.4904, test_acc: 0.7951, best: 0.8024, time: 0:02:34
 Epoch: 211, lr: 2.0e-03, train_loss: 0.1419, train_acc: 0.9538 test_loss: 1.5469, test_acc: 0.7805, best: 0.8024, time: 0:02:35
 Epoch: 212, lr: 2.0e-03, train_loss: 0.1479, train_acc: 0.9488 test_loss: 1.4572, test_acc: 0.7790, best: 0.8024, time: 0:02:34
 Epoch: 213, lr: 2.0e-03, train_loss: 0.1595, train_acc: 0.9450 test_loss: 0.9906, test_acc: 0.7959, best: 0.8024, time: 0:02:34
 Epoch: 214, lr: 2.0e-03, train_loss: 0.1349, train_acc: 0.9510 test_loss: 1.2952, test_acc: 0.7893, best: 0.8024, time: 0:02:35
 Epoch: 215, lr: 2.0e-03, train_loss: 0.1418, train_acc: 0.9534 test_loss: 1.2983, test_acc: 0.7884, best: 0.8024, time: 0:02:34
 Epoch: 216, lr: 2.0e-03, train_loss: 0.1452, train_acc: 0.9504 test_loss: 1.4118, test_acc: 0.7923, best: 0.8024, time: 0:02:34
 Epoch: 217, lr: 2.0e-03, train_loss: 0.1457, train_acc: 0.9494 test_loss: 1.2860, test_acc: 0.7907, best: 0.8024, time: 0:02:35
 Epoch: 218, lr: 2.0e-03, train_loss: 0.1444, train_acc: 0.9508 test_loss: 1.2180, test_acc: 0.7925, best: 0.8024, time: 0:02:34
 Epoch: 219, lr: 2.0e-03, train_loss: 0.1596, train_acc: 0.9448 test_loss: 1.6309, test_acc: 0.7975, best: 0.8024, time: 0:02:34
 Epoch: 220, lr: 2.0e-03, train_loss: 0.1511, train_acc: 0.9486 test_loss: 1.1088, test_acc: 0.7926, best: 0.8024, time: 0:02:34
 Epoch: 221, lr: 2.0e-03, train_loss: 0.1416, train_acc: 0.9498 test_loss: 1.6012, test_acc: 0.7821, best: 0.8024, time: 0:02:34
 Epoch: 222, lr: 2.0e-03, train_loss: 0.1395, train_acc: 0.9542 test_loss: 1.7261, test_acc: 0.7885, best: 0.8024, time: 0:02:34
 Epoch: 223, lr: 2.0e-03, train_loss: 0.1495, train_acc: 0.9468 test_loss: 1.5861, test_acc: 0.7885, best: 0.8024, time: 0:02:34
 Epoch: 224, lr: 2.0e-03, train_loss: 0.1402, train_acc: 0.9548 test_loss: 1.0724, test_acc: 0.7999, best: 0.8024, time: 0:02:34
 Epoch: 225, lr: 2.0e-03, train_loss: 0.1333, train_acc: 0.9548 test_loss: 3.1504, test_acc: 0.7586, best: 0.8024, time: 0:02:34
 Epoch: 226, lr: 2.0e-03, train_loss: 0.1370, train_acc: 0.9520 test_loss: 1.8274, test_acc: 0.7786, best: 0.8024, time: 0:02:34
 Epoch: 227, lr: 2.0e-03, train_loss: 0.1396, train_acc: 0.9484 test_loss: 3.2924, test_acc: 0.7488, best: 0.8024, time: 0:02:34
 Epoch: 228, lr: 2.0e-03, train_loss: 0.1515, train_acc: 0.9474 test_loss: 1.5297, test_acc: 0.7896, best: 0.8024, time: 0:02:35
 Epoch: 229, lr: 2.0e-03, train_loss: 0.1297, train_acc: 0.9562 test_loss: 2.5833, test_acc: 0.7688, best: 0.8024, time: 0:02:35
 Epoch: 230, lr: 2.0e-03, train_loss: 0.1536, train_acc: 0.9472 test_loss: 2.9397, test_acc: 0.7619, best: 0.8024, time: 0:02:34
 Epoch: 231, lr: 2.0e-03, train_loss: 0.1425, train_acc: 0.9516 test_loss: 1.1882, test_acc: 0.7949, best: 0.8024, time: 0:02:34
 Epoch: 232, lr: 2.0e-03, train_loss: 0.1381, train_acc: 0.9528 test_loss: 2.7972, test_acc: 0.7684, best: 0.8024, time: 0:02:34
 Epoch: 233, lr: 2.0e-03, train_loss: 0.1509, train_acc: 0.9480 test_loss: 1.5121, test_acc: 0.7974, best: 0.8024, time: 0:02:34
 Epoch: 234, lr: 2.0e-03, train_loss: 0.1337, train_acc: 0.9564 test_loss: 4.3543, test_acc: 0.7546, best: 0.8024, time: 0:02:34
 Epoch: 235, lr: 2.0e-03, train_loss: 0.1462, train_acc: 0.9502 test_loss: 1.6630, test_acc: 0.7894, best: 0.8024, time: 0:02:35
 Epoch: 236, lr: 2.0e-03, train_loss: 0.1315, train_acc: 0.9514 test_loss: 2.9920, test_acc: 0.7708, best: 0.8024, time: 0:02:35
 Epoch: 237, lr: 2.0e-03, train_loss: 0.1267, train_acc: 0.9566 test_loss: 1.4527, test_acc: 0.7909, best: 0.8024, time: 0:02:35
 Epoch: 238, lr: 2.0e-03, train_loss: 0.1295, train_acc: 0.9574 test_loss: 1.5812, test_acc: 0.7849, best: 0.8024, time: 0:02:34
 Epoch: 239, lr: 2.0e-03, train_loss: 0.1406, train_acc: 0.9544 test_loss: 3.7250, test_acc: 0.7579, best: 0.8024, time: 0:02:35
 Epoch: 240, lr: 4.0e-04, train_loss: 0.1287, train_acc: 0.9546 test_loss: 1.5050, test_acc: 0.7960, best: 0.8024, time: 0:02:35
 Epoch: 241, lr: 4.0e-04, train_loss: 0.1245, train_acc: 0.9582 test_loss: 2.1512, test_acc: 0.7845, best: 0.8024, time: 0:02:35
 Epoch: 242, lr: 4.0e-04, train_loss: 0.1256, train_acc: 0.9580 test_loss: 4.6730, test_acc: 0.7626, best: 0.8024, time: 0:02:34
 Epoch: 243, lr: 4.0e-04, train_loss: 0.1331, train_acc: 0.9550 test_loss: 2.3769, test_acc: 0.7752, best: 0.8024, time: 0:02:35
 Epoch: 244, lr: 4.0e-04, train_loss: 0.1240, train_acc: 0.9594 test_loss: 1.7313, test_acc: 0.7933, best: 0.8024, time: 0:02:34
 Epoch: 245, lr: 4.0e-04, train_loss: 0.1287, train_acc: 0.9558 test_loss: 3.3958, test_acc: 0.7724, best: 0.8024, time: 0:02:34
 Epoch: 246, lr: 4.0e-04, train_loss: 0.1325, train_acc: 0.9548 test_loss: 2.1257, test_acc: 0.7817, best: 0.8024, time: 0:02:34
 Epoch: 247, lr: 4.0e-04, train_loss: 0.1262, train_acc: 0.9568 test_loss: 2.8400, test_acc: 0.7708, best: 0.8024, time: 0:02:35
 Epoch: 248, lr: 4.0e-04, train_loss: 0.1037, train_acc: 0.9658 test_loss: 2.9547, test_acc: 0.7709, best: 0.8024, time: 0:02:35
 Epoch: 249, lr: 4.0e-04, train_loss: 0.1259, train_acc: 0.9572 test_loss: 2.4255, test_acc: 0.7913, best: 0.8024, time: 0:02:35
 Epoch: 250, lr: 4.0e-04, train_loss: 0.1434, train_acc: 0.9508 test_loss: 1.7218, test_acc: 0.8003, best: 0.8024, time: 0:02:35
 Epoch: 251, lr: 4.0e-04, train_loss: 0.1273, train_acc: 0.9572 test_loss: 3.2204, test_acc: 0.7765, best: 0.8024, time: 0:02:35
 Epoch: 252, lr: 4.0e-04, train_loss: 0.1237, train_acc: 0.9566 test_loss: 3.8826, test_acc: 0.7591, best: 0.8024, time: 0:02:34
 Epoch: 253, lr: 4.0e-04, train_loss: 0.1268, train_acc: 0.9554 test_loss: 2.7167, test_acc: 0.7694, best: 0.8024, time: 0:02:33
 Epoch: 254, lr: 4.0e-04, train_loss: 0.1155, train_acc: 0.9636 test_loss: 1.8035, test_acc: 0.7880, best: 0.8024, time: 0:02:33
 Epoch: 255, lr: 4.0e-04, train_loss: 0.1286, train_acc: 0.9572 test_loss: 2.6606, test_acc: 0.7688, best: 0.8024, time: 0:02:33
 Epoch: 256, lr: 4.0e-04, train_loss: 0.1210, train_acc: 0.9614 test_loss: 2.9692, test_acc: 0.7843, best: 0.8024, time: 0:02:33
 Epoch: 257, lr: 4.0e-04, train_loss: 0.1426, train_acc: 0.9506 test_loss: 2.3893, test_acc: 0.7788, best: 0.8024, time: 0:02:33
 Epoch: 258, lr: 4.0e-04, train_loss: 0.1289, train_acc: 0.9572 test_loss: 2.1671, test_acc: 0.7876, best: 0.8024, time: 0:02:33
 Epoch: 259, lr: 4.0e-04, train_loss: 0.1281, train_acc: 0.9590 test_loss: 3.1036, test_acc: 0.7778, best: 0.8024, time: 0:02:31
 Epoch: 260, lr: 4.0e-04, train_loss: 0.1184, train_acc: 0.9600 test_loss: 2.8483, test_acc: 0.7897, best: 0.8024, time: 0:02:29
 Epoch: 261, lr: 4.0e-04, train_loss: 0.1175, train_acc: 0.9622 test_loss: 4.1035, test_acc: 0.7680, best: 0.8024, time: 0:02:29
 Epoch: 262, lr: 4.0e-04, train_loss: 0.1221, train_acc: 0.9606 test_loss: 2.4210, test_acc: 0.7845, best: 0.8024, time: 0:02:28
 Epoch: 263, lr: 4.0e-04, train_loss: 0.1224, train_acc: 0.9610 test_loss: 5.0772, test_acc: 0.7474, best: 0.8024, time: 0:02:28
 Epoch: 264, lr: 4.0e-04, train_loss: 0.1223, train_acc: 0.9604 test_loss: 1.9882, test_acc: 0.7877, best: 0.8024, time: 0:02:28
 Epoch: 265, lr: 4.0e-04, train_loss: 0.1210, train_acc: 0.9588 test_loss: 5.1129, test_acc: 0.7475, best: 0.8024, time: 0:02:29
 Epoch: 266, lr: 4.0e-04, train_loss: 0.1218, train_acc: 0.9608 test_loss: 1.4795, test_acc: 0.8055, best: 0.8055, time: 0:02:29
 Epoch: 267, lr: 4.0e-04, train_loss: 0.1195, train_acc: 0.9608 test_loss: 4.2736, test_acc: 0.7561, best: 0.8055, time: 0:02:28
 Epoch: 268, lr: 4.0e-04, train_loss: 0.1130, train_acc: 0.9618 test_loss: 6.3753, test_acc: 0.7472, best: 0.8055, time: 0:02:28
 Epoch: 269, lr: 4.0e-04, train_loss: 0.1278, train_acc: 0.9582 test_loss: 1.8073, test_acc: 0.7923, best: 0.8055, time: 0:02:28
 Epoch: 270, lr: 8.0e-05, train_loss: 0.1036, train_acc: 0.9646 test_loss: 2.3965, test_acc: 0.7866, best: 0.8055, time: 0:02:28
 Epoch: 271, lr: 8.0e-05, train_loss: 0.1172, train_acc: 0.9604 test_loss: 3.2473, test_acc: 0.7776, best: 0.8055, time: 0:02:28
 Epoch: 272, lr: 8.0e-05, train_loss: 0.1145, train_acc: 0.9622 test_loss: 4.0759, test_acc: 0.7599, best: 0.8055, time: 0:02:28
 Epoch: 273, lr: 8.0e-05, train_loss: 0.1233, train_acc: 0.9604 test_loss: 4.9599, test_acc: 0.7612, best: 0.8055, time: 0:02:28
 Epoch: 274, lr: 8.0e-05, train_loss: 0.1148, train_acc: 0.9618 test_loss: 3.6460, test_acc: 0.7775, best: 0.8055, time: 0:02:25
 Epoch: 275, lr: 8.0e-05, train_loss: 0.1190, train_acc: 0.9618 test_loss: 5.2546, test_acc: 0.7569, best: 0.8055, time: 0:02:24
 Epoch: 276, lr: 8.0e-05, train_loss: 0.1196, train_acc: 0.9586 test_loss: 2.2484, test_acc: 0.7815, best: 0.8055, time: 0:02:24
 Epoch: 277, lr: 8.0e-05, train_loss: 0.1142, train_acc: 0.9598 test_loss: 2.7510, test_acc: 0.7768, best: 0.8055, time: 0:02:24
 Epoch: 278, lr: 8.0e-05, train_loss: 0.1093, train_acc: 0.9636 test_loss: 3.0547, test_acc: 0.7804, best: 0.8055, time: 0:02:24
 Epoch: 279, lr: 8.0e-05, train_loss: 0.1253, train_acc: 0.9572 test_loss: 10.4977, test_acc: 0.7171, best: 0.8055, time: 0:02:24
 Epoch: 280, lr: 8.0e-05, train_loss: 0.1188, train_acc: 0.9588 test_loss: 3.1862, test_acc: 0.7749, best: 0.8055, time: 0:02:24
 Epoch: 281, lr: 8.0e-05, train_loss: 0.1147, train_acc: 0.9624 test_loss: 1.7595, test_acc: 0.7984, best: 0.8055, time: 0:02:24
 Epoch: 282, lr: 8.0e-05, train_loss: 0.1174, train_acc: 0.9588 test_loss: 2.7492, test_acc: 0.7780, best: 0.8055, time: 0:02:24
 Epoch: 283, lr: 8.0e-05, train_loss: 0.1210, train_acc: 0.9580 test_loss: 3.4227, test_acc: 0.7750, best: 0.8055, time: 0:02:23
 Epoch: 284, lr: 8.0e-05, train_loss: 0.1221, train_acc: 0.9590 test_loss: 2.1808, test_acc: 0.7877, best: 0.8055, time: 0:02:23
 Epoch: 285, lr: 8.0e-05, train_loss: 0.1208, train_acc: 0.9628 test_loss: 4.7078, test_acc: 0.7558, best: 0.8055, time: 0:02:23
 Epoch: 286, lr: 8.0e-05, train_loss: 0.1117, train_acc: 0.9616 test_loss: 2.7636, test_acc: 0.7857, best: 0.8055, time: 0:02:23
 Epoch: 287, lr: 8.0e-05, train_loss: 0.1109, train_acc: 0.9582 test_loss: 5.9123, test_acc: 0.7591, best: 0.8055, time: 0:02:23
 Epoch: 288, lr: 8.0e-05, train_loss: 0.1108, train_acc: 0.9628 test_loss: 2.3287, test_acc: 0.7909, best: 0.8055, time: 0:02:23
 Epoch: 289, lr: 8.0e-05, train_loss: 0.1172, train_acc: 0.9596 test_loss: 5.6747, test_acc: 0.7582, best: 0.8055, time: 0:02:23
 Epoch: 290, lr: 8.0e-05, train_loss: 0.1123, train_acc: 0.9602 test_loss: 3.4395, test_acc: 0.7745, best: 0.8055, time: 0:02:23
 Epoch: 291, lr: 8.0e-05, train_loss: 0.1046, train_acc: 0.9664 test_loss: 7.0053, test_acc: 0.7481, best: 0.8055, time: 0:02:23
 Epoch: 292, lr: 8.0e-05, train_loss: 0.1265, train_acc: 0.9590 test_loss: 3.5230, test_acc: 0.7804, best: 0.8055, time: 0:02:23
 Epoch: 293, lr: 8.0e-05, train_loss: 0.1114, train_acc: 0.9642 test_loss: 4.4255, test_acc: 0.7661, best: 0.8055, time: 0:02:23
 Epoch: 294, lr: 8.0e-05, train_loss: 0.1155, train_acc: 0.9612 test_loss: 2.3237, test_acc: 0.7831, best: 0.8055, time: 0:02:23
 Epoch: 295, lr: 8.0e-05, train_loss: 0.1322, train_acc: 0.9548 test_loss: 2.6116, test_acc: 0.7794, best: 0.8055, time: 0:02:23
 Epoch: 296, lr: 8.0e-05, train_loss: 0.1194, train_acc: 0.9592 test_loss: 3.4545, test_acc: 0.7722, best: 0.8055, time: 0:02:23
 Epoch: 297, lr: 8.0e-05, train_loss: 0.1196, train_acc: 0.9578 test_loss: 5.3689, test_acc: 0.7602, best: 0.8055, time: 0:02:23
 Epoch: 298, lr: 8.0e-05, train_loss: 0.1117, train_acc: 0.9634 test_loss: 1.6931, test_acc: 0.7993, best: 0.8055, time: 0:02:23
 Epoch: 299, lr: 8.0e-05, train_loss: 0.1170, train_acc: 0.9574 test_loss: 2.1578, test_acc: 0.7969, best: 0.8055, time: 0:02:23
 Highest accuracy: 0.8055