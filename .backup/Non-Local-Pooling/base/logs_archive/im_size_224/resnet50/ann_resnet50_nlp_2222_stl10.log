
 Run on time: 2022-06-25 13:21:13.169924

 Arguments:
	 gpu                  : True
	 log                  : True
	 seed                 : 0
	 momentum             : 0.9
	 amsgrad              : True
	 dataset              : STL10
	 batch_size           : 8
	 architecture         : RESNET50_NLP_2222
	 im_size              : 224
	 relu_threshold       : 4.0
	 learning_rate        : 0.01
	 pretrained_backbone  : 
	 pretrained_ann       : 
	 weight_decay         : 0.0
	 test_only            : False
	 epochs               : 300
	 lr_interval          : [180, 240, 270]
	 lr_reduce            : 5
	 optimizer            : SGD
	 dropout              : 0.2
	 kernel_size          : 3
	 dont_save            : False
	 visualize            : False
	 devices              : 0,1
 DataParallel(
  (module): NetworkByName(
    (net): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (pool): Pool2d(
          (logit): Sequential(
            (pool_weight): NLP_BASE(
              (downsample): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (multihead_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (restore): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Sigmoid()
              )
              (pos_embed): PositionEmbeddingLearned(
                (row_embed): Embedding(256, 128)
                (col_embed): Embedding(256, 128)
              )
            )
          )
          (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (pool): Pool2d(
          (logit): Sequential(
            (pool_weight): NLP_BASE(
              (downsample): Sequential(
                (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
                (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (multihead_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (restore): Sequential(
                (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
                (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Sigmoid()
              )
              (pos_embed): PositionEmbeddingLearned(
                (row_embed): Embedding(256, 256)
                (col_embed): Embedding(256, 256)
              )
            )
          )
          (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (pool): Pool2d(
          (logit): Sequential(
            (pool_weight): NLP_BASE(
              (downsample): Sequential(
                (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
                (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (multihead_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (restore): Sequential(
                (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
                (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Sigmoid()
              )
              (pos_embed): PositionEmbeddingLearned(
                (row_embed): Embedding(256, 512)
                (col_embed): Embedding(256, 512)
              )
            )
          )
          (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        )
      )
      (layer4): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (pool): Pool2d(
          (logit): Sequential(
            (pool_weight): NLP_BASE(
              (downsample): Sequential(
                (0): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1))
                (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (multihead_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=2048, out_features=2048, bias=True)
              )
              (restore): Sequential(
                (0): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1))
                (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Sigmoid()
              )
              (pos_embed): PositionEmbeddingLearned(
                (row_embed): Embedding(256, 1024)
                (col_embed): Embedding(256, 1024)
              )
            )
          )
          (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        )
      )
      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
      (fc): Linear(in_features=2048, out_features=10, bias=True)
    )
  )
)
 SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0
)
 Epoch: 1, lr: 1.0e-02, train_loss: 2.6928, train_acc: 0.1510 test_loss: 2.2944, test_acc: 0.2042, best: 0.2042, time: 0:04:53
 Epoch: 2, lr: 1.0e-02, train_loss: 2.0542, train_acc: 0.2184 test_loss: 2.4627, test_acc: 0.2530, best: 0.2530, time: 0:04:51
 Epoch: 3, lr: 1.0e-02, train_loss: 1.9309, train_acc: 0.2672 test_loss: 1.7982, test_acc: 0.3194, best: 0.3194, time: 0:04:51
 Epoch: 4, lr: 1.0e-02, train_loss: 1.8593, train_acc: 0.3010 test_loss: 1.6995, test_acc: 0.3736, best: 0.3736, time: 0:04:51
 Epoch: 5, lr: 1.0e-02, train_loss: 1.8068, train_acc: 0.3222 test_loss: 1.6453, test_acc: 0.3814, best: 0.3814, time: 0:04:51
 Epoch: 6, lr: 1.0e-02, train_loss: 1.7429, train_acc: 0.3406 test_loss: 1.5490, test_acc: 0.4040, best: 0.4040, time: 0:04:51
 Epoch: 7, lr: 1.0e-02, train_loss: 1.6900, train_acc: 0.3530 test_loss: 1.7138, test_acc: 0.3685, best: 0.4040, time: 0:04:50
 Epoch: 8, lr: 1.0e-02, train_loss: 1.6290, train_acc: 0.3820 test_loss: 1.8258, test_acc: 0.4050, best: 0.4050, time: 0:04:51
 Epoch: 9, lr: 1.0e-02, train_loss: 1.6149, train_acc: 0.3922 test_loss: 2.4048, test_acc: 0.4085, best: 0.4085, time: 0:04:51
 Epoch: 10, lr: 1.0e-02, train_loss: 1.5745, train_acc: 0.4022 test_loss: 1.5556, test_acc: 0.4390, best: 0.4390, time: 0:04:51
 Epoch: 11, lr: 1.0e-02, train_loss: 1.5557, train_acc: 0.4136 test_loss: 1.5730, test_acc: 0.4220, best: 0.4390, time: 0:04:49
 Epoch: 12, lr: 1.0e-02, train_loss: 1.4980, train_acc: 0.4466 test_loss: 1.5233, test_acc: 0.4704, best: 0.4704, time: 0:04:51
 Epoch: 13, lr: 1.0e-02, train_loss: 1.4693, train_acc: 0.4552 test_loss: 1.3215, test_acc: 0.5324, best: 0.5324, time: 0:04:51
 Epoch: 14, lr: 1.0e-02, train_loss: 1.4726, train_acc: 0.4658 test_loss: 1.6135, test_acc: 0.4916, best: 0.5324, time: 0:04:50
 Epoch: 15, lr: 1.0e-02, train_loss: 1.4094, train_acc: 0.4812 test_loss: 1.4645, test_acc: 0.5130, best: 0.5324, time: 0:04:50
 Epoch: 16, lr: 1.0e-02, train_loss: 1.4000, train_acc: 0.4886 test_loss: 1.7569, test_acc: 0.5005, best: 0.5324, time: 0:04:50
 Epoch: 17, lr: 1.0e-02, train_loss: 1.3720, train_acc: 0.4996 test_loss: 1.5049, test_acc: 0.5198, best: 0.5324, time: 0:04:50
 Epoch: 18, lr: 1.0e-02, train_loss: 1.3480, train_acc: 0.5002 test_loss: 1.7429, test_acc: 0.5210, best: 0.5324, time: 0:04:50
 Epoch: 19, lr: 1.0e-02, train_loss: 1.2956, train_acc: 0.5192 test_loss: 1.6116, test_acc: 0.5519, best: 0.5519, time: 0:04:51
 Epoch: 20, lr: 1.0e-02, train_loss: 1.2831, train_acc: 0.5292 test_loss: 1.9655, test_acc: 0.5140, best: 0.5519, time: 0:04:50
 Epoch: 21, lr: 1.0e-02, train_loss: 1.2545, train_acc: 0.5398 test_loss: 1.5396, test_acc: 0.5436, best: 0.5519, time: 0:04:50
 Epoch: 22, lr: 1.0e-02, train_loss: 1.2218, train_acc: 0.5562 test_loss: 1.8321, test_acc: 0.5484, best: 0.5519, time: 0:04:50
 Epoch: 23, lr: 1.0e-02, train_loss: 1.2144, train_acc: 0.5576 test_loss: 1.4333, test_acc: 0.5735, best: 0.5735, time: 0:04:51
 Epoch: 24, lr: 1.0e-02, train_loss: 1.2132, train_acc: 0.5566 test_loss: 1.2648, test_acc: 0.5679, best: 0.5735, time: 0:04:50
 Epoch: 25, lr: 1.0e-02, train_loss: 1.1782, train_acc: 0.5686 test_loss: 1.2785, test_acc: 0.5694, best: 0.5735, time: 0:04:50
 Epoch: 26, lr: 1.0e-02, train_loss: 1.1731, train_acc: 0.5724 test_loss: 1.4244, test_acc: 0.5723, best: 0.5735, time: 0:04:50
 Epoch: 27, lr: 1.0e-02, train_loss: 1.1356, train_acc: 0.5926 test_loss: 2.8508, test_acc: 0.5504, best: 0.5735, time: 0:04:49
 Epoch: 28, lr: 1.0e-02, train_loss: 1.1136, train_acc: 0.5940 test_loss: 1.7046, test_acc: 0.5679, best: 0.5735, time: 0:04:50
 Epoch: 29, lr: 1.0e-02, train_loss: 1.1155, train_acc: 0.5964 test_loss: 1.1665, test_acc: 0.5995, best: 0.5995, time: 0:04:52
 Epoch: 30, lr: 1.0e-02, train_loss: 1.1105, train_acc: 0.5920 test_loss: 1.1416, test_acc: 0.6158, best: 0.6158, time: 0:04:52
 Epoch: 31, lr: 1.0e-02, train_loss: 1.0584, train_acc: 0.6226 test_loss: 1.7013, test_acc: 0.6218, best: 0.6218, time: 0:04:51
 Epoch: 32, lr: 1.0e-02, train_loss: 1.0481, train_acc: 0.6230 test_loss: 2.9464, test_acc: 0.5809, best: 0.6218, time: 0:04:50
 Epoch: 33, lr: 1.0e-02, train_loss: 1.0348, train_acc: 0.6296 test_loss: 1.7920, test_acc: 0.6088, best: 0.6218, time: 0:04:50
 Epoch: 34, lr: 1.0e-02, train_loss: 1.0150, train_acc: 0.6356 test_loss: 1.6978, test_acc: 0.6005, best: 0.6218, time: 0:04:50
 Epoch: 35, lr: 1.0e-02, train_loss: 0.9879, train_acc: 0.6414 test_loss: 1.6698, test_acc: 0.6230, best: 0.6230, time: 0:04:51
 Epoch: 36, lr: 1.0e-02, train_loss: 0.9842, train_acc: 0.6472 test_loss: 1.8661, test_acc: 0.6100, best: 0.6230, time: 0:04:50
 Epoch: 37, lr: 1.0e-02, train_loss: 0.9622, train_acc: 0.6470 test_loss: 1.5448, test_acc: 0.6285, best: 0.6285, time: 0:04:51
 Epoch: 38, lr: 1.0e-02, train_loss: 0.9624, train_acc: 0.6538 test_loss: 2.1425, test_acc: 0.6151, best: 0.6285, time: 0:04:50
 Epoch: 39, lr: 1.0e-02, train_loss: 0.9191, train_acc: 0.6662 test_loss: 1.8029, test_acc: 0.6308, best: 0.6308, time: 0:04:52
 Epoch: 40, lr: 1.0e-02, train_loss: 0.9199, train_acc: 0.6678 test_loss: 2.3229, test_acc: 0.5927, best: 0.6308, time: 0:04:50
 Epoch: 41, lr: 1.0e-02, train_loss: 0.9068, train_acc: 0.6758 test_loss: 1.4543, test_acc: 0.6432, best: 0.6432, time: 0:04:52
 Epoch: 42, lr: 1.0e-02, train_loss: 0.9079, train_acc: 0.6770 test_loss: 1.8689, test_acc: 0.6421, best: 0.6432, time: 0:04:50
 Epoch: 43, lr: 1.0e-02, train_loss: 0.8992, train_acc: 0.6820 test_loss: 3.2053, test_acc: 0.6206, best: 0.6432, time: 0:04:50
 Epoch: 44, lr: 1.0e-02, train_loss: 0.8906, train_acc: 0.6842 test_loss: 3.0231, test_acc: 0.6120, best: 0.6432, time: 0:04:50
 Epoch: 45, lr: 1.0e-02, train_loss: 0.8663, train_acc: 0.6822 test_loss: 9.0231, test_acc: 0.5925, best: 0.6432, time: 0:04:51
 Epoch: 46, lr: 1.0e-02, train_loss: 0.8626, train_acc: 0.7000 test_loss: 3.6280, test_acc: 0.6125, best: 0.6432, time: 0:04:51
 Epoch: 47, lr: 1.0e-02, train_loss: 0.8510, train_acc: 0.6960 test_loss: 2.5397, test_acc: 0.6488, best: 0.6488, time: 0:04:52
 Epoch: 48, lr: 1.0e-02, train_loss: 0.8458, train_acc: 0.6932 test_loss: 1.8211, test_acc: 0.6641, best: 0.6641, time: 0:04:52
 Epoch: 49, lr: 1.0e-02, train_loss: 0.8072, train_acc: 0.7086 test_loss: 4.0219, test_acc: 0.6560, best: 0.6641, time: 0:04:50
 Epoch: 50, lr: 1.0e-02, train_loss: 0.7849, train_acc: 0.7214 test_loss: 1.5630, test_acc: 0.6643, best: 0.6643, time: 0:04:52
 Epoch: 51, lr: 1.0e-02, train_loss: 0.7961, train_acc: 0.7124 test_loss: 5.2378, test_acc: 0.6104, best: 0.6643, time: 0:04:50
 Epoch: 52, lr: 1.0e-02, train_loss: 0.7947, train_acc: 0.7144 test_loss: 2.2246, test_acc: 0.6874, best: 0.6874, time: 0:04:51
 Epoch: 53, lr: 1.0e-02, train_loss: 0.7395, train_acc: 0.7382 test_loss: 2.4830, test_acc: 0.6595, best: 0.6874, time: 0:04:50
 Epoch: 54, lr: 1.0e-02, train_loss: 0.7580, train_acc: 0.7370 test_loss: 1.3861, test_acc: 0.6777, best: 0.6874, time: 0:04:50
 Epoch: 55, lr: 1.0e-02, train_loss: 0.7252, train_acc: 0.7418 test_loss: 3.2340, test_acc: 0.6119, best: 0.6874, time: 0:04:50
 Epoch: 56, lr: 1.0e-02, train_loss: 0.7224, train_acc: 0.7434 test_loss: 1.9818, test_acc: 0.6616, best: 0.6874, time: 0:04:50
 Epoch: 57, lr: 1.0e-02, train_loss: 0.7153, train_acc: 0.7494 test_loss: 2.6618, test_acc: 0.6492, best: 0.6874, time: 0:04:50
 Epoch: 58, lr: 1.0e-02, train_loss: 0.6767, train_acc: 0.7640 test_loss: 1.2910, test_acc: 0.6931, best: 0.6931, time: 0:05:16
 Epoch: 59, lr: 1.0e-02, train_loss: 0.6805, train_acc: 0.7508 test_loss: 7.4233, test_acc: 0.6172, best: 0.6931, time: 0:04:50
 Epoch: 60, lr: 1.0e-02, train_loss: 0.7042, train_acc: 0.7472 test_loss: 3.6728, test_acc: 0.6596, best: 0.6931, time: 0:04:51
 Epoch: 61, lr: 1.0e-02, train_loss: 0.6787, train_acc: 0.7568 test_loss: 3.4074, test_acc: 0.6292, best: 0.6931, time: 0:04:50
 Epoch: 62, lr: 1.0e-02, train_loss: 0.6767, train_acc: 0.7572 test_loss: 1.6672, test_acc: 0.6640, best: 0.6931, time: 0:04:49
 Epoch: 63, lr: 1.0e-02, train_loss: 0.6442, train_acc: 0.7790 test_loss: 1.6588, test_acc: 0.6625, best: 0.6931, time: 0:04:50
 Epoch: 64, lr: 1.0e-02, train_loss: 0.6452, train_acc: 0.7710 test_loss: 1.3443, test_acc: 0.6919, best: 0.6931, time: 0:04:50
 Epoch: 65, lr: 1.0e-02, train_loss: 0.6592, train_acc: 0.7714 test_loss: 1.8404, test_acc: 0.7026, best: 0.7026, time: 0:04:52
 Epoch: 66, lr: 1.0e-02, train_loss: 0.6413, train_acc: 0.7718 test_loss: 1.1547, test_acc: 0.6979, best: 0.7026, time: 0:04:50
 Epoch: 67, lr: 1.0e-02, train_loss: 0.6247, train_acc: 0.7800 test_loss: 2.3009, test_acc: 0.6824, best: 0.7026, time: 0:04:50
 Epoch: 68, lr: 1.0e-02, train_loss: 0.6276, train_acc: 0.7778 test_loss: 1.4533, test_acc: 0.6976, best: 0.7026, time: 0:04:50
 Epoch: 69, lr: 1.0e-02, train_loss: 0.6200, train_acc: 0.7742 test_loss: 1.2726, test_acc: 0.7007, best: 0.7026, time: 0:04:50
 Epoch: 70, lr: 1.0e-02, train_loss: 0.6075, train_acc: 0.7840 test_loss: 1.6457, test_acc: 0.6999, best: 0.7026, time: 0:04:50
 Epoch: 71, lr: 1.0e-02, train_loss: 0.5762, train_acc: 0.7970 test_loss: 1.7919, test_acc: 0.6909, best: 0.7026, time: 0:04:51
 Epoch: 72, lr: 1.0e-02, train_loss: 0.5835, train_acc: 0.7904 test_loss: 1.2133, test_acc: 0.7202, best: 0.7202, time: 0:04:53
 Epoch: 73, lr: 1.0e-02, train_loss: 0.5780, train_acc: 0.8044 test_loss: 1.7341, test_acc: 0.6936, best: 0.7202, time: 0:04:51
 Epoch: 74, lr: 1.0e-02, train_loss: 0.5647, train_acc: 0.8052 test_loss: 1.2692, test_acc: 0.7023, best: 0.7202, time: 0:04:51
 Epoch: 75, lr: 1.0e-02, train_loss: 0.5665, train_acc: 0.7992 test_loss: 1.1306, test_acc: 0.7146, best: 0.7202, time: 0:04:51
 Epoch: 76, lr: 1.0e-02, train_loss: 0.5462, train_acc: 0.8146 test_loss: 2.0018, test_acc: 0.6915, best: 0.7202, time: 0:04:51
 Epoch: 77, lr: 1.0e-02, train_loss: 0.5692, train_acc: 0.7970 test_loss: 1.3743, test_acc: 0.6966, best: 0.7202, time: 0:04:51
 Epoch: 78, lr: 1.0e-02, train_loss: 0.5290, train_acc: 0.8100 test_loss: 0.9367, test_acc: 0.7319, best: 0.7319, time: 0:04:53
 Epoch: 79, lr: 1.0e-02, train_loss: 0.5398, train_acc: 0.8128 test_loss: 0.9326, test_acc: 0.7300, best: 0.7319, time: 0:04:52
 Epoch: 80, lr: 1.0e-02, train_loss: 0.5071, train_acc: 0.8178 test_loss: 1.2204, test_acc: 0.7131, best: 0.7319, time: 0:04:51
 Epoch: 81, lr: 1.0e-02, train_loss: 0.5196, train_acc: 0.8204 test_loss: 1.3093, test_acc: 0.7059, best: 0.7319, time: 0:04:51
 Epoch: 82, lr: 1.0e-02, train_loss: 0.5134, train_acc: 0.8242 test_loss: 0.9264, test_acc: 0.7458, best: 0.7458, time: 0:04:52
 Epoch: 83, lr: 1.0e-02, train_loss: 0.5294, train_acc: 0.8150 test_loss: 1.5003, test_acc: 0.7007, best: 0.7458, time: 0:04:51
 Epoch: 84, lr: 1.0e-02, train_loss: 0.4927, train_acc: 0.8282 test_loss: 1.4559, test_acc: 0.7161, best: 0.7458, time: 0:04:51
 Epoch: 85, lr: 1.0e-02, train_loss: 0.4958, train_acc: 0.8308 test_loss: 1.2435, test_acc: 0.7235, best: 0.7458, time: 0:04:51
 Epoch: 86, lr: 1.0e-02, train_loss: 0.4898, train_acc: 0.8310 test_loss: 0.9963, test_acc: 0.7206, best: 0.7458, time: 0:04:51
 Epoch: 87, lr: 1.0e-02, train_loss: 0.4640, train_acc: 0.8376 test_loss: 1.1252, test_acc: 0.7224, best: 0.7458, time: 0:04:51
 Epoch: 88, lr: 1.0e-02, train_loss: 0.4941, train_acc: 0.8246 test_loss: 1.3464, test_acc: 0.7050, best: 0.7458, time: 0:04:51
 Epoch: 89, lr: 1.0e-02, train_loss: 0.4782, train_acc: 0.8320 test_loss: 1.1319, test_acc: 0.7225, best: 0.7458, time: 0:04:51
 Epoch: 90, lr: 1.0e-02, train_loss: 0.4998, train_acc: 0.8216 test_loss: 1.0577, test_acc: 0.7232, best: 0.7458, time: 0:04:51
 Epoch: 91, lr: 1.0e-02, train_loss: 0.4740, train_acc: 0.8356 test_loss: 0.8556, test_acc: 0.7504, best: 0.7504, time: 0:04:54
 Epoch: 92, lr: 1.0e-02, train_loss: 0.4328, train_acc: 0.8536 test_loss: 0.9417, test_acc: 0.7471, best: 0.7504, time: 0:04:51
 Epoch: 93, lr: 1.0e-02, train_loss: 0.4462, train_acc: 0.8462 test_loss: 1.0183, test_acc: 0.7311, best: 0.7504, time: 0:04:51
 Epoch: 94, lr: 1.0e-02, train_loss: 0.4676, train_acc: 0.8424 test_loss: 1.2646, test_acc: 0.7375, best: 0.7504, time: 0:04:51
 Epoch: 95, lr: 1.0e-02, train_loss: 0.4524, train_acc: 0.8450 test_loss: 1.0020, test_acc: 0.7406, best: 0.7504, time: 0:04:51
 Epoch: 96, lr: 1.0e-02, train_loss: 0.4407, train_acc: 0.8520 test_loss: 0.9574, test_acc: 0.7388, best: 0.7504, time: 0:04:50
 Epoch: 97, lr: 1.0e-02, train_loss: 0.4264, train_acc: 0.8514 test_loss: 1.1057, test_acc: 0.7344, best: 0.7504, time: 0:04:51
 Epoch: 98, lr: 1.0e-02, train_loss: 0.4220, train_acc: 0.8526 test_loss: 0.9696, test_acc: 0.7370, best: 0.7504, time: 0:04:51
 Epoch: 99, lr: 1.0e-02, train_loss: 0.4307, train_acc: 0.8498 test_loss: 0.9457, test_acc: 0.7384, best: 0.7504, time: 0:04:51
 Epoch: 100, lr: 1.0e-02, train_loss: 0.4185, train_acc: 0.8464 test_loss: 0.9543, test_acc: 0.7344, best: 0.7504, time: 0:04:51
 Epoch: 101, lr: 1.0e-02, train_loss: 0.4130, train_acc: 0.8562 test_loss: 0.9122, test_acc: 0.7488, best: 0.7504, time: 0:04:51
 Epoch: 102, lr: 1.0e-02, train_loss: 0.4266, train_acc: 0.8542 test_loss: 1.1347, test_acc: 0.7431, best: 0.7504, time: 0:04:51
 Epoch: 103, lr: 1.0e-02, train_loss: 0.4253, train_acc: 0.8526 test_loss: 0.8193, test_acc: 0.7672, best: 0.7672, time: 0:04:54
 Epoch: 104, lr: 1.0e-02, train_loss: 0.4313, train_acc: 0.8530 test_loss: 1.0967, test_acc: 0.7486, best: 0.7672, time: 0:04:51
 Epoch: 105, lr: 1.0e-02, train_loss: 0.4203, train_acc: 0.8564 test_loss: 1.0065, test_acc: 0.7456, best: 0.7672, time: 0:04:51
 Epoch: 106, lr: 1.0e-02, train_loss: 0.4056, train_acc: 0.8598 test_loss: 1.0515, test_acc: 0.7350, best: 0.7672, time: 0:04:51
 Epoch: 107, lr: 1.0e-02, train_loss: 0.3937, train_acc: 0.8650 test_loss: 1.0404, test_acc: 0.7446, best: 0.7672, time: 0:04:51
 Epoch: 108, lr: 1.0e-02, train_loss: 0.3995, train_acc: 0.8578 test_loss: 1.2006, test_acc: 0.7286, best: 0.7672, time: 0:04:51
 Epoch: 109, lr: 1.0e-02, train_loss: 0.3764, train_acc: 0.8688 test_loss: 1.0599, test_acc: 0.7406, best: 0.7672, time: 0:04:51
 Epoch: 110, lr: 1.0e-02, train_loss: 0.3746, train_acc: 0.8756 test_loss: 0.9667, test_acc: 0.7500, best: 0.7672, time: 0:04:51
 Epoch: 111, lr: 1.0e-02, train_loss: 0.3685, train_acc: 0.8738 test_loss: 0.8971, test_acc: 0.7628, best: 0.7672, time: 0:04:51
 Epoch: 112, lr: 1.0e-02, train_loss: 0.3801, train_acc: 0.8732 test_loss: 0.9657, test_acc: 0.7545, best: 0.7672, time: 0:04:51
 Epoch: 113, lr: 1.0e-02, train_loss: 0.3647, train_acc: 0.8742 test_loss: 1.0975, test_acc: 0.7451, best: 0.7672, time: 0:04:51
 Epoch: 114, lr: 1.0e-02, train_loss: 0.3815, train_acc: 0.8720 test_loss: 0.9871, test_acc: 0.7445, best: 0.7672, time: 0:04:51
 Epoch: 115, lr: 1.0e-02, train_loss: 0.3608, train_acc: 0.8786 test_loss: 0.9573, test_acc: 0.7538, best: 0.7672, time: 0:04:51
 Epoch: 116, lr: 1.0e-02, train_loss: 0.3626, train_acc: 0.8744 test_loss: 1.1149, test_acc: 0.7380, best: 0.7672, time: 0:04:51
 Epoch: 117, lr: 1.0e-02, train_loss: 0.3585, train_acc: 0.8740 test_loss: 1.1167, test_acc: 0.7431, best: 0.7672, time: 0:04:51
 Epoch: 118, lr: 1.0e-02, train_loss: 0.3545, train_acc: 0.8778 test_loss: 0.9091, test_acc: 0.7489, best: 0.7672, time: 0:04:51
 Epoch: 119, lr: 1.0e-02, train_loss: 0.3332, train_acc: 0.8788 test_loss: 0.8560, test_acc: 0.7705, best: 0.7705, time: 0:04:53
 Epoch: 120, lr: 1.0e-02, train_loss: 0.3585, train_acc: 0.8776 test_loss: 0.9916, test_acc: 0.7506, best: 0.7705, time: 0:04:51
 Epoch: 121, lr: 1.0e-02, train_loss: 0.3497, train_acc: 0.8782 test_loss: 0.9709, test_acc: 0.7601, best: 0.7705, time: 0:04:51
 Epoch: 122, lr: 1.0e-02, train_loss: 0.3568, train_acc: 0.8780 test_loss: 1.1455, test_acc: 0.7304, best: 0.7705, time: 0:04:51
 Epoch: 123, lr: 1.0e-02, train_loss: 0.3429, train_acc: 0.8850 test_loss: 1.0956, test_acc: 0.7285, best: 0.7705, time: 0:04:55
 Epoch: 124, lr: 1.0e-02, train_loss: 0.3572, train_acc: 0.8800 test_loss: 1.0989, test_acc: 0.7480, best: 0.7705, time: 0:04:57
 Epoch: 125, lr: 1.0e-02, train_loss: 0.3387, train_acc: 0.8876 test_loss: 1.0512, test_acc: 0.7499, best: 0.7705, time: 0:04:58
 Epoch: 126, lr: 1.0e-02, train_loss: 0.3381, train_acc: 0.8816 test_loss: 0.9835, test_acc: 0.7468, best: 0.7705, time: 0:04:58
 Epoch: 127, lr: 1.0e-02, train_loss: 0.3362, train_acc: 0.8864 test_loss: 1.1482, test_acc: 0.7231, best: 0.7705, time: 0:04:57
 Epoch: 128, lr: 1.0e-02, train_loss: 0.3380, train_acc: 0.8842 test_loss: 0.9795, test_acc: 0.7662, best: 0.7705, time: 0:04:57
 Epoch: 129, lr: 1.0e-02, train_loss: 0.3463, train_acc: 0.8812 test_loss: 0.9279, test_acc: 0.7689, best: 0.7705, time: 0:04:57
 Epoch: 130, lr: 1.0e-02, train_loss: 0.3410, train_acc: 0.8836 test_loss: 0.9031, test_acc: 0.7610, best: 0.7705, time: 0:04:58
 Epoch: 131, lr: 1.0e-02, train_loss: 0.3113, train_acc: 0.8958 test_loss: 1.0098, test_acc: 0.7535, best: 0.7705, time: 0:04:58
 Epoch: 132, lr: 1.0e-02, train_loss: 0.3264, train_acc: 0.8904 test_loss: 0.9848, test_acc: 0.7595, best: 0.7705, time: 0:04:58
 Epoch: 133, lr: 1.0e-02, train_loss: 0.3369, train_acc: 0.8838 test_loss: 0.9296, test_acc: 0.7641, best: 0.7705, time: 0:04:58
 Epoch: 134, lr: 1.0e-02, train_loss: 0.3278, train_acc: 0.8858 test_loss: 0.8947, test_acc: 0.7675, best: 0.7705, time: 0:04:58
 Epoch: 135, lr: 1.0e-02, train_loss: 0.3268, train_acc: 0.8850 test_loss: 0.9506, test_acc: 0.7609, best: 0.7705, time: 0:04:58
 Epoch: 136, lr: 1.0e-02, train_loss: 0.2997, train_acc: 0.8994 test_loss: 1.0019, test_acc: 0.7631, best: 0.7705, time: 0:04:58
 Epoch: 137, lr: 1.0e-02, train_loss: 0.3151, train_acc: 0.8942 test_loss: 0.9637, test_acc: 0.7655, best: 0.7705, time: 0:04:58
 Epoch: 138, lr: 1.0e-02, train_loss: 0.3251, train_acc: 0.8874 test_loss: 0.8975, test_acc: 0.7586, best: 0.7705, time: 0:04:58
 Epoch: 139, lr: 1.0e-02, train_loss: 0.3145, train_acc: 0.8956 test_loss: 1.0095, test_acc: 0.7678, best: 0.7705, time: 0:04:58
 Epoch: 140, lr: 1.0e-02, train_loss: 0.3093, train_acc: 0.8950 test_loss: 1.0180, test_acc: 0.7502, best: 0.7705, time: 0:04:57
 Epoch: 141, lr: 1.0e-02, train_loss: 0.3080, train_acc: 0.8942 test_loss: 0.9093, test_acc: 0.7664, best: 0.7705, time: 0:04:58
 Epoch: 142, lr: 1.0e-02, train_loss: 0.2973, train_acc: 0.9002 test_loss: 0.9435, test_acc: 0.7645, best: 0.7705, time: 0:04:58
 Epoch: 143, lr: 1.0e-02, train_loss: 0.2972, train_acc: 0.9002 test_loss: 1.0775, test_acc: 0.7466, best: 0.7705, time: 0:04:58
 Epoch: 144, lr: 1.0e-02, train_loss: 0.2778, train_acc: 0.9046 test_loss: 1.1019, test_acc: 0.7452, best: 0.7705, time: 0:04:53
 Epoch: 145, lr: 1.0e-02, train_loss: 0.2825, train_acc: 0.9032 test_loss: 0.9173, test_acc: 0.7706, best: 0.7706, time: 0:04:55
 Epoch: 146, lr: 1.0e-02, train_loss: 0.2797, train_acc: 0.9040 test_loss: 0.9147, test_acc: 0.7729, best: 0.7729, time: 0:04:53
 Epoch: 147, lr: 1.0e-02, train_loss: 0.2633, train_acc: 0.9092 test_loss: 1.0792, test_acc: 0.7651, best: 0.7729, time: 0:04:52
 Epoch: 148, lr: 1.0e-02, train_loss: 0.3032, train_acc: 0.8970 test_loss: 1.0542, test_acc: 0.7535, best: 0.7729, time: 0:04:52
 Epoch: 149, lr: 1.0e-02, train_loss: 0.2851, train_acc: 0.8982 test_loss: 1.1203, test_acc: 0.7396, best: 0.7729, time: 0:04:53
 Epoch: 150, lr: 1.0e-02, train_loss: 0.2851, train_acc: 0.9036 test_loss: 1.0869, test_acc: 0.7526, best: 0.7729, time: 0:04:53
 Epoch: 151, lr: 1.0e-02, train_loss: 0.2835, train_acc: 0.9046 test_loss: 1.0378, test_acc: 0.7576, best: 0.7729, time: 0:04:53
 Epoch: 152, lr: 1.0e-02, train_loss: 0.2954, train_acc: 0.8968 test_loss: 0.9512, test_acc: 0.7675, best: 0.7729, time: 0:04:54
 Epoch: 153, lr: 1.0e-02, train_loss: 0.2998, train_acc: 0.8986 test_loss: 0.9903, test_acc: 0.7688, best: 0.7729, time: 0:04:56
 Epoch: 154, lr: 1.0e-02, train_loss: 0.2772, train_acc: 0.9086 test_loss: 0.9604, test_acc: 0.7716, best: 0.7729, time: 0:04:58
 Epoch: 155, lr: 1.0e-02, train_loss: 0.2777, train_acc: 0.9052 test_loss: 0.9658, test_acc: 0.7764, best: 0.7764, time: 0:04:59
 Epoch: 156, lr: 1.0e-02, train_loss: 0.2646, train_acc: 0.9074 test_loss: 1.0418, test_acc: 0.7589, best: 0.7764, time: 0:04:58
 Epoch: 157, lr: 1.0e-02, train_loss: 0.2762, train_acc: 0.9084 test_loss: 0.9980, test_acc: 0.7645, best: 0.7764, time: 0:04:58
 Epoch: 158, lr: 1.0e-02, train_loss: 0.2764, train_acc: 0.9060 test_loss: 0.9748, test_acc: 0.7714, best: 0.7764, time: 0:04:58
 Epoch: 159, lr: 1.0e-02, train_loss: 0.2594, train_acc: 0.9098 test_loss: 0.9947, test_acc: 0.7731, best: 0.7764, time: 0:04:57
 Epoch: 160, lr: 1.0e-02, train_loss: 0.2692, train_acc: 0.9076 test_loss: 0.9763, test_acc: 0.7718, best: 0.7764, time: 0:04:57
 Epoch: 161, lr: 1.0e-02, train_loss: 0.2652, train_acc: 0.9102 test_loss: 0.9692, test_acc: 0.7682, best: 0.7764, time: 0:04:58
 Epoch: 162, lr: 1.0e-02, train_loss: 0.2616, train_acc: 0.9130 test_loss: 1.0720, test_acc: 0.7606, best: 0.7764, time: 0:04:58
 Epoch: 163, lr: 1.0e-02, train_loss: 0.2648, train_acc: 0.9042 test_loss: 0.9245, test_acc: 0.7652, best: 0.7764, time: 0:04:58
 Epoch: 164, lr: 1.0e-02, train_loss: 0.2583, train_acc: 0.9088 test_loss: 1.1819, test_acc: 0.7562, best: 0.7764, time: 0:04:58
 Epoch: 165, lr: 1.0e-02, train_loss: 0.2576, train_acc: 0.9120 test_loss: 1.1108, test_acc: 0.7549, best: 0.7764, time: 0:04:58
 Epoch: 166, lr: 1.0e-02, train_loss: 0.2901, train_acc: 0.9012 test_loss: 1.0901, test_acc: 0.7495, best: 0.7764, time: 0:04:58
 Epoch: 167, lr: 1.0e-02, train_loss: 0.2621, train_acc: 0.9110 test_loss: 0.9672, test_acc: 0.7659, best: 0.7764, time: 0:04:58
 Epoch: 168, lr: 1.0e-02, train_loss: 0.2456, train_acc: 0.9130 test_loss: 1.1942, test_acc: 0.7464, best: 0.7764, time: 0:04:58
 Epoch: 169, lr: 1.0e-02, train_loss: 0.2680, train_acc: 0.9094 test_loss: 1.1102, test_acc: 0.7569, best: 0.7764, time: 0:04:57
 Epoch: 170, lr: 1.0e-02, train_loss: 0.2263, train_acc: 0.9248 test_loss: 1.0633, test_acc: 0.7565, best: 0.7764, time: 0:04:58
 Epoch: 171, lr: 1.0e-02, train_loss: 0.2630, train_acc: 0.9114 test_loss: 1.0527, test_acc: 0.7625, best: 0.7764, time: 0:04:57
 Epoch: 172, lr: 1.0e-02, train_loss: 0.2443, train_acc: 0.9126 test_loss: 0.9178, test_acc: 0.7751, best: 0.7764, time: 0:04:57
 Epoch: 173, lr: 1.0e-02, train_loss: 0.2555, train_acc: 0.9106 test_loss: 1.1518, test_acc: 0.7574, best: 0.7764, time: 0:04:58
 Epoch: 174, lr: 1.0e-02, train_loss: 0.2527, train_acc: 0.9116 test_loss: 1.1033, test_acc: 0.7474, best: 0.7764, time: 0:04:58
 Epoch: 175, lr: 1.0e-02, train_loss: 0.2581, train_acc: 0.9150 test_loss: 0.9883, test_acc: 0.7718, best: 0.7764, time: 0:04:58
 Epoch: 176, lr: 1.0e-02, train_loss: 0.2577, train_acc: 0.9120 test_loss: 1.0292, test_acc: 0.7615, best: 0.7764, time: 0:04:58
 Epoch: 177, lr: 1.0e-02, train_loss: 0.2466, train_acc: 0.9170 test_loss: 1.0400, test_acc: 0.7695, best: 0.7764, time: 0:04:58
 Epoch: 178, lr: 1.0e-02, train_loss: 0.2472, train_acc: 0.9150 test_loss: 0.9823, test_acc: 0.7758, best: 0.7764, time: 0:04:58
 Epoch: 179, lr: 1.0e-02, train_loss: 0.2515, train_acc: 0.9140 test_loss: 0.9486, test_acc: 0.7796, best: 0.7796, time: 0:05:00
 Epoch: 180, lr: 2.0e-03, train_loss: 0.1980, train_acc: 0.9324 test_loss: 0.8849, test_acc: 0.7857, best: 0.7857, time: 0:05:00
 Epoch: 181, lr: 2.0e-03, train_loss: 0.1928, train_acc: 0.9358 test_loss: 0.9224, test_acc: 0.7835, best: 0.7857, time: 0:04:58
 Epoch: 182, lr: 2.0e-03, train_loss: 0.1731, train_acc: 0.9450 test_loss: 0.9017, test_acc: 0.7900, best: 0.7900, time: 0:04:59
 Epoch: 183, lr: 2.0e-03, train_loss: 0.1590, train_acc: 0.9474 test_loss: 0.9771, test_acc: 0.7821, best: 0.7900, time: 0:04:58
 Epoch: 184, lr: 2.0e-03, train_loss: 0.1673, train_acc: 0.9450 test_loss: 0.9347, test_acc: 0.7866, best: 0.7900, time: 0:04:58
 Epoch: 185, lr: 2.0e-03, train_loss: 0.1758, train_acc: 0.9386 test_loss: 0.9343, test_acc: 0.7904, best: 0.7904, time: 0:05:00
 Epoch: 186, lr: 2.0e-03, train_loss: 0.1698, train_acc: 0.9416 test_loss: 0.9117, test_acc: 0.7987, best: 0.7987, time: 0:05:00
 Epoch: 187, lr: 2.0e-03, train_loss: 0.1669, train_acc: 0.9436 test_loss: 0.9007, test_acc: 0.7944, best: 0.7987, time: 0:04:59
 Epoch: 188, lr: 2.0e-03, train_loss: 0.1762, train_acc: 0.9424 test_loss: 0.8836, test_acc: 0.7923, best: 0.7987, time: 0:04:58
 Epoch: 189, lr: 2.0e-03, train_loss: 0.1637, train_acc: 0.9440 test_loss: 0.9682, test_acc: 0.7897, best: 0.7987, time: 0:04:58
 Epoch: 190, lr: 2.0e-03, train_loss: 0.1480, train_acc: 0.9492 test_loss: 0.9959, test_acc: 0.7850, best: 0.7987, time: 0:04:57
 Epoch: 191, lr: 2.0e-03, train_loss: 0.1564, train_acc: 0.9498 test_loss: 0.9840, test_acc: 0.7805, best: 0.7987, time: 0:04:59
 Epoch: 192, lr: 2.0e-03, train_loss: 0.1598, train_acc: 0.9466 test_loss: 0.9777, test_acc: 0.7826, best: 0.7987, time: 0:04:58
 Epoch: 193, lr: 2.0e-03, train_loss: 0.1567, train_acc: 0.9446 test_loss: 1.0478, test_acc: 0.7806, best: 0.7987, time: 0:04:58
 Epoch: 194, lr: 2.0e-03, train_loss: 0.1537, train_acc: 0.9448 test_loss: 0.9113, test_acc: 0.7930, best: 0.7987, time: 0:04:58
 Epoch: 195, lr: 2.0e-03, train_loss: 0.1700, train_acc: 0.9410 test_loss: 0.9135, test_acc: 0.7964, best: 0.7987, time: 0:04:58
 Epoch: 196, lr: 2.0e-03, train_loss: 0.1447, train_acc: 0.9548 test_loss: 1.0272, test_acc: 0.7830, best: 0.7987, time: 0:04:58
 Epoch: 197, lr: 2.0e-03, train_loss: 0.1617, train_acc: 0.9418 test_loss: 0.9229, test_acc: 0.7955, best: 0.7987, time: 0:04:58
 Epoch: 198, lr: 2.0e-03, train_loss: 0.1596, train_acc: 0.9482 test_loss: 0.9670, test_acc: 0.7857, best: 0.7987, time: 0:04:58
 Epoch: 199, lr: 2.0e-03, train_loss: 0.1565, train_acc: 0.9498 test_loss: 0.9631, test_acc: 0.7909, best: 0.7987, time: 0:04:58
 Epoch: 200, lr: 2.0e-03, train_loss: 0.1498, train_acc: 0.9484 test_loss: 0.9173, test_acc: 0.7910, best: 0.7987, time: 0:04:58
 Epoch: 201, lr: 2.0e-03, train_loss: 0.1497, train_acc: 0.9504 test_loss: 0.9577, test_acc: 0.7916, best: 0.7987, time: 0:04:58
 Epoch: 202, lr: 2.0e-03, train_loss: 0.1581, train_acc: 0.9422 test_loss: 0.9179, test_acc: 0.7943, best: 0.7987, time: 0:04:58
 Epoch: 203, lr: 2.0e-03, train_loss: 0.1465, train_acc: 0.9510 test_loss: 1.0526, test_acc: 0.7840, best: 0.7987, time: 0:04:58
 Epoch: 204, lr: 2.0e-03, train_loss: 0.1516, train_acc: 0.9490 test_loss: 0.9934, test_acc: 0.7905, best: 0.7987, time: 0:04:59
 Epoch: 205, lr: 2.0e-03, train_loss: 0.1510, train_acc: 0.9488 test_loss: 0.9694, test_acc: 0.7930, best: 0.7987, time: 0:04:58
 Epoch: 206, lr: 2.0e-03, train_loss: 0.1604, train_acc: 0.9444 test_loss: 0.9832, test_acc: 0.7906, best: 0.7987, time: 0:04:57
 Epoch: 207, lr: 2.0e-03, train_loss: 0.1357, train_acc: 0.9546 test_loss: 1.0035, test_acc: 0.7849, best: 0.7987, time: 0:04:58
 Epoch: 208, lr: 2.0e-03, train_loss: 0.1382, train_acc: 0.9526 test_loss: 0.9624, test_acc: 0.7886, best: 0.7987, time: 0:04:58
 Epoch: 209, lr: 2.0e-03, train_loss: 0.1464, train_acc: 0.9496 test_loss: 1.0226, test_acc: 0.7873, best: 0.7987, time: 0:04:58
 Epoch: 210, lr: 2.0e-03, train_loss: 0.1500, train_acc: 0.9462 test_loss: 1.0179, test_acc: 0.7867, best: 0.7987, time: 0:04:58
 Epoch: 211, lr: 2.0e-03, train_loss: 0.1323, train_acc: 0.9518 test_loss: 0.9235, test_acc: 0.7943, best: 0.7987, time: 0:04:58
 Epoch: 212, lr: 2.0e-03, train_loss: 0.1430, train_acc: 0.9498 test_loss: 1.0617, test_acc: 0.7816, best: 0.7987, time: 0:04:58
 Epoch: 213, lr: 2.0e-03, train_loss: 0.1368, train_acc: 0.9512 test_loss: 0.9316, test_acc: 0.7967, best: 0.7987, time: 0:04:58
 Epoch: 214, lr: 2.0e-03, train_loss: 0.1362, train_acc: 0.9526 test_loss: 1.0091, test_acc: 0.7869, best: 0.7987, time: 0:04:58
 Epoch: 215, lr: 2.0e-03, train_loss: 0.1429, train_acc: 0.9490 test_loss: 1.0336, test_acc: 0.7855, best: 0.7987, time: 0:04:59
 Epoch: 216, lr: 2.0e-03, train_loss: 0.1367, train_acc: 0.9566 test_loss: 0.9228, test_acc: 0.7960, best: 0.7987, time: 0:04:59
 Epoch: 217, lr: 2.0e-03, train_loss: 0.1429, train_acc: 0.9536 test_loss: 1.0563, test_acc: 0.7806, best: 0.7987, time: 0:04:58
 Epoch: 218, lr: 2.0e-03, train_loss: 0.1403, train_acc: 0.9536 test_loss: 0.9264, test_acc: 0.7945, best: 0.7987, time: 0:04:58
 Epoch: 219, lr: 2.0e-03, train_loss: 0.1427, train_acc: 0.9548 test_loss: 1.0444, test_acc: 0.7821, best: 0.7987, time: 0:04:58
 Epoch: 220, lr: 2.0e-03, train_loss: 0.1444, train_acc: 0.9492 test_loss: 0.9463, test_acc: 0.7940, best: 0.7987, time: 0:04:58
 Epoch: 221, lr: 2.0e-03, train_loss: 0.1377, train_acc: 0.9522 test_loss: 0.9862, test_acc: 0.7919, best: 0.7987, time: 0:04:59
 Epoch: 222, lr: 2.0e-03, train_loss: 0.1379, train_acc: 0.9520 test_loss: 0.9469, test_acc: 0.7950, best: 0.7987, time: 0:04:58
 Epoch: 223, lr: 2.0e-03, train_loss: 0.1333, train_acc: 0.9578 test_loss: 0.9766, test_acc: 0.7867, best: 0.7987, time: 0:04:59
 Epoch: 224, lr: 2.0e-03, train_loss: 0.1486, train_acc: 0.9500 test_loss: 1.0128, test_acc: 0.7885, best: 0.7987, time: 0:04:58
 Epoch: 225, lr: 2.0e-03, train_loss: 0.1439, train_acc: 0.9514 test_loss: 1.0902, test_acc: 0.7768, best: 0.7987, time: 0:04:58
 Epoch: 226, lr: 2.0e-03, train_loss: 0.1410, train_acc: 0.9528 test_loss: 0.9997, test_acc: 0.7820, best: 0.7987, time: 0:04:58
 Epoch: 227, lr: 2.0e-03, train_loss: 0.1498, train_acc: 0.9476 test_loss: 1.0275, test_acc: 0.7837, best: 0.7987, time: 0:04:59
 Epoch: 228, lr: 2.0e-03, train_loss: 0.1398, train_acc: 0.9526 test_loss: 0.9461, test_acc: 0.7867, best: 0.7987, time: 0:04:59
 Epoch: 229, lr: 2.0e-03, train_loss: 0.1282, train_acc: 0.9584 test_loss: 0.9572, test_acc: 0.7906, best: 0.7987, time: 0:04:58
 Epoch: 230, lr: 2.0e-03, train_loss: 0.1371, train_acc: 0.9552 test_loss: 1.0176, test_acc: 0.7883, best: 0.7987, time: 0:04:58
 Epoch: 231, lr: 2.0e-03, train_loss: 0.1281, train_acc: 0.9598 test_loss: 0.9448, test_acc: 0.7937, best: 0.7987, time: 0:04:58
 Epoch: 232, lr: 2.0e-03, train_loss: 0.1289, train_acc: 0.9530 test_loss: 1.0536, test_acc: 0.7877, best: 0.7987, time: 0:04:58
 Epoch: 233, lr: 2.0e-03, train_loss: 0.1366, train_acc: 0.9528 test_loss: 0.9253, test_acc: 0.7934, best: 0.7987, time: 0:04:58
 Epoch: 234, lr: 2.0e-03, train_loss: 0.1384, train_acc: 0.9548 test_loss: 1.0000, test_acc: 0.7846, best: 0.7987, time: 0:04:58
 Epoch: 235, lr: 2.0e-03, train_loss: 0.1345, train_acc: 0.9524 test_loss: 0.9424, test_acc: 0.7925, best: 0.7987, time: 0:04:58
 Epoch: 236, lr: 2.0e-03, train_loss: 0.1408, train_acc: 0.9520 test_loss: 0.8996, test_acc: 0.7925, best: 0.7987, time: 0:04:58
 Epoch: 237, lr: 2.0e-03, train_loss: 0.1279, train_acc: 0.9576 test_loss: 1.0641, test_acc: 0.7802, best: 0.7987, time: 0:04:58
 Epoch: 238, lr: 2.0e-03, train_loss: 0.1316, train_acc: 0.9568 test_loss: 0.9624, test_acc: 0.7947, best: 0.7987, time: 0:04:58
 Epoch: 239, lr: 2.0e-03, train_loss: 0.1337, train_acc: 0.9548 test_loss: 0.9608, test_acc: 0.7945, best: 0.7987, time: 0:04:59
 Epoch: 240, lr: 4.0e-04, train_loss: 0.1280, train_acc: 0.9556 test_loss: 0.9576, test_acc: 0.7887, best: 0.7987, time: 0:04:58
 Epoch: 241, lr: 4.0e-04, train_loss: 0.1193, train_acc: 0.9612 test_loss: 0.9421, test_acc: 0.7895, best: 0.7987, time: 0:04:58
 Epoch: 242, lr: 4.0e-04, train_loss: 0.1303, train_acc: 0.9554 test_loss: 0.9747, test_acc: 0.7910, best: 0.7987, time: 0:04:58
 Epoch: 243, lr: 4.0e-04, train_loss: 0.1180, train_acc: 0.9638 test_loss: 0.9182, test_acc: 0.7937, best: 0.7987, time: 0:04:58
 Epoch: 244, lr: 4.0e-04, train_loss: 0.1325, train_acc: 0.9538 test_loss: 1.0510, test_acc: 0.7895, best: 0.7987, time: 0:04:59
 Epoch: 245, lr: 4.0e-04, train_loss: 0.1261, train_acc: 0.9574 test_loss: 1.0251, test_acc: 0.7876, best: 0.7987, time: 0:04:59
 Epoch: 246, lr: 4.0e-04, train_loss: 0.1249, train_acc: 0.9568 test_loss: 0.9600, test_acc: 0.7957, best: 0.7987, time: 0:04:59
 Epoch: 247, lr: 4.0e-04, train_loss: 0.1357, train_acc: 0.9522 test_loss: 0.9797, test_acc: 0.7939, best: 0.7987, time: 0:04:59
 Epoch: 248, lr: 4.0e-04, train_loss: 0.1241, train_acc: 0.9538 test_loss: 0.9784, test_acc: 0.7867, best: 0.7987, time: 0:04:59
 Epoch: 249, lr: 4.0e-04, train_loss: 0.1239, train_acc: 0.9554 test_loss: 1.0572, test_acc: 0.7857, best: 0.7987, time: 0:04:59
 Epoch: 250, lr: 4.0e-04, train_loss: 0.1350, train_acc: 0.9536 test_loss: 1.0382, test_acc: 0.7903, best: 0.7987, time: 0:04:58
 Epoch: 251, lr: 4.0e-04, train_loss: 0.1265, train_acc: 0.9592 test_loss: 0.9911, test_acc: 0.7924, best: 0.7987, time: 0:04:58
 Epoch: 252, lr: 4.0e-04, train_loss: 0.1140, train_acc: 0.9602 test_loss: 0.9205, test_acc: 0.7917, best: 0.7987, time: 0:04:58
 Epoch: 253, lr: 4.0e-04, train_loss: 0.1108, train_acc: 0.9620 test_loss: 0.9593, test_acc: 0.7960, best: 0.7987, time: 0:04:58
 Epoch: 254, lr: 4.0e-04, train_loss: 0.1227, train_acc: 0.9574 test_loss: 1.0190, test_acc: 0.7859, best: 0.7987, time: 0:04:58
 Epoch: 255, lr: 4.0e-04, train_loss: 0.1165, train_acc: 0.9620 test_loss: 0.9678, test_acc: 0.7944, best: 0.7987, time: 0:04:58
 Epoch: 256, lr: 4.0e-04, train_loss: 0.1290, train_acc: 0.9578 test_loss: 1.0568, test_acc: 0.7897, best: 0.7987, time: 0:04:58
 Epoch: 257, lr: 4.0e-04, train_loss: 0.1177, train_acc: 0.9596 test_loss: 1.0137, test_acc: 0.7895, best: 0.7987, time: 0:04:58
 Epoch: 258, lr: 4.0e-04, train_loss: 0.1223, train_acc: 0.9584 test_loss: 0.9914, test_acc: 0.7891, best: 0.7987, time: 0:04:58
 Epoch: 259, lr: 4.0e-04, train_loss: 0.1261, train_acc: 0.9608 test_loss: 1.0001, test_acc: 0.7916, best: 0.7987, time: 0:04:58
 Epoch: 260, lr: 4.0e-04, train_loss: 0.1100, train_acc: 0.9640 test_loss: 0.9571, test_acc: 0.7970, best: 0.7987, time: 0:04:59
 Epoch: 261, lr: 4.0e-04, train_loss: 0.1280, train_acc: 0.9552 test_loss: 0.9921, test_acc: 0.7909, best: 0.7987, time: 0:04:58
 Epoch: 262, lr: 4.0e-04, train_loss: 0.1147, train_acc: 0.9628 test_loss: 0.9665, test_acc: 0.7933, best: 0.7987, time: 0:04:58
 Epoch: 263, lr: 4.0e-04, train_loss: 0.1090, train_acc: 0.9634 test_loss: 1.0452, test_acc: 0.7874, best: 0.7987, time: 0:04:58
 Epoch: 264, lr: 4.0e-04, train_loss: 0.1263, train_acc: 0.9552 test_loss: 1.0713, test_acc: 0.7874, best: 0.7987, time: 0:04:58
 Epoch: 265, lr: 4.0e-04, train_loss: 0.1018, train_acc: 0.9650 test_loss: 0.9497, test_acc: 0.7991, best: 0.7991, time: 0:05:00
 Epoch: 266, lr: 4.0e-04, train_loss: 0.1173, train_acc: 0.9604 test_loss: 0.9170, test_acc: 0.7976, best: 0.7991, time: 0:04:58
 Epoch: 267, lr: 4.0e-04, train_loss: 0.1126, train_acc: 0.9622 test_loss: 0.9744, test_acc: 0.7921, best: 0.7991, time: 0:04:58
 Epoch: 268, lr: 4.0e-04, train_loss: 0.1221, train_acc: 0.9586 test_loss: 0.9994, test_acc: 0.7933, best: 0.7991, time: 0:04:59
 Epoch: 269, lr: 4.0e-04, train_loss: 0.1155, train_acc: 0.9624 test_loss: 1.0467, test_acc: 0.7879, best: 0.7991, time: 0:04:58
 Epoch: 270, lr: 8.0e-05, train_loss: 0.1128, train_acc: 0.9610 test_loss: 0.9645, test_acc: 0.7939, best: 0.7991, time: 0:04:58
 Epoch: 271, lr: 8.0e-05, train_loss: 0.1134, train_acc: 0.9596 test_loss: 0.9990, test_acc: 0.7933, best: 0.7991, time: 0:04:58
 Epoch: 272, lr: 8.0e-05, train_loss: 0.1135, train_acc: 0.9590 test_loss: 0.9539, test_acc: 0.7977, best: 0.7991, time: 0:04:59
 Epoch: 273, lr: 8.0e-05, train_loss: 0.1254, train_acc: 0.9558 test_loss: 0.9806, test_acc: 0.7933, best: 0.7991, time: 0:04:58
 Epoch: 274, lr: 8.0e-05, train_loss: 0.1312, train_acc: 0.9576 test_loss: 0.9573, test_acc: 0.7940, best: 0.7991, time: 0:04:59
 Epoch: 275, lr: 8.0e-05, train_loss: 0.1079, train_acc: 0.9640 test_loss: 0.9251, test_acc: 0.7991, best: 0.7991, time: 0:04:59
 Epoch: 276, lr: 8.0e-05, train_loss: 0.1180, train_acc: 0.9598 test_loss: 1.0038, test_acc: 0.7923, best: 0.7991, time: 0:04:58
 Epoch: 277, lr: 8.0e-05, train_loss: 0.1186, train_acc: 0.9604 test_loss: 0.9895, test_acc: 0.7917, best: 0.7991, time: 0:04:58
 Epoch: 278, lr: 8.0e-05, train_loss: 0.1256, train_acc: 0.9564 test_loss: 0.9701, test_acc: 0.7936, best: 0.7991, time: 0:04:58
 Epoch: 279, lr: 8.0e-05, train_loss: 0.1046, train_acc: 0.9662 test_loss: 1.0264, test_acc: 0.7849, best: 0.7991, time: 0:04:58
 Epoch: 280, lr: 8.0e-05, train_loss: 0.1170, train_acc: 0.9598 test_loss: 0.9971, test_acc: 0.7901, best: 0.7991, time: 0:04:58
 Epoch: 281, lr: 8.0e-05, train_loss: 0.1229, train_acc: 0.9584 test_loss: 1.0174, test_acc: 0.7889, best: 0.7991, time: 0:04:59
 Epoch: 282, lr: 8.0e-05, train_loss: 0.1125, train_acc: 0.9614 test_loss: 0.9802, test_acc: 0.7924, best: 0.7991, time: 0:04:59
 Epoch: 283, lr: 8.0e-05, train_loss: 0.1088, train_acc: 0.9652 test_loss: 0.9317, test_acc: 0.7984, best: 0.7991, time: 0:04:58
 Epoch: 284, lr: 8.0e-05, train_loss: 0.1093, train_acc: 0.9650 test_loss: 1.1680, test_acc: 0.7798, best: 0.7991, time: 0:04:58
 Epoch: 285, lr: 8.0e-05, train_loss: 0.1117, train_acc: 0.9616 test_loss: 1.0477, test_acc: 0.7849, best: 0.7991, time: 0:04:58
 Epoch: 286, lr: 8.0e-05, train_loss: 0.1085, train_acc: 0.9642 test_loss: 1.0369, test_acc: 0.7890, best: 0.7991, time: 0:04:58
 Epoch: 287, lr: 8.0e-05, train_loss: 0.1122, train_acc: 0.9624 test_loss: 0.9761, test_acc: 0.7937, best: 0.7991, time: 0:04:58
 Epoch: 288, lr: 8.0e-05, train_loss: 0.1217, train_acc: 0.9576 test_loss: 1.0181, test_acc: 0.7881, best: 0.7991, time: 0:04:58
 Epoch: 289, lr: 8.0e-05, train_loss: 0.1028, train_acc: 0.9656 test_loss: 0.9947, test_acc: 0.7887, best: 0.7991, time: 0:04:58
 Epoch: 290, lr: 8.0e-05, train_loss: 0.1161, train_acc: 0.9578 test_loss: 0.9529, test_acc: 0.7940, best: 0.7991, time: 0:04:58
 Epoch: 291, lr: 8.0e-05, train_loss: 0.1207, train_acc: 0.9580 test_loss: 0.9739, test_acc: 0.7934, best: 0.7991, time: 0:04:58
 Epoch: 292, lr: 8.0e-05, train_loss: 0.1132, train_acc: 0.9604 test_loss: 0.9639, test_acc: 0.7987, best: 0.7991, time: 0:04:58
 Epoch: 293, lr: 8.0e-05, train_loss: 0.1244, train_acc: 0.9578 test_loss: 1.0365, test_acc: 0.7885, best: 0.7991, time: 0:04:58
 Epoch: 294, lr: 8.0e-05, train_loss: 0.1245, train_acc: 0.9544 test_loss: 1.1020, test_acc: 0.7843, best: 0.7991, time: 0:04:58
 Epoch: 295, lr: 8.0e-05, train_loss: 0.1087, train_acc: 0.9606 test_loss: 0.9736, test_acc: 0.7919, best: 0.7991, time: 0:04:58
 Epoch: 296, lr: 8.0e-05, train_loss: 0.1045, train_acc: 0.9652 test_loss: 0.9968, test_acc: 0.7945, best: 0.7991, time: 0:04:58
 Epoch: 297, lr: 8.0e-05, train_loss: 0.1185, train_acc: 0.9616 test_loss: 0.9722, test_acc: 0.7926, best: 0.7991, time: 0:04:58
 Epoch: 298, lr: 8.0e-05, train_loss: 0.1084, train_acc: 0.9604 test_loss: 1.0065, test_acc: 0.7887, best: 0.7991, time: 0:04:58
 Epoch: 299, lr: 8.0e-05, train_loss: 0.1088, train_acc: 0.9628 test_loss: 0.9761, test_acc: 0.7925, best: 0.7991, time: 0:04:58
 Highest accuracy: 0.7991