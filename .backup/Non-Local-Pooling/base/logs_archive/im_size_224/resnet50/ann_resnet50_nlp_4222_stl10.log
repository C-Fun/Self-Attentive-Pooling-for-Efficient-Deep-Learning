
 Run on time: 2022-06-25 13:21:40.365630

 Arguments:
	 gpu                  : True
	 log                  : True
	 seed                 : 0
	 momentum             : 0.9
	 amsgrad              : True
	 dataset              : STL10
	 batch_size           : 8
	 architecture         : RESNET50_NLP_4222
	 im_size              : 224
	 relu_threshold       : 4.0
	 learning_rate        : 0.01
	 pretrained_backbone  : 
	 pretrained_ann       : 
	 weight_decay         : 0.0
	 test_only            : False
	 epochs               : 300
	 lr_interval          : [180, 240, 270]
	 lr_reduce            : 5
	 optimizer            : SGD
	 dropout              : 0.2
	 kernel_size          : 3
	 dont_save            : False
	 visualize            : False
	 devices              : 0,1
 DataParallel(
  (module): NetworkByName(
    (net): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (pool): Pool2d(
          (logit): Sequential(
            (pool_weight): NLP_BASE(
              (downsample): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (multihead_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (restore): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Sigmoid()
              )
              (pos_embed): PositionEmbeddingLearned(
                (row_embed): Embedding(256, 128)
                (col_embed): Embedding(256, 128)
              )
            )
          )
          (pool): AvgPool2d(kernel_size=4, stride=4, padding=0)
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (pool): Pool2d(
          (logit): Sequential(
            (pool_weight): NLP_BASE(
              (downsample): Sequential(
                (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
                (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (multihead_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (restore): Sequential(
                (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
                (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Sigmoid()
              )
              (pos_embed): PositionEmbeddingLearned(
                (row_embed): Embedding(256, 256)
                (col_embed): Embedding(256, 256)
              )
            )
          )
          (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (pool): Pool2d(
          (logit): Sequential(
            (pool_weight): NLP_BASE(
              (downsample): Sequential(
                (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
                (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (multihead_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (restore): Sequential(
                (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
                (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Sigmoid()
              )
              (pos_embed): PositionEmbeddingLearned(
                (row_embed): Embedding(256, 512)
                (col_embed): Embedding(256, 512)
              )
            )
          )
          (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        )
      )
      (layer4): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (pool): Pool2d(
          (logit): Sequential(
            (pool_weight): NLP_BASE(
              (downsample): Sequential(
                (0): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1))
                (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (multihead_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=2048, out_features=2048, bias=True)
              )
              (restore): Sequential(
                (0): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1))
                (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Sigmoid()
              )
              (pos_embed): PositionEmbeddingLearned(
                (row_embed): Embedding(256, 1024)
                (col_embed): Embedding(256, 1024)
              )
            )
          )
          (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        )
      )
      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
      (fc): Linear(in_features=2048, out_features=10, bias=True)
    )
  )
)
 SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0
)
 Epoch: 1, lr: 1.0e-02, train_loss: 2.4473, train_acc: 0.1710 test_loss: 2.0997, test_acc: 0.2293, best: 0.2293, time: 0:03:53
 Epoch: 2, lr: 1.0e-02, train_loss: 2.0583, train_acc: 0.2104 test_loss: 1.8403, test_acc: 0.2802, best: 0.2802, time: 0:03:48
 Epoch: 3, lr: 1.0e-02, train_loss: 1.9722, train_acc: 0.2548 test_loss: 1.9758, test_acc: 0.3068, best: 0.3068, time: 0:03:48
 Epoch: 4, lr: 1.0e-02, train_loss: 1.9388, train_acc: 0.2760 test_loss: 1.8291, test_acc: 0.3172, best: 0.3172, time: 0:03:48
 Epoch: 5, lr: 1.0e-02, train_loss: 1.8986, train_acc: 0.2810 test_loss: 1.7583, test_acc: 0.3351, best: 0.3351, time: 0:03:48
 Epoch: 6, lr: 1.0e-02, train_loss: 1.8619, train_acc: 0.2954 test_loss: 1.7422, test_acc: 0.3377, best: 0.3377, time: 0:03:48
 Epoch: 7, lr: 1.0e-02, train_loss: 1.8265, train_acc: 0.3028 test_loss: 1.9980, test_acc: 0.3001, best: 0.3377, time: 0:03:47
 Epoch: 8, lr: 1.0e-02, train_loss: 1.7967, train_acc: 0.3266 test_loss: 1.8651, test_acc: 0.3339, best: 0.3377, time: 0:03:47
 Epoch: 9, lr: 1.0e-02, train_loss: 1.7887, train_acc: 0.3242 test_loss: 1.7153, test_acc: 0.3496, best: 0.3496, time: 0:03:48
 Epoch: 10, lr: 1.0e-02, train_loss: 1.7553, train_acc: 0.3290 test_loss: 1.7838, test_acc: 0.3584, best: 0.3584, time: 0:03:48
 Epoch: 11, lr: 1.0e-02, train_loss: 1.7380, train_acc: 0.3510 test_loss: 1.8025, test_acc: 0.3588, best: 0.3588, time: 0:03:49
 Epoch: 12, lr: 1.0e-02, train_loss: 1.7100, train_acc: 0.3636 test_loss: 1.5907, test_acc: 0.4098, best: 0.4098, time: 0:03:49
 Epoch: 13, lr: 1.0e-02, train_loss: 1.6868, train_acc: 0.3626 test_loss: 1.6160, test_acc: 0.4154, best: 0.4154, time: 0:03:49
 Epoch: 14, lr: 1.0e-02, train_loss: 1.6751, train_acc: 0.3774 test_loss: 1.6330, test_acc: 0.3949, best: 0.4154, time: 0:03:48
 Epoch: 15, lr: 1.0e-02, train_loss: 1.6331, train_acc: 0.3836 test_loss: 1.6291, test_acc: 0.4101, best: 0.4154, time: 0:03:48
 Epoch: 16, lr: 1.0e-02, train_loss: 1.6260, train_acc: 0.3918 test_loss: 1.5086, test_acc: 0.4576, best: 0.4576, time: 0:03:49
 Epoch: 17, lr: 1.0e-02, train_loss: 1.5998, train_acc: 0.4030 test_loss: 1.5054, test_acc: 0.4596, best: 0.4596, time: 0:03:49
 Epoch: 18, lr: 1.0e-02, train_loss: 1.5852, train_acc: 0.4168 test_loss: 1.5205, test_acc: 0.4611, best: 0.4611, time: 0:03:49
 Epoch: 19, lr: 1.0e-02, train_loss: 1.5424, train_acc: 0.4204 test_loss: 1.5225, test_acc: 0.4610, best: 0.4611, time: 0:03:48
 Epoch: 20, lr: 1.0e-02, train_loss: 1.5379, train_acc: 0.4208 test_loss: 1.5442, test_acc: 0.4515, best: 0.4611, time: 0:03:48
 Epoch: 21, lr: 1.0e-02, train_loss: 1.5093, train_acc: 0.4414 test_loss: 1.5320, test_acc: 0.4640, best: 0.4640, time: 0:03:49
 Epoch: 22, lr: 1.0e-02, train_loss: 1.4839, train_acc: 0.4528 test_loss: 1.5052, test_acc: 0.4659, best: 0.4659, time: 0:03:49
 Epoch: 23, lr: 1.0e-02, train_loss: 1.4873, train_acc: 0.4458 test_loss: 1.4791, test_acc: 0.4956, best: 0.4956, time: 0:03:49
 Epoch: 24, lr: 1.0e-02, train_loss: 1.4670, train_acc: 0.4608 test_loss: 1.5486, test_acc: 0.4874, best: 0.4956, time: 0:03:48
 Epoch: 25, lr: 1.0e-02, train_loss: 1.4155, train_acc: 0.4778 test_loss: 1.4239, test_acc: 0.5018, best: 0.5018, time: 0:03:49
 Epoch: 26, lr: 1.0e-02, train_loss: 1.4414, train_acc: 0.4706 test_loss: 1.4549, test_acc: 0.4856, best: 0.5018, time: 0:03:48
 Epoch: 27, lr: 1.0e-02, train_loss: 1.3969, train_acc: 0.4862 test_loss: 1.3561, test_acc: 0.5231, best: 0.5231, time: 0:03:49
 Epoch: 28, lr: 1.0e-02, train_loss: 1.3883, train_acc: 0.4934 test_loss: 1.2862, test_acc: 0.5315, best: 0.5315, time: 0:03:49
 Epoch: 29, lr: 1.0e-02, train_loss: 1.3657, train_acc: 0.5022 test_loss: 1.4043, test_acc: 0.5220, best: 0.5315, time: 0:03:48
 Epoch: 30, lr: 1.0e-02, train_loss: 1.3380, train_acc: 0.5054 test_loss: 1.2812, test_acc: 0.5371, best: 0.5371, time: 0:03:49
 Epoch: 31, lr: 1.0e-02, train_loss: 1.3255, train_acc: 0.5068 test_loss: 1.2581, test_acc: 0.5604, best: 0.5604, time: 0:03:49
 Epoch: 32, lr: 1.0e-02, train_loss: 1.3127, train_acc: 0.5078 test_loss: 1.3463, test_acc: 0.5406, best: 0.5604, time: 0:03:48
 Epoch: 33, lr: 1.0e-02, train_loss: 1.3108, train_acc: 0.5258 test_loss: 1.3116, test_acc: 0.5511, best: 0.5604, time: 0:03:48
 Epoch: 34, lr: 1.0e-02, train_loss: 1.2863, train_acc: 0.5308 test_loss: 1.3241, test_acc: 0.5531, best: 0.5604, time: 0:03:48
 Epoch: 35, lr: 1.0e-02, train_loss: 1.2809, train_acc: 0.5318 test_loss: 1.2231, test_acc: 0.5663, best: 0.5663, time: 0:03:49
 Epoch: 36, lr: 1.0e-02, train_loss: 1.2617, train_acc: 0.5416 test_loss: 1.2672, test_acc: 0.5607, best: 0.5663, time: 0:03:48
 Epoch: 37, lr: 1.0e-02, train_loss: 1.2342, train_acc: 0.5474 test_loss: 1.2591, test_acc: 0.5700, best: 0.5700, time: 0:03:49
 Epoch: 38, lr: 1.0e-02, train_loss: 1.2313, train_acc: 0.5570 test_loss: 1.2843, test_acc: 0.5657, best: 0.5700, time: 0:03:48
 Epoch: 39, lr: 1.0e-02, train_loss: 1.1928, train_acc: 0.5606 test_loss: 1.2080, test_acc: 0.5891, best: 0.5891, time: 0:03:49
 Epoch: 40, lr: 1.0e-02, train_loss: 1.1836, train_acc: 0.5776 test_loss: 1.2570, test_acc: 0.5577, best: 0.5891, time: 0:03:48
 Epoch: 41, lr: 1.0e-02, train_loss: 1.1803, train_acc: 0.5738 test_loss: 1.1781, test_acc: 0.5975, best: 0.5975, time: 0:03:49
 Epoch: 42, lr: 1.0e-02, train_loss: 1.1701, train_acc: 0.5750 test_loss: 1.3195, test_acc: 0.5823, best: 0.5975, time: 0:03:48
 Epoch: 43, lr: 1.0e-02, train_loss: 1.1713, train_acc: 0.5774 test_loss: 1.1585, test_acc: 0.6034, best: 0.6034, time: 0:03:49
 Epoch: 44, lr: 1.0e-02, train_loss: 1.1417, train_acc: 0.5894 test_loss: 1.1315, test_acc: 0.6031, best: 0.6034, time: 0:03:48
 Epoch: 45, lr: 1.0e-02, train_loss: 1.1287, train_acc: 0.5848 test_loss: 1.1942, test_acc: 0.6074, best: 0.6074, time: 0:03:49
 Epoch: 46, lr: 1.0e-02, train_loss: 1.1134, train_acc: 0.5976 test_loss: 1.2052, test_acc: 0.5956, best: 0.6074, time: 0:03:48
 Epoch: 47, lr: 1.0e-02, train_loss: 1.1033, train_acc: 0.6090 test_loss: 1.1852, test_acc: 0.6180, best: 0.6180, time: 0:03:49
 Epoch: 48, lr: 1.0e-02, train_loss: 1.0920, train_acc: 0.6058 test_loss: 1.1812, test_acc: 0.6090, best: 0.6180, time: 0:03:48
 Epoch: 49, lr: 1.0e-02, train_loss: 1.0798, train_acc: 0.6056 test_loss: 1.1363, test_acc: 0.6254, best: 0.6254, time: 0:03:49
 Epoch: 50, lr: 1.0e-02, train_loss: 1.0537, train_acc: 0.6240 test_loss: 1.1188, test_acc: 0.6175, best: 0.6254, time: 0:03:48
 Epoch: 51, lr: 1.0e-02, train_loss: 1.0642, train_acc: 0.6174 test_loss: 1.1454, test_acc: 0.6340, best: 0.6340, time: 0:03:49
 Epoch: 52, lr: 1.0e-02, train_loss: 1.0624, train_acc: 0.6170 test_loss: 1.1688, test_acc: 0.6190, best: 0.6340, time: 0:03:48
 Epoch: 53, lr: 1.0e-02, train_loss: 1.0125, train_acc: 0.6374 test_loss: 1.1299, test_acc: 0.6289, best: 0.6340, time: 0:03:48
 Epoch: 54, lr: 1.0e-02, train_loss: 1.0404, train_acc: 0.6220 test_loss: 1.2559, test_acc: 0.6092, best: 0.6340, time: 0:03:48
 Epoch: 55, lr: 1.0e-02, train_loss: 1.0179, train_acc: 0.6284 test_loss: 1.2030, test_acc: 0.6210, best: 0.6340, time: 0:03:48
 Epoch: 56, lr: 1.0e-02, train_loss: 0.9917, train_acc: 0.6456 test_loss: 1.1717, test_acc: 0.6305, best: 0.6340, time: 0:03:48
 Epoch: 57, lr: 1.0e-02, train_loss: 0.9792, train_acc: 0.6504 test_loss: 1.1668, test_acc: 0.6332, best: 0.6340, time: 0:03:48
 Epoch: 58, lr: 1.0e-02, train_loss: 0.9589, train_acc: 0.6540 test_loss: 1.0250, test_acc: 0.6576, best: 0.6576, time: 0:03:49
 Epoch: 59, lr: 1.0e-02, train_loss: 0.9606, train_acc: 0.6562 test_loss: 1.1127, test_acc: 0.6484, best: 0.6576, time: 0:03:48
 Epoch: 60, lr: 1.0e-02, train_loss: 0.9672, train_acc: 0.6492 test_loss: 1.2425, test_acc: 0.6322, best: 0.6576, time: 0:03:48
 Epoch: 61, lr: 1.0e-02, train_loss: 0.9538, train_acc: 0.6494 test_loss: 1.1554, test_acc: 0.6220, best: 0.6576, time: 0:03:48
 Epoch: 62, lr: 1.0e-02, train_loss: 0.9369, train_acc: 0.6542 test_loss: 1.3312, test_acc: 0.6285, best: 0.6576, time: 0:03:48
 Epoch: 63, lr: 1.0e-02, train_loss: 0.9357, train_acc: 0.6698 test_loss: 1.1661, test_acc: 0.6338, best: 0.6576, time: 0:03:48
 Epoch: 64, lr: 1.0e-02, train_loss: 0.9280, train_acc: 0.6766 test_loss: 1.0565, test_acc: 0.6566, best: 0.6576, time: 0:03:48
 Epoch: 65, lr: 1.0e-02, train_loss: 0.9271, train_acc: 0.6688 test_loss: 1.2389, test_acc: 0.6488, best: 0.6576, time: 0:03:48
 Epoch: 66, lr: 1.0e-02, train_loss: 0.9231, train_acc: 0.6712 test_loss: 1.0962, test_acc: 0.6528, best: 0.6576, time: 0:03:48
 Epoch: 67, lr: 1.0e-02, train_loss: 0.8911, train_acc: 0.6762 test_loss: 1.2581, test_acc: 0.6451, best: 0.6576, time: 0:03:48
 Epoch: 68, lr: 1.0e-02, train_loss: 0.8936, train_acc: 0.6800 test_loss: 1.0579, test_acc: 0.6654, best: 0.6654, time: 0:03:49
 Epoch: 69, lr: 1.0e-02, train_loss: 0.9088, train_acc: 0.6772 test_loss: 1.0377, test_acc: 0.6516, best: 0.6654, time: 0:03:48
 Epoch: 70, lr: 1.0e-02, train_loss: 0.8774, train_acc: 0.6846 test_loss: 1.0328, test_acc: 0.6681, best: 0.6681, time: 0:03:49
 Epoch: 71, lr: 1.0e-02, train_loss: 0.8619, train_acc: 0.6990 test_loss: 1.1406, test_acc: 0.6549, best: 0.6681, time: 0:03:48
 Epoch: 72, lr: 1.0e-02, train_loss: 0.8570, train_acc: 0.6898 test_loss: 1.0783, test_acc: 0.6737, best: 0.6737, time: 0:03:49
 Epoch: 73, lr: 1.0e-02, train_loss: 0.8767, train_acc: 0.6830 test_loss: 1.2464, test_acc: 0.6526, best: 0.6737, time: 0:03:48
 Epoch: 74, lr: 1.0e-02, train_loss: 0.8375, train_acc: 0.7040 test_loss: 1.1978, test_acc: 0.6571, best: 0.6737, time: 0:03:48
 Epoch: 75, lr: 1.0e-02, train_loss: 0.8345, train_acc: 0.7000 test_loss: 1.2035, test_acc: 0.6514, best: 0.6737, time: 0:03:48
 Epoch: 76, lr: 1.0e-02, train_loss: 0.8206, train_acc: 0.7086 test_loss: 1.0995, test_acc: 0.6747, best: 0.6747, time: 0:03:49
 Epoch: 77, lr: 1.0e-02, train_loss: 0.8301, train_acc: 0.7054 test_loss: 1.1815, test_acc: 0.6656, best: 0.6747, time: 0:03:48
 Epoch: 78, lr: 1.0e-02, train_loss: 0.7943, train_acc: 0.7196 test_loss: 1.0232, test_acc: 0.6797, best: 0.6797, time: 0:03:49
 Epoch: 79, lr: 1.0e-02, train_loss: 0.8185, train_acc: 0.7094 test_loss: 0.9736, test_acc: 0.6830, best: 0.6830, time: 0:03:49
 Epoch: 80, lr: 1.0e-02, train_loss: 0.7839, train_acc: 0.7146 test_loss: 1.0064, test_acc: 0.6905, best: 0.6905, time: 0:03:49
 Epoch: 81, lr: 1.0e-02, train_loss: 0.7840, train_acc: 0.7258 test_loss: 1.1663, test_acc: 0.6706, best: 0.6905, time: 0:03:48
 Epoch: 82, lr: 1.0e-02, train_loss: 0.7737, train_acc: 0.7314 test_loss: 1.0684, test_acc: 0.6880, best: 0.6905, time: 0:03:48
 Epoch: 83, lr: 1.0e-02, train_loss: 0.7868, train_acc: 0.7220 test_loss: 1.0983, test_acc: 0.6731, best: 0.6905, time: 0:03:48
 Epoch: 84, lr: 1.0e-02, train_loss: 0.7521, train_acc: 0.7370 test_loss: 1.0853, test_acc: 0.6940, best: 0.6940, time: 0:03:49
 Epoch: 85, lr: 1.0e-02, train_loss: 0.7539, train_acc: 0.7310 test_loss: 1.0075, test_acc: 0.6916, best: 0.6940, time: 0:03:48
 Epoch: 86, lr: 1.0e-02, train_loss: 0.7597, train_acc: 0.7266 test_loss: 1.2138, test_acc: 0.6522, best: 0.6940, time: 0:03:48
 Epoch: 87, lr: 1.0e-02, train_loss: 0.7349, train_acc: 0.7382 test_loss: 1.0628, test_acc: 0.6843, best: 0.6940, time: 0:03:48
 Epoch: 88, lr: 1.0e-02, train_loss: 0.7423, train_acc: 0.7386 test_loss: 1.0786, test_acc: 0.6935, best: 0.6940, time: 0:03:48
 Epoch: 89, lr: 1.0e-02, train_loss: 0.7368, train_acc: 0.7364 test_loss: 1.0336, test_acc: 0.6889, best: 0.6940, time: 0:03:48
 Epoch: 90, lr: 1.0e-02, train_loss: 0.7546, train_acc: 0.7348 test_loss: 1.0910, test_acc: 0.6859, best: 0.6940, time: 0:03:48
 Epoch: 91, lr: 1.0e-02, train_loss: 0.7172, train_acc: 0.7462 test_loss: 1.1229, test_acc: 0.6907, best: 0.6940, time: 0:03:48
 Epoch: 92, lr: 1.0e-02, train_loss: 0.6857, train_acc: 0.7574 test_loss: 1.1472, test_acc: 0.6779, best: 0.6940, time: 0:03:48
 Epoch: 93, lr: 1.0e-02, train_loss: 0.6980, train_acc: 0.7518 test_loss: 1.1441, test_acc: 0.6910, best: 0.6940, time: 0:03:48
 Epoch: 94, lr: 1.0e-02, train_loss: 0.7126, train_acc: 0.7474 test_loss: 1.0003, test_acc: 0.7202, best: 0.7202, time: 0:03:49
 Epoch: 95, lr: 1.0e-02, train_loss: 0.7150, train_acc: 0.7404 test_loss: 1.0879, test_acc: 0.7025, best: 0.7202, time: 0:03:48
 Epoch: 96, lr: 1.0e-02, train_loss: 0.6796, train_acc: 0.7600 test_loss: 1.0442, test_acc: 0.6980, best: 0.7202, time: 0:03:48
 Epoch: 97, lr: 1.0e-02, train_loss: 0.6716, train_acc: 0.7704 test_loss: 1.0616, test_acc: 0.7039, best: 0.7202, time: 0:03:48
 Epoch: 98, lr: 1.0e-02, train_loss: 0.6792, train_acc: 0.7650 test_loss: 1.0731, test_acc: 0.7037, best: 0.7202, time: 0:03:48
 Epoch: 99, lr: 1.0e-02, train_loss: 0.6805, train_acc: 0.7562 test_loss: 1.1384, test_acc: 0.6985, best: 0.7202, time: 0:03:48
 Epoch: 100, lr: 1.0e-02, train_loss: 0.6659, train_acc: 0.7636 test_loss: 1.2598, test_acc: 0.6609, best: 0.7202, time: 0:03:48
 Epoch: 101, lr: 1.0e-02, train_loss: 0.6606, train_acc: 0.7638 test_loss: 0.9849, test_acc: 0.7173, best: 0.7202, time: 0:03:48
 Epoch: 102, lr: 1.0e-02, train_loss: 0.6698, train_acc: 0.7590 test_loss: 0.9871, test_acc: 0.7214, best: 0.7214, time: 0:03:49
 Epoch: 103, lr: 1.0e-02, train_loss: 0.6632, train_acc: 0.7644 test_loss: 1.0388, test_acc: 0.6981, best: 0.7214, time: 0:03:48
 Epoch: 104, lr: 1.0e-02, train_loss: 0.6648, train_acc: 0.7650 test_loss: 1.0233, test_acc: 0.7116, best: 0.7214, time: 0:03:48
 Epoch: 105, lr: 1.0e-02, train_loss: 0.6806, train_acc: 0.7666 test_loss: 1.0652, test_acc: 0.7095, best: 0.7214, time: 0:03:48
 Epoch: 106, lr: 1.0e-02, train_loss: 0.6497, train_acc: 0.7730 test_loss: 1.0609, test_acc: 0.7149, best: 0.7214, time: 0:03:48
 Epoch: 107, lr: 1.0e-02, train_loss: 0.6327, train_acc: 0.7780 test_loss: 1.1738, test_acc: 0.6879, best: 0.7214, time: 0:03:48
 Epoch: 108, lr: 1.0e-02, train_loss: 0.6450, train_acc: 0.7716 test_loss: 1.1121, test_acc: 0.7040, best: 0.7214, time: 0:03:48
 Epoch: 109, lr: 1.0e-02, train_loss: 0.6070, train_acc: 0.7818 test_loss: 1.0525, test_acc: 0.7144, best: 0.7214, time: 0:03:48
 Epoch: 110, lr: 1.0e-02, train_loss: 0.6361, train_acc: 0.7770 test_loss: 1.0435, test_acc: 0.7190, best: 0.7214, time: 0:03:48
 Epoch: 111, lr: 1.0e-02, train_loss: 0.6262, train_acc: 0.7788 test_loss: 1.0830, test_acc: 0.7091, best: 0.7214, time: 0:03:48
 Epoch: 112, lr: 1.0e-02, train_loss: 0.6130, train_acc: 0.7822 test_loss: 1.0944, test_acc: 0.7242, best: 0.7242, time: 0:03:50
 Epoch: 113, lr: 1.0e-02, train_loss: 0.5964, train_acc: 0.7898 test_loss: 1.0908, test_acc: 0.7070, best: 0.7242, time: 0:03:48
 Epoch: 114, lr: 1.0e-02, train_loss: 0.6005, train_acc: 0.7862 test_loss: 1.0766, test_acc: 0.7030, best: 0.7242, time: 0:03:48
 Epoch: 115, lr: 1.0e-02, train_loss: 0.5851, train_acc: 0.7944 test_loss: 0.9862, test_acc: 0.7249, best: 0.7249, time: 0:03:50
 Epoch: 116, lr: 1.0e-02, train_loss: 0.5780, train_acc: 0.7978 test_loss: 1.0806, test_acc: 0.7166, best: 0.7249, time: 0:03:48
 Epoch: 117, lr: 1.0e-02, train_loss: 0.5726, train_acc: 0.7924 test_loss: 1.1455, test_acc: 0.7281, best: 0.7281, time: 0:03:49
 Epoch: 118, lr: 1.0e-02, train_loss: 0.5911, train_acc: 0.7908 test_loss: 1.1725, test_acc: 0.7080, best: 0.7281, time: 0:03:48
 Epoch: 119, lr: 1.0e-02, train_loss: 0.5771, train_acc: 0.7996 test_loss: 1.0659, test_acc: 0.7060, best: 0.7281, time: 0:03:47
 Epoch: 120, lr: 1.0e-02, train_loss: 0.5617, train_acc: 0.8020 test_loss: 1.0451, test_acc: 0.7228, best: 0.7281, time: 0:03:48
 Epoch: 121, lr: 1.0e-02, train_loss: 0.5786, train_acc: 0.7984 test_loss: 0.9837, test_acc: 0.7279, best: 0.7281, time: 0:03:48
 Epoch: 122, lr: 1.0e-02, train_loss: 0.5654, train_acc: 0.7994 test_loss: 1.1952, test_acc: 0.7086, best: 0.7281, time: 0:03:47
 Epoch: 123, lr: 1.0e-02, train_loss: 0.5768, train_acc: 0.7980 test_loss: 1.2507, test_acc: 0.7027, best: 0.7281, time: 0:03:48
 Epoch: 124, lr: 1.0e-02, train_loss: 0.5518, train_acc: 0.8066 test_loss: 1.0123, test_acc: 0.7264, best: 0.7281, time: 0:03:48
 Epoch: 125, lr: 1.0e-02, train_loss: 0.5598, train_acc: 0.8030 test_loss: 1.0421, test_acc: 0.7165, best: 0.7281, time: 0:03:47
 Epoch: 126, lr: 1.0e-02, train_loss: 0.5512, train_acc: 0.8076 test_loss: 1.1461, test_acc: 0.7009, best: 0.7281, time: 0:03:48
 Epoch: 127, lr: 1.0e-02, train_loss: 0.5768, train_acc: 0.8060 test_loss: 1.0630, test_acc: 0.7191, best: 0.7281, time: 0:03:47
 Epoch: 128, lr: 1.0e-02, train_loss: 0.5432, train_acc: 0.8008 test_loss: 1.1386, test_acc: 0.7156, best: 0.7281, time: 0:03:48
 Epoch: 129, lr: 1.0e-02, train_loss: 0.5523, train_acc: 0.8086 test_loss: 1.1328, test_acc: 0.7234, best: 0.7281, time: 0:03:48
 Epoch: 130, lr: 1.0e-02, train_loss: 0.5464, train_acc: 0.8112 test_loss: 1.1696, test_acc: 0.7134, best: 0.7281, time: 0:03:48
 Epoch: 131, lr: 1.0e-02, train_loss: 0.5423, train_acc: 0.8094 test_loss: 1.1158, test_acc: 0.7214, best: 0.7281, time: 0:03:48
 Epoch: 132, lr: 1.0e-02, train_loss: 0.5326, train_acc: 0.8104 test_loss: 1.0575, test_acc: 0.7163, best: 0.7281, time: 0:03:48
 Epoch: 133, lr: 1.0e-02, train_loss: 0.5432, train_acc: 0.8074 test_loss: 1.1221, test_acc: 0.7309, best: 0.7309, time: 0:03:49
 Epoch: 134, lr: 1.0e-02, train_loss: 0.5235, train_acc: 0.8214 test_loss: 1.0502, test_acc: 0.7266, best: 0.7309, time: 0:03:48
 Epoch: 135, lr: 1.0e-02, train_loss: 0.5231, train_acc: 0.8156 test_loss: 1.1662, test_acc: 0.7365, best: 0.7365, time: 0:03:49
 Epoch: 136, lr: 1.0e-02, train_loss: 0.5040, train_acc: 0.8214 test_loss: 1.0462, test_acc: 0.7334, best: 0.7365, time: 0:03:48
 Epoch: 137, lr: 1.0e-02, train_loss: 0.5103, train_acc: 0.8234 test_loss: 1.1072, test_acc: 0.7339, best: 0.7365, time: 0:03:48
 Epoch: 138, lr: 1.0e-02, train_loss: 0.5291, train_acc: 0.8138 test_loss: 1.0211, test_acc: 0.7340, best: 0.7365, time: 0:03:48
 Epoch: 139, lr: 1.0e-02, train_loss: 0.5233, train_acc: 0.8162 test_loss: 1.1115, test_acc: 0.7312, best: 0.7365, time: 0:03:48
 Epoch: 140, lr: 1.0e-02, train_loss: 0.4975, train_acc: 0.8278 test_loss: 0.9879, test_acc: 0.7381, best: 0.7381, time: 0:03:49
 Epoch: 141, lr: 1.0e-02, train_loss: 0.5035, train_acc: 0.8240 test_loss: 1.0519, test_acc: 0.7368, best: 0.7381, time: 0:03:48
 Epoch: 142, lr: 1.0e-02, train_loss: 0.4942, train_acc: 0.8270 test_loss: 1.0234, test_acc: 0.7325, best: 0.7381, time: 0:03:47
 Epoch: 143, lr: 1.0e-02, train_loss: 0.4826, train_acc: 0.8316 test_loss: 1.0975, test_acc: 0.7309, best: 0.7381, time: 0:03:47
 Epoch: 144, lr: 1.0e-02, train_loss: 0.4807, train_acc: 0.8306 test_loss: 1.1369, test_acc: 0.7336, best: 0.7381, time: 0:03:48
 Epoch: 145, lr: 1.0e-02, train_loss: 0.4834, train_acc: 0.8252 test_loss: 1.1590, test_acc: 0.7224, best: 0.7381, time: 0:03:47
 Epoch: 146, lr: 1.0e-02, train_loss: 0.4747, train_acc: 0.8326 test_loss: 1.1695, test_acc: 0.7252, best: 0.7381, time: 0:03:48
 Epoch: 147, lr: 1.0e-02, train_loss: 0.4476, train_acc: 0.8440 test_loss: 1.1413, test_acc: 0.7298, best: 0.7381, time: 0:03:48
 Epoch: 148, lr: 1.0e-02, train_loss: 0.4872, train_acc: 0.8316 test_loss: 1.0860, test_acc: 0.7269, best: 0.7381, time: 0:03:48
 Epoch: 149, lr: 1.0e-02, train_loss: 0.4787, train_acc: 0.8314 test_loss: 1.1387, test_acc: 0.7298, best: 0.7381, time: 0:03:48
 Epoch: 150, lr: 1.0e-02, train_loss: 0.4749, train_acc: 0.8326 test_loss: 1.1716, test_acc: 0.7309, best: 0.7381, time: 0:03:48
 Epoch: 151, lr: 1.0e-02, train_loss: 0.4757, train_acc: 0.8302 test_loss: 1.1736, test_acc: 0.7334, best: 0.7381, time: 0:03:48
 Epoch: 152, lr: 1.0e-02, train_loss: 0.4747, train_acc: 0.8370 test_loss: 1.1663, test_acc: 0.7270, best: 0.7381, time: 0:03:48
 Epoch: 153, lr: 1.0e-02, train_loss: 0.4864, train_acc: 0.8308 test_loss: 1.1020, test_acc: 0.7362, best: 0.7381, time: 0:03:47
 Epoch: 154, lr: 1.0e-02, train_loss: 0.4571, train_acc: 0.8410 test_loss: 1.0464, test_acc: 0.7430, best: 0.7430, time: 0:03:49
 Epoch: 155, lr: 1.0e-02, train_loss: 0.4595, train_acc: 0.8428 test_loss: 1.0219, test_acc: 0.7446, best: 0.7446, time: 0:03:49
 Epoch: 156, lr: 1.0e-02, train_loss: 0.4547, train_acc: 0.8396 test_loss: 1.1917, test_acc: 0.7372, best: 0.7446, time: 0:03:48
 Epoch: 157, lr: 1.0e-02, train_loss: 0.4464, train_acc: 0.8424 test_loss: 1.1156, test_acc: 0.7458, best: 0.7458, time: 0:03:49
 Epoch: 158, lr: 1.0e-02, train_loss: 0.4662, train_acc: 0.8360 test_loss: 1.0834, test_acc: 0.7399, best: 0.7458, time: 0:03:48
 Epoch: 159, lr: 1.0e-02, train_loss: 0.4401, train_acc: 0.8490 test_loss: 1.1397, test_acc: 0.7352, best: 0.7458, time: 0:03:48
 Epoch: 160, lr: 1.0e-02, train_loss: 0.4465, train_acc: 0.8450 test_loss: 1.1853, test_acc: 0.7321, best: 0.7458, time: 0:03:48
 Epoch: 161, lr: 1.0e-02, train_loss: 0.4549, train_acc: 0.8338 test_loss: 1.0628, test_acc: 0.7336, best: 0.7458, time: 0:03:48
 Epoch: 162, lr: 1.0e-02, train_loss: 0.4372, train_acc: 0.8450 test_loss: 1.2035, test_acc: 0.7422, best: 0.7458, time: 0:03:48
 Epoch: 163, lr: 1.0e-02, train_loss: 0.4375, train_acc: 0.8444 test_loss: 1.1460, test_acc: 0.7310, best: 0.7458, time: 0:03:48
 Epoch: 164, lr: 1.0e-02, train_loss: 0.4381, train_acc: 0.8468 test_loss: 1.3346, test_acc: 0.7064, best: 0.7458, time: 0:03:48
 Epoch: 165, lr: 1.0e-02, train_loss: 0.4298, train_acc: 0.8530 test_loss: 1.1388, test_acc: 0.7389, best: 0.7458, time: 0:03:48
 Epoch: 166, lr: 1.0e-02, train_loss: 0.4517, train_acc: 0.8454 test_loss: 1.2179, test_acc: 0.7322, best: 0.7458, time: 0:03:48
 Epoch: 167, lr: 1.0e-02, train_loss: 0.4290, train_acc: 0.8500 test_loss: 1.1060, test_acc: 0.7472, best: 0.7472, time: 0:03:49
 Epoch: 168, lr: 1.0e-02, train_loss: 0.4366, train_acc: 0.8478 test_loss: 1.1660, test_acc: 0.7315, best: 0.7472, time: 0:03:48
 Epoch: 169, lr: 1.0e-02, train_loss: 0.4245, train_acc: 0.8528 test_loss: 1.2158, test_acc: 0.7341, best: 0.7472, time: 0:03:48
 Epoch: 170, lr: 1.0e-02, train_loss: 0.3964, train_acc: 0.8616 test_loss: 1.1159, test_acc: 0.7312, best: 0.7472, time: 0:03:48
 Epoch: 171, lr: 1.0e-02, train_loss: 0.4141, train_acc: 0.8580 test_loss: 1.1772, test_acc: 0.7375, best: 0.7472, time: 0:03:48
 Epoch: 172, lr: 1.0e-02, train_loss: 0.4134, train_acc: 0.8536 test_loss: 1.1604, test_acc: 0.7411, best: 0.7472, time: 0:03:48
 Epoch: 173, lr: 1.0e-02, train_loss: 0.4246, train_acc: 0.8510 test_loss: 1.2621, test_acc: 0.7406, best: 0.7472, time: 0:03:47
 Epoch: 174, lr: 1.0e-02, train_loss: 0.4298, train_acc: 0.8540 test_loss: 1.1402, test_acc: 0.7460, best: 0.7472, time: 0:03:48
 Epoch: 175, lr: 1.0e-02, train_loss: 0.4169, train_acc: 0.8536 test_loss: 1.1281, test_acc: 0.7464, best: 0.7472, time: 0:03:48
 Epoch: 176, lr: 1.0e-02, train_loss: 0.3928, train_acc: 0.8654 test_loss: 1.0631, test_acc: 0.7585, best: 0.7585, time: 0:03:50
 Epoch: 177, lr: 1.0e-02, train_loss: 0.4127, train_acc: 0.8572 test_loss: 1.1242, test_acc: 0.7560, best: 0.7585, time: 0:03:48
 Epoch: 178, lr: 1.0e-02, train_loss: 0.3974, train_acc: 0.8604 test_loss: 1.1036, test_acc: 0.7452, best: 0.7585, time: 0:03:48
 Epoch: 179, lr: 1.0e-02, train_loss: 0.4110, train_acc: 0.8610 test_loss: 1.0717, test_acc: 0.7526, best: 0.7585, time: 0:03:48
 Epoch: 180, lr: 2.0e-03, train_loss: 0.3613, train_acc: 0.8722 test_loss: 1.1326, test_acc: 0.7580, best: 0.7585, time: 0:03:48
 Epoch: 181, lr: 2.0e-03, train_loss: 0.3383, train_acc: 0.8886 test_loss: 1.1466, test_acc: 0.7562, best: 0.7585, time: 0:03:48
 Epoch: 182, lr: 2.0e-03, train_loss: 0.3315, train_acc: 0.8844 test_loss: 1.0399, test_acc: 0.7705, best: 0.7705, time: 0:03:50
 Epoch: 183, lr: 2.0e-03, train_loss: 0.3019, train_acc: 0.8954 test_loss: 1.0775, test_acc: 0.7679, best: 0.7705, time: 0:03:48
 Epoch: 184, lr: 2.0e-03, train_loss: 0.2998, train_acc: 0.8972 test_loss: 1.0974, test_acc: 0.7655, best: 0.7705, time: 0:03:48
 Epoch: 185, lr: 2.0e-03, train_loss: 0.3099, train_acc: 0.8874 test_loss: 1.1157, test_acc: 0.7702, best: 0.7705, time: 0:03:48
 Epoch: 186, lr: 2.0e-03, train_loss: 0.3043, train_acc: 0.8986 test_loss: 1.0978, test_acc: 0.7731, best: 0.7731, time: 0:03:50
 Epoch: 187, lr: 2.0e-03, train_loss: 0.3001, train_acc: 0.8950 test_loss: 1.0261, test_acc: 0.7725, best: 0.7731, time: 0:03:48
 Epoch: 188, lr: 2.0e-03, train_loss: 0.2963, train_acc: 0.8988 test_loss: 1.0573, test_acc: 0.7651, best: 0.7731, time: 0:03:48
 Epoch: 189, lr: 2.0e-03, train_loss: 0.3070, train_acc: 0.8984 test_loss: 1.0536, test_acc: 0.7740, best: 0.7740, time: 0:03:49
 Epoch: 190, lr: 2.0e-03, train_loss: 0.2829, train_acc: 0.9056 test_loss: 1.1201, test_acc: 0.7699, best: 0.7740, time: 0:03:48
 Epoch: 191, lr: 2.0e-03, train_loss: 0.2836, train_acc: 0.8988 test_loss: 1.0847, test_acc: 0.7645, best: 0.7740, time: 0:03:48
 Epoch: 192, lr: 2.0e-03, train_loss: 0.2863, train_acc: 0.9006 test_loss: 1.0943, test_acc: 0.7642, best: 0.7740, time: 0:03:48
 Epoch: 193, lr: 2.0e-03, train_loss: 0.2767, train_acc: 0.9078 test_loss: 1.1491, test_acc: 0.7688, best: 0.7740, time: 0:03:48
 Epoch: 194, lr: 2.0e-03, train_loss: 0.2850, train_acc: 0.9040 test_loss: 1.0410, test_acc: 0.7750, best: 0.7750, time: 0:03:49
 Epoch: 195, lr: 2.0e-03, train_loss: 0.2994, train_acc: 0.8942 test_loss: 1.0299, test_acc: 0.7765, best: 0.7765, time: 0:03:49
 Epoch: 196, lr: 2.0e-03, train_loss: 0.2760, train_acc: 0.9048 test_loss: 1.1393, test_acc: 0.7672, best: 0.7765, time: 0:03:48
 Epoch: 197, lr: 2.0e-03, train_loss: 0.2794, train_acc: 0.9018 test_loss: 1.1027, test_acc: 0.7716, best: 0.7765, time: 0:03:48
 Epoch: 198, lr: 2.0e-03, train_loss: 0.2727, train_acc: 0.9048 test_loss: 1.0985, test_acc: 0.7708, best: 0.7765, time: 0:03:48
 Epoch: 199, lr: 2.0e-03, train_loss: 0.2823, train_acc: 0.9008 test_loss: 1.0734, test_acc: 0.7704, best: 0.7765, time: 0:03:48
 Epoch: 200, lr: 2.0e-03, train_loss: 0.2818, train_acc: 0.9028 test_loss: 1.0805, test_acc: 0.7702, best: 0.7765, time: 0:03:48
 Epoch: 201, lr: 2.0e-03, train_loss: 0.2766, train_acc: 0.9008 test_loss: 1.1299, test_acc: 0.7619, best: 0.7765, time: 0:03:48
 Epoch: 202, lr: 2.0e-03, train_loss: 0.2817, train_acc: 0.9002 test_loss: 1.1468, test_acc: 0.7674, best: 0.7765, time: 0:03:48
 Epoch: 203, lr: 2.0e-03, train_loss: 0.2641, train_acc: 0.9112 test_loss: 1.1569, test_acc: 0.7665, best: 0.7765, time: 0:03:47
 Epoch: 204, lr: 2.0e-03, train_loss: 0.2688, train_acc: 0.9058 test_loss: 1.1169, test_acc: 0.7652, best: 0.7765, time: 0:03:48
 Epoch: 205, lr: 2.0e-03, train_loss: 0.2900, train_acc: 0.9008 test_loss: 1.0849, test_acc: 0.7735, best: 0.7765, time: 0:03:48
 Epoch: 206, lr: 2.0e-03, train_loss: 0.2771, train_acc: 0.9050 test_loss: 1.1712, test_acc: 0.7692, best: 0.7765, time: 0:03:48
 Epoch: 207, lr: 2.0e-03, train_loss: 0.2499, train_acc: 0.9178 test_loss: 1.1205, test_acc: 0.7582, best: 0.7765, time: 0:03:48
 Epoch: 208, lr: 2.0e-03, train_loss: 0.2591, train_acc: 0.9126 test_loss: 1.0641, test_acc: 0.7710, best: 0.7765, time: 0:03:47
 Epoch: 209, lr: 2.0e-03, train_loss: 0.2613, train_acc: 0.9058 test_loss: 1.1731, test_acc: 0.7634, best: 0.7765, time: 0:03:48
 Epoch: 210, lr: 2.0e-03, train_loss: 0.2703, train_acc: 0.9078 test_loss: 1.1428, test_acc: 0.7682, best: 0.7765, time: 0:03:48
 Epoch: 211, lr: 2.0e-03, train_loss: 0.2484, train_acc: 0.9146 test_loss: 1.1302, test_acc: 0.7658, best: 0.7765, time: 0:03:48
 Epoch: 212, lr: 2.0e-03, train_loss: 0.2841, train_acc: 0.9030 test_loss: 1.2120, test_acc: 0.7601, best: 0.7765, time: 0:03:47
 Epoch: 213, lr: 2.0e-03, train_loss: 0.2560, train_acc: 0.9096 test_loss: 1.0591, test_acc: 0.7774, best: 0.7774, time: 0:03:49
 Epoch: 214, lr: 2.0e-03, train_loss: 0.2365, train_acc: 0.9156 test_loss: 1.1893, test_acc: 0.7674, best: 0.7774, time: 0:03:48
 Epoch: 215, lr: 2.0e-03, train_loss: 0.2584, train_acc: 0.9092 test_loss: 1.1586, test_acc: 0.7740, best: 0.7774, time: 0:03:48
 Epoch: 216, lr: 2.0e-03, train_loss: 0.2646, train_acc: 0.9104 test_loss: 1.0424, test_acc: 0.7738, best: 0.7774, time: 0:03:47
 Epoch: 217, lr: 2.0e-03, train_loss: 0.2639, train_acc: 0.9090 test_loss: 1.1566, test_acc: 0.7642, best: 0.7774, time: 0:03:48
 Epoch: 218, lr: 2.0e-03, train_loss: 0.2556, train_acc: 0.9118 test_loss: 1.0916, test_acc: 0.7698, best: 0.7774, time: 0:03:47
 Epoch: 219, lr: 2.0e-03, train_loss: 0.2519, train_acc: 0.9150 test_loss: 1.1343, test_acc: 0.7705, best: 0.7774, time: 0:03:48
 Epoch: 220, lr: 2.0e-03, train_loss: 0.2585, train_acc: 0.9138 test_loss: 1.1272, test_acc: 0.7681, best: 0.7774, time: 0:03:48
 Epoch: 221, lr: 2.0e-03, train_loss: 0.2451, train_acc: 0.9128 test_loss: 1.1308, test_acc: 0.7705, best: 0.7774, time: 0:03:48
 Epoch: 222, lr: 2.0e-03, train_loss: 0.2436, train_acc: 0.9130 test_loss: 1.1048, test_acc: 0.7694, best: 0.7774, time: 0:03:48
 Epoch: 223, lr: 2.0e-03, train_loss: 0.2421, train_acc: 0.9192 test_loss: 1.0783, test_acc: 0.7781, best: 0.7781, time: 0:03:49
 Epoch: 224, lr: 2.0e-03, train_loss: 0.2611, train_acc: 0.9076 test_loss: 1.1673, test_acc: 0.7685, best: 0.7781, time: 0:03:48
 Epoch: 225, lr: 2.0e-03, train_loss: 0.2766, train_acc: 0.9050 test_loss: 1.2215, test_acc: 0.7589, best: 0.7781, time: 0:03:48
 Epoch: 226, lr: 2.0e-03, train_loss: 0.2522, train_acc: 0.9156 test_loss: 1.1246, test_acc: 0.7745, best: 0.7781, time: 0:03:48
 Epoch: 227, lr: 2.0e-03, train_loss: 0.2515, train_acc: 0.9130 test_loss: 1.1243, test_acc: 0.7675, best: 0.7781, time: 0:03:48
 Epoch: 228, lr: 2.0e-03, train_loss: 0.2475, train_acc: 0.9144 test_loss: 1.0752, test_acc: 0.7694, best: 0.7781, time: 0:03:48
 Epoch: 229, lr: 2.0e-03, train_loss: 0.2382, train_acc: 0.9194 test_loss: 1.1424, test_acc: 0.7646, best: 0.7781, time: 0:03:48
 Epoch: 230, lr: 2.0e-03, train_loss: 0.2540, train_acc: 0.9150 test_loss: 1.2403, test_acc: 0.7594, best: 0.7781, time: 0:03:48
 Epoch: 231, lr: 2.0e-03, train_loss: 0.2383, train_acc: 0.9148 test_loss: 1.1385, test_acc: 0.7674, best: 0.7781, time: 0:03:48
 Epoch: 232, lr: 2.0e-03, train_loss: 0.2508, train_acc: 0.9096 test_loss: 1.2117, test_acc: 0.7660, best: 0.7781, time: 0:03:48
 Epoch: 233, lr: 2.0e-03, train_loss: 0.2439, train_acc: 0.9222 test_loss: 1.1520, test_acc: 0.7656, best: 0.7781, time: 0:03:48
 Epoch: 234, lr: 2.0e-03, train_loss: 0.2466, train_acc: 0.9196 test_loss: 1.1813, test_acc: 0.7642, best: 0.7781, time: 0:03:48
 Epoch: 235, lr: 2.0e-03, train_loss: 0.2455, train_acc: 0.9142 test_loss: 1.1176, test_acc: 0.7692, best: 0.7781, time: 0:03:48
 Epoch: 236, lr: 2.0e-03, train_loss: 0.2413, train_acc: 0.9160 test_loss: 1.0924, test_acc: 0.7730, best: 0.7781, time: 0:03:48
 Epoch: 237, lr: 2.0e-03, train_loss: 0.2392, train_acc: 0.9184 test_loss: 1.1782, test_acc: 0.7714, best: 0.7781, time: 0:03:48
 Epoch: 238, lr: 2.0e-03, train_loss: 0.2526, train_acc: 0.9152 test_loss: 1.1618, test_acc: 0.7689, best: 0.7781, time: 0:03:48
 Epoch: 239, lr: 2.0e-03, train_loss: 0.2371, train_acc: 0.9196 test_loss: 1.1123, test_acc: 0.7776, best: 0.7781, time: 0:03:48
 Epoch: 240, lr: 4.0e-04, train_loss: 0.2442, train_acc: 0.9156 test_loss: 1.1291, test_acc: 0.7756, best: 0.7781, time: 0:03:48
 Epoch: 241, lr: 4.0e-04, train_loss: 0.2339, train_acc: 0.9220 test_loss: 1.1007, test_acc: 0.7732, best: 0.7781, time: 0:03:48
 Epoch: 242, lr: 4.0e-04, train_loss: 0.2328, train_acc: 0.9194 test_loss: 1.1058, test_acc: 0.7760, best: 0.7781, time: 0:03:48
 Epoch: 243, lr: 4.0e-04, train_loss: 0.2289, train_acc: 0.9228 test_loss: 1.0627, test_acc: 0.7789, best: 0.7789, time: 0:03:49
 Epoch: 244, lr: 4.0e-04, train_loss: 0.2332, train_acc: 0.9192 test_loss: 1.1695, test_acc: 0.7739, best: 0.7789, time: 0:03:48
 Epoch: 245, lr: 4.0e-04, train_loss: 0.2283, train_acc: 0.9190 test_loss: 1.1844, test_acc: 0.7709, best: 0.7789, time: 0:03:48
 Epoch: 246, lr: 4.0e-04, train_loss: 0.2304, train_acc: 0.9210 test_loss: 1.1102, test_acc: 0.7805, best: 0.7805, time: 0:03:49
 Epoch: 247, lr: 4.0e-04, train_loss: 0.2262, train_acc: 0.9190 test_loss: 1.1612, test_acc: 0.7739, best: 0.7805, time: 0:03:48
 Epoch: 248, lr: 4.0e-04, train_loss: 0.2349, train_acc: 0.9142 test_loss: 1.1167, test_acc: 0.7746, best: 0.7805, time: 0:03:48
 Epoch: 249, lr: 4.0e-04, train_loss: 0.2274, train_acc: 0.9226 test_loss: 1.1447, test_acc: 0.7704, best: 0.7805, time: 0:03:48
 Epoch: 250, lr: 4.0e-04, train_loss: 0.2405, train_acc: 0.9178 test_loss: 1.1519, test_acc: 0.7730, best: 0.7805, time: 0:03:48
 Epoch: 251, lr: 4.0e-04, train_loss: 0.2247, train_acc: 0.9214 test_loss: 1.1176, test_acc: 0.7768, best: 0.7805, time: 0:03:48
 Epoch: 252, lr: 4.0e-04, train_loss: 0.2258, train_acc: 0.9210 test_loss: 1.0912, test_acc: 0.7739, best: 0.7805, time: 0:03:48
 Epoch: 253, lr: 4.0e-04, train_loss: 0.2060, train_acc: 0.9284 test_loss: 1.1098, test_acc: 0.7780, best: 0.7805, time: 0:03:48
 Epoch: 254, lr: 4.0e-04, train_loss: 0.2341, train_acc: 0.9170 test_loss: 1.1613, test_acc: 0.7689, best: 0.7805, time: 0:03:48
 Epoch: 255, lr: 4.0e-04, train_loss: 0.2228, train_acc: 0.9218 test_loss: 1.1715, test_acc: 0.7734, best: 0.7805, time: 0:03:48
 Epoch: 256, lr: 4.0e-04, train_loss: 0.2431, train_acc: 0.9176 test_loss: 1.2518, test_acc: 0.7748, best: 0.7805, time: 0:03:48
 Epoch: 257, lr: 4.0e-04, train_loss: 0.2159, train_acc: 0.9248 test_loss: 1.1228, test_acc: 0.7788, best: 0.7805, time: 0:03:48
 Epoch: 258, lr: 4.0e-04, train_loss: 0.2201, train_acc: 0.9232 test_loss: 1.1604, test_acc: 0.7758, best: 0.7805, time: 0:03:48
 Epoch: 259, lr: 4.0e-04, train_loss: 0.2244, train_acc: 0.9230 test_loss: 1.1116, test_acc: 0.7766, best: 0.7805, time: 0:03:48
 Epoch: 260, lr: 4.0e-04, train_loss: 0.2074, train_acc: 0.9252 test_loss: 1.1463, test_acc: 0.7755, best: 0.7805, time: 0:03:49
 Epoch: 261, lr: 4.0e-04, train_loss: 0.2247, train_acc: 0.9176 test_loss: 1.1678, test_acc: 0.7716, best: 0.7805, time: 0:03:48
 Epoch: 262, lr: 4.0e-04, train_loss: 0.2106, train_acc: 0.9304 test_loss: 1.1297, test_acc: 0.7801, best: 0.7805, time: 0:03:48
 Epoch: 263, lr: 4.0e-04, train_loss: 0.2196, train_acc: 0.9260 test_loss: 1.2206, test_acc: 0.7722, best: 0.7805, time: 0:03:48
 Epoch: 264, lr: 4.0e-04, train_loss: 0.2236, train_acc: 0.9224 test_loss: 1.1549, test_acc: 0.7760, best: 0.7805, time: 0:03:48
 Epoch: 265, lr: 4.0e-04, train_loss: 0.2135, train_acc: 0.9278 test_loss: 1.1718, test_acc: 0.7735, best: 0.7805, time: 0:03:48
 Epoch: 266, lr: 4.0e-04, train_loss: 0.2310, train_acc: 0.9194 test_loss: 1.1046, test_acc: 0.7769, best: 0.7805, time: 0:03:48
 Epoch: 267, lr: 4.0e-04, train_loss: 0.2072, train_acc: 0.9274 test_loss: 1.1121, test_acc: 0.7736, best: 0.7805, time: 0:03:48
 Epoch: 268, lr: 4.0e-04, train_loss: 0.2272, train_acc: 0.9212 test_loss: 1.2121, test_acc: 0.7734, best: 0.7805, time: 0:03:48
 Epoch: 269, lr: 4.0e-04, train_loss: 0.2223, train_acc: 0.9262 test_loss: 1.1731, test_acc: 0.7745, best: 0.7805, time: 0:03:48
 Epoch: 270, lr: 8.0e-05, train_loss: 0.2176, train_acc: 0.9270 test_loss: 1.1073, test_acc: 0.7780, best: 0.7805, time: 0:03:48
 Epoch: 271, lr: 8.0e-05, train_loss: 0.2096, train_acc: 0.9232 test_loss: 1.1520, test_acc: 0.7719, best: 0.7805, time: 0:03:48
 Epoch: 272, lr: 8.0e-05, train_loss: 0.2116, train_acc: 0.9270 test_loss: 1.1448, test_acc: 0.7795, best: 0.7805, time: 0:03:48
 Epoch: 273, lr: 8.0e-05, train_loss: 0.2253, train_acc: 0.9222 test_loss: 1.1819, test_acc: 0.7765, best: 0.7805, time: 0:03:48
 Epoch: 274, lr: 8.0e-05, train_loss: 0.2363, train_acc: 0.9220 test_loss: 1.1258, test_acc: 0.7734, best: 0.7805, time: 0:03:48
 Epoch: 275, lr: 8.0e-05, train_loss: 0.1947, train_acc: 0.9354 test_loss: 1.0853, test_acc: 0.7835, best: 0.7835, time: 0:03:49
 Epoch: 276, lr: 8.0e-05, train_loss: 0.2093, train_acc: 0.9246 test_loss: 1.1762, test_acc: 0.7750, best: 0.7835, time: 0:03:48
 Epoch: 277, lr: 8.0e-05, train_loss: 0.2222, train_acc: 0.9212 test_loss: 1.1674, test_acc: 0.7752, best: 0.7835, time: 0:03:48
 Epoch: 278, lr: 8.0e-05, train_loss: 0.2152, train_acc: 0.9278 test_loss: 1.1855, test_acc: 0.7705, best: 0.7835, time: 0:03:48
 Epoch: 279, lr: 8.0e-05, train_loss: 0.2037, train_acc: 0.9284 test_loss: 1.1977, test_acc: 0.7684, best: 0.7835, time: 0:03:47
 Epoch: 280, lr: 8.0e-05, train_loss: 0.2169, train_acc: 0.9236 test_loss: 1.1374, test_acc: 0.7774, best: 0.7835, time: 0:03:48
 Epoch: 281, lr: 8.0e-05, train_loss: 0.2332, train_acc: 0.9178 test_loss: 1.1699, test_acc: 0.7699, best: 0.7835, time: 0:03:48
 Epoch: 282, lr: 8.0e-05, train_loss: 0.2035, train_acc: 0.9308 test_loss: 1.1868, test_acc: 0.7732, best: 0.7835, time: 0:03:48
 Epoch: 283, lr: 8.0e-05, train_loss: 0.2164, train_acc: 0.9252 test_loss: 1.1036, test_acc: 0.7772, best: 0.7835, time: 0:03:48
 Epoch: 284, lr: 8.0e-05, train_loss: 0.2093, train_acc: 0.9298 test_loss: 1.2399, test_acc: 0.7700, best: 0.7835, time: 0:03:48
 Epoch: 285, lr: 8.0e-05, train_loss: 0.2079, train_acc: 0.9260 test_loss: 1.1363, test_acc: 0.7765, best: 0.7835, time: 0:03:48
 Epoch: 286, lr: 8.0e-05, train_loss: 0.2073, train_acc: 0.9284 test_loss: 1.1582, test_acc: 0.7734, best: 0.7835, time: 0:03:48
 Epoch: 287, lr: 8.0e-05, train_loss: 0.2201, train_acc: 0.9248 test_loss: 1.1738, test_acc: 0.7749, best: 0.7835, time: 0:03:48
 Epoch: 288, lr: 8.0e-05, train_loss: 0.1986, train_acc: 0.9296 test_loss: 1.1829, test_acc: 0.7720, best: 0.7835, time: 0:03:48
 Epoch: 289, lr: 8.0e-05, train_loss: 0.2089, train_acc: 0.9250 test_loss: 1.1557, test_acc: 0.7686, best: 0.7835, time: 0:03:48
 Epoch: 290, lr: 8.0e-05, train_loss: 0.2074, train_acc: 0.9248 test_loss: 1.1023, test_acc: 0.7745, best: 0.7835, time: 0:03:48
 Epoch: 291, lr: 8.0e-05, train_loss: 0.2145, train_acc: 0.9290 test_loss: 1.1765, test_acc: 0.7746, best: 0.7835, time: 0:03:48
 Epoch: 292, lr: 8.0e-05, train_loss: 0.2052, train_acc: 0.9260 test_loss: 1.1428, test_acc: 0.7806, best: 0.7835, time: 0:03:48
 Epoch: 293, lr: 8.0e-05, train_loss: 0.2241, train_acc: 0.9216 test_loss: 1.1556, test_acc: 0.7769, best: 0.7835, time: 0:03:48
 Epoch: 294, lr: 8.0e-05, train_loss: 0.2285, train_acc: 0.9242 test_loss: 1.2252, test_acc: 0.7712, best: 0.7835, time: 0:03:48
 Epoch: 295, lr: 8.0e-05, train_loss: 0.2078, train_acc: 0.9278 test_loss: 1.0957, test_acc: 0.7791, best: 0.7835, time: 0:03:48
 Epoch: 296, lr: 8.0e-05, train_loss: 0.2016, train_acc: 0.9314 test_loss: 1.1693, test_acc: 0.7776, best: 0.7835, time: 0:03:48
 Epoch: 297, lr: 8.0e-05, train_loss: 0.2186, train_acc: 0.9246 test_loss: 1.1648, test_acc: 0.7738, best: 0.7835, time: 0:03:48
 Epoch: 298, lr: 8.0e-05, train_loss: 0.2038, train_acc: 0.9300 test_loss: 1.1716, test_acc: 0.7726, best: 0.7835, time: 0:03:48
 Epoch: 299, lr: 8.0e-05, train_loss: 0.2059, train_acc: 0.9286 test_loss: 1.1416, test_acc: 0.7790, best: 0.7835, time: 0:03:48
 Highest accuracy: 0.7835