
 Run on time: 2022-07-01 02:49:48.405360

 Architecture: mobilenet-lip-2121

 Pool Config: {
    "arch": "mobilenet",
    "conv1": {
        "_conv2d": "norm",
        "pool_cfg": {}
    },
    "layer1": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "lip",
            "_stride": 2,
            "_psize": null,
            "_dim_reduced_ratio": null,
            "_num_heads": null,
            "_conv2d": null,
            "_win_norm": null
        }
    },
    "layer2": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "skip",
            "_stride": 1,
            "_psize": null,
            "_dim_reduced_ratio": null,
            "_num_heads": null,
            "_conv2d": null,
            "_win_norm": null
        }
    },
    "layer3": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "lip",
            "_stride": 2,
            "_psize": null,
            "_dim_reduced_ratio": null,
            "_num_heads": null,
            "_conv2d": null,
            "_win_norm": null
        }
    },
    "layer4": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "skip",
            "_stride": 1,
            "_psize": null,
            "_dim_reduced_ratio": null,
            "_num_heads": null,
            "_conv2d": null,
            "_win_norm": null
        }
    },
    "conv2": {
        "_conv2d": "norm",
        "pool_cfg": {}
    }
}

 Arguments:
	 gpu                  : True
	 log                  : True
	 seed                 : 0
	 momentum             : 0.9
	 amsgrad              : True
	 dataset              : STL10
	 batch_size           : 8
	 architecture         : MOBILENET-LIP-2121
	 im_size              : None
	 relu_threshold       : 4.0
	 learning_rate        : 0.01
	 pretrained_backbone  : 
	 pretrained_ann       : 
	 weight_decay         : 0.0
	 test_only            : False
	 epochs               : 300
	 lr_interval          : [180, 240, 270]
	 lr_reduce            : 5
	 optimizer            : SGD
	 dropout              : 0.2
	 kernel_size          : 3
	 dont_save            : False
	 visualize            : False
	 devices              : 0
 DataParallel(
  (module): Network(
    (net): MobileNetV2(
      (features): Sequential(
        (0): Sequential(
          (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): Pool2d(
          (logit): Sequential(
            (pool_weight): LIP_BASE(
              (logit): Sequential(
                (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Sigmoid()
              )
            )
          )
          (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        )
        (2): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (3): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (4): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (5): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (6): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (7): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
            (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (8): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
            (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (9): Pool2d(
          (logit): Sequential(
            (pool_weight): LIP_BASE(
              (logit): Sequential(
                (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
                (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Sigmoid()
              )
            )
          )
          (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        )
        (10): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
            (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (11): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
            (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (12): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
            (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (13): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
            (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
      (conv): Sequential(
        (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
      (classifier): Linear(in_features=1280, out_features=10, bias=True)
    )
  )
)
 SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0
)
 Epoch: 1, lr: 1.0e-02, train_loss: 2.3454, train_acc: 0.2030 test_loss: 1.9349, test_acc: 0.2431, best: 0.2431, time: 0:00:37
 Epoch: 2, lr: 1.0e-02, train_loss: 1.9407, train_acc: 0.2528 test_loss: 1.6892, test_acc: 0.3180, best: 0.3180, time: 0:00:36
 Epoch: 3, lr: 1.0e-02, train_loss: 1.8544, train_acc: 0.2904 test_loss: 1.5836, test_acc: 0.3691, best: 0.3691, time: 0:00:36
 Epoch: 4, lr: 1.0e-02, train_loss: 1.7905, train_acc: 0.3258 test_loss: 1.5803, test_acc: 0.3842, best: 0.3842, time: 0:00:36
 Epoch: 5, lr: 1.0e-02, train_loss: 1.7139, train_acc: 0.3530 test_loss: 1.5794, test_acc: 0.3912, best: 0.3912, time: 0:00:37
 Epoch: 6, lr: 1.0e-02, train_loss: 1.6697, train_acc: 0.3704 test_loss: 1.5366, test_acc: 0.4134, best: 0.4134, time: 0:00:36
 Epoch: 7, lr: 1.0e-02, train_loss: 1.6086, train_acc: 0.4012 test_loss: 1.3874, test_acc: 0.4800, best: 0.4800, time: 0:00:36
 Epoch: 8, lr: 1.0e-02, train_loss: 1.5782, train_acc: 0.4096 test_loss: 1.3786, test_acc: 0.4639, best: 0.4800, time: 0:00:36
 Epoch: 9, lr: 1.0e-02, train_loss: 1.5181, train_acc: 0.4284 test_loss: 1.4350, test_acc: 0.4740, best: 0.4800, time: 0:00:36
 Epoch: 10, lr: 1.0e-02, train_loss: 1.4870, train_acc: 0.4406 test_loss: 1.2880, test_acc: 0.5118, best: 0.5118, time: 0:00:36
 Epoch: 11, lr: 1.0e-02, train_loss: 1.4647, train_acc: 0.4548 test_loss: 1.3874, test_acc: 0.4893, best: 0.5118, time: 0:00:36
 Epoch: 12, lr: 1.0e-02, train_loss: 1.4069, train_acc: 0.4822 test_loss: 1.2144, test_acc: 0.5603, best: 0.5603, time: 0:00:36
 Epoch: 13, lr: 1.0e-02, train_loss: 1.3812, train_acc: 0.4910 test_loss: 1.3183, test_acc: 0.5169, best: 0.5603, time: 0:00:36
 Epoch: 14, lr: 1.0e-02, train_loss: 1.3591, train_acc: 0.5066 test_loss: 1.2110, test_acc: 0.5567, best: 0.5603, time: 0:00:36
 Epoch: 15, lr: 1.0e-02, train_loss: 1.3413, train_acc: 0.5154 test_loss: 1.1866, test_acc: 0.5621, best: 0.5621, time: 0:00:36
 Epoch: 16, lr: 1.0e-02, train_loss: 1.3046, train_acc: 0.5134 test_loss: 1.2002, test_acc: 0.5561, best: 0.5621, time: 0:00:37
 Epoch: 17, lr: 1.0e-02, train_loss: 1.2922, train_acc: 0.5308 test_loss: 1.1751, test_acc: 0.5741, best: 0.5741, time: 0:00:37
 Epoch: 18, lr: 1.0e-02, train_loss: 1.2860, train_acc: 0.5372 test_loss: 1.0863, test_acc: 0.5966, best: 0.5966, time: 0:00:37
 Epoch: 19, lr: 1.0e-02, train_loss: 1.2369, train_acc: 0.5606 test_loss: 1.0936, test_acc: 0.5999, best: 0.5999, time: 0:00:37
 Epoch: 20, lr: 1.0e-02, train_loss: 1.2204, train_acc: 0.5592 test_loss: 1.0880, test_acc: 0.6080, best: 0.6080, time: 0:00:37
 Epoch: 21, lr: 1.0e-02, train_loss: 1.1980, train_acc: 0.5616 test_loss: 1.0551, test_acc: 0.6202, best: 0.6202, time: 0:00:37
 Epoch: 22, lr: 1.0e-02, train_loss: 1.1902, train_acc: 0.5690 test_loss: 1.0443, test_acc: 0.6108, best: 0.6202, time: 0:00:37
 Epoch: 23, lr: 1.0e-02, train_loss: 1.1632, train_acc: 0.5690 test_loss: 1.0333, test_acc: 0.6331, best: 0.6331, time: 0:00:37
 Epoch: 24, lr: 1.0e-02, train_loss: 1.1635, train_acc: 0.5776 test_loss: 1.0023, test_acc: 0.6416, best: 0.6416, time: 0:00:37
 Epoch: 25, lr: 1.0e-02, train_loss: 1.1428, train_acc: 0.5876 test_loss: 1.0116, test_acc: 0.6330, best: 0.6416, time: 0:00:37
 Epoch: 26, lr: 1.0e-02, train_loss: 1.1241, train_acc: 0.5956 test_loss: 0.9819, test_acc: 0.6500, best: 0.6500, time: 0:00:37
 Epoch: 27, lr: 1.0e-02, train_loss: 1.1040, train_acc: 0.6040 test_loss: 1.0410, test_acc: 0.6300, best: 0.6500, time: 0:00:37
 Epoch: 28, lr: 1.0e-02, train_loss: 1.1206, train_acc: 0.5942 test_loss: 0.9838, test_acc: 0.6431, best: 0.6500, time: 0:00:37
 Epoch: 29, lr: 1.0e-02, train_loss: 1.0572, train_acc: 0.6196 test_loss: 1.1064, test_acc: 0.6134, best: 0.6500, time: 0:00:37
 Epoch: 30, lr: 1.0e-02, train_loss: 1.0559, train_acc: 0.6206 test_loss: 1.0592, test_acc: 0.6331, best: 0.6500, time: 0:00:37
 Epoch: 31, lr: 1.0e-02, train_loss: 1.0424, train_acc: 0.6274 test_loss: 0.9706, test_acc: 0.6554, best: 0.6554, time: 0:00:37
 Epoch: 32, lr: 1.0e-02, train_loss: 1.0455, train_acc: 0.6330 test_loss: 0.9849, test_acc: 0.6462, best: 0.6554, time: 0:00:37
 Epoch: 33, lr: 1.0e-02, train_loss: 1.0312, train_acc: 0.6358 test_loss: 0.9218, test_acc: 0.6739, best: 0.6739, time: 0:00:37
 Epoch: 34, lr: 1.0e-02, train_loss: 1.0060, train_acc: 0.6348 test_loss: 1.0661, test_acc: 0.6292, best: 0.6739, time: 0:00:37
 Epoch: 35, lr: 1.0e-02, train_loss: 1.0149, train_acc: 0.6398 test_loss: 0.9697, test_acc: 0.6559, best: 0.6739, time: 0:00:37
 Epoch: 36, lr: 1.0e-02, train_loss: 0.9943, train_acc: 0.6424 test_loss: 0.9042, test_acc: 0.6813, best: 0.6813, time: 0:00:37
 Epoch: 37, lr: 1.0e-02, train_loss: 0.9700, train_acc: 0.6616 test_loss: 0.9334, test_acc: 0.6645, best: 0.6813, time: 0:00:37
 Epoch: 38, lr: 1.0e-02, train_loss: 0.9696, train_acc: 0.6532 test_loss: 0.9883, test_acc: 0.6542, best: 0.6813, time: 0:00:37
 Epoch: 39, lr: 1.0e-02, train_loss: 0.9503, train_acc: 0.6648 test_loss: 0.9996, test_acc: 0.6426, best: 0.6813, time: 0:00:37
 Epoch: 40, lr: 1.0e-02, train_loss: 0.9248, train_acc: 0.6736 test_loss: 0.9516, test_acc: 0.6625, best: 0.6813, time: 0:00:37
 Epoch: 41, lr: 1.0e-02, train_loss: 0.9243, train_acc: 0.6686 test_loss: 0.9381, test_acc: 0.6746, best: 0.6813, time: 0:00:37
 Epoch: 42, lr: 1.0e-02, train_loss: 0.9112, train_acc: 0.6740 test_loss: 0.9132, test_acc: 0.6803, best: 0.6813, time: 0:00:37
 Epoch: 43, lr: 1.0e-02, train_loss: 0.9225, train_acc: 0.6684 test_loss: 0.9466, test_acc: 0.6706, best: 0.6813, time: 0:00:37
 Epoch: 44, lr: 1.0e-02, train_loss: 0.8862, train_acc: 0.6894 test_loss: 0.8766, test_acc: 0.6916, best: 0.6916, time: 0:00:37
 Epoch: 45, lr: 1.0e-02, train_loss: 0.8868, train_acc: 0.6790 test_loss: 0.8425, test_acc: 0.6999, best: 0.6999, time: 0:00:37
 Epoch: 46, lr: 1.0e-02, train_loss: 0.8610, train_acc: 0.6918 test_loss: 0.9783, test_acc: 0.6587, best: 0.6999, time: 0:00:37
 Epoch: 47, lr: 1.0e-02, train_loss: 0.8494, train_acc: 0.7076 test_loss: 0.8842, test_acc: 0.6904, best: 0.6999, time: 0:00:37
 Epoch: 48, lr: 1.0e-02, train_loss: 0.8641, train_acc: 0.6934 test_loss: 0.8495, test_acc: 0.7084, best: 0.7084, time: 0:00:37
 Epoch: 49, lr: 1.0e-02, train_loss: 0.8489, train_acc: 0.6908 test_loss: 0.8241, test_acc: 0.7119, best: 0.7119, time: 0:00:37
 Epoch: 50, lr: 1.0e-02, train_loss: 0.8363, train_acc: 0.7002 test_loss: 0.9483, test_acc: 0.6746, best: 0.7119, time: 0:00:37
 Epoch: 51, lr: 1.0e-02, train_loss: 0.8303, train_acc: 0.7050 test_loss: 0.8909, test_acc: 0.6921, best: 0.7119, time: 0:00:36
 Epoch: 52, lr: 1.0e-02, train_loss: 0.8063, train_acc: 0.7154 test_loss: 0.8324, test_acc: 0.7175, best: 0.7175, time: 0:00:37
 Epoch: 53, lr: 1.0e-02, train_loss: 0.8154, train_acc: 0.7156 test_loss: 0.8812, test_acc: 0.6980, best: 0.7175, time: 0:00:37
 Epoch: 54, lr: 1.0e-02, train_loss: 0.8077, train_acc: 0.7130 test_loss: 0.8112, test_acc: 0.7175, best: 0.7175, time: 0:00:37
 Epoch: 55, lr: 1.0e-02, train_loss: 0.7986, train_acc: 0.7218 test_loss: 0.8657, test_acc: 0.7007, best: 0.7175, time: 0:00:37
 Epoch: 56, lr: 1.0e-02, train_loss: 0.7774, train_acc: 0.7242 test_loss: 0.8514, test_acc: 0.7095, best: 0.7175, time: 0:00:37
 Epoch: 57, lr: 1.0e-02, train_loss: 0.8110, train_acc: 0.7148 test_loss: 0.8744, test_acc: 0.7045, best: 0.7175, time: 0:00:37
 Epoch: 58, lr: 1.0e-02, train_loss: 0.7666, train_acc: 0.7324 test_loss: 0.8440, test_acc: 0.7081, best: 0.7175, time: 0:00:37
 Epoch: 59, lr: 1.0e-02, train_loss: 0.7569, train_acc: 0.7330 test_loss: 0.8644, test_acc: 0.6984, best: 0.7175, time: 0:00:36
 Epoch: 60, lr: 1.0e-02, train_loss: 0.7597, train_acc: 0.7334 test_loss: 0.8180, test_acc: 0.7220, best: 0.7220, time: 0:00:37
 Epoch: 61, lr: 1.0e-02, train_loss: 0.7434, train_acc: 0.7408 test_loss: 0.8799, test_acc: 0.7080, best: 0.7220, time: 0:00:37
 Epoch: 62, lr: 1.0e-02, train_loss: 0.7584, train_acc: 0.7376 test_loss: 0.8918, test_acc: 0.7011, best: 0.7220, time: 0:00:37
 Epoch: 63, lr: 1.0e-02, train_loss: 0.7259, train_acc: 0.7482 test_loss: 0.8086, test_acc: 0.7359, best: 0.7359, time: 0:00:37
 Epoch: 64, lr: 1.0e-02, train_loss: 0.7403, train_acc: 0.7344 test_loss: 0.8214, test_acc: 0.7255, best: 0.7359, time: 0:00:37
 Epoch: 65, lr: 1.0e-02, train_loss: 0.7251, train_acc: 0.7534 test_loss: 0.7972, test_acc: 0.7268, best: 0.7359, time: 0:00:37
 Epoch: 66, lr: 1.0e-02, train_loss: 0.7235, train_acc: 0.7450 test_loss: 0.8000, test_acc: 0.7239, best: 0.7359, time: 0:00:37
 Epoch: 67, lr: 1.0e-02, train_loss: 0.7011, train_acc: 0.7606 test_loss: 0.8220, test_acc: 0.7271, best: 0.7359, time: 0:00:37
 Epoch: 68, lr: 1.0e-02, train_loss: 0.7112, train_acc: 0.7498 test_loss: 0.7842, test_acc: 0.7301, best: 0.7359, time: 0:00:37
 Epoch: 69, lr: 1.0e-02, train_loss: 0.6784, train_acc: 0.7636 test_loss: 0.8092, test_acc: 0.7216, best: 0.7359, time: 0:00:37
 Epoch: 70, lr: 1.0e-02, train_loss: 0.6664, train_acc: 0.7666 test_loss: 0.8247, test_acc: 0.7276, best: 0.7359, time: 0:00:37
 Epoch: 71, lr: 1.0e-02, train_loss: 0.6781, train_acc: 0.7594 test_loss: 0.7791, test_acc: 0.7342, best: 0.7359, time: 0:00:37
 Epoch: 72, lr: 1.0e-02, train_loss: 0.6692, train_acc: 0.7688 test_loss: 0.7864, test_acc: 0.7365, best: 0.7365, time: 0:00:37
 Epoch: 73, lr: 1.0e-02, train_loss: 0.6723, train_acc: 0.7626 test_loss: 0.7975, test_acc: 0.7335, best: 0.7365, time: 0:00:37
 Epoch: 74, lr: 1.0e-02, train_loss: 0.6971, train_acc: 0.7498 test_loss: 0.8312, test_acc: 0.7165, best: 0.7365, time: 0:00:37
 Epoch: 75, lr: 1.0e-02, train_loss: 0.6800, train_acc: 0.7630 test_loss: 0.8102, test_acc: 0.7249, best: 0.7365, time: 0:00:37
 Epoch: 76, lr: 1.0e-02, train_loss: 0.6538, train_acc: 0.7700 test_loss: 0.8730, test_acc: 0.7096, best: 0.7365, time: 0:00:37
 Epoch: 77, lr: 1.0e-02, train_loss: 0.6479, train_acc: 0.7774 test_loss: 0.8048, test_acc: 0.7270, best: 0.7365, time: 0:00:37
 Epoch: 78, lr: 1.0e-02, train_loss: 0.6574, train_acc: 0.7726 test_loss: 0.7576, test_acc: 0.7488, best: 0.7488, time: 0:00:37
 Epoch: 79, lr: 1.0e-02, train_loss: 0.6394, train_acc: 0.7820 test_loss: 0.7978, test_acc: 0.7384, best: 0.7488, time: 0:00:37
 Epoch: 80, lr: 1.0e-02, train_loss: 0.6124, train_acc: 0.7854 test_loss: 0.7614, test_acc: 0.7349, best: 0.7488, time: 0:00:37
 Epoch: 81, lr: 1.0e-02, train_loss: 0.6300, train_acc: 0.7782 test_loss: 0.7941, test_acc: 0.7389, best: 0.7488, time: 0:00:37
 Epoch: 82, lr: 1.0e-02, train_loss: 0.6029, train_acc: 0.7938 test_loss: 0.8162, test_acc: 0.7328, best: 0.7488, time: 0:00:37
 Epoch: 83, lr: 1.0e-02, train_loss: 0.6148, train_acc: 0.7904 test_loss: 0.8004, test_acc: 0.7369, best: 0.7488, time: 0:00:37
 Epoch: 84, lr: 1.0e-02, train_loss: 0.5800, train_acc: 0.7956 test_loss: 0.9320, test_acc: 0.7089, best: 0.7488, time: 0:00:37
 Epoch: 85, lr: 1.0e-02, train_loss: 0.6257, train_acc: 0.7822 test_loss: 0.7848, test_acc: 0.7469, best: 0.7488, time: 0:00:37
 Epoch: 86, lr: 1.0e-02, train_loss: 0.6129, train_acc: 0.7872 test_loss: 0.7433, test_acc: 0.7540, best: 0.7540, time: 0:00:37
 Epoch: 87, lr: 1.0e-02, train_loss: 0.5819, train_acc: 0.7930 test_loss: 0.8210, test_acc: 0.7304, best: 0.7540, time: 0:00:37
 Epoch: 88, lr: 1.0e-02, train_loss: 0.5811, train_acc: 0.8052 test_loss: 0.7979, test_acc: 0.7442, best: 0.7540, time: 0:00:37
 Epoch: 89, lr: 1.0e-02, train_loss: 0.5715, train_acc: 0.8004 test_loss: 0.8129, test_acc: 0.7474, best: 0.7540, time: 0:00:37
 Epoch: 90, lr: 1.0e-02, train_loss: 0.5950, train_acc: 0.7972 test_loss: 0.7994, test_acc: 0.7438, best: 0.7540, time: 0:00:37
 Epoch: 91, lr: 1.0e-02, train_loss: 0.5653, train_acc: 0.8038 test_loss: 0.7588, test_acc: 0.7492, best: 0.7540, time: 0:00:37
 Epoch: 92, lr: 1.0e-02, train_loss: 0.5480, train_acc: 0.8046 test_loss: 0.7956, test_acc: 0.7396, best: 0.7540, time: 0:00:37
 Epoch: 93, lr: 1.0e-02, train_loss: 0.5837, train_acc: 0.7986 test_loss: 0.7882, test_acc: 0.7389, best: 0.7540, time: 0:00:37
 Epoch: 94, lr: 1.0e-02, train_loss: 0.5822, train_acc: 0.7982 test_loss: 0.7657, test_acc: 0.7516, best: 0.7540, time: 0:00:37
 Epoch: 95, lr: 1.0e-02, train_loss: 0.5756, train_acc: 0.7956 test_loss: 0.7426, test_acc: 0.7588, best: 0.7588, time: 0:00:37
 Epoch: 96, lr: 1.0e-02, train_loss: 0.5413, train_acc: 0.8188 test_loss: 0.7919, test_acc: 0.7474, best: 0.7588, time: 0:00:36
 Epoch: 97, lr: 1.0e-02, train_loss: 0.5408, train_acc: 0.8160 test_loss: 0.8093, test_acc: 0.7470, best: 0.7588, time: 0:00:37
 Epoch: 98, lr: 1.0e-02, train_loss: 0.5259, train_acc: 0.8180 test_loss: 0.8386, test_acc: 0.7434, best: 0.7588, time: 0:00:37
 Epoch: 99, lr: 1.0e-02, train_loss: 0.5438, train_acc: 0.8170 test_loss: 0.7715, test_acc: 0.7538, best: 0.7588, time: 0:00:37
 Epoch: 100, lr: 1.0e-02, train_loss: 0.5204, train_acc: 0.8156 test_loss: 0.7835, test_acc: 0.7542, best: 0.7588, time: 0:00:37
 Epoch: 101, lr: 1.0e-02, train_loss: 0.5247, train_acc: 0.8182 test_loss: 0.8082, test_acc: 0.7570, best: 0.7588, time: 0:00:37
 Epoch: 102, lr: 1.0e-02, train_loss: 0.5151, train_acc: 0.8250 test_loss: 0.8423, test_acc: 0.7385, best: 0.7588, time: 0:00:37
 Epoch: 103, lr: 1.0e-02, train_loss: 0.5205, train_acc: 0.8194 test_loss: 0.7995, test_acc: 0.7526, best: 0.7588, time: 0:00:37
 Epoch: 104, lr: 1.0e-02, train_loss: 0.4966, train_acc: 0.8284 test_loss: 0.8007, test_acc: 0.7498, best: 0.7588, time: 0:00:37
 Epoch: 105, lr: 1.0e-02, train_loss: 0.5073, train_acc: 0.8242 test_loss: 0.8222, test_acc: 0.7446, best: 0.7588, time: 0:00:37
 Epoch: 106, lr: 1.0e-02, train_loss: 0.5102, train_acc: 0.8268 test_loss: 0.7972, test_acc: 0.7566, best: 0.7588, time: 0:00:37
 Epoch: 107, lr: 1.0e-02, train_loss: 0.5149, train_acc: 0.8206 test_loss: 0.7462, test_acc: 0.7592, best: 0.7592, time: 0:00:37
 Epoch: 108, lr: 1.0e-02, train_loss: 0.4886, train_acc: 0.8306 test_loss: 0.7816, test_acc: 0.7568, best: 0.7592, time: 0:00:37
 Epoch: 109, lr: 1.0e-02, train_loss: 0.5092, train_acc: 0.8252 test_loss: 0.8414, test_acc: 0.7334, best: 0.7592, time: 0:00:37
 Epoch: 110, lr: 1.0e-02, train_loss: 0.4873, train_acc: 0.8362 test_loss: 0.8185, test_acc: 0.7465, best: 0.7592, time: 0:00:37
 Epoch: 111, lr: 1.0e-02, train_loss: 0.4812, train_acc: 0.8348 test_loss: 0.7379, test_acc: 0.7705, best: 0.7705, time: 0:00:37
 Epoch: 112, lr: 1.0e-02, train_loss: 0.4991, train_acc: 0.8288 test_loss: 0.8226, test_acc: 0.7468, best: 0.7705, time: 0:00:37
 Epoch: 113, lr: 1.0e-02, train_loss: 0.4790, train_acc: 0.8412 test_loss: 0.7376, test_acc: 0.7676, best: 0.7705, time: 0:00:37
 Epoch: 114, lr: 1.0e-02, train_loss: 0.4808, train_acc: 0.8346 test_loss: 0.7682, test_acc: 0.7632, best: 0.7705, time: 0:00:37
 Epoch: 115, lr: 1.0e-02, train_loss: 0.4698, train_acc: 0.8340 test_loss: 0.8364, test_acc: 0.7596, best: 0.7705, time: 0:00:37
 Epoch: 116, lr: 1.0e-02, train_loss: 0.4743, train_acc: 0.8324 test_loss: 0.8433, test_acc: 0.7504, best: 0.7705, time: 0:00:37
 Epoch: 117, lr: 1.0e-02, train_loss: 0.4672, train_acc: 0.8358 test_loss: 0.8347, test_acc: 0.7409, best: 0.7705, time: 0:00:37
 Epoch: 118, lr: 1.0e-02, train_loss: 0.4670, train_acc: 0.8414 test_loss: 0.7581, test_acc: 0.7600, best: 0.7705, time: 0:00:37
 Epoch: 119, lr: 1.0e-02, train_loss: 0.4689, train_acc: 0.8378 test_loss: 0.7621, test_acc: 0.7649, best: 0.7705, time: 0:00:37
 Epoch: 120, lr: 1.0e-02, train_loss: 0.4919, train_acc: 0.8272 test_loss: 0.8303, test_acc: 0.7620, best: 0.7705, time: 0:00:37
 Epoch: 121, lr: 1.0e-02, train_loss: 0.4623, train_acc: 0.8416 test_loss: 0.8029, test_acc: 0.7658, best: 0.7705, time: 0:00:37
 Epoch: 122, lr: 1.0e-02, train_loss: 0.4722, train_acc: 0.8334 test_loss: 0.7610, test_acc: 0.7610, best: 0.7705, time: 0:00:37
 Epoch: 123, lr: 1.0e-02, train_loss: 0.4531, train_acc: 0.8442 test_loss: 0.7498, test_acc: 0.7651, best: 0.7705, time: 0:00:37
 Epoch: 124, lr: 1.0e-02, train_loss: 0.4396, train_acc: 0.8514 test_loss: 0.8462, test_acc: 0.7625, best: 0.7705, time: 0:00:37
 Epoch: 125, lr: 1.0e-02, train_loss: 0.4517, train_acc: 0.8474 test_loss: 0.7672, test_acc: 0.7600, best: 0.7705, time: 0:00:37
 Epoch: 126, lr: 1.0e-02, train_loss: 0.4215, train_acc: 0.8572 test_loss: 0.7577, test_acc: 0.7639, best: 0.7705, time: 0:00:37
 Epoch: 127, lr: 1.0e-02, train_loss: 0.4214, train_acc: 0.8528 test_loss: 0.8102, test_acc: 0.7505, best: 0.7705, time: 0:00:37
 Epoch: 128, lr: 1.0e-02, train_loss: 0.4436, train_acc: 0.8456 test_loss: 0.8423, test_acc: 0.7475, best: 0.7705, time: 0:00:37
 Epoch: 129, lr: 1.0e-02, train_loss: 0.4374, train_acc: 0.8522 test_loss: 0.7518, test_acc: 0.7712, best: 0.7712, time: 0:00:37
 Epoch: 130, lr: 1.0e-02, train_loss: 0.4224, train_acc: 0.8552 test_loss: 0.7860, test_acc: 0.7750, best: 0.7750, time: 0:00:37
 Epoch: 131, lr: 1.0e-02, train_loss: 0.4394, train_acc: 0.8494 test_loss: 0.7804, test_acc: 0.7650, best: 0.7750, time: 0:00:37
 Epoch: 132, lr: 1.0e-02, train_loss: 0.4226, train_acc: 0.8588 test_loss: 0.8490, test_acc: 0.7632, best: 0.7750, time: 0:00:37
 Epoch: 133, lr: 1.0e-02, train_loss: 0.4315, train_acc: 0.8504 test_loss: 0.8527, test_acc: 0.7580, best: 0.7750, time: 0:00:37
 Epoch: 134, lr: 1.0e-02, train_loss: 0.4318, train_acc: 0.8544 test_loss: 0.7610, test_acc: 0.7681, best: 0.7750, time: 0:00:37
 Epoch: 135, lr: 1.0e-02, train_loss: 0.3974, train_acc: 0.8618 test_loss: 0.8358, test_acc: 0.7541, best: 0.7750, time: 0:00:37
 Epoch: 136, lr: 1.0e-02, train_loss: 0.3959, train_acc: 0.8624 test_loss: 0.8390, test_acc: 0.7691, best: 0.7750, time: 0:00:37
 Epoch: 137, lr: 1.0e-02, train_loss: 0.4235, train_acc: 0.8562 test_loss: 0.8372, test_acc: 0.7535, best: 0.7750, time: 0:00:37
 Epoch: 138, lr: 1.0e-02, train_loss: 0.4256, train_acc: 0.8580 test_loss: 0.8568, test_acc: 0.7514, best: 0.7750, time: 0:00:37
 Epoch: 139, lr: 1.0e-02, train_loss: 0.4138, train_acc: 0.8590 test_loss: 0.8024, test_acc: 0.7678, best: 0.7750, time: 0:00:37
 Epoch: 140, lr: 1.0e-02, train_loss: 0.3919, train_acc: 0.8650 test_loss: 0.8784, test_acc: 0.7631, best: 0.7750, time: 0:00:37
 Epoch: 141, lr: 1.0e-02, train_loss: 0.4028, train_acc: 0.8664 test_loss: 0.7829, test_acc: 0.7634, best: 0.7750, time: 0:00:37
 Epoch: 142, lr: 1.0e-02, train_loss: 0.4177, train_acc: 0.8584 test_loss: 0.8022, test_acc: 0.7624, best: 0.7750, time: 0:00:37
 Epoch: 143, lr: 1.0e-02, train_loss: 0.4153, train_acc: 0.8574 test_loss: 0.8077, test_acc: 0.7611, best: 0.7750, time: 0:00:37
 Epoch: 144, lr: 1.0e-02, train_loss: 0.4016, train_acc: 0.8638 test_loss: 0.8148, test_acc: 0.7639, best: 0.7750, time: 0:00:37
 Epoch: 145, lr: 1.0e-02, train_loss: 0.3953, train_acc: 0.8664 test_loss: 0.8419, test_acc: 0.7551, best: 0.7750, time: 0:00:37
 Epoch: 146, lr: 1.0e-02, train_loss: 0.3971, train_acc: 0.8642 test_loss: 0.8024, test_acc: 0.7700, best: 0.7750, time: 0:00:37
 Epoch: 147, lr: 1.0e-02, train_loss: 0.4100, train_acc: 0.8622 test_loss: 0.8217, test_acc: 0.7638, best: 0.7750, time: 0:00:37
 Epoch: 148, lr: 1.0e-02, train_loss: 0.3833, train_acc: 0.8696 test_loss: 0.7730, test_acc: 0.7718, best: 0.7750, time: 0:00:37
 Epoch: 149, lr: 1.0e-02, train_loss: 0.4048, train_acc: 0.8608 test_loss: 0.8449, test_acc: 0.7606, best: 0.7750, time: 0:00:37
 Epoch: 150, lr: 1.0e-02, train_loss: 0.3692, train_acc: 0.8726 test_loss: 0.8638, test_acc: 0.7590, best: 0.7750, time: 0:00:37
 Epoch: 151, lr: 1.0e-02, train_loss: 0.3870, train_acc: 0.8630 test_loss: 0.8052, test_acc: 0.7692, best: 0.7750, time: 0:00:37
 Epoch: 152, lr: 1.0e-02, train_loss: 0.3645, train_acc: 0.8746 test_loss: 0.8577, test_acc: 0.7639, best: 0.7750, time: 0:00:37
 Epoch: 153, lr: 1.0e-02, train_loss: 0.4116, train_acc: 0.8624 test_loss: 0.7929, test_acc: 0.7662, best: 0.7750, time: 0:00:37
 Epoch: 154, lr: 1.0e-02, train_loss: 0.3629, train_acc: 0.8728 test_loss: 0.8502, test_acc: 0.7592, best: 0.7750, time: 0:00:37
 Epoch: 155, lr: 1.0e-02, train_loss: 0.3627, train_acc: 0.8752 test_loss: 0.8755, test_acc: 0.7610, best: 0.7750, time: 0:00:37
 Epoch: 156, lr: 1.0e-02, train_loss: 0.3835, train_acc: 0.8724 test_loss: 0.8291, test_acc: 0.7621, best: 0.7750, time: 0:00:37
 Epoch: 157, lr: 1.0e-02, train_loss: 0.3740, train_acc: 0.8694 test_loss: 0.8800, test_acc: 0.7571, best: 0.7750, time: 0:00:36
 Epoch: 158, lr: 1.0e-02, train_loss: 0.3976, train_acc: 0.8668 test_loss: 0.8157, test_acc: 0.7556, best: 0.7750, time: 0:00:37
 Epoch: 159, lr: 1.0e-02, train_loss: 0.3593, train_acc: 0.8734 test_loss: 0.8113, test_acc: 0.7644, best: 0.7750, time: 0:00:37
 Epoch: 160, lr: 1.0e-02, train_loss: 0.3733, train_acc: 0.8666 test_loss: 0.7805, test_acc: 0.7679, best: 0.7750, time: 0:00:37
 Epoch: 161, lr: 1.0e-02, train_loss: 0.3613, train_acc: 0.8832 test_loss: 0.7906, test_acc: 0.7648, best: 0.7750, time: 0:00:37
 Epoch: 162, lr: 1.0e-02, train_loss: 0.3728, train_acc: 0.8718 test_loss: 0.8862, test_acc: 0.7504, best: 0.7750, time: 0:00:37
 Epoch: 163, lr: 1.0e-02, train_loss: 0.3656, train_acc: 0.8748 test_loss: 0.8180, test_acc: 0.7611, best: 0.7750, time: 0:00:37
 Epoch: 164, lr: 1.0e-02, train_loss: 0.3618, train_acc: 0.8760 test_loss: 0.8216, test_acc: 0.7731, best: 0.7750, time: 0:00:37
 Epoch: 165, lr: 1.0e-02, train_loss: 0.3520, train_acc: 0.8790 test_loss: 0.8447, test_acc: 0.7610, best: 0.7750, time: 0:00:37
 Epoch: 166, lr: 1.0e-02, train_loss: 0.3401, train_acc: 0.8836 test_loss: 0.8000, test_acc: 0.7748, best: 0.7750, time: 0:00:37
 Epoch: 167, lr: 1.0e-02, train_loss: 0.3544, train_acc: 0.8792 test_loss: 0.8395, test_acc: 0.7724, best: 0.7750, time: 0:00:37
 Epoch: 168, lr: 1.0e-02, train_loss: 0.3573, train_acc: 0.8776 test_loss: 0.8892, test_acc: 0.7564, best: 0.7750, time: 0:00:37
 Epoch: 169, lr: 1.0e-02, train_loss: 0.3576, train_acc: 0.8776 test_loss: 0.8491, test_acc: 0.7670, best: 0.7750, time: 0:00:37
 Epoch: 170, lr: 1.0e-02, train_loss: 0.3492, train_acc: 0.8830 test_loss: 0.8010, test_acc: 0.7729, best: 0.7750, time: 0:00:37
 Epoch: 171, lr: 1.0e-02, train_loss: 0.3365, train_acc: 0.8874 test_loss: 0.7880, test_acc: 0.7739, best: 0.7750, time: 0:00:37
 Epoch: 172, lr: 1.0e-02, train_loss: 0.3520, train_acc: 0.8844 test_loss: 0.8410, test_acc: 0.7619, best: 0.7750, time: 0:00:37
 Epoch: 173, lr: 1.0e-02, train_loss: 0.3438, train_acc: 0.8834 test_loss: 0.8757, test_acc: 0.7599, best: 0.7750, time: 0:00:37
 Epoch: 174, lr: 1.0e-02, train_loss: 0.3388, train_acc: 0.8830 test_loss: 0.8206, test_acc: 0.7756, best: 0.7756, time: 0:00:37
 Epoch: 175, lr: 1.0e-02, train_loss: 0.3619, train_acc: 0.8764 test_loss: 0.8693, test_acc: 0.7619, best: 0.7756, time: 0:00:37
 Epoch: 176, lr: 1.0e-02, train_loss: 0.3235, train_acc: 0.8914 test_loss: 0.8335, test_acc: 0.7745, best: 0.7756, time: 0:00:37
 Epoch: 177, lr: 1.0e-02, train_loss: 0.3323, train_acc: 0.8826 test_loss: 0.9485, test_acc: 0.7551, best: 0.7756, time: 0:00:37
 Epoch: 178, lr: 1.0e-02, train_loss: 0.3393, train_acc: 0.8844 test_loss: 0.8496, test_acc: 0.7594, best: 0.7756, time: 0:00:37
 Epoch: 179, lr: 1.0e-02, train_loss: 0.3271, train_acc: 0.8876 test_loss: 0.9503, test_acc: 0.7511, best: 0.7756, time: 0:00:37
 Epoch: 180, lr: 2.0e-03, train_loss: 0.2731, train_acc: 0.9062 test_loss: 0.8176, test_acc: 0.7802, best: 0.7802, time: 0:00:37
 Epoch: 181, lr: 2.0e-03, train_loss: 0.2618, train_acc: 0.9112 test_loss: 0.7563, test_acc: 0.7893, best: 0.7893, time: 0:00:37
 Epoch: 182, lr: 2.0e-03, train_loss: 0.2500, train_acc: 0.9168 test_loss: 0.7646, test_acc: 0.7896, best: 0.7896, time: 0:00:37
 Epoch: 183, lr: 2.0e-03, train_loss: 0.2299, train_acc: 0.9232 test_loss: 0.7916, test_acc: 0.7923, best: 0.7923, time: 0:00:37
 Epoch: 184, lr: 2.0e-03, train_loss: 0.2240, train_acc: 0.9232 test_loss: 0.7852, test_acc: 0.7895, best: 0.7923, time: 0:00:37
 Epoch: 185, lr: 2.0e-03, train_loss: 0.2272, train_acc: 0.9240 test_loss: 0.8097, test_acc: 0.7884, best: 0.7923, time: 0:00:37
 Epoch: 186, lr: 2.0e-03, train_loss: 0.2208, train_acc: 0.9228 test_loss: 0.8129, test_acc: 0.7915, best: 0.7923, time: 0:00:37
 Epoch: 187, lr: 2.0e-03, train_loss: 0.2266, train_acc: 0.9280 test_loss: 0.7834, test_acc: 0.7957, best: 0.7957, time: 0:00:37
 Epoch: 188, lr: 2.0e-03, train_loss: 0.2039, train_acc: 0.9278 test_loss: 0.8088, test_acc: 0.7877, best: 0.7957, time: 0:00:37
 Epoch: 189, lr: 2.0e-03, train_loss: 0.2188, train_acc: 0.9260 test_loss: 0.7710, test_acc: 0.7936, best: 0.7957, time: 0:00:37
 Epoch: 190, lr: 2.0e-03, train_loss: 0.2174, train_acc: 0.9252 test_loss: 0.7906, test_acc: 0.7935, best: 0.7957, time: 0:00:37
 Epoch: 191, lr: 2.0e-03, train_loss: 0.2189, train_acc: 0.9246 test_loss: 0.7839, test_acc: 0.7931, best: 0.7957, time: 0:00:37
 Epoch: 192, lr: 2.0e-03, train_loss: 0.2087, train_acc: 0.9298 test_loss: 0.8350, test_acc: 0.7865, best: 0.7957, time: 0:00:37
 Epoch: 193, lr: 2.0e-03, train_loss: 0.2034, train_acc: 0.9298 test_loss: 0.7850, test_acc: 0.7954, best: 0.7957, time: 0:00:37
 Epoch: 194, lr: 2.0e-03, train_loss: 0.2052, train_acc: 0.9304 test_loss: 0.8057, test_acc: 0.7949, best: 0.7957, time: 0:00:37
 Epoch: 195, lr: 2.0e-03, train_loss: 0.2092, train_acc: 0.9316 test_loss: 0.8059, test_acc: 0.7891, best: 0.7957, time: 0:00:37
 Epoch: 196, lr: 2.0e-03, train_loss: 0.2281, train_acc: 0.9208 test_loss: 0.8111, test_acc: 0.7871, best: 0.7957, time: 0:00:37
 Epoch: 197, lr: 2.0e-03, train_loss: 0.2036, train_acc: 0.9308 test_loss: 0.8048, test_acc: 0.7939, best: 0.7957, time: 0:00:37
 Epoch: 198, lr: 2.0e-03, train_loss: 0.2029, train_acc: 0.9352 test_loss: 0.8196, test_acc: 0.7905, best: 0.7957, time: 0:00:37
 Epoch: 199, lr: 2.0e-03, train_loss: 0.2117, train_acc: 0.9298 test_loss: 0.8071, test_acc: 0.7917, best: 0.7957, time: 0:00:37
 Epoch: 200, lr: 2.0e-03, train_loss: 0.2271, train_acc: 0.9258 test_loss: 0.7992, test_acc: 0.7875, best: 0.7957, time: 0:00:37
 Epoch: 201, lr: 2.0e-03, train_loss: 0.2020, train_acc: 0.9330 test_loss: 0.8200, test_acc: 0.7867, best: 0.7957, time: 0:00:37
 Epoch: 202, lr: 2.0e-03, train_loss: 0.2033, train_acc: 0.9280 test_loss: 0.8276, test_acc: 0.7945, best: 0.7957, time: 0:00:37
 Epoch: 203, lr: 2.0e-03, train_loss: 0.2056, train_acc: 0.9294 test_loss: 0.7865, test_acc: 0.7931, best: 0.7957, time: 0:00:37
 Epoch: 204, lr: 2.0e-03, train_loss: 0.1951, train_acc: 0.9358 test_loss: 0.8186, test_acc: 0.7926, best: 0.7957, time: 0:00:37
 Epoch: 205, lr: 2.0e-03, train_loss: 0.2038, train_acc: 0.9278 test_loss: 0.8111, test_acc: 0.7899, best: 0.7957, time: 0:00:37
 Epoch: 206, lr: 2.0e-03, train_loss: 0.2038, train_acc: 0.9302 test_loss: 0.8286, test_acc: 0.7903, best: 0.7957, time: 0:00:37
 Epoch: 207, lr: 2.0e-03, train_loss: 0.1946, train_acc: 0.9322 test_loss: 0.7808, test_acc: 0.7937, best: 0.7957, time: 0:00:37
 Epoch: 208, lr: 2.0e-03, train_loss: 0.1988, train_acc: 0.9334 test_loss: 0.8110, test_acc: 0.7903, best: 0.7957, time: 0:00:37
 Epoch: 209, lr: 2.0e-03, train_loss: 0.1950, train_acc: 0.9336 test_loss: 0.8532, test_acc: 0.7853, best: 0.7957, time: 0:00:37
 Epoch: 210, lr: 2.0e-03, train_loss: 0.1998, train_acc: 0.9308 test_loss: 0.8029, test_acc: 0.7937, best: 0.7957, time: 0:00:37
 Epoch: 211, lr: 2.0e-03, train_loss: 0.2039, train_acc: 0.9316 test_loss: 0.7992, test_acc: 0.7943, best: 0.7957, time: 0:00:37
 Epoch: 212, lr: 2.0e-03, train_loss: 0.2030, train_acc: 0.9318 test_loss: 0.7871, test_acc: 0.7934, best: 0.7957, time: 0:00:37
 Epoch: 213, lr: 2.0e-03, train_loss: 0.1917, train_acc: 0.9376 test_loss: 0.8022, test_acc: 0.7929, best: 0.7957, time: 0:00:37
 Epoch: 214, lr: 2.0e-03, train_loss: 0.1969, train_acc: 0.9310 test_loss: 0.7938, test_acc: 0.7943, best: 0.7957, time: 0:00:37
 Epoch: 215, lr: 2.0e-03, train_loss: 0.2037, train_acc: 0.9324 test_loss: 0.8019, test_acc: 0.7983, best: 0.7983, time: 0:00:37
 Epoch: 216, lr: 2.0e-03, train_loss: 0.1927, train_acc: 0.9368 test_loss: 0.7998, test_acc: 0.7963, best: 0.7983, time: 0:00:37
 Epoch: 217, lr: 2.0e-03, train_loss: 0.2101, train_acc: 0.9304 test_loss: 0.7836, test_acc: 0.7915, best: 0.7983, time: 0:00:37
 Epoch: 218, lr: 2.0e-03, train_loss: 0.1930, train_acc: 0.9346 test_loss: 0.8021, test_acc: 0.7943, best: 0.7983, time: 0:00:37
 Epoch: 219, lr: 2.0e-03, train_loss: 0.1947, train_acc: 0.9332 test_loss: 0.8157, test_acc: 0.7953, best: 0.7983, time: 0:00:37
 Epoch: 220, lr: 2.0e-03, train_loss: 0.1799, train_acc: 0.9406 test_loss: 0.8057, test_acc: 0.7980, best: 0.7983, time: 0:00:37
 Epoch: 221, lr: 2.0e-03, train_loss: 0.1891, train_acc: 0.9360 test_loss: 0.8157, test_acc: 0.7845, best: 0.7983, time: 0:00:37
 Epoch: 222, lr: 2.0e-03, train_loss: 0.1928, train_acc: 0.9358 test_loss: 0.8005, test_acc: 0.7936, best: 0.7983, time: 0:00:37
 Epoch: 223, lr: 2.0e-03, train_loss: 0.1761, train_acc: 0.9414 test_loss: 0.8242, test_acc: 0.7915, best: 0.7983, time: 0:00:37
 Epoch: 224, lr: 2.0e-03, train_loss: 0.1826, train_acc: 0.9350 test_loss: 0.7707, test_acc: 0.7989, best: 0.7989, time: 0:00:37
 Epoch: 225, lr: 2.0e-03, train_loss: 0.2114, train_acc: 0.9324 test_loss: 0.7599, test_acc: 0.7916, best: 0.7989, time: 0:00:37
 Epoch: 226, lr: 2.0e-03, train_loss: 0.1828, train_acc: 0.9394 test_loss: 0.8236, test_acc: 0.7846, best: 0.7989, time: 0:00:37
 Epoch: 227, lr: 2.0e-03, train_loss: 0.1888, train_acc: 0.9374 test_loss: 0.8043, test_acc: 0.7935, best: 0.7989, time: 0:00:37
 Epoch: 228, lr: 2.0e-03, train_loss: 0.1857, train_acc: 0.9416 test_loss: 0.8083, test_acc: 0.7895, best: 0.7989, time: 0:00:36
 Epoch: 229, lr: 2.0e-03, train_loss: 0.1917, train_acc: 0.9362 test_loss: 0.8164, test_acc: 0.7887, best: 0.7989, time: 0:00:36
 Epoch: 230, lr: 2.0e-03, train_loss: 0.1990, train_acc: 0.9294 test_loss: 0.7963, test_acc: 0.7949, best: 0.7989, time: 0:00:37
 Epoch: 231, lr: 2.0e-03, train_loss: 0.1999, train_acc: 0.9340 test_loss: 0.8285, test_acc: 0.7891, best: 0.7989, time: 0:00:37
 Epoch: 232, lr: 2.0e-03, train_loss: 0.2038, train_acc: 0.9332 test_loss: 0.8124, test_acc: 0.7901, best: 0.7989, time: 0:00:37
 Epoch: 233, lr: 2.0e-03, train_loss: 0.1800, train_acc: 0.9400 test_loss: 0.8252, test_acc: 0.7909, best: 0.7989, time: 0:00:37
 Epoch: 234, lr: 2.0e-03, train_loss: 0.1912, train_acc: 0.9394 test_loss: 0.8143, test_acc: 0.7893, best: 0.7989, time: 0:00:37
 Epoch: 235, lr: 2.0e-03, train_loss: 0.1904, train_acc: 0.9362 test_loss: 0.8261, test_acc: 0.7945, best: 0.7989, time: 0:00:37
 Epoch: 236, lr: 2.0e-03, train_loss: 0.2032, train_acc: 0.9306 test_loss: 0.7973, test_acc: 0.7879, best: 0.7989, time: 0:00:37
 Epoch: 237, lr: 2.0e-03, train_loss: 0.1709, train_acc: 0.9408 test_loss: 0.8297, test_acc: 0.7913, best: 0.7989, time: 0:00:37
 Epoch: 238, lr: 2.0e-03, train_loss: 0.1894, train_acc: 0.9358 test_loss: 0.8145, test_acc: 0.7905, best: 0.7989, time: 0:00:37
 Epoch: 239, lr: 2.0e-03, train_loss: 0.1875, train_acc: 0.9340 test_loss: 0.7983, test_acc: 0.7919, best: 0.7989, time: 0:00:37
 Epoch: 240, lr: 4.0e-04, train_loss: 0.1923, train_acc: 0.9374 test_loss: 0.7931, test_acc: 0.7947, best: 0.7989, time: 0:00:37
 Epoch: 241, lr: 4.0e-04, train_loss: 0.1730, train_acc: 0.9402 test_loss: 0.8122, test_acc: 0.7949, best: 0.7989, time: 0:00:37
 Epoch: 242, lr: 4.0e-04, train_loss: 0.1794, train_acc: 0.9404 test_loss: 0.7912, test_acc: 0.7959, best: 0.7989, time: 0:00:37
 Epoch: 243, lr: 4.0e-04, train_loss: 0.1720, train_acc: 0.9408 test_loss: 0.7902, test_acc: 0.8006, best: 0.8006, time: 0:00:37
 Epoch: 244, lr: 4.0e-04, train_loss: 0.1565, train_acc: 0.9496 test_loss: 0.8101, test_acc: 0.7897, best: 0.8006, time: 0:00:37
 Epoch: 245, lr: 4.0e-04, train_loss: 0.1754, train_acc: 0.9436 test_loss: 0.8030, test_acc: 0.7963, best: 0.8006, time: 0:00:37
 Epoch: 246, lr: 4.0e-04, train_loss: 0.1607, train_acc: 0.9460 test_loss: 0.8126, test_acc: 0.7950, best: 0.8006, time: 0:00:37
 Epoch: 247, lr: 4.0e-04, train_loss: 0.1554, train_acc: 0.9482 test_loss: 0.8315, test_acc: 0.7930, best: 0.8006, time: 0:00:37
 Epoch: 248, lr: 4.0e-04, train_loss: 0.1775, train_acc: 0.9394 test_loss: 0.8236, test_acc: 0.7953, best: 0.8006, time: 0:00:37
 Epoch: 249, lr: 4.0e-04, train_loss: 0.1844, train_acc: 0.9394 test_loss: 0.8045, test_acc: 0.7933, best: 0.8006, time: 0:00:37
 Epoch: 250, lr: 4.0e-04, train_loss: 0.1824, train_acc: 0.9374 test_loss: 0.8161, test_acc: 0.7975, best: 0.8006, time: 0:00:37
 Epoch: 251, lr: 4.0e-04, train_loss: 0.1598, train_acc: 0.9466 test_loss: 0.8052, test_acc: 0.7961, best: 0.8006, time: 0:00:37
 Epoch: 252, lr: 4.0e-04, train_loss: 0.1597, train_acc: 0.9456 test_loss: 0.8117, test_acc: 0.7977, best: 0.8006, time: 0:00:37
 Epoch: 253, lr: 4.0e-04, train_loss: 0.1721, train_acc: 0.9442 test_loss: 0.8230, test_acc: 0.7949, best: 0.8006, time: 0:00:37
 Epoch: 254, lr: 4.0e-04, train_loss: 0.1646, train_acc: 0.9434 test_loss: 0.8068, test_acc: 0.7957, best: 0.8006, time: 0:00:38
 Epoch: 255, lr: 4.0e-04, train_loss: 0.1801, train_acc: 0.9396 test_loss: 0.8213, test_acc: 0.7923, best: 0.8006, time: 0:00:37
 Epoch: 256, lr: 4.0e-04, train_loss: 0.1688, train_acc: 0.9404 test_loss: 0.8491, test_acc: 0.7943, best: 0.8006, time: 0:00:37
 Epoch: 257, lr: 4.0e-04, train_loss: 0.1545, train_acc: 0.9480 test_loss: 0.8208, test_acc: 0.7956, best: 0.8006, time: 0:00:37
 Epoch: 258, lr: 4.0e-04, train_loss: 0.1794, train_acc: 0.9382 test_loss: 0.8225, test_acc: 0.7937, best: 0.8006, time: 0:00:37
 Epoch: 259, lr: 4.0e-04, train_loss: 0.1627, train_acc: 0.9472 test_loss: 0.8134, test_acc: 0.7953, best: 0.8006, time: 0:00:37
 Epoch: 260, lr: 4.0e-04, train_loss: 0.1531, train_acc: 0.9488 test_loss: 0.8294, test_acc: 0.7967, best: 0.8006, time: 0:00:37
 Epoch: 261, lr: 4.0e-04, train_loss: 0.1749, train_acc: 0.9388 test_loss: 0.8389, test_acc: 0.7929, best: 0.8006, time: 0:00:36
 Epoch: 262, lr: 4.0e-04, train_loss: 0.1782, train_acc: 0.9404 test_loss: 0.8334, test_acc: 0.7930, best: 0.8006, time: 0:00:37
 Epoch: 263, lr: 4.0e-04, train_loss: 0.1730, train_acc: 0.9440 test_loss: 0.8144, test_acc: 0.7945, best: 0.8006, time: 0:00:37
 Epoch: 264, lr: 4.0e-04, train_loss: 0.1554, train_acc: 0.9492 test_loss: 0.8237, test_acc: 0.7970, best: 0.8006, time: 0:00:37
 Epoch: 265, lr: 4.0e-04, train_loss: 0.1785, train_acc: 0.9436 test_loss: 0.8310, test_acc: 0.7927, best: 0.8006, time: 0:00:37
 Epoch: 266, lr: 4.0e-04, train_loss: 0.1761, train_acc: 0.9436 test_loss: 0.8149, test_acc: 0.7961, best: 0.8006, time: 0:00:36
 Epoch: 267, lr: 4.0e-04, train_loss: 0.1598, train_acc: 0.9462 test_loss: 0.8164, test_acc: 0.7966, best: 0.8006, time: 0:00:37
 Epoch: 268, lr: 4.0e-04, train_loss: 0.1570, train_acc: 0.9464 test_loss: 0.8112, test_acc: 0.7973, best: 0.8006, time: 0:00:37
 Epoch: 269, lr: 4.0e-04, train_loss: 0.1710, train_acc: 0.9418 test_loss: 0.8142, test_acc: 0.7990, best: 0.8006, time: 0:00:37
 Epoch: 270, lr: 8.0e-05, train_loss: 0.1587, train_acc: 0.9476 test_loss: 0.8274, test_acc: 0.7953, best: 0.8006, time: 0:00:37
 Epoch: 271, lr: 8.0e-05, train_loss: 0.1599, train_acc: 0.9452 test_loss: 0.8032, test_acc: 0.7966, best: 0.8006, time: 0:00:37
 Epoch: 272, lr: 8.0e-05, train_loss: 0.1702, train_acc: 0.9420 test_loss: 0.8304, test_acc: 0.7947, best: 0.8006, time: 0:00:37
 Epoch: 273, lr: 8.0e-05, train_loss: 0.1563, train_acc: 0.9456 test_loss: 0.8307, test_acc: 0.7967, best: 0.8006, time: 0:00:37
 Epoch: 274, lr: 8.0e-05, train_loss: 0.1603, train_acc: 0.9472 test_loss: 0.8142, test_acc: 0.7973, best: 0.8006, time: 0:00:37
 Epoch: 275, lr: 8.0e-05, train_loss: 0.1653, train_acc: 0.9462 test_loss: 0.8309, test_acc: 0.7960, best: 0.8006, time: 0:00:37
 Epoch: 276, lr: 8.0e-05, train_loss: 0.1730, train_acc: 0.9444 test_loss: 0.8219, test_acc: 0.7965, best: 0.8006, time: 0:00:37
 Epoch: 277, lr: 8.0e-05, train_loss: 0.1626, train_acc: 0.9454 test_loss: 0.8301, test_acc: 0.7971, best: 0.8006, time: 0:00:37
 Epoch: 278, lr: 8.0e-05, train_loss: 0.1687, train_acc: 0.9426 test_loss: 0.8372, test_acc: 0.7945, best: 0.8006, time: 0:00:37
 Epoch: 279, lr: 8.0e-05, train_loss: 0.1797, train_acc: 0.9398 test_loss: 0.8247, test_acc: 0.7969, best: 0.8006, time: 0:00:37
 Epoch: 280, lr: 8.0e-05, train_loss: 0.1544, train_acc: 0.9518 test_loss: 0.8114, test_acc: 0.7959, best: 0.8006, time: 0:00:37
 Epoch: 281, lr: 8.0e-05, train_loss: 0.1715, train_acc: 0.9388 test_loss: 0.8207, test_acc: 0.7955, best: 0.8006, time: 0:00:37
 Epoch: 282, lr: 8.0e-05, train_loss: 0.1689, train_acc: 0.9434 test_loss: 0.8247, test_acc: 0.7925, best: 0.8006, time: 0:00:37
 Epoch: 283, lr: 8.0e-05, train_loss: 0.1562, train_acc: 0.9488 test_loss: 0.8150, test_acc: 0.7957, best: 0.8006, time: 0:00:37
 Epoch: 284, lr: 8.0e-05, train_loss: 0.1627, train_acc: 0.9452 test_loss: 0.8556, test_acc: 0.7900, best: 0.8006, time: 0:00:37
 Epoch: 285, lr: 8.0e-05, train_loss: 0.1689, train_acc: 0.9412 test_loss: 0.8180, test_acc: 0.7967, best: 0.8006, time: 0:00:37
 Epoch: 286, lr: 8.0e-05, train_loss: 0.1483, train_acc: 0.9500 test_loss: 0.8207, test_acc: 0.7933, best: 0.8006, time: 0:00:37
 Epoch: 287, lr: 8.0e-05, train_loss: 0.1504, train_acc: 0.9508 test_loss: 0.8175, test_acc: 0.7984, best: 0.8006, time: 0:00:37
 Epoch: 288, lr: 8.0e-05, train_loss: 0.1618, train_acc: 0.9458 test_loss: 0.8332, test_acc: 0.7920, best: 0.8006, time: 0:00:37
 Epoch: 289, lr: 8.0e-05, train_loss: 0.1700, train_acc: 0.9466 test_loss: 0.8218, test_acc: 0.7993, best: 0.8006, time: 0:00:37
 Epoch: 290, lr: 8.0e-05, train_loss: 0.1496, train_acc: 0.9508 test_loss: 0.8141, test_acc: 0.7959, best: 0.8006, time: 0:00:37
 Epoch: 291, lr: 8.0e-05, train_loss: 0.1724, train_acc: 0.9464 test_loss: 0.8119, test_acc: 0.7965, best: 0.8006, time: 0:00:37
 Epoch: 292, lr: 8.0e-05, train_loss: 0.1715, train_acc: 0.9442 test_loss: 0.8007, test_acc: 0.7966, best: 0.8006, time: 0:00:37
 Epoch: 293, lr: 8.0e-05, train_loss: 0.1586, train_acc: 0.9450 test_loss: 0.8196, test_acc: 0.7983, best: 0.8006, time: 0:00:37
 Epoch: 294, lr: 8.0e-05, train_loss: 0.1587, train_acc: 0.9492 test_loss: 0.8390, test_acc: 0.7914, best: 0.8006, time: 0:00:37
 Epoch: 295, lr: 8.0e-05, train_loss: 0.1793, train_acc: 0.9418 test_loss: 0.8371, test_acc: 0.7966, best: 0.8006, time: 0:00:37
 Epoch: 296, lr: 8.0e-05, train_loss: 0.1695, train_acc: 0.9434 test_loss: 0.8339, test_acc: 0.7935, best: 0.8006, time: 0:00:37
 Epoch: 297, lr: 8.0e-05, train_loss: 0.1503, train_acc: 0.9526 test_loss: 0.8362, test_acc: 0.7949, best: 0.8006, time: 0:00:37
 Epoch: 298, lr: 8.0e-05, train_loss: 0.1622, train_acc: 0.9448 test_loss: 0.8431, test_acc: 0.7940, best: 0.8006, time: 0:00:37
 Epoch: 299, lr: 8.0e-05, train_loss: 0.1665, train_acc: 0.9454 test_loss: 0.8118, test_acc: 0.7954, best: 0.8006, time: 0:00:37
 Highest accuracy: 0.8006