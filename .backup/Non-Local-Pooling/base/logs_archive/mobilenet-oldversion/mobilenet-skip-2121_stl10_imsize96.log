
 Run on time: 2022-07-01 02:49:03.181484

 Architecture: mobilenet-skip-2121

 Pool Config: {
    "arch": "mobilenet",
    "conv1": {
        "_conv2d": "norm",
        "pool_cfg": {}
    },
    "layer1": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "skip",
            "_stride": 2,
            "_psize": null,
            "_dim_reduced_ratio": null,
            "_num_heads": null,
            "_conv2d": null,
            "_win_norm": null
        }
    },
    "layer2": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "skip",
            "_stride": 1,
            "_psize": null,
            "_dim_reduced_ratio": null,
            "_num_heads": null,
            "_conv2d": null,
            "_win_norm": null
        }
    },
    "layer3": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "skip",
            "_stride": 2,
            "_psize": null,
            "_dim_reduced_ratio": null,
            "_num_heads": null,
            "_conv2d": null,
            "_win_norm": null
        }
    },
    "layer4": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "skip",
            "_stride": 1,
            "_psize": null,
            "_dim_reduced_ratio": null,
            "_num_heads": null,
            "_conv2d": null,
            "_win_norm": null
        }
    },
    "conv2": {
        "_conv2d": "norm",
        "pool_cfg": {}
    }
}

 Arguments:
	 gpu                  : True
	 log                  : True
	 seed                 : 0
	 momentum             : 0.9
	 amsgrad              : True
	 dataset              : STL10
	 batch_size           : 8
	 architecture         : MOBILENET-SKIP-2121
	 im_size              : None
	 relu_threshold       : 4.0
	 learning_rate        : 0.01
	 pretrained_backbone  : 
	 pretrained_ann       : 
	 weight_decay         : 0.0
	 test_only            : False
	 epochs               : 300
	 lr_interval          : [180, 240, 270]
	 lr_reduce            : 5
	 optimizer            : SGD
	 dropout              : 0.2
	 kernel_size          : 3
	 dont_save            : False
	 visualize            : False
	 devices              : 0
 DataParallel(
  (module): Network(
    (net): MobileNetV2(
      (features): Sequential(
        (0): Sequential(
          (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
            (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (2): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (3): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (4): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (5): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (6): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
            (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (7): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
            (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (8): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)
            (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (9): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
            (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (10): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
            (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (11): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
            (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
      (conv): Sequential(
        (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
      (classifier): Linear(in_features=1280, out_features=10, bias=True)
    )
  )
)
 SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0
)
 Epoch: 1, lr: 1.0e-02, train_loss: 2.3048, train_acc: 0.2090 test_loss: 1.7539, test_acc: 0.2988, best: 0.2988, time: 0:00:41
 Epoch: 2, lr: 1.0e-02, train_loss: 1.9295, train_acc: 0.2728 test_loss: 1.7084, test_acc: 0.3182, best: 0.3182, time: 0:00:41
 Epoch: 3, lr: 1.0e-02, train_loss: 1.8313, train_acc: 0.3010 test_loss: 1.5438, test_acc: 0.3951, best: 0.3951, time: 0:00:41
 Epoch: 4, lr: 1.0e-02, train_loss: 1.7660, train_acc: 0.3346 test_loss: 1.6129, test_acc: 0.3882, best: 0.3951, time: 0:00:41
 Epoch: 5, lr: 1.0e-02, train_loss: 1.6847, train_acc: 0.3636 test_loss: 1.5112, test_acc: 0.4345, best: 0.4345, time: 0:00:41
 Epoch: 6, lr: 1.0e-02, train_loss: 1.6033, train_acc: 0.3988 test_loss: 1.4193, test_acc: 0.4467, best: 0.4467, time: 0:00:41
 Epoch: 7, lr: 1.0e-02, train_loss: 1.5647, train_acc: 0.4176 test_loss: 1.4593, test_acc: 0.4497, best: 0.4497, time: 0:00:41
 Epoch: 8, lr: 1.0e-02, train_loss: 1.5221, train_acc: 0.4260 test_loss: 1.3168, test_acc: 0.5096, best: 0.5096, time: 0:00:41
 Epoch: 9, lr: 1.0e-02, train_loss: 1.4808, train_acc: 0.4538 test_loss: 1.2820, test_acc: 0.5250, best: 0.5250, time: 0:00:41
 Epoch: 10, lr: 1.0e-02, train_loss: 1.4502, train_acc: 0.4708 test_loss: 1.2501, test_acc: 0.5339, best: 0.5339, time: 0:00:41
 Epoch: 11, lr: 1.0e-02, train_loss: 1.4099, train_acc: 0.4834 test_loss: 1.1832, test_acc: 0.5546, best: 0.5546, time: 0:00:41
 Epoch: 12, lr: 1.0e-02, train_loss: 1.3900, train_acc: 0.4966 test_loss: 1.2244, test_acc: 0.5379, best: 0.5546, time: 0:00:41
 Epoch: 13, lr: 1.0e-02, train_loss: 1.3506, train_acc: 0.5068 test_loss: 1.2402, test_acc: 0.5484, best: 0.5546, time: 0:00:41
 Epoch: 14, lr: 1.0e-02, train_loss: 1.3182, train_acc: 0.5088 test_loss: 1.2379, test_acc: 0.5423, best: 0.5546, time: 0:00:41
 Epoch: 15, lr: 1.0e-02, train_loss: 1.3031, train_acc: 0.5290 test_loss: 1.1897, test_acc: 0.5543, best: 0.5546, time: 0:00:41
 Epoch: 16, lr: 1.0e-02, train_loss: 1.2803, train_acc: 0.5378 test_loss: 1.1254, test_acc: 0.5874, best: 0.5874, time: 0:00:41
 Epoch: 17, lr: 1.0e-02, train_loss: 1.2593, train_acc: 0.5420 test_loss: 1.0786, test_acc: 0.6162, best: 0.6162, time: 0:00:41
 Epoch: 18, lr: 1.0e-02, train_loss: 1.2511, train_acc: 0.5378 test_loss: 1.1203, test_acc: 0.5972, best: 0.6162, time: 0:00:41
 Epoch: 19, lr: 1.0e-02, train_loss: 1.2291, train_acc: 0.5490 test_loss: 1.1076, test_acc: 0.5966, best: 0.6162, time: 0:00:41
 Epoch: 20, lr: 1.0e-02, train_loss: 1.1899, train_acc: 0.5678 test_loss: 1.0557, test_acc: 0.6246, best: 0.6246, time: 0:00:41
 Epoch: 21, lr: 1.0e-02, train_loss: 1.1745, train_acc: 0.5698 test_loss: 1.0185, test_acc: 0.6236, best: 0.6246, time: 0:00:41
 Epoch: 22, lr: 1.0e-02, train_loss: 1.1480, train_acc: 0.5890 test_loss: 1.0811, test_acc: 0.6191, best: 0.6246, time: 0:00:41
 Epoch: 23, lr: 1.0e-02, train_loss: 1.1433, train_acc: 0.5860 test_loss: 1.0443, test_acc: 0.6230, best: 0.6246, time: 0:00:41
 Epoch: 24, lr: 1.0e-02, train_loss: 1.1211, train_acc: 0.5992 test_loss: 1.0012, test_acc: 0.6539, best: 0.6539, time: 0:00:41
 Epoch: 25, lr: 1.0e-02, train_loss: 1.1114, train_acc: 0.5944 test_loss: 1.0377, test_acc: 0.6349, best: 0.6539, time: 0:00:41
 Epoch: 26, lr: 1.0e-02, train_loss: 1.0889, train_acc: 0.6112 test_loss: 0.9862, test_acc: 0.6365, best: 0.6539, time: 0:00:41
 Epoch: 27, lr: 1.0e-02, train_loss: 1.0720, train_acc: 0.6198 test_loss: 0.9707, test_acc: 0.6506, best: 0.6539, time: 0:00:41
 Epoch: 28, lr: 1.0e-02, train_loss: 1.0531, train_acc: 0.6212 test_loss: 1.1251, test_acc: 0.6046, best: 0.6539, time: 0:00:41
 Epoch: 29, lr: 1.0e-02, train_loss: 1.0419, train_acc: 0.6218 test_loss: 1.0120, test_acc: 0.6448, best: 0.6539, time: 0:00:41
 Epoch: 30, lr: 1.0e-02, train_loss: 1.0492, train_acc: 0.6270 test_loss: 0.9119, test_acc: 0.6714, best: 0.6714, time: 0:00:41
 Epoch: 31, lr: 1.0e-02, train_loss: 1.0132, train_acc: 0.6378 test_loss: 0.9868, test_acc: 0.6522, best: 0.6714, time: 0:00:41
 Epoch: 32, lr: 1.0e-02, train_loss: 0.9968, train_acc: 0.6384 test_loss: 0.9402, test_acc: 0.6663, best: 0.6714, time: 0:00:41
 Epoch: 33, lr: 1.0e-02, train_loss: 1.0064, train_acc: 0.6494 test_loss: 0.8779, test_acc: 0.6876, best: 0.6876, time: 0:00:41
 Epoch: 34, lr: 1.0e-02, train_loss: 0.9611, train_acc: 0.6570 test_loss: 0.9843, test_acc: 0.6474, best: 0.6876, time: 0:00:41
 Epoch: 35, lr: 1.0e-02, train_loss: 0.9752, train_acc: 0.6476 test_loss: 0.9501, test_acc: 0.6716, best: 0.6876, time: 0:00:41
 Epoch: 36, lr: 1.0e-02, train_loss: 0.9652, train_acc: 0.6494 test_loss: 0.9265, test_acc: 0.6711, best: 0.6876, time: 0:00:41
 Epoch: 37, lr: 1.0e-02, train_loss: 0.9516, train_acc: 0.6566 test_loss: 0.9252, test_acc: 0.6694, best: 0.6876, time: 0:00:41
 Epoch: 38, lr: 1.0e-02, train_loss: 0.9530, train_acc: 0.6548 test_loss: 0.8589, test_acc: 0.6925, best: 0.6925, time: 0:00:41
 Epoch: 39, lr: 1.0e-02, train_loss: 0.9223, train_acc: 0.6718 test_loss: 0.8675, test_acc: 0.7013, best: 0.7013, time: 0:00:41
 Epoch: 40, lr: 1.0e-02, train_loss: 0.9042, train_acc: 0.6690 test_loss: 0.9419, test_acc: 0.6769, best: 0.7013, time: 0:00:41
 Epoch: 41, lr: 1.0e-02, train_loss: 0.9056, train_acc: 0.6794 test_loss: 0.8607, test_acc: 0.6890, best: 0.7013, time: 0:00:41
 Epoch: 42, lr: 1.0e-02, train_loss: 0.8883, train_acc: 0.6778 test_loss: 0.9295, test_acc: 0.6735, best: 0.7013, time: 0:00:41
 Epoch: 43, lr: 1.0e-02, train_loss: 0.8655, train_acc: 0.6894 test_loss: 0.9453, test_acc: 0.6707, best: 0.7013, time: 0:00:41
 Epoch: 44, lr: 1.0e-02, train_loss: 0.8712, train_acc: 0.6918 test_loss: 0.8937, test_acc: 0.6777, best: 0.7013, time: 0:00:41
 Epoch: 45, lr: 1.0e-02, train_loss: 0.8578, train_acc: 0.6958 test_loss: 0.8269, test_acc: 0.7143, best: 0.7143, time: 0:00:41
 Epoch: 46, lr: 1.0e-02, train_loss: 0.8374, train_acc: 0.7042 test_loss: 0.8079, test_acc: 0.7167, best: 0.7167, time: 0:00:41
 Epoch: 47, lr: 1.0e-02, train_loss: 0.8608, train_acc: 0.6976 test_loss: 0.8599, test_acc: 0.7160, best: 0.7167, time: 0:00:41
 Epoch: 48, lr: 1.0e-02, train_loss: 0.8149, train_acc: 0.7154 test_loss: 0.8006, test_acc: 0.7219, best: 0.7219, time: 0:00:41
 Epoch: 49, lr: 1.0e-02, train_loss: 0.8329, train_acc: 0.7090 test_loss: 0.7832, test_acc: 0.7284, best: 0.7284, time: 0:00:41
 Epoch: 50, lr: 1.0e-02, train_loss: 0.8075, train_acc: 0.7142 test_loss: 0.8285, test_acc: 0.7184, best: 0.7284, time: 0:00:41
 Epoch: 51, lr: 1.0e-02, train_loss: 0.8066, train_acc: 0.7138 test_loss: 0.7538, test_acc: 0.7375, best: 0.7375, time: 0:00:41
 Epoch: 52, lr: 1.0e-02, train_loss: 0.8261, train_acc: 0.7064 test_loss: 0.8373, test_acc: 0.7173, best: 0.7375, time: 0:00:41
 Epoch: 53, lr: 1.0e-02, train_loss: 0.7832, train_acc: 0.7212 test_loss: 0.8591, test_acc: 0.7044, best: 0.7375, time: 0:00:41
 Epoch: 54, lr: 1.0e-02, train_loss: 0.7774, train_acc: 0.7230 test_loss: 0.7998, test_acc: 0.7269, best: 0.7375, time: 0:00:41
 Epoch: 55, lr: 1.0e-02, train_loss: 0.7590, train_acc: 0.7292 test_loss: 0.7635, test_acc: 0.7389, best: 0.7389, time: 0:00:41
 Epoch: 56, lr: 1.0e-02, train_loss: 0.7653, train_acc: 0.7306 test_loss: 0.7788, test_acc: 0.7271, best: 0.7389, time: 0:00:41
 Epoch: 57, lr: 1.0e-02, train_loss: 0.7773, train_acc: 0.7230 test_loss: 0.7831, test_acc: 0.7231, best: 0.7389, time: 0:00:41
 Epoch: 58, lr: 1.0e-02, train_loss: 0.7622, train_acc: 0.7270 test_loss: 0.8039, test_acc: 0.7250, best: 0.7389, time: 0:00:41
 Epoch: 59, lr: 1.0e-02, train_loss: 0.7233, train_acc: 0.7464 test_loss: 0.7974, test_acc: 0.7252, best: 0.7389, time: 0:00:41
 Epoch: 60, lr: 1.0e-02, train_loss: 0.7341, train_acc: 0.7432 test_loss: 0.7052, test_acc: 0.7555, best: 0.7555, time: 0:00:41
 Epoch: 61, lr: 1.0e-02, train_loss: 0.7207, train_acc: 0.7464 test_loss: 0.7950, test_acc: 0.7350, best: 0.7555, time: 0:00:41
 Epoch: 62, lr: 1.0e-02, train_loss: 0.7164, train_acc: 0.7482 test_loss: 0.7274, test_acc: 0.7548, best: 0.7555, time: 0:00:41
 Epoch: 63, lr: 1.0e-02, train_loss: 0.7074, train_acc: 0.7500 test_loss: 0.7062, test_acc: 0.7618, best: 0.7618, time: 0:00:41
 Epoch: 64, lr: 1.0e-02, train_loss: 0.7100, train_acc: 0.7484 test_loss: 0.7660, test_acc: 0.7442, best: 0.7618, time: 0:00:41
 Epoch: 65, lr: 1.0e-02, train_loss: 0.6976, train_acc: 0.7576 test_loss: 0.7857, test_acc: 0.7422, best: 0.7618, time: 0:00:41
 Epoch: 66, lr: 1.0e-02, train_loss: 0.6877, train_acc: 0.7544 test_loss: 0.7611, test_acc: 0.7495, best: 0.7618, time: 0:00:41
 Epoch: 67, lr: 1.0e-02, train_loss: 0.6716, train_acc: 0.7634 test_loss: 0.7653, test_acc: 0.7451, best: 0.7618, time: 0:00:41
 Epoch: 68, lr: 1.0e-02, train_loss: 0.6661, train_acc: 0.7654 test_loss: 0.7678, test_acc: 0.7435, best: 0.7618, time: 0:00:41
 Epoch: 69, lr: 1.0e-02, train_loss: 0.6690, train_acc: 0.7702 test_loss: 0.7201, test_acc: 0.7542, best: 0.7618, time: 0:00:41
 Epoch: 70, lr: 1.0e-02, train_loss: 0.6599, train_acc: 0.7726 test_loss: 0.8253, test_acc: 0.7376, best: 0.7618, time: 0:00:41
 Epoch: 71, lr: 1.0e-02, train_loss: 0.6387, train_acc: 0.7768 test_loss: 0.7045, test_acc: 0.7678, best: 0.7678, time: 0:00:41
 Epoch: 72, lr: 1.0e-02, train_loss: 0.6535, train_acc: 0.7758 test_loss: 0.6758, test_acc: 0.7661, best: 0.7678, time: 0:00:41
 Epoch: 73, lr: 1.0e-02, train_loss: 0.6576, train_acc: 0.7670 test_loss: 0.6900, test_acc: 0.7726, best: 0.7726, time: 0:00:41
 Epoch: 74, lr: 1.0e-02, train_loss: 0.6324, train_acc: 0.7782 test_loss: 0.8404, test_acc: 0.7288, best: 0.7726, time: 0:00:41
 Epoch: 75, lr: 1.0e-02, train_loss: 0.6329, train_acc: 0.7728 test_loss: 0.6941, test_acc: 0.7724, best: 0.7726, time: 0:00:41
 Epoch: 76, lr: 1.0e-02, train_loss: 0.6383, train_acc: 0.7792 test_loss: 0.7343, test_acc: 0.7618, best: 0.7726, time: 0:00:41
 Epoch: 77, lr: 1.0e-02, train_loss: 0.6287, train_acc: 0.7870 test_loss: 0.7879, test_acc: 0.7368, best: 0.7726, time: 0:00:41
 Epoch: 78, lr: 1.0e-02, train_loss: 0.6186, train_acc: 0.7860 test_loss: 0.7141, test_acc: 0.7689, best: 0.7726, time: 0:00:41
 Epoch: 79, lr: 1.0e-02, train_loss: 0.5907, train_acc: 0.7964 test_loss: 0.8053, test_acc: 0.7454, best: 0.7726, time: 0:00:41
 Epoch: 80, lr: 1.0e-02, train_loss: 0.6108, train_acc: 0.7852 test_loss: 0.7532, test_acc: 0.7594, best: 0.7726, time: 0:00:41
 Epoch: 81, lr: 1.0e-02, train_loss: 0.6054, train_acc: 0.7784 test_loss: 0.7368, test_acc: 0.7636, best: 0.7726, time: 0:00:41
 Epoch: 82, lr: 1.0e-02, train_loss: 0.5836, train_acc: 0.7946 test_loss: 0.7093, test_acc: 0.7609, best: 0.7726, time: 0:00:41
 Epoch: 83, lr: 1.0e-02, train_loss: 0.5864, train_acc: 0.7992 test_loss: 0.7123, test_acc: 0.7590, best: 0.7726, time: 0:00:41
 Epoch: 84, lr: 1.0e-02, train_loss: 0.5867, train_acc: 0.7946 test_loss: 0.7096, test_acc: 0.7639, best: 0.7726, time: 0:00:41
 Epoch: 85, lr: 1.0e-02, train_loss: 0.5774, train_acc: 0.7910 test_loss: 0.7466, test_acc: 0.7594, best: 0.7726, time: 0:00:41
 Epoch: 86, lr: 1.0e-02, train_loss: 0.5854, train_acc: 0.8016 test_loss: 0.7189, test_acc: 0.7696, best: 0.7726, time: 0:00:41
 Epoch: 87, lr: 1.0e-02, train_loss: 0.5640, train_acc: 0.8044 test_loss: 0.7513, test_acc: 0.7671, best: 0.7726, time: 0:00:41
 Epoch: 88, lr: 1.0e-02, train_loss: 0.5316, train_acc: 0.8166 test_loss: 0.6786, test_acc: 0.7805, best: 0.7805, time: 0:00:41
 Epoch: 89, lr: 1.0e-02, train_loss: 0.5653, train_acc: 0.8016 test_loss: 0.6819, test_acc: 0.7781, best: 0.7805, time: 0:00:41
 Epoch: 90, lr: 1.0e-02, train_loss: 0.5600, train_acc: 0.8026 test_loss: 0.7842, test_acc: 0.7526, best: 0.7805, time: 0:00:41
 Epoch: 91, lr: 1.0e-02, train_loss: 0.5687, train_acc: 0.7958 test_loss: 0.7645, test_acc: 0.7490, best: 0.7805, time: 0:00:41
 Epoch: 92, lr: 1.0e-02, train_loss: 0.5347, train_acc: 0.8174 test_loss: 0.7883, test_acc: 0.7515, best: 0.7805, time: 0:00:41
 Epoch: 93, lr: 1.0e-02, train_loss: 0.5493, train_acc: 0.8126 test_loss: 0.7018, test_acc: 0.7762, best: 0.7805, time: 0:00:41
 Epoch: 94, lr: 1.0e-02, train_loss: 0.5154, train_acc: 0.8220 test_loss: 0.6770, test_acc: 0.7857, best: 0.7857, time: 0:00:41
 Epoch: 95, lr: 1.0e-02, train_loss: 0.5266, train_acc: 0.8164 test_loss: 0.7008, test_acc: 0.7756, best: 0.7857, time: 0:00:41
 Epoch: 96, lr: 1.0e-02, train_loss: 0.5352, train_acc: 0.8170 test_loss: 0.6816, test_acc: 0.7815, best: 0.7857, time: 0:00:41
 Epoch: 97, lr: 1.0e-02, train_loss: 0.5377, train_acc: 0.8118 test_loss: 0.7141, test_acc: 0.7728, best: 0.7857, time: 0:00:41
 Epoch: 98, lr: 1.0e-02, train_loss: 0.5345, train_acc: 0.8228 test_loss: 0.6947, test_acc: 0.7754, best: 0.7857, time: 0:00:41
 Epoch: 99, lr: 1.0e-02, train_loss: 0.5249, train_acc: 0.8214 test_loss: 0.6985, test_acc: 0.7764, best: 0.7857, time: 0:00:41
 Epoch: 100, lr: 1.0e-02, train_loss: 0.4936, train_acc: 0.8282 test_loss: 0.7066, test_acc: 0.7719, best: 0.7857, time: 0:00:41
 Epoch: 101, lr: 1.0e-02, train_loss: 0.5161, train_acc: 0.8182 test_loss: 0.7656, test_acc: 0.7585, best: 0.7857, time: 0:00:41
 Epoch: 102, lr: 1.0e-02, train_loss: 0.4974, train_acc: 0.8278 test_loss: 0.7363, test_acc: 0.7684, best: 0.7857, time: 0:00:41
 Epoch: 103, lr: 1.0e-02, train_loss: 0.4884, train_acc: 0.8346 test_loss: 0.7585, test_acc: 0.7598, best: 0.7857, time: 0:00:41
 Epoch: 104, lr: 1.0e-02, train_loss: 0.4884, train_acc: 0.8278 test_loss: 0.7362, test_acc: 0.7782, best: 0.7857, time: 0:00:41
 Epoch: 105, lr: 1.0e-02, train_loss: 0.4874, train_acc: 0.8328 test_loss: 0.9022, test_acc: 0.7381, best: 0.7857, time: 0:00:41
 Epoch: 106, lr: 1.0e-02, train_loss: 0.4708, train_acc: 0.8394 test_loss: 0.7208, test_acc: 0.7846, best: 0.7857, time: 0:00:41
 Epoch: 107, lr: 1.0e-02, train_loss: 0.4829, train_acc: 0.8330 test_loss: 0.7730, test_acc: 0.7640, best: 0.7857, time: 0:00:41
 Epoch: 108, lr: 1.0e-02, train_loss: 0.4890, train_acc: 0.8336 test_loss: 0.7813, test_acc: 0.7591, best: 0.7857, time: 0:00:41
 Epoch: 109, lr: 1.0e-02, train_loss: 0.4732, train_acc: 0.8402 test_loss: 0.7116, test_acc: 0.7802, best: 0.7857, time: 0:00:41
 Epoch: 110, lr: 1.0e-02, train_loss: 0.4798, train_acc: 0.8354 test_loss: 0.7546, test_acc: 0.7626, best: 0.7857, time: 0:00:41
 Epoch: 111, lr: 1.0e-02, train_loss: 0.4608, train_acc: 0.8388 test_loss: 0.6926, test_acc: 0.7857, best: 0.7857, time: 0:00:41
 Epoch: 112, lr: 1.0e-02, train_loss: 0.4606, train_acc: 0.8424 test_loss: 0.7169, test_acc: 0.7847, best: 0.7857, time: 0:00:41
 Epoch: 113, lr: 1.0e-02, train_loss: 0.4602, train_acc: 0.8386 test_loss: 0.7765, test_acc: 0.7724, best: 0.7857, time: 0:00:41
 Epoch: 114, lr: 1.0e-02, train_loss: 0.4472, train_acc: 0.8476 test_loss: 0.7865, test_acc: 0.7704, best: 0.7857, time: 0:00:41
 Epoch: 115, lr: 1.0e-02, train_loss: 0.4515, train_acc: 0.8430 test_loss: 0.7541, test_acc: 0.7758, best: 0.7857, time: 0:00:41
 Epoch: 116, lr: 1.0e-02, train_loss: 0.4719, train_acc: 0.8306 test_loss: 0.7274, test_acc: 0.7786, best: 0.7857, time: 0:00:41
 Epoch: 117, lr: 1.0e-02, train_loss: 0.4746, train_acc: 0.8364 test_loss: 0.7473, test_acc: 0.7711, best: 0.7857, time: 0:00:41
 Epoch: 118, lr: 1.0e-02, train_loss: 0.4376, train_acc: 0.8492 test_loss: 0.7010, test_acc: 0.7841, best: 0.7857, time: 0:00:41
 Epoch: 119, lr: 1.0e-02, train_loss: 0.4449, train_acc: 0.8476 test_loss: 0.7495, test_acc: 0.7806, best: 0.7857, time: 0:00:41
 Epoch: 120, lr: 1.0e-02, train_loss: 0.4473, train_acc: 0.8456 test_loss: 0.6960, test_acc: 0.7870, best: 0.7870, time: 0:00:41
 Epoch: 121, lr: 1.0e-02, train_loss: 0.4672, train_acc: 0.8396 test_loss: 0.7099, test_acc: 0.7883, best: 0.7883, time: 0:00:41
 Epoch: 122, lr: 1.0e-02, train_loss: 0.4309, train_acc: 0.8484 test_loss: 0.6862, test_acc: 0.7923, best: 0.7923, time: 0:00:41
 Epoch: 123, lr: 1.0e-02, train_loss: 0.4150, train_acc: 0.8568 test_loss: 0.7088, test_acc: 0.7880, best: 0.7923, time: 0:00:41
 Epoch: 124, lr: 1.0e-02, train_loss: 0.4233, train_acc: 0.8506 test_loss: 0.7313, test_acc: 0.7833, best: 0.7923, time: 0:00:41
 Epoch: 125, lr: 1.0e-02, train_loss: 0.4127, train_acc: 0.8570 test_loss: 0.6915, test_acc: 0.7936, best: 0.7936, time: 0:00:41
 Epoch: 126, lr: 1.0e-02, train_loss: 0.4113, train_acc: 0.8590 test_loss: 0.7463, test_acc: 0.7860, best: 0.7936, time: 0:00:41
 Epoch: 127, lr: 1.0e-02, train_loss: 0.4206, train_acc: 0.8560 test_loss: 0.6761, test_acc: 0.7931, best: 0.7936, time: 0:00:41
 Epoch: 128, lr: 1.0e-02, train_loss: 0.4114, train_acc: 0.8546 test_loss: 0.7340, test_acc: 0.7790, best: 0.7936, time: 0:00:41
 Epoch: 129, lr: 1.0e-02, train_loss: 0.4171, train_acc: 0.8570 test_loss: 0.7129, test_acc: 0.7956, best: 0.7956, time: 0:00:41
 Epoch: 130, lr: 1.0e-02, train_loss: 0.4133, train_acc: 0.8584 test_loss: 0.7353, test_acc: 0.7836, best: 0.7956, time: 0:00:41
 Epoch: 131, lr: 1.0e-02, train_loss: 0.3890, train_acc: 0.8660 test_loss: 0.7177, test_acc: 0.7857, best: 0.7956, time: 0:00:41
 Epoch: 132, lr: 1.0e-02, train_loss: 0.3939, train_acc: 0.8634 test_loss: 0.7000, test_acc: 0.7933, best: 0.7956, time: 0:00:41
 Epoch: 133, lr: 1.0e-02, train_loss: 0.4225, train_acc: 0.8572 test_loss: 0.7484, test_acc: 0.7909, best: 0.7956, time: 0:00:41
 Epoch: 134, lr: 1.0e-02, train_loss: 0.3858, train_acc: 0.8636 test_loss: 0.7166, test_acc: 0.7939, best: 0.7956, time: 0:00:41
 Epoch: 135, lr: 1.0e-02, train_loss: 0.4109, train_acc: 0.8560 test_loss: 0.7442, test_acc: 0.7861, best: 0.7956, time: 0:00:41
 Epoch: 136, lr: 1.0e-02, train_loss: 0.3847, train_acc: 0.8648 test_loss: 0.7962, test_acc: 0.7859, best: 0.7956, time: 0:00:41
 Epoch: 137, lr: 1.0e-02, train_loss: 0.3977, train_acc: 0.8656 test_loss: 0.7828, test_acc: 0.7904, best: 0.7956, time: 0:00:41
 Epoch: 138, lr: 1.0e-02, train_loss: 0.3996, train_acc: 0.8640 test_loss: 0.7936, test_acc: 0.7792, best: 0.7956, time: 0:00:41
 Epoch: 139, lr: 1.0e-02, train_loss: 0.4027, train_acc: 0.8606 test_loss: 0.7438, test_acc: 0.7890, best: 0.7956, time: 0:00:41
 Epoch: 140, lr: 1.0e-02, train_loss: 0.4015, train_acc: 0.8560 test_loss: 0.7560, test_acc: 0.7846, best: 0.7956, time: 0:00:41
 Epoch: 141, lr: 1.0e-02, train_loss: 0.3794, train_acc: 0.8712 test_loss: 0.7785, test_acc: 0.7885, best: 0.7956, time: 0:00:41
 Epoch: 142, lr: 1.0e-02, train_loss: 0.3715, train_acc: 0.8716 test_loss: 0.7587, test_acc: 0.7874, best: 0.7956, time: 0:00:41
 Epoch: 143, lr: 1.0e-02, train_loss: 0.3702, train_acc: 0.8720 test_loss: 0.7414, test_acc: 0.7895, best: 0.7956, time: 0:00:41
 Epoch: 144, lr: 1.0e-02, train_loss: 0.3599, train_acc: 0.8738 test_loss: 0.7108, test_acc: 0.7965, best: 0.7965, time: 0:00:41
 Epoch: 145, lr: 1.0e-02, train_loss: 0.3747, train_acc: 0.8730 test_loss: 0.8154, test_acc: 0.7845, best: 0.7965, time: 0:00:41
 Epoch: 146, lr: 1.0e-02, train_loss: 0.3668, train_acc: 0.8758 test_loss: 0.7267, test_acc: 0.7970, best: 0.7970, time: 0:00:41
 Epoch: 147, lr: 1.0e-02, train_loss: 0.3977, train_acc: 0.8594 test_loss: 0.7262, test_acc: 0.7969, best: 0.7970, time: 0:00:41
 Epoch: 148, lr: 1.0e-02, train_loss: 0.3760, train_acc: 0.8680 test_loss: 0.7581, test_acc: 0.7965, best: 0.7970, time: 0:00:41
 Epoch: 149, lr: 1.0e-02, train_loss: 0.3646, train_acc: 0.8754 test_loss: 0.8384, test_acc: 0.7700, best: 0.7970, time: 0:00:41
 Epoch: 150, lr: 1.0e-02, train_loss: 0.3698, train_acc: 0.8706 test_loss: 0.7484, test_acc: 0.7940, best: 0.7970, time: 0:00:41
 Epoch: 151, lr: 1.0e-02, train_loss: 0.3690, train_acc: 0.8722 test_loss: 0.7661, test_acc: 0.7903, best: 0.7970, time: 0:00:41
 Epoch: 152, lr: 1.0e-02, train_loss: 0.3732, train_acc: 0.8718 test_loss: 0.7456, test_acc: 0.7955, best: 0.7970, time: 0:00:41
 Epoch: 153, lr: 1.0e-02, train_loss: 0.3532, train_acc: 0.8774 test_loss: 0.7088, test_acc: 0.7961, best: 0.7970, time: 0:00:41
 Epoch: 154, lr: 1.0e-02, train_loss: 0.3573, train_acc: 0.8764 test_loss: 0.6754, test_acc: 0.8037, best: 0.8037, time: 0:00:41
 Epoch: 155, lr: 1.0e-02, train_loss: 0.3412, train_acc: 0.8836 test_loss: 0.7295, test_acc: 0.7935, best: 0.8037, time: 0:00:41
 Epoch: 156, lr: 1.0e-02, train_loss: 0.3645, train_acc: 0.8702 test_loss: 0.7385, test_acc: 0.7847, best: 0.8037, time: 0:00:41
 Epoch: 157, lr: 1.0e-02, train_loss: 0.3537, train_acc: 0.8794 test_loss: 0.7189, test_acc: 0.8014, best: 0.8037, time: 0:00:41
 Epoch: 158, lr: 1.0e-02, train_loss: 0.3562, train_acc: 0.8768 test_loss: 0.6715, test_acc: 0.7977, best: 0.8037, time: 0:00:41
 Epoch: 159, lr: 1.0e-02, train_loss: 0.3540, train_acc: 0.8810 test_loss: 0.7674, test_acc: 0.7934, best: 0.8037, time: 0:00:41
 Epoch: 160, lr: 1.0e-02, train_loss: 0.3465, train_acc: 0.8830 test_loss: 0.8547, test_acc: 0.7731, best: 0.8037, time: 0:00:41
 Epoch: 161, lr: 1.0e-02, train_loss: 0.3508, train_acc: 0.8812 test_loss: 0.7168, test_acc: 0.7967, best: 0.8037, time: 0:00:41
 Epoch: 162, lr: 1.0e-02, train_loss: 0.3576, train_acc: 0.8790 test_loss: 0.7693, test_acc: 0.7899, best: 0.8037, time: 0:00:41
 Epoch: 163, lr: 1.0e-02, train_loss: 0.3354, train_acc: 0.8842 test_loss: 0.7670, test_acc: 0.7964, best: 0.8037, time: 0:00:41
 Epoch: 164, lr: 1.0e-02, train_loss: 0.3503, train_acc: 0.8770 test_loss: 0.8355, test_acc: 0.7764, best: 0.8037, time: 0:00:41
 Epoch: 165, lr: 1.0e-02, train_loss: 0.3525, train_acc: 0.8770 test_loss: 0.7945, test_acc: 0.7819, best: 0.8037, time: 0:00:41
 Epoch: 166, lr: 1.0e-02, train_loss: 0.3446, train_acc: 0.8784 test_loss: 0.7781, test_acc: 0.7817, best: 0.8037, time: 0:00:41
 Epoch: 167, lr: 1.0e-02, train_loss: 0.3264, train_acc: 0.8902 test_loss: 0.7447, test_acc: 0.7980, best: 0.8037, time: 0:00:41
 Epoch: 168, lr: 1.0e-02, train_loss: 0.3389, train_acc: 0.8854 test_loss: 0.8325, test_acc: 0.7920, best: 0.8037, time: 0:00:41
 Epoch: 169, lr: 1.0e-02, train_loss: 0.3069, train_acc: 0.8896 test_loss: 0.8910, test_acc: 0.7873, best: 0.8037, time: 0:00:41
 Epoch: 170, lr: 1.0e-02, train_loss: 0.3458, train_acc: 0.8804 test_loss: 0.7591, test_acc: 0.7884, best: 0.8037, time: 0:00:41
 Epoch: 171, lr: 1.0e-02, train_loss: 0.3261, train_acc: 0.8884 test_loss: 0.8018, test_acc: 0.8011, best: 0.8037, time: 0:00:41
 Epoch: 172, lr: 1.0e-02, train_loss: 0.3400, train_acc: 0.8800 test_loss: 0.7045, test_acc: 0.8016, best: 0.8037, time: 0:00:41
 Epoch: 173, lr: 1.0e-02, train_loss: 0.3438, train_acc: 0.8822 test_loss: 0.7037, test_acc: 0.7954, best: 0.8037, time: 0:00:41
 Epoch: 174, lr: 1.0e-02, train_loss: 0.3201, train_acc: 0.8884 test_loss: 0.7361, test_acc: 0.8061, best: 0.8061, time: 0:00:41
 Epoch: 175, lr: 1.0e-02, train_loss: 0.3131, train_acc: 0.8952 test_loss: 0.8078, test_acc: 0.7875, best: 0.8061, time: 0:00:41
 Epoch: 176, lr: 1.0e-02, train_loss: 0.3257, train_acc: 0.8906 test_loss: 0.7480, test_acc: 0.7979, best: 0.8061, time: 0:00:41
 Epoch: 177, lr: 1.0e-02, train_loss: 0.3065, train_acc: 0.8936 test_loss: 0.7259, test_acc: 0.8083, best: 0.8083, time: 0:00:41
 Epoch: 178, lr: 1.0e-02, train_loss: 0.3131, train_acc: 0.8918 test_loss: 0.8854, test_acc: 0.7701, best: 0.8083, time: 0:00:41
 Epoch: 179, lr: 1.0e-02, train_loss: 0.3262, train_acc: 0.8866 test_loss: 0.7637, test_acc: 0.7970, best: 0.8083, time: 0:00:41
 Epoch: 180, lr: 2.0e-03, train_loss: 0.2726, train_acc: 0.9090 test_loss: 0.6965, test_acc: 0.8120, best: 0.8120, time: 0:00:41
 Epoch: 181, lr: 2.0e-03, train_loss: 0.2440, train_acc: 0.9168 test_loss: 0.7335, test_acc: 0.8094, best: 0.8120, time: 0:00:41
 Epoch: 182, lr: 2.0e-03, train_loss: 0.2347, train_acc: 0.9204 test_loss: 0.7131, test_acc: 0.8135, best: 0.8135, time: 0:00:41
 Epoch: 183, lr: 2.0e-03, train_loss: 0.2152, train_acc: 0.9250 test_loss: 0.7161, test_acc: 0.8151, best: 0.8151, time: 0:00:41
 Epoch: 184, lr: 2.0e-03, train_loss: 0.2145, train_acc: 0.9288 test_loss: 0.7206, test_acc: 0.8123, best: 0.8151, time: 0:00:41
 Epoch: 185, lr: 2.0e-03, train_loss: 0.2188, train_acc: 0.9228 test_loss: 0.7327, test_acc: 0.8170, best: 0.8170, time: 0:00:41
 Epoch: 186, lr: 2.0e-03, train_loss: 0.2198, train_acc: 0.9236 test_loss: 0.6949, test_acc: 0.8249, best: 0.8249, time: 0:00:41
 Epoch: 187, lr: 2.0e-03, train_loss: 0.2183, train_acc: 0.9286 test_loss: 0.7133, test_acc: 0.8173, best: 0.8249, time: 0:00:41
 Epoch: 188, lr: 2.0e-03, train_loss: 0.1958, train_acc: 0.9292 test_loss: 0.7136, test_acc: 0.8204, best: 0.8249, time: 0:00:41
 Epoch: 189, lr: 2.0e-03, train_loss: 0.1977, train_acc: 0.9324 test_loss: 0.7514, test_acc: 0.8170, best: 0.8249, time: 0:00:41
 Epoch: 190, lr: 2.0e-03, train_loss: 0.2148, train_acc: 0.9290 test_loss: 0.7152, test_acc: 0.8194, best: 0.8249, time: 0:00:41
 Epoch: 191, lr: 2.0e-03, train_loss: 0.1931, train_acc: 0.9394 test_loss: 0.7636, test_acc: 0.8126, best: 0.8249, time: 0:00:41
 Epoch: 192, lr: 2.0e-03, train_loss: 0.1969, train_acc: 0.9326 test_loss: 0.7283, test_acc: 0.8196, best: 0.8249, time: 0:00:41
 Epoch: 193, lr: 2.0e-03, train_loss: 0.2190, train_acc: 0.9278 test_loss: 0.7140, test_acc: 0.8135, best: 0.8249, time: 0:00:41
 Epoch: 194, lr: 2.0e-03, train_loss: 0.1992, train_acc: 0.9304 test_loss: 0.7136, test_acc: 0.8215, best: 0.8249, time: 0:00:41
 Epoch: 195, lr: 2.0e-03, train_loss: 0.1914, train_acc: 0.9348 test_loss: 0.7202, test_acc: 0.8187, best: 0.8249, time: 0:00:41
 Epoch: 196, lr: 2.0e-03, train_loss: 0.1997, train_acc: 0.9342 test_loss: 0.7599, test_acc: 0.8174, best: 0.8249, time: 0:00:41
 Epoch: 197, lr: 2.0e-03, train_loss: 0.2155, train_acc: 0.9298 test_loss: 0.7356, test_acc: 0.8159, best: 0.8249, time: 0:00:41
 Epoch: 198, lr: 2.0e-03, train_loss: 0.1924, train_acc: 0.9356 test_loss: 0.7069, test_acc: 0.8196, best: 0.8249, time: 0:00:41
 Epoch: 199, lr: 2.0e-03, train_loss: 0.2022, train_acc: 0.9342 test_loss: 0.7378, test_acc: 0.8146, best: 0.8249, time: 0:00:41
 Epoch: 200, lr: 2.0e-03, train_loss: 0.2041, train_acc: 0.9328 test_loss: 0.7279, test_acc: 0.8154, best: 0.8249, time: 0:00:41
 Epoch: 201, lr: 2.0e-03, train_loss: 0.1981, train_acc: 0.9290 test_loss: 0.7529, test_acc: 0.8090, best: 0.8249, time: 0:00:41
 Epoch: 202, lr: 2.0e-03, train_loss: 0.1809, train_acc: 0.9382 test_loss: 0.7555, test_acc: 0.8121, best: 0.8249, time: 0:00:41
 Epoch: 203, lr: 2.0e-03, train_loss: 0.1995, train_acc: 0.9300 test_loss: 0.7218, test_acc: 0.8173, best: 0.8249, time: 0:00:41
 Epoch: 204, lr: 2.0e-03, train_loss: 0.1856, train_acc: 0.9362 test_loss: 0.7353, test_acc: 0.8184, best: 0.8249, time: 0:00:41
 Epoch: 205, lr: 2.0e-03, train_loss: 0.2046, train_acc: 0.9286 test_loss: 0.7513, test_acc: 0.8197, best: 0.8249, time: 0:00:41
 Epoch: 206, lr: 2.0e-03, train_loss: 0.1768, train_acc: 0.9458 test_loss: 0.7494, test_acc: 0.8179, best: 0.8249, time: 0:00:41
 Epoch: 207, lr: 2.0e-03, train_loss: 0.1964, train_acc: 0.9316 test_loss: 0.7757, test_acc: 0.8115, best: 0.8249, time: 0:00:41
 Epoch: 208, lr: 2.0e-03, train_loss: 0.1870, train_acc: 0.9378 test_loss: 0.7266, test_acc: 0.8150, best: 0.8249, time: 0:00:41
 Epoch: 209, lr: 2.0e-03, train_loss: 0.1977, train_acc: 0.9312 test_loss: 0.7027, test_acc: 0.8165, best: 0.8249, time: 0:00:41
 Epoch: 210, lr: 2.0e-03, train_loss: 0.1761, train_acc: 0.9412 test_loss: 0.7344, test_acc: 0.8176, best: 0.8249, time: 0:00:41
 Epoch: 211, lr: 2.0e-03, train_loss: 0.1694, train_acc: 0.9428 test_loss: 0.7264, test_acc: 0.8195, best: 0.8249, time: 0:00:41
 Epoch: 212, lr: 2.0e-03, train_loss: 0.1943, train_acc: 0.9370 test_loss: 0.7336, test_acc: 0.8105, best: 0.8249, time: 0:00:41
 Epoch: 213, lr: 2.0e-03, train_loss: 0.1855, train_acc: 0.9394 test_loss: 0.7141, test_acc: 0.8171, best: 0.8249, time: 0:00:41
 Epoch: 214, lr: 2.0e-03, train_loss: 0.1847, train_acc: 0.9370 test_loss: 0.7148, test_acc: 0.8236, best: 0.8249, time: 0:00:41
 Epoch: 215, lr: 2.0e-03, train_loss: 0.1899, train_acc: 0.9334 test_loss: 0.7085, test_acc: 0.8177, best: 0.8249, time: 0:00:41
 Epoch: 216, lr: 2.0e-03, train_loss: 0.1863, train_acc: 0.9412 test_loss: 0.7163, test_acc: 0.8190, best: 0.8249, time: 0:00:41
 Epoch: 217, lr: 2.0e-03, train_loss: 0.1860, train_acc: 0.9382 test_loss: 0.7461, test_acc: 0.8139, best: 0.8249, time: 0:00:41
 Epoch: 218, lr: 2.0e-03, train_loss: 0.1874, train_acc: 0.9382 test_loss: 0.7444, test_acc: 0.8077, best: 0.8249, time: 0:00:41
 Epoch: 219, lr: 2.0e-03, train_loss: 0.1896, train_acc: 0.9376 test_loss: 0.7318, test_acc: 0.8130, best: 0.8249, time: 0:00:41
 Epoch: 220, lr: 2.0e-03, train_loss: 0.1924, train_acc: 0.9322 test_loss: 0.7369, test_acc: 0.8186, best: 0.8249, time: 0:00:41
 Epoch: 221, lr: 2.0e-03, train_loss: 0.1799, train_acc: 0.9364 test_loss: 0.7267, test_acc: 0.8159, best: 0.8249, time: 0:00:41
 Epoch: 222, lr: 2.0e-03, train_loss: 0.1782, train_acc: 0.9430 test_loss: 0.6886, test_acc: 0.8220, best: 0.8249, time: 0:00:41
 Epoch: 223, lr: 2.0e-03, train_loss: 0.1940, train_acc: 0.9356 test_loss: 0.7137, test_acc: 0.8181, best: 0.8249, time: 0:00:41
 Epoch: 224, lr: 2.0e-03, train_loss: 0.1711, train_acc: 0.9392 test_loss: 0.7455, test_acc: 0.8115, best: 0.8249, time: 0:00:41
 Epoch: 225, lr: 2.0e-03, train_loss: 0.1849, train_acc: 0.9384 test_loss: 0.7925, test_acc: 0.8110, best: 0.8249, time: 0:00:41
 Epoch: 226, lr: 2.0e-03, train_loss: 0.1925, train_acc: 0.9360 test_loss: 0.7315, test_acc: 0.8164, best: 0.8249, time: 0:00:41
 Epoch: 227, lr: 2.0e-03, train_loss: 0.1837, train_acc: 0.9398 test_loss: 0.7241, test_acc: 0.8206, best: 0.8249, time: 0:00:41
 Epoch: 228, lr: 2.0e-03, train_loss: 0.1797, train_acc: 0.9412 test_loss: 0.7656, test_acc: 0.8116, best: 0.8249, time: 0:00:41
 Epoch: 229, lr: 2.0e-03, train_loss: 0.1895, train_acc: 0.9334 test_loss: 0.7237, test_acc: 0.8160, best: 0.8249, time: 0:00:41
 Epoch: 230, lr: 2.0e-03, train_loss: 0.1763, train_acc: 0.9390 test_loss: 0.7383, test_acc: 0.8189, best: 0.8249, time: 0:00:41
 Epoch: 231, lr: 2.0e-03, train_loss: 0.1858, train_acc: 0.9378 test_loss: 0.7354, test_acc: 0.8133, best: 0.8249, time: 0:00:41
 Epoch: 232, lr: 2.0e-03, train_loss: 0.1911, train_acc: 0.9392 test_loss: 0.7766, test_acc: 0.8093, best: 0.8249, time: 0:00:41
 Epoch: 233, lr: 2.0e-03, train_loss: 0.1821, train_acc: 0.9390 test_loss: 0.7536, test_acc: 0.8151, best: 0.8249, time: 0:00:41
 Epoch: 234, lr: 2.0e-03, train_loss: 0.1663, train_acc: 0.9438 test_loss: 0.7922, test_acc: 0.8044, best: 0.8249, time: 0:00:41
 Epoch: 235, lr: 2.0e-03, train_loss: 0.1765, train_acc: 0.9358 test_loss: 0.7319, test_acc: 0.8170, best: 0.8249, time: 0:00:41
 Epoch: 236, lr: 2.0e-03, train_loss: 0.1859, train_acc: 0.9388 test_loss: 0.7880, test_acc: 0.8031, best: 0.8249, time: 0:00:41
 Epoch: 237, lr: 2.0e-03, train_loss: 0.1896, train_acc: 0.9382 test_loss: 0.7398, test_acc: 0.8076, best: 0.8249, time: 0:00:41
 Epoch: 238, lr: 2.0e-03, train_loss: 0.1793, train_acc: 0.9372 test_loss: 0.7508, test_acc: 0.8103, best: 0.8249, time: 0:00:41
 Epoch: 239, lr: 2.0e-03, train_loss: 0.1777, train_acc: 0.9372 test_loss: 0.7446, test_acc: 0.8146, best: 0.8249, time: 0:00:41
 Epoch: 240, lr: 4.0e-04, train_loss: 0.1602, train_acc: 0.9430 test_loss: 0.7792, test_acc: 0.8090, best: 0.8249, time: 0:00:41
 Epoch: 241, lr: 4.0e-04, train_loss: 0.1735, train_acc: 0.9394 test_loss: 0.7313, test_acc: 0.8154, best: 0.8249, time: 0:00:41
 Epoch: 242, lr: 4.0e-04, train_loss: 0.1687, train_acc: 0.9388 test_loss: 0.7400, test_acc: 0.8134, best: 0.8249, time: 0:00:41
 Epoch: 243, lr: 4.0e-04, train_loss: 0.1725, train_acc: 0.9386 test_loss: 0.7299, test_acc: 0.8164, best: 0.8249, time: 0:00:41
 Epoch: 244, lr: 4.0e-04, train_loss: 0.1576, train_acc: 0.9486 test_loss: 0.7181, test_acc: 0.8199, best: 0.8249, time: 0:00:41
 Epoch: 245, lr: 4.0e-04, train_loss: 0.1634, train_acc: 0.9466 test_loss: 0.7234, test_acc: 0.8181, best: 0.8249, time: 0:00:41
 Epoch: 246, lr: 4.0e-04, train_loss: 0.1483, train_acc: 0.9478 test_loss: 0.7203, test_acc: 0.8203, best: 0.8249, time: 0:00:41
 Epoch: 247, lr: 4.0e-04, train_loss: 0.1587, train_acc: 0.9466 test_loss: 0.7309, test_acc: 0.8175, best: 0.8249, time: 0:00:41
 Epoch: 248, lr: 4.0e-04, train_loss: 0.1656, train_acc: 0.9430 test_loss: 0.7253, test_acc: 0.8196, best: 0.8249, time: 0:00:41
 Epoch: 249, lr: 4.0e-04, train_loss: 0.1663, train_acc: 0.9418 test_loss: 0.7321, test_acc: 0.8204, best: 0.8249, time: 0:00:41
 Epoch: 250, lr: 4.0e-04, train_loss: 0.1641, train_acc: 0.9450 test_loss: 0.7480, test_acc: 0.8184, best: 0.8249, time: 0:00:41
 Epoch: 251, lr: 4.0e-04, train_loss: 0.1576, train_acc: 0.9458 test_loss: 0.7420, test_acc: 0.8159, best: 0.8249, time: 0:00:41
 Epoch: 252, lr: 4.0e-04, train_loss: 0.1573, train_acc: 0.9474 test_loss: 0.7201, test_acc: 0.8221, best: 0.8249, time: 0:00:41
 Epoch: 253, lr: 4.0e-04, train_loss: 0.1550, train_acc: 0.9508 test_loss: 0.7447, test_acc: 0.8174, best: 0.8249, time: 0:00:41
 Epoch: 254, lr: 4.0e-04, train_loss: 0.1503, train_acc: 0.9518 test_loss: 0.7645, test_acc: 0.8134, best: 0.8249, time: 0:00:41
 Epoch: 255, lr: 4.0e-04, train_loss: 0.1593, train_acc: 0.9472 test_loss: 0.7269, test_acc: 0.8169, best: 0.8249, time: 0:00:41
 Epoch: 256, lr: 4.0e-04, train_loss: 0.1565, train_acc: 0.9464 test_loss: 0.7349, test_acc: 0.8233, best: 0.8249, time: 0:00:41
 Epoch: 257, lr: 4.0e-04, train_loss: 0.1642, train_acc: 0.9424 test_loss: 0.7163, test_acc: 0.8206, best: 0.8249, time: 0:00:41
 Epoch: 258, lr: 4.0e-04, train_loss: 0.1661, train_acc: 0.9438 test_loss: 0.7460, test_acc: 0.8147, best: 0.8249, time: 0:00:41
 Epoch: 259, lr: 4.0e-04, train_loss: 0.1638, train_acc: 0.9460 test_loss: 0.7289, test_acc: 0.8203, best: 0.8249, time: 0:00:41
 Epoch: 260, lr: 4.0e-04, train_loss: 0.1571, train_acc: 0.9472 test_loss: 0.7232, test_acc: 0.8176, best: 0.8249, time: 0:00:41
 Epoch: 261, lr: 4.0e-04, train_loss: 0.1630, train_acc: 0.9436 test_loss: 0.7423, test_acc: 0.8166, best: 0.8249, time: 0:00:41
 Epoch: 262, lr: 4.0e-04, train_loss: 0.1742, train_acc: 0.9376 test_loss: 0.7662, test_acc: 0.8139, best: 0.8249, time: 0:00:41
 Epoch: 263, lr: 4.0e-04, train_loss: 0.1544, train_acc: 0.9440 test_loss: 0.7336, test_acc: 0.8149, best: 0.8249, time: 0:00:41
 Epoch: 264, lr: 4.0e-04, train_loss: 0.1539, train_acc: 0.9472 test_loss: 0.7370, test_acc: 0.8157, best: 0.8249, time: 0:00:41
 Epoch: 265, lr: 4.0e-04, train_loss: 0.1598, train_acc: 0.9440 test_loss: 0.7475, test_acc: 0.8160, best: 0.8249, time: 0:00:41
 Epoch: 266, lr: 4.0e-04, train_loss: 0.1504, train_acc: 0.9474 test_loss: 0.7464, test_acc: 0.8180, best: 0.8249, time: 0:00:41
 Epoch: 267, lr: 4.0e-04, train_loss: 0.1565, train_acc: 0.9476 test_loss: 0.7409, test_acc: 0.8139, best: 0.8249, time: 0:00:41
 Epoch: 268, lr: 4.0e-04, train_loss: 0.1607, train_acc: 0.9448 test_loss: 0.7431, test_acc: 0.8146, best: 0.8249, time: 0:00:41
 Epoch: 269, lr: 4.0e-04, train_loss: 0.1474, train_acc: 0.9494 test_loss: 0.7342, test_acc: 0.8154, best: 0.8249, time: 0:00:41
 Epoch: 270, lr: 8.0e-05, train_loss: 0.1571, train_acc: 0.9496 test_loss: 0.7570, test_acc: 0.8169, best: 0.8249, time: 0:00:41
 Epoch: 271, lr: 8.0e-05, train_loss: 0.1492, train_acc: 0.9498 test_loss: 0.7541, test_acc: 0.8137, best: 0.8249, time: 0:00:41
 Epoch: 272, lr: 8.0e-05, train_loss: 0.1413, train_acc: 0.9520 test_loss: 0.7330, test_acc: 0.8170, best: 0.8249, time: 0:00:41
 Epoch: 273, lr: 8.0e-05, train_loss: 0.1480, train_acc: 0.9492 test_loss: 0.7368, test_acc: 0.8167, best: 0.8249, time: 0:00:41
 Epoch: 274, lr: 8.0e-05, train_loss: 0.1606, train_acc: 0.9462 test_loss: 0.7322, test_acc: 0.8189, best: 0.8249, time: 0:00:41
 Epoch: 275, lr: 8.0e-05, train_loss: 0.1522, train_acc: 0.9498 test_loss: 0.7392, test_acc: 0.8177, best: 0.8249, time: 0:00:41
 Epoch: 276, lr: 8.0e-05, train_loss: 0.1551, train_acc: 0.9490 test_loss: 0.7507, test_acc: 0.8161, best: 0.8249, time: 0:00:41
 Epoch: 277, lr: 8.0e-05, train_loss: 0.1499, train_acc: 0.9518 test_loss: 0.7442, test_acc: 0.8170, best: 0.8249, time: 0:00:41
 Epoch: 278, lr: 8.0e-05, train_loss: 0.1759, train_acc: 0.9414 test_loss: 0.7329, test_acc: 0.8180, best: 0.8249, time: 0:00:41
 Epoch: 279, lr: 8.0e-05, train_loss: 0.1519, train_acc: 0.9476 test_loss: 0.7509, test_acc: 0.8160, best: 0.8249, time: 0:00:41
 Epoch: 280, lr: 8.0e-05, train_loss: 0.1599, train_acc: 0.9448 test_loss: 0.7506, test_acc: 0.8154, best: 0.8249, time: 0:00:41
 Epoch: 281, lr: 8.0e-05, train_loss: 0.1526, train_acc: 0.9498 test_loss: 0.7373, test_acc: 0.8167, best: 0.8249, time: 0:00:41
 Epoch: 282, lr: 8.0e-05, train_loss: 0.1593, train_acc: 0.9428 test_loss: 0.7325, test_acc: 0.8156, best: 0.8249, time: 0:00:40
 Epoch: 283, lr: 8.0e-05, train_loss: 0.1627, train_acc: 0.9450 test_loss: 0.7250, test_acc: 0.8185, best: 0.8249, time: 0:00:40
 Epoch: 284, lr: 8.0e-05, train_loss: 0.1464, train_acc: 0.9484 test_loss: 0.7300, test_acc: 0.8144, best: 0.8249, time: 0:00:40
 Epoch: 285, lr: 8.0e-05, train_loss: 0.1624, train_acc: 0.9452 test_loss: 0.7223, test_acc: 0.8157, best: 0.8249, time: 0:00:40
 Epoch: 286, lr: 8.0e-05, train_loss: 0.1565, train_acc: 0.9476 test_loss: 0.7340, test_acc: 0.8186, best: 0.8249, time: 0:00:40
 Epoch: 287, lr: 8.0e-05, train_loss: 0.1538, train_acc: 0.9470 test_loss: 0.7481, test_acc: 0.8146, best: 0.8249, time: 0:00:40
 Epoch: 288, lr: 8.0e-05, train_loss: 0.1508, train_acc: 0.9480 test_loss: 0.7296, test_acc: 0.8203, best: 0.8249, time: 0:00:40
 Epoch: 289, lr: 8.0e-05, train_loss: 0.1583, train_acc: 0.9480 test_loss: 0.7434, test_acc: 0.8167, best: 0.8249, time: 0:00:40
 Epoch: 290, lr: 8.0e-05, train_loss: 0.1571, train_acc: 0.9478 test_loss: 0.7474, test_acc: 0.8125, best: 0.8249, time: 0:00:40
 Epoch: 291, lr: 8.0e-05, train_loss: 0.1483, train_acc: 0.9488 test_loss: 0.7342, test_acc: 0.8207, best: 0.8249, time: 0:00:40
 Epoch: 292, lr: 8.0e-05, train_loss: 0.1565, train_acc: 0.9504 test_loss: 0.7409, test_acc: 0.8176, best: 0.8249, time: 0:00:40
 Epoch: 293, lr: 8.0e-05, train_loss: 0.1638, train_acc: 0.9450 test_loss: 0.7320, test_acc: 0.8171, best: 0.8249, time: 0:00:40
 Epoch: 294, lr: 8.0e-05, train_loss: 0.1595, train_acc: 0.9488 test_loss: 0.7538, test_acc: 0.8166, best: 0.8249, time: 0:00:40
 Epoch: 295, lr: 8.0e-05, train_loss: 0.1668, train_acc: 0.9414 test_loss: 0.7438, test_acc: 0.8135, best: 0.8249, time: 0:00:40
 Epoch: 296, lr: 8.0e-05, train_loss: 0.1558, train_acc: 0.9484 test_loss: 0.7439, test_acc: 0.8144, best: 0.8249, time: 0:00:40
 Epoch: 297, lr: 8.0e-05, train_loss: 0.1579, train_acc: 0.9458 test_loss: 0.7191, test_acc: 0.8215, best: 0.8249, time: 0:00:40
 Epoch: 298, lr: 8.0e-05, train_loss: 0.1490, train_acc: 0.9474 test_loss: 0.7497, test_acc: 0.8174, best: 0.8249, time: 0:00:40
 Epoch: 299, lr: 8.0e-05, train_loss: 0.1493, train_acc: 0.9506 test_loss: 0.7211, test_acc: 0.8193, best: 0.8249, time: 0:00:40
 Highest accuracy: 0.8249