
 Run on time: 2022-07-01 02:59:33.296356

 Architecture: mobilenet_v2-gaussian_pool-2121

 Pool Config: {
    "arch": "mobilenet_v2",
    "conv1": {
        "_conv2d": "norm",
        "pool_cfg": {}
    },
    "layer1": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "gaussian_pool",
            "_stride": 2,
            "_psize": null,
            "_dim_reduced_ratio": null,
            "_num_heads": null,
            "_conv2d": null,
            "_win_norm": null
        }
    },
    "layer2": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "skip",
            "_stride": 1,
            "_psize": null,
            "_dim_reduced_ratio": null,
            "_num_heads": null,
            "_conv2d": null,
            "_win_norm": null
        }
    },
    "layer3": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "gaussian_pool",
            "_stride": 2,
            "_psize": null,
            "_dim_reduced_ratio": null,
            "_num_heads": null,
            "_conv2d": null,
            "_win_norm": null
        }
    },
    "layer4": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "skip",
            "_stride": 1,
            "_psize": null,
            "_dim_reduced_ratio": null,
            "_num_heads": null,
            "_conv2d": null,
            "_win_norm": null
        }
    },
    "conv2": {
        "_conv2d": "norm",
        "pool_cfg": {}
    }
}

 Arguments:
	 gpu                  : True
	 log                  : True
	 seed                 : 0
	 momentum             : 0.9
	 amsgrad              : True
	 dataset              : STL10
	 batch_size           : 8
	 architecture         : MOBILENET_V2-GAUSSIAN_POOL-2121
	 im_size              : None
	 relu_threshold       : 4.0
	 learning_rate        : 0.01
	 pretrained_backbone  : 
	 pretrained_ann       : 
	 weight_decay         : 0.0
	 test_only            : False
	 epochs               : 300
	 lr_interval          : [180, 240, 270]
	 lr_reduce            : 5
	 optimizer            : SGD
	 dropout              : 0.2
	 kernel_size          : 3
	 dont_save            : False
	 visualize            : False
	 devices              : 0
 DataParallel(
  (module): Network(
    (net): MobileNetV2(
      (features): Sequential(
        (0): Sequential(
          (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): InvertedResidual(
          (pooling): GaussianPooling2d(
            kernel_size=2, stride=2, padding=0
            (ToHidden): Sequential(
              (0): AdaptiveAvgPool2d(output_size=(1, 1))
              (1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (3): ReLU()
            )
            (ToMean): Sequential(
              (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (ToSigma): Sequential(
              (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Sigmoid()
            )
            (activation): Softplus(beta=1, threshold=20)
          )
          (conv): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (2): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (3): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (4): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (5): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (6): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
            (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (7): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
            (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (8): InvertedResidual(
          (pooling): GaussianPooling2d(
            kernel_size=2, stride=2, padding=0
            (ToHidden): Sequential(
              (0): AdaptiveAvgPool2d(output_size=(1, 1))
              (1): Conv2d(160, 80, kernel_size=(1, 1), stride=(1, 1))
              (2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (3): ReLU()
            )
            (ToMean): Sequential(
              (0): Conv2d(80, 160, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (ToSigma): Sequential(
              (0): Conv2d(80, 160, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Sigmoid()
            )
            (activation): Softplus(beta=1, threshold=20)
          )
          (conv): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
            (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (9): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
            (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (10): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
            (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (11): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
            (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
      (conv): Sequential(
        (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
      (classifier): Linear(in_features=1280, out_features=10, bias=True)
    )
  )
)
 SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0
)
 Epoch: 1, lr: 1.0e-02, train_loss: 2.3461, train_acc: 0.2114 test_loss: 1.8925, test_acc: 0.2691, best: 0.2691, time: 0:00:42
 Epoch: 2, lr: 1.0e-02, train_loss: 1.9284, train_acc: 0.2696 test_loss: 1.6585, test_acc: 0.3448, best: 0.3448, time: 0:00:40
 Epoch: 3, lr: 1.0e-02, train_loss: 1.8243, train_acc: 0.3084 test_loss: 1.6442, test_acc: 0.3653, best: 0.3653, time: 0:00:41
 Epoch: 4, lr: 1.0e-02, train_loss: 1.7645, train_acc: 0.3388 test_loss: 1.5762, test_acc: 0.4009, best: 0.4009, time: 0:00:40
 Epoch: 5, lr: 1.0e-02, train_loss: 1.6830, train_acc: 0.3676 test_loss: 1.4968, test_acc: 0.4305, best: 0.4305, time: 0:00:40
 Epoch: 6, lr: 1.0e-02, train_loss: 1.6179, train_acc: 0.4020 test_loss: 1.4486, test_acc: 0.4375, best: 0.4375, time: 0:00:40
 Epoch: 7, lr: 1.0e-02, train_loss: 1.5683, train_acc: 0.4172 test_loss: 1.3267, test_acc: 0.5006, best: 0.5006, time: 0:00:40
 Epoch: 8, lr: 1.0e-02, train_loss: 1.5337, train_acc: 0.4288 test_loss: 1.2783, test_acc: 0.5284, best: 0.5284, time: 0:00:40
 Epoch: 9, lr: 1.0e-02, train_loss: 1.4829, train_acc: 0.4638 test_loss: 1.3357, test_acc: 0.5032, best: 0.5284, time: 0:00:40
 Epoch: 10, lr: 1.0e-02, train_loss: 1.4216, train_acc: 0.4720 test_loss: 1.2639, test_acc: 0.5394, best: 0.5394, time: 0:00:41
 Epoch: 11, lr: 1.0e-02, train_loss: 1.3772, train_acc: 0.4982 test_loss: 1.1826, test_acc: 0.5530, best: 0.5530, time: 0:00:41
 Epoch: 12, lr: 1.0e-02, train_loss: 1.3454, train_acc: 0.5122 test_loss: 1.2061, test_acc: 0.5481, best: 0.5530, time: 0:00:40
 Epoch: 13, lr: 1.0e-02, train_loss: 1.3067, train_acc: 0.5256 test_loss: 1.0825, test_acc: 0.6035, best: 0.6035, time: 0:00:41
 Epoch: 14, lr: 1.0e-02, train_loss: 1.2747, train_acc: 0.5320 test_loss: 1.1409, test_acc: 0.5844, best: 0.6035, time: 0:00:41
 Epoch: 15, lr: 1.0e-02, train_loss: 1.2606, train_acc: 0.5378 test_loss: 1.1050, test_acc: 0.5911, best: 0.6035, time: 0:00:41
 Epoch: 16, lr: 1.0e-02, train_loss: 1.2145, train_acc: 0.5618 test_loss: 1.0551, test_acc: 0.6191, best: 0.6191, time: 0:00:41
 Epoch: 17, lr: 1.0e-02, train_loss: 1.1820, train_acc: 0.5846 test_loss: 1.0768, test_acc: 0.6061, best: 0.6191, time: 0:00:40
 Epoch: 18, lr: 1.0e-02, train_loss: 1.1699, train_acc: 0.5772 test_loss: 0.9805, test_acc: 0.6445, best: 0.6445, time: 0:00:40
 Epoch: 19, lr: 1.0e-02, train_loss: 1.1319, train_acc: 0.5886 test_loss: 0.9649, test_acc: 0.6484, best: 0.6484, time: 0:00:41
 Epoch: 20, lr: 1.0e-02, train_loss: 1.1489, train_acc: 0.5856 test_loss: 1.0086, test_acc: 0.6440, best: 0.6484, time: 0:00:41
 Epoch: 21, lr: 1.0e-02, train_loss: 1.1127, train_acc: 0.5940 test_loss: 0.9881, test_acc: 0.6420, best: 0.6484, time: 0:00:41
 Epoch: 22, lr: 1.0e-02, train_loss: 1.0914, train_acc: 0.6074 test_loss: 1.0682, test_acc: 0.6180, best: 0.6484, time: 0:00:40
 Epoch: 23, lr: 1.0e-02, train_loss: 1.0809, train_acc: 0.6094 test_loss: 1.0224, test_acc: 0.6259, best: 0.6484, time: 0:00:40
 Epoch: 24, lr: 1.0e-02, train_loss: 1.0655, train_acc: 0.6226 test_loss: 0.9501, test_acc: 0.6624, best: 0.6624, time: 0:00:40
 Epoch: 25, lr: 1.0e-02, train_loss: 1.0501, train_acc: 0.6178 test_loss: 0.9399, test_acc: 0.6584, best: 0.6624, time: 0:00:40
 Epoch: 26, lr: 1.0e-02, train_loss: 1.0360, train_acc: 0.6310 test_loss: 0.9344, test_acc: 0.6695, best: 0.6695, time: 0:00:40
 Epoch: 27, lr: 1.0e-02, train_loss: 1.0026, train_acc: 0.6378 test_loss: 0.9412, test_acc: 0.6685, best: 0.6695, time: 0:00:40
 Epoch: 28, lr: 1.0e-02, train_loss: 0.9871, train_acc: 0.6408 test_loss: 0.9144, test_acc: 0.6761, best: 0.6761, time: 0:00:40
 Epoch: 29, lr: 1.0e-02, train_loss: 0.9732, train_acc: 0.6524 test_loss: 0.9027, test_acc: 0.6705, best: 0.6761, time: 0:00:41
 Epoch: 30, lr: 1.0e-02, train_loss: 0.9556, train_acc: 0.6580 test_loss: 0.8669, test_acc: 0.6913, best: 0.6913, time: 0:00:40
 Epoch: 31, lr: 1.0e-02, train_loss: 0.9456, train_acc: 0.6636 test_loss: 0.8871, test_acc: 0.6781, best: 0.6913, time: 0:00:40
 Epoch: 32, lr: 1.0e-02, train_loss: 0.9321, train_acc: 0.6710 test_loss: 0.8329, test_acc: 0.7077, best: 0.7077, time: 0:00:40
 Epoch: 33, lr: 1.0e-02, train_loss: 0.9132, train_acc: 0.6744 test_loss: 0.8949, test_acc: 0.6913, best: 0.7077, time: 0:00:41
 Epoch: 34, lr: 1.0e-02, train_loss: 0.9080, train_acc: 0.6784 test_loss: 0.8594, test_acc: 0.7065, best: 0.7077, time: 0:00:41
 Epoch: 35, lr: 1.0e-02, train_loss: 0.8802, train_acc: 0.6850 test_loss: 0.8820, test_acc: 0.6939, best: 0.7077, time: 0:00:40
 Epoch: 36, lr: 1.0e-02, train_loss: 0.8716, train_acc: 0.6912 test_loss: 0.7998, test_acc: 0.7119, best: 0.7119, time: 0:00:40
 Epoch: 37, lr: 1.0e-02, train_loss: 0.8484, train_acc: 0.7022 test_loss: 0.8367, test_acc: 0.7109, best: 0.7119, time: 0:00:40
 Epoch: 38, lr: 1.0e-02, train_loss: 0.8712, train_acc: 0.6862 test_loss: 0.8126, test_acc: 0.7093, best: 0.7119, time: 0:00:40
 Epoch: 39, lr: 1.0e-02, train_loss: 0.8479, train_acc: 0.7002 test_loss: 0.9700, test_acc: 0.6910, best: 0.7119, time: 0:00:40
 Epoch: 40, lr: 1.0e-02, train_loss: 0.8500, train_acc: 0.7054 test_loss: 0.8254, test_acc: 0.7136, best: 0.7136, time: 0:00:40
 Epoch: 41, lr: 1.0e-02, train_loss: 0.8231, train_acc: 0.7048 test_loss: 0.8822, test_acc: 0.6987, best: 0.7136, time: 0:00:40
 Epoch: 42, lr: 1.0e-02, train_loss: 0.8171, train_acc: 0.7048 test_loss: 0.8513, test_acc: 0.7184, best: 0.7184, time: 0:00:40
 Epoch: 43, lr: 1.0e-02, train_loss: 0.8015, train_acc: 0.7184 test_loss: 0.7829, test_acc: 0.7265, best: 0.7265, time: 0:00:40
 Epoch: 44, lr: 1.0e-02, train_loss: 0.7929, train_acc: 0.7164 test_loss: 0.8186, test_acc: 0.7199, best: 0.7265, time: 0:00:40
 Epoch: 45, lr: 1.0e-02, train_loss: 0.7841, train_acc: 0.7174 test_loss: 0.7758, test_acc: 0.7262, best: 0.7265, time: 0:00:40
 Epoch: 46, lr: 1.0e-02, train_loss: 0.7860, train_acc: 0.7206 test_loss: 0.7036, test_acc: 0.7496, best: 0.7496, time: 0:00:41
 Epoch: 47, lr: 1.0e-02, train_loss: 0.7454, train_acc: 0.7360 test_loss: 0.7795, test_acc: 0.7269, best: 0.7496, time: 0:00:41
 Epoch: 48, lr: 1.0e-02, train_loss: 0.7733, train_acc: 0.7250 test_loss: 0.6947, test_acc: 0.7556, best: 0.7556, time: 0:00:41
 Epoch: 49, lr: 1.0e-02, train_loss: 0.7459, train_acc: 0.7372 test_loss: 0.7217, test_acc: 0.7534, best: 0.7556, time: 0:00:40
 Epoch: 50, lr: 1.0e-02, train_loss: 0.7348, train_acc: 0.7440 test_loss: 0.7458, test_acc: 0.7489, best: 0.7556, time: 0:00:40
 Epoch: 51, lr: 1.0e-02, train_loss: 0.7141, train_acc: 0.7478 test_loss: 0.7395, test_acc: 0.7445, best: 0.7556, time: 0:00:40
 Epoch: 52, lr: 1.0e-02, train_loss: 0.7140, train_acc: 0.7488 test_loss: 0.7322, test_acc: 0.7464, best: 0.7556, time: 0:00:40
 Epoch: 53, lr: 1.0e-02, train_loss: 0.7256, train_acc: 0.7374 test_loss: 0.7515, test_acc: 0.7472, best: 0.7556, time: 0:00:40
 Epoch: 54, lr: 1.0e-02, train_loss: 0.7102, train_acc: 0.7504 test_loss: 0.7129, test_acc: 0.7546, best: 0.7556, time: 0:00:41
 Epoch: 55, lr: 1.0e-02, train_loss: 0.6903, train_acc: 0.7562 test_loss: 0.7082, test_acc: 0.7619, best: 0.7619, time: 0:00:41
 Epoch: 56, lr: 1.0e-02, train_loss: 0.6764, train_acc: 0.7634 test_loss: 0.7387, test_acc: 0.7530, best: 0.7619, time: 0:00:40
 Epoch: 57, lr: 1.0e-02, train_loss: 0.6817, train_acc: 0.7594 test_loss: 0.6742, test_acc: 0.7720, best: 0.7720, time: 0:00:41
 Epoch: 58, lr: 1.0e-02, train_loss: 0.6465, train_acc: 0.7758 test_loss: 0.6552, test_acc: 0.7694, best: 0.7720, time: 0:00:41
 Epoch: 59, lr: 1.0e-02, train_loss: 0.6713, train_acc: 0.7582 test_loss: 0.7669, test_acc: 0.7382, best: 0.7720, time: 0:00:41
 Epoch: 60, lr: 1.0e-02, train_loss: 0.6615, train_acc: 0.7720 test_loss: 0.6802, test_acc: 0.7716, best: 0.7720, time: 0:00:41
 Epoch: 61, lr: 1.0e-02, train_loss: 0.6577, train_acc: 0.7722 test_loss: 0.7110, test_acc: 0.7585, best: 0.7720, time: 0:00:41
 Epoch: 62, lr: 1.0e-02, train_loss: 0.6569, train_acc: 0.7750 test_loss: 0.7363, test_acc: 0.7572, best: 0.7720, time: 0:00:41
 Epoch: 63, lr: 1.0e-02, train_loss: 0.6241, train_acc: 0.7808 test_loss: 0.7083, test_acc: 0.7632, best: 0.7720, time: 0:00:40
 Epoch: 64, lr: 1.0e-02, train_loss: 0.6087, train_acc: 0.7876 test_loss: 0.8511, test_acc: 0.7312, best: 0.7720, time: 0:00:41
 Epoch: 65, lr: 1.0e-02, train_loss: 0.6196, train_acc: 0.7866 test_loss: 0.7500, test_acc: 0.7491, best: 0.7720, time: 0:00:41
 Epoch: 66, lr: 1.0e-02, train_loss: 0.6355, train_acc: 0.7764 test_loss: 0.6732, test_acc: 0.7672, best: 0.7720, time: 0:00:41
 Epoch: 67, lr: 1.0e-02, train_loss: 0.5864, train_acc: 0.7916 test_loss: 0.6975, test_acc: 0.7716, best: 0.7720, time: 0:00:40
 Epoch: 68, lr: 1.0e-02, train_loss: 0.6032, train_acc: 0.7892 test_loss: 0.7696, test_acc: 0.7489, best: 0.7720, time: 0:00:40
 Epoch: 69, lr: 1.0e-02, train_loss: 0.5965, train_acc: 0.7894 test_loss: 0.6826, test_acc: 0.7731, best: 0.7731, time: 0:00:40
 Epoch: 70, lr: 1.0e-02, train_loss: 0.5737, train_acc: 0.8034 test_loss: 0.8074, test_acc: 0.7591, best: 0.7731, time: 0:00:40
 Epoch: 71, lr: 1.0e-02, train_loss: 0.6125, train_acc: 0.7818 test_loss: 0.7691, test_acc: 0.7620, best: 0.7731, time: 0:00:40
 Epoch: 72, lr: 1.0e-02, train_loss: 0.5964, train_acc: 0.7898 test_loss: 0.6388, test_acc: 0.7857, best: 0.7857, time: 0:00:41
 Epoch: 73, lr: 1.0e-02, train_loss: 0.5672, train_acc: 0.8010 test_loss: 0.6941, test_acc: 0.7762, best: 0.7857, time: 0:00:40
 Epoch: 74, lr: 1.0e-02, train_loss: 0.5692, train_acc: 0.8014 test_loss: 0.7462, test_acc: 0.7654, best: 0.7857, time: 0:00:40
 Epoch: 75, lr: 1.0e-02, train_loss: 0.5590, train_acc: 0.8006 test_loss: 0.7310, test_acc: 0.7716, best: 0.7857, time: 0:00:40
 Epoch: 76, lr: 1.0e-02, train_loss: 0.5688, train_acc: 0.7972 test_loss: 0.6930, test_acc: 0.7791, best: 0.7857, time: 0:00:41
 Epoch: 77, lr: 1.0e-02, train_loss: 0.5484, train_acc: 0.8096 test_loss: 0.6424, test_acc: 0.7907, best: 0.7907, time: 0:00:41
 Epoch: 78, lr: 1.0e-02, train_loss: 0.5382, train_acc: 0.8150 test_loss: 0.6610, test_acc: 0.7866, best: 0.7907, time: 0:00:41
 Epoch: 79, lr: 1.0e-02, train_loss: 0.5426, train_acc: 0.8140 test_loss: 0.6961, test_acc: 0.7766, best: 0.7907, time: 0:00:40
 Epoch: 80, lr: 1.0e-02, train_loss: 0.5546, train_acc: 0.8042 test_loss: 0.6581, test_acc: 0.7817, best: 0.7907, time: 0:00:41
 Epoch: 81, lr: 1.0e-02, train_loss: 0.4974, train_acc: 0.8232 test_loss: 0.7070, test_acc: 0.7865, best: 0.7907, time: 0:00:41
 Epoch: 82, lr: 1.0e-02, train_loss: 0.5223, train_acc: 0.8194 test_loss: 0.7075, test_acc: 0.7796, best: 0.7907, time: 0:00:41
 Epoch: 83, lr: 1.0e-02, train_loss: 0.5132, train_acc: 0.8182 test_loss: 0.7921, test_acc: 0.7525, best: 0.7907, time: 0:00:41
 Epoch: 84, lr: 1.0e-02, train_loss: 0.5199, train_acc: 0.8192 test_loss: 0.6725, test_acc: 0.7814, best: 0.7907, time: 0:00:41
 Epoch: 85, lr: 1.0e-02, train_loss: 0.5184, train_acc: 0.8242 test_loss: 0.7385, test_acc: 0.7679, best: 0.7907, time: 0:00:41
 Epoch: 86, lr: 1.0e-02, train_loss: 0.5127, train_acc: 0.8176 test_loss: 0.6846, test_acc: 0.7801, best: 0.7907, time: 0:00:40
 Epoch: 87, lr: 1.0e-02, train_loss: 0.5012, train_acc: 0.8254 test_loss: 0.7061, test_acc: 0.7725, best: 0.7907, time: 0:00:41
 Epoch: 88, lr: 1.0e-02, train_loss: 0.5080, train_acc: 0.8274 test_loss: 0.6744, test_acc: 0.7861, best: 0.7907, time: 0:00:41
 Epoch: 89, lr: 1.0e-02, train_loss: 0.4930, train_acc: 0.8346 test_loss: 0.6492, test_acc: 0.7970, best: 0.7970, time: 0:00:40
 Epoch: 90, lr: 1.0e-02, train_loss: 0.4920, train_acc: 0.8292 test_loss: 0.7042, test_acc: 0.7829, best: 0.7970, time: 0:00:40
 Epoch: 91, lr: 1.0e-02, train_loss: 0.4855, train_acc: 0.8272 test_loss: 0.7522, test_acc: 0.7854, best: 0.7970, time: 0:00:41
 Epoch: 92, lr: 1.0e-02, train_loss: 0.4798, train_acc: 0.8324 test_loss: 0.7173, test_acc: 0.7801, best: 0.7970, time: 0:00:40
 Epoch: 93, lr: 1.0e-02, train_loss: 0.4752, train_acc: 0.8334 test_loss: 0.6540, test_acc: 0.8073, best: 0.8073, time: 0:00:40
 Epoch: 94, lr: 1.0e-02, train_loss: 0.4917, train_acc: 0.8308 test_loss: 0.7182, test_acc: 0.7774, best: 0.8073, time: 0:00:40
 Epoch: 95, lr: 1.0e-02, train_loss: 0.4647, train_acc: 0.8420 test_loss: 0.6980, test_acc: 0.7896, best: 0.8073, time: 0:00:40
 Epoch: 96, lr: 1.0e-02, train_loss: 0.4645, train_acc: 0.8360 test_loss: 0.7235, test_acc: 0.7947, best: 0.8073, time: 0:00:40
 Epoch: 97, lr: 1.0e-02, train_loss: 0.4681, train_acc: 0.8370 test_loss: 0.6329, test_acc: 0.8045, best: 0.8073, time: 0:00:40
 Epoch: 98, lr: 1.0e-02, train_loss: 0.4545, train_acc: 0.8356 test_loss: 0.6943, test_acc: 0.7889, best: 0.8073, time: 0:00:40
 Epoch: 99, lr: 1.0e-02, train_loss: 0.4701, train_acc: 0.8326 test_loss: 0.6296, test_acc: 0.8023, best: 0.8073, time: 0:00:40
 Epoch: 100, lr: 1.0e-02, train_loss: 0.4546, train_acc: 0.8406 test_loss: 0.7075, test_acc: 0.7873, best: 0.8073, time: 0:00:40
 Epoch: 101, lr: 1.0e-02, train_loss: 0.4372, train_acc: 0.8490 test_loss: 0.6557, test_acc: 0.7965, best: 0.8073, time: 0:00:40
 Epoch: 102, lr: 1.0e-02, train_loss: 0.4405, train_acc: 0.8484 test_loss: 0.7011, test_acc: 0.7905, best: 0.8073, time: 0:00:40
 Epoch: 103, lr: 1.0e-02, train_loss: 0.4319, train_acc: 0.8472 test_loss: 0.6992, test_acc: 0.7860, best: 0.8073, time: 0:00:40
 Epoch: 104, lr: 1.0e-02, train_loss: 0.4274, train_acc: 0.8490 test_loss: 0.7061, test_acc: 0.7886, best: 0.8073, time: 0:00:41
 Epoch: 105, lr: 1.0e-02, train_loss: 0.4343, train_acc: 0.8484 test_loss: 0.7811, test_acc: 0.7776, best: 0.8073, time: 0:00:40
 Epoch: 106, lr: 1.0e-02, train_loss: 0.4246, train_acc: 0.8570 test_loss: 0.6925, test_acc: 0.7914, best: 0.8073, time: 0:00:40
 Epoch: 107, lr: 1.0e-02, train_loss: 0.4444, train_acc: 0.8440 test_loss: 0.7225, test_acc: 0.7861, best: 0.8073, time: 0:00:40
 Epoch: 108, lr: 1.0e-02, train_loss: 0.4152, train_acc: 0.8524 test_loss: 0.7304, test_acc: 0.7917, best: 0.8073, time: 0:00:41
 Epoch: 109, lr: 1.0e-02, train_loss: 0.4236, train_acc: 0.8520 test_loss: 0.6596, test_acc: 0.7983, best: 0.8073, time: 0:00:40
 Epoch: 110, lr: 1.0e-02, train_loss: 0.4379, train_acc: 0.8504 test_loss: 0.7701, test_acc: 0.7764, best: 0.8073, time: 0:00:40
 Epoch: 111, lr: 1.0e-02, train_loss: 0.4045, train_acc: 0.8616 test_loss: 0.7358, test_acc: 0.7839, best: 0.8073, time: 0:00:41
 Epoch: 112, lr: 1.0e-02, train_loss: 0.3958, train_acc: 0.8656 test_loss: 0.6805, test_acc: 0.8044, best: 0.8073, time: 0:00:41
 Epoch: 113, lr: 1.0e-02, train_loss: 0.4158, train_acc: 0.8574 test_loss: 0.6996, test_acc: 0.7986, best: 0.8073, time: 0:00:41
 Epoch: 114, lr: 1.0e-02, train_loss: 0.3860, train_acc: 0.8640 test_loss: 0.6626, test_acc: 0.8100, best: 0.8100, time: 0:00:41
 Epoch: 115, lr: 1.0e-02, train_loss: 0.3860, train_acc: 0.8682 test_loss: 0.7974, test_acc: 0.7879, best: 0.8100, time: 0:00:40
 Epoch: 116, lr: 1.0e-02, train_loss: 0.4028, train_acc: 0.8624 test_loss: 0.7343, test_acc: 0.7933, best: 0.8100, time: 0:00:41
 Epoch: 117, lr: 1.0e-02, train_loss: 0.4204, train_acc: 0.8544 test_loss: 0.7152, test_acc: 0.7881, best: 0.8100, time: 0:00:40
 Epoch: 118, lr: 1.0e-02, train_loss: 0.4011, train_acc: 0.8640 test_loss: 0.7189, test_acc: 0.7973, best: 0.8100, time: 0:00:41
 Epoch: 119, lr: 1.0e-02, train_loss: 0.4092, train_acc: 0.8538 test_loss: 0.7269, test_acc: 0.7925, best: 0.8100, time: 0:00:41
 Epoch: 120, lr: 1.0e-02, train_loss: 0.3757, train_acc: 0.8718 test_loss: 0.6340, test_acc: 0.8133, best: 0.8133, time: 0:00:41
 Epoch: 121, lr: 1.0e-02, train_loss: 0.3917, train_acc: 0.8662 test_loss: 0.6854, test_acc: 0.8023, best: 0.8133, time: 0:00:41
 Epoch: 122, lr: 1.0e-02, train_loss: 0.3719, train_acc: 0.8698 test_loss: 0.8366, test_acc: 0.7809, best: 0.8133, time: 0:00:41
 Epoch: 123, lr: 1.0e-02, train_loss: 0.3805, train_acc: 0.8698 test_loss: 0.7408, test_acc: 0.8060, best: 0.8133, time: 0:00:40
 Epoch: 124, lr: 1.0e-02, train_loss: 0.3755, train_acc: 0.8730 test_loss: 0.7469, test_acc: 0.7831, best: 0.8133, time: 0:00:40
 Epoch: 125, lr: 1.0e-02, train_loss: 0.3759, train_acc: 0.8716 test_loss: 0.6639, test_acc: 0.8080, best: 0.8133, time: 0:00:40
 Epoch: 126, lr: 1.0e-02, train_loss: 0.3591, train_acc: 0.8726 test_loss: 0.7062, test_acc: 0.7966, best: 0.8133, time: 0:00:40
 Epoch: 127, lr: 1.0e-02, train_loss: 0.3772, train_acc: 0.8660 test_loss: 0.6335, test_acc: 0.8125, best: 0.8133, time: 0:00:40
 Epoch: 128, lr: 1.0e-02, train_loss: 0.3725, train_acc: 0.8714 test_loss: 0.6558, test_acc: 0.8054, best: 0.8133, time: 0:00:40
 Epoch: 129, lr: 1.0e-02, train_loss: 0.3516, train_acc: 0.8794 test_loss: 0.7218, test_acc: 0.8050, best: 0.8133, time: 0:00:40
 Epoch: 130, lr: 1.0e-02, train_loss: 0.3627, train_acc: 0.8728 test_loss: 0.7388, test_acc: 0.7973, best: 0.8133, time: 0:00:40
 Epoch: 131, lr: 1.0e-02, train_loss: 0.3503, train_acc: 0.8770 test_loss: 0.6834, test_acc: 0.8106, best: 0.8133, time: 0:00:40
 Epoch: 132, lr: 1.0e-02, train_loss: 0.3618, train_acc: 0.8742 test_loss: 0.7590, test_acc: 0.7991, best: 0.8133, time: 0:00:40
 Epoch: 133, lr: 1.0e-02, train_loss: 0.3839, train_acc: 0.8672 test_loss: 0.6956, test_acc: 0.8057, best: 0.8133, time: 0:00:41
 Epoch: 134, lr: 1.0e-02, train_loss: 0.3459, train_acc: 0.8820 test_loss: 0.6522, test_acc: 0.8121, best: 0.8133, time: 0:00:40
 Epoch: 135, lr: 1.0e-02, train_loss: 0.3700, train_acc: 0.8700 test_loss: 0.6800, test_acc: 0.8149, best: 0.8149, time: 0:00:41
 Epoch: 136, lr: 1.0e-02, train_loss: 0.3453, train_acc: 0.8828 test_loss: 0.6478, test_acc: 0.8139, best: 0.8149, time: 0:00:40
 Epoch: 137, lr: 1.0e-02, train_loss: 0.3628, train_acc: 0.8796 test_loss: 0.7265, test_acc: 0.7926, best: 0.8149, time: 0:00:40
 Epoch: 138, lr: 1.0e-02, train_loss: 0.3519, train_acc: 0.8810 test_loss: 0.6983, test_acc: 0.7971, best: 0.8149, time: 0:00:40
 Epoch: 139, lr: 1.0e-02, train_loss: 0.3271, train_acc: 0.8908 test_loss: 0.7141, test_acc: 0.8009, best: 0.8149, time: 0:00:40
 Epoch: 140, lr: 1.0e-02, train_loss: 0.3452, train_acc: 0.8828 test_loss: 0.7395, test_acc: 0.7997, best: 0.8149, time: 0:00:41
 Epoch: 141, lr: 1.0e-02, train_loss: 0.3575, train_acc: 0.8752 test_loss: 0.7103, test_acc: 0.8050, best: 0.8149, time: 0:00:40
 Epoch: 142, lr: 1.0e-02, train_loss: 0.3501, train_acc: 0.8754 test_loss: 0.6785, test_acc: 0.8074, best: 0.8149, time: 0:00:41
 Epoch: 143, lr: 1.0e-02, train_loss: 0.3247, train_acc: 0.8898 test_loss: 0.7261, test_acc: 0.7987, best: 0.8149, time: 0:00:41
 Epoch: 144, lr: 1.0e-02, train_loss: 0.3292, train_acc: 0.8830 test_loss: 0.7726, test_acc: 0.7896, best: 0.8149, time: 0:00:40
 Epoch: 145, lr: 1.0e-02, train_loss: 0.3396, train_acc: 0.8850 test_loss: 0.7670, test_acc: 0.8037, best: 0.8149, time: 0:00:41
 Epoch: 146, lr: 1.0e-02, train_loss: 0.3375, train_acc: 0.8828 test_loss: 0.7048, test_acc: 0.8094, best: 0.8149, time: 0:00:40
 Epoch: 147, lr: 1.0e-02, train_loss: 0.3192, train_acc: 0.8894 test_loss: 0.7141, test_acc: 0.8117, best: 0.8149, time: 0:00:41
 Epoch: 148, lr: 1.0e-02, train_loss: 0.3441, train_acc: 0.8826 test_loss: 0.6770, test_acc: 0.8197, best: 0.8197, time: 0:00:41
 Epoch: 149, lr: 1.0e-02, train_loss: 0.3229, train_acc: 0.8866 test_loss: 0.7164, test_acc: 0.8113, best: 0.8197, time: 0:00:41
 Epoch: 150, lr: 1.0e-02, train_loss: 0.3248, train_acc: 0.8898 test_loss: 0.7577, test_acc: 0.8000, best: 0.8197, time: 0:00:41
 Epoch: 151, lr: 1.0e-02, train_loss: 0.3318, train_acc: 0.8840 test_loss: 0.7677, test_acc: 0.8011, best: 0.8197, time: 0:00:41
 Epoch: 152, lr: 1.0e-02, train_loss: 0.3097, train_acc: 0.8938 test_loss: 0.9204, test_acc: 0.7709, best: 0.8197, time: 0:00:40
 Epoch: 153, lr: 1.0e-02, train_loss: 0.3374, train_acc: 0.8870 test_loss: 0.7747, test_acc: 0.7874, best: 0.8197, time: 0:00:41
 Epoch: 154, lr: 1.0e-02, train_loss: 0.3346, train_acc: 0.8856 test_loss: 0.7379, test_acc: 0.8020, best: 0.8197, time: 0:00:41
 Epoch: 155, lr: 1.0e-02, train_loss: 0.3269, train_acc: 0.8884 test_loss: 0.6974, test_acc: 0.8130, best: 0.8197, time: 0:00:41
 Epoch: 156, lr: 1.0e-02, train_loss: 0.3155, train_acc: 0.8910 test_loss: 0.8172, test_acc: 0.7907, best: 0.8197, time: 0:00:41
 Epoch: 157, lr: 1.0e-02, train_loss: 0.3118, train_acc: 0.8932 test_loss: 0.7127, test_acc: 0.8085, best: 0.8197, time: 0:00:41
 Epoch: 158, lr: 1.0e-02, train_loss: 0.3211, train_acc: 0.8848 test_loss: 0.7379, test_acc: 0.8069, best: 0.8197, time: 0:00:40
 Epoch: 159, lr: 1.0e-02, train_loss: 0.3160, train_acc: 0.8902 test_loss: 0.7440, test_acc: 0.8026, best: 0.8197, time: 0:00:40
 Epoch: 160, lr: 1.0e-02, train_loss: 0.3077, train_acc: 0.8938 test_loss: 0.7222, test_acc: 0.8041, best: 0.8197, time: 0:00:40
 Epoch: 161, lr: 1.0e-02, train_loss: 0.3269, train_acc: 0.8894 test_loss: 0.7566, test_acc: 0.7981, best: 0.8197, time: 0:00:40
 Epoch: 162, lr: 1.0e-02, train_loss: 0.3059, train_acc: 0.8966 test_loss: 0.6833, test_acc: 0.8099, best: 0.8197, time: 0:00:40
 Epoch: 163, lr: 1.0e-02, train_loss: 0.3008, train_acc: 0.8938 test_loss: 0.7346, test_acc: 0.8003, best: 0.8197, time: 0:00:41
 Epoch: 164, lr: 1.0e-02, train_loss: 0.3148, train_acc: 0.8924 test_loss: 0.7044, test_acc: 0.8097, best: 0.8197, time: 0:00:41
 Epoch: 165, lr: 1.0e-02, train_loss: 0.2948, train_acc: 0.9004 test_loss: 0.7772, test_acc: 0.7983, best: 0.8197, time: 0:00:41
 Epoch: 166, lr: 1.0e-02, train_loss: 0.3007, train_acc: 0.8982 test_loss: 0.8218, test_acc: 0.7967, best: 0.8197, time: 0:00:41
 Epoch: 167, lr: 1.0e-02, train_loss: 0.2843, train_acc: 0.8990 test_loss: 0.7268, test_acc: 0.8097, best: 0.8197, time: 0:00:40
 Epoch: 168, lr: 1.0e-02, train_loss: 0.2941, train_acc: 0.8990 test_loss: 0.8073, test_acc: 0.7989, best: 0.8197, time: 0:00:41
 Epoch: 169, lr: 1.0e-02, train_loss: 0.2938, train_acc: 0.9046 test_loss: 0.7654, test_acc: 0.7987, best: 0.8197, time: 0:00:40
 Epoch: 170, lr: 1.0e-02, train_loss: 0.3006, train_acc: 0.8962 test_loss: 0.7603, test_acc: 0.8059, best: 0.8197, time: 0:00:41
 Epoch: 171, lr: 1.0e-02, train_loss: 0.2814, train_acc: 0.9034 test_loss: 0.7380, test_acc: 0.8030, best: 0.8197, time: 0:00:41
 Epoch: 172, lr: 1.0e-02, train_loss: 0.2989, train_acc: 0.9000 test_loss: 0.7653, test_acc: 0.8044, best: 0.8197, time: 0:00:41
 Epoch: 173, lr: 1.0e-02, train_loss: 0.2900, train_acc: 0.9012 test_loss: 0.7500, test_acc: 0.8057, best: 0.8197, time: 0:00:41
 Epoch: 174, lr: 1.0e-02, train_loss: 0.2877, train_acc: 0.9012 test_loss: 0.8111, test_acc: 0.7975, best: 0.8197, time: 0:00:40
 Epoch: 175, lr: 1.0e-02, train_loss: 0.2935, train_acc: 0.8962 test_loss: 0.7837, test_acc: 0.8019, best: 0.8197, time: 0:00:40
 Epoch: 176, lr: 1.0e-02, train_loss: 0.2842, train_acc: 0.9014 test_loss: 0.7302, test_acc: 0.8093, best: 0.8197, time: 0:00:41
 Epoch: 177, lr: 1.0e-02, train_loss: 0.3046, train_acc: 0.8966 test_loss: 0.6940, test_acc: 0.8177, best: 0.8197, time: 0:00:41
 Epoch: 178, lr: 1.0e-02, train_loss: 0.2838, train_acc: 0.8980 test_loss: 0.7719, test_acc: 0.8044, best: 0.8197, time: 0:00:41
 Epoch: 179, lr: 1.0e-02, train_loss: 0.2864, train_acc: 0.8974 test_loss: 0.7492, test_acc: 0.8130, best: 0.8197, time: 0:00:40
 Epoch: 180, lr: 2.0e-03, train_loss: 0.2196, train_acc: 0.9274 test_loss: 0.7073, test_acc: 0.8251, best: 0.8251, time: 0:00:41
 Epoch: 181, lr: 2.0e-03, train_loss: 0.2185, train_acc: 0.9284 test_loss: 0.6636, test_acc: 0.8303, best: 0.8303, time: 0:00:40
 Epoch: 182, lr: 2.0e-03, train_loss: 0.2149, train_acc: 0.9270 test_loss: 0.6903, test_acc: 0.8309, best: 0.8309, time: 0:00:40
 Epoch: 183, lr: 2.0e-03, train_loss: 0.2197, train_acc: 0.9244 test_loss: 0.7144, test_acc: 0.8261, best: 0.8309, time: 0:00:40
 Epoch: 184, lr: 2.0e-03, train_loss: 0.1915, train_acc: 0.9294 test_loss: 0.6904, test_acc: 0.8303, best: 0.8309, time: 0:00:40
 Epoch: 185, lr: 2.0e-03, train_loss: 0.2080, train_acc: 0.9332 test_loss: 0.6912, test_acc: 0.8264, best: 0.8309, time: 0:00:40
 Epoch: 186, lr: 2.0e-03, train_loss: 0.1939, train_acc: 0.9340 test_loss: 0.7071, test_acc: 0.8240, best: 0.8309, time: 0:00:40
 Epoch: 187, lr: 2.0e-03, train_loss: 0.1937, train_acc: 0.9330 test_loss: 0.6936, test_acc: 0.8246, best: 0.8309, time: 0:00:40
 Epoch: 188, lr: 2.0e-03, train_loss: 0.1840, train_acc: 0.9382 test_loss: 0.7172, test_acc: 0.8229, best: 0.8309, time: 0:00:40
 Epoch: 189, lr: 2.0e-03, train_loss: 0.1903, train_acc: 0.9368 test_loss: 0.7172, test_acc: 0.8235, best: 0.8309, time: 0:00:41
 Epoch: 190, lr: 2.0e-03, train_loss: 0.1861, train_acc: 0.9350 test_loss: 0.7053, test_acc: 0.8253, best: 0.8309, time: 0:00:41
 Epoch: 191, lr: 2.0e-03, train_loss: 0.1760, train_acc: 0.9412 test_loss: 0.6851, test_acc: 0.8315, best: 0.8315, time: 0:00:41
 Epoch: 192, lr: 2.0e-03, train_loss: 0.1973, train_acc: 0.9320 test_loss: 0.7164, test_acc: 0.8277, best: 0.8315, time: 0:00:40
 Epoch: 193, lr: 2.0e-03, train_loss: 0.1932, train_acc: 0.9358 test_loss: 0.6855, test_acc: 0.8297, best: 0.8315, time: 0:00:40
 Epoch: 194, lr: 2.0e-03, train_loss: 0.1827, train_acc: 0.9368 test_loss: 0.7217, test_acc: 0.8251, best: 0.8315, time: 0:00:40
 Epoch: 195, lr: 2.0e-03, train_loss: 0.1824, train_acc: 0.9380 test_loss: 0.7043, test_acc: 0.8263, best: 0.8315, time: 0:00:40
 Epoch: 196, lr: 2.0e-03, train_loss: 0.1881, train_acc: 0.9368 test_loss: 0.7376, test_acc: 0.8279, best: 0.8315, time: 0:00:41
 Epoch: 197, lr: 2.0e-03, train_loss: 0.1865, train_acc: 0.9328 test_loss: 0.7246, test_acc: 0.8231, best: 0.8315, time: 0:00:40
 Epoch: 198, lr: 2.0e-03, train_loss: 0.1937, train_acc: 0.9346 test_loss: 0.6915, test_acc: 0.8271, best: 0.8315, time: 0:00:40
 Epoch: 199, lr: 2.0e-03, train_loss: 0.1797, train_acc: 0.9396 test_loss: 0.6744, test_acc: 0.8281, best: 0.8315, time: 0:00:41
 Epoch: 200, lr: 2.0e-03, train_loss: 0.1712, train_acc: 0.9398 test_loss: 0.6911, test_acc: 0.8279, best: 0.8315, time: 0:00:40
 Epoch: 201, lr: 2.0e-03, train_loss: 0.1636, train_acc: 0.9414 test_loss: 0.7447, test_acc: 0.8221, best: 0.8315, time: 0:00:40
 Epoch: 202, lr: 2.0e-03, train_loss: 0.1665, train_acc: 0.9432 test_loss: 0.7213, test_acc: 0.8251, best: 0.8315, time: 0:00:41
 Epoch: 203, lr: 2.0e-03, train_loss: 0.1840, train_acc: 0.9392 test_loss: 0.7534, test_acc: 0.8244, best: 0.8315, time: 0:00:41
 Epoch: 204, lr: 2.0e-03, train_loss: 0.1730, train_acc: 0.9390 test_loss: 0.7114, test_acc: 0.8237, best: 0.8315, time: 0:00:41
 Epoch: 205, lr: 2.0e-03, train_loss: 0.1664, train_acc: 0.9408 test_loss: 0.7259, test_acc: 0.8265, best: 0.8315, time: 0:00:40
 Epoch: 206, lr: 2.0e-03, train_loss: 0.1796, train_acc: 0.9370 test_loss: 0.7126, test_acc: 0.8306, best: 0.8315, time: 0:00:40
 Epoch: 207, lr: 2.0e-03, train_loss: 0.1595, train_acc: 0.9446 test_loss: 0.7227, test_acc: 0.8236, best: 0.8315, time: 0:00:41
 Epoch: 208, lr: 2.0e-03, train_loss: 0.1854, train_acc: 0.9376 test_loss: 0.7157, test_acc: 0.8275, best: 0.8315, time: 0:00:40
 Epoch: 209, lr: 2.0e-03, train_loss: 0.1609, train_acc: 0.9494 test_loss: 0.7330, test_acc: 0.8290, best: 0.8315, time: 0:00:40
 Epoch: 210, lr: 2.0e-03, train_loss: 0.1917, train_acc: 0.9360 test_loss: 0.7342, test_acc: 0.8254, best: 0.8315, time: 0:00:41
 Epoch: 211, lr: 2.0e-03, train_loss: 0.1580, train_acc: 0.9454 test_loss: 0.6946, test_acc: 0.8294, best: 0.8315, time: 0:00:41
 Epoch: 212, lr: 2.0e-03, train_loss: 0.1793, train_acc: 0.9418 test_loss: 0.7364, test_acc: 0.8259, best: 0.8315, time: 0:00:41
 Epoch: 213, lr: 2.0e-03, train_loss: 0.1588, train_acc: 0.9478 test_loss: 0.7826, test_acc: 0.8135, best: 0.8315, time: 0:00:40
 Epoch: 214, lr: 2.0e-03, train_loss: 0.1623, train_acc: 0.9446 test_loss: 0.7690, test_acc: 0.8243, best: 0.8315, time: 0:00:41
 Epoch: 215, lr: 2.0e-03, train_loss: 0.1818, train_acc: 0.9348 test_loss: 0.7654, test_acc: 0.8174, best: 0.8315, time: 0:00:41
 Epoch: 216, lr: 2.0e-03, train_loss: 0.1640, train_acc: 0.9404 test_loss: 0.7391, test_acc: 0.8240, best: 0.8315, time: 0:00:40
 Epoch: 217, lr: 2.0e-03, train_loss: 0.1706, train_acc: 0.9438 test_loss: 0.7250, test_acc: 0.8253, best: 0.8315, time: 0:00:41
 Epoch: 218, lr: 2.0e-03, train_loss: 0.1572, train_acc: 0.9450 test_loss: 0.7439, test_acc: 0.8249, best: 0.8315, time: 0:00:41
 Epoch: 219, lr: 2.0e-03, train_loss: 0.1752, train_acc: 0.9410 test_loss: 0.7678, test_acc: 0.8184, best: 0.8315, time: 0:00:41
 Epoch: 220, lr: 2.0e-03, train_loss: 0.1767, train_acc: 0.9410 test_loss: 0.7281, test_acc: 0.8266, best: 0.8315, time: 0:00:41
 Epoch: 221, lr: 2.0e-03, train_loss: 0.1622, train_acc: 0.9460 test_loss: 0.7303, test_acc: 0.8270, best: 0.8315, time: 0:00:41
 Epoch: 222, lr: 2.0e-03, train_loss: 0.1777, train_acc: 0.9384 test_loss: 0.7272, test_acc: 0.8235, best: 0.8315, time: 0:00:40
 Epoch: 223, lr: 2.0e-03, train_loss: 0.1743, train_acc: 0.9414 test_loss: 0.7030, test_acc: 0.8205, best: 0.8315, time: 0:00:40
 Epoch: 224, lr: 2.0e-03, train_loss: 0.1703, train_acc: 0.9438 test_loss: 0.7233, test_acc: 0.8261, best: 0.8315, time: 0:00:40
 Epoch: 225, lr: 2.0e-03, train_loss: 0.1660, train_acc: 0.9466 test_loss: 0.7456, test_acc: 0.8213, best: 0.8315, time: 0:00:40
 Epoch: 226, lr: 2.0e-03, train_loss: 0.1768, train_acc: 0.9386 test_loss: 0.7394, test_acc: 0.8200, best: 0.8315, time: 0:00:41
 Epoch: 227, lr: 2.0e-03, train_loss: 0.1661, train_acc: 0.9412 test_loss: 0.7524, test_acc: 0.8196, best: 0.8315, time: 0:00:40
 Epoch: 228, lr: 2.0e-03, train_loss: 0.1799, train_acc: 0.9374 test_loss: 0.7388, test_acc: 0.8246, best: 0.8315, time: 0:00:41
 Epoch: 229, lr: 2.0e-03, train_loss: 0.1714, train_acc: 0.9412 test_loss: 0.7168, test_acc: 0.8240, best: 0.8315, time: 0:00:40
 Epoch: 230, lr: 2.0e-03, train_loss: 0.1591, train_acc: 0.9482 test_loss: 0.7154, test_acc: 0.8297, best: 0.8315, time: 0:00:40
 Epoch: 231, lr: 2.0e-03, train_loss: 0.1826, train_acc: 0.9358 test_loss: 0.7234, test_acc: 0.8226, best: 0.8315, time: 0:00:40
 Epoch: 232, lr: 2.0e-03, train_loss: 0.1835, train_acc: 0.9342 test_loss: 0.7136, test_acc: 0.8273, best: 0.8315, time: 0:00:41
 Epoch: 233, lr: 2.0e-03, train_loss: 0.1650, train_acc: 0.9420 test_loss: 0.6962, test_acc: 0.8306, best: 0.8315, time: 0:00:40
 Epoch: 234, lr: 2.0e-03, train_loss: 0.1711, train_acc: 0.9398 test_loss: 0.7691, test_acc: 0.8205, best: 0.8315, time: 0:00:40
 Epoch: 235, lr: 2.0e-03, train_loss: 0.1558, train_acc: 0.9464 test_loss: 0.7296, test_acc: 0.8293, best: 0.8315, time: 0:00:41
 Epoch: 236, lr: 2.0e-03, train_loss: 0.1585, train_acc: 0.9472 test_loss: 0.7179, test_acc: 0.8264, best: 0.8315, time: 0:00:41
 Epoch: 237, lr: 2.0e-03, train_loss: 0.1590, train_acc: 0.9496 test_loss: 0.7092, test_acc: 0.8273, best: 0.8315, time: 0:00:41
 Epoch: 238, lr: 2.0e-03, train_loss: 0.1565, train_acc: 0.9432 test_loss: 0.7291, test_acc: 0.8254, best: 0.8315, time: 0:00:41
 Epoch: 239, lr: 2.0e-03, train_loss: 0.1716, train_acc: 0.9404 test_loss: 0.7186, test_acc: 0.8287, best: 0.8315, time: 0:00:41
 Epoch: 240, lr: 4.0e-04, train_loss: 0.1643, train_acc: 0.9444 test_loss: 0.7229, test_acc: 0.8250, best: 0.8315, time: 0:00:41
 Epoch: 241, lr: 4.0e-04, train_loss: 0.1484, train_acc: 0.9536 test_loss: 0.7048, test_acc: 0.8285, best: 0.8315, time: 0:00:41
 Epoch: 242, lr: 4.0e-04, train_loss: 0.1462, train_acc: 0.9490 test_loss: 0.7225, test_acc: 0.8285, best: 0.8315, time: 0:00:41
 Epoch: 243, lr: 4.0e-04, train_loss: 0.1587, train_acc: 0.9454 test_loss: 0.7141, test_acc: 0.8306, best: 0.8315, time: 0:00:41
 Epoch: 244, lr: 4.0e-04, train_loss: 0.1514, train_acc: 0.9496 test_loss: 0.7108, test_acc: 0.8304, best: 0.8315, time: 0:00:41
 Epoch: 245, lr: 4.0e-04, train_loss: 0.1494, train_acc: 0.9484 test_loss: 0.7048, test_acc: 0.8299, best: 0.8315, time: 0:00:41
 Epoch: 246, lr: 4.0e-04, train_loss: 0.1672, train_acc: 0.9448 test_loss: 0.7268, test_acc: 0.8246, best: 0.8315, time: 0:00:41
 Epoch: 247, lr: 4.0e-04, train_loss: 0.1522, train_acc: 0.9474 test_loss: 0.7028, test_acc: 0.8299, best: 0.8315, time: 0:00:41
 Epoch: 248, lr: 4.0e-04, train_loss: 0.1504, train_acc: 0.9492 test_loss: 0.7347, test_acc: 0.8257, best: 0.8315, time: 0:00:41
 Epoch: 249, lr: 4.0e-04, train_loss: 0.1592, train_acc: 0.9452 test_loss: 0.7154, test_acc: 0.8256, best: 0.8315, time: 0:00:41
 Epoch: 250, lr: 4.0e-04, train_loss: 0.1528, train_acc: 0.9484 test_loss: 0.7292, test_acc: 0.8210, best: 0.8315, time: 0:00:41
 Epoch: 251, lr: 4.0e-04, train_loss: 0.1508, train_acc: 0.9514 test_loss: 0.7009, test_acc: 0.8305, best: 0.8315, time: 0:00:41
 Epoch: 252, lr: 4.0e-04, train_loss: 0.1483, train_acc: 0.9498 test_loss: 0.7087, test_acc: 0.8280, best: 0.8315, time: 0:00:40
 Epoch: 253, lr: 4.0e-04, train_loss: 0.1631, train_acc: 0.9404 test_loss: 0.7101, test_acc: 0.8296, best: 0.8315, time: 0:00:40
 Epoch: 254, lr: 4.0e-04, train_loss: 0.1422, train_acc: 0.9530 test_loss: 0.7391, test_acc: 0.8273, best: 0.8315, time: 0:00:40
 Epoch: 255, lr: 4.0e-04, train_loss: 0.1466, train_acc: 0.9496 test_loss: 0.7023, test_acc: 0.8333, best: 0.8333, time: 0:00:41
 Epoch: 256, lr: 4.0e-04, train_loss: 0.1507, train_acc: 0.9476 test_loss: 0.7132, test_acc: 0.8266, best: 0.8333, time: 0:00:40
 Epoch: 257, lr: 4.0e-04, train_loss: 0.1500, train_acc: 0.9516 test_loss: 0.7064, test_acc: 0.8311, best: 0.8333, time: 0:00:40
 Epoch: 258, lr: 4.0e-04, train_loss: 0.1609, train_acc: 0.9468 test_loss: 0.7355, test_acc: 0.8267, best: 0.8333, time: 0:00:40
 Epoch: 259, lr: 4.0e-04, train_loss: 0.1543, train_acc: 0.9462 test_loss: 0.7120, test_acc: 0.8325, best: 0.8333, time: 0:00:40
 Epoch: 260, lr: 4.0e-04, train_loss: 0.1492, train_acc: 0.9490 test_loss: 0.7206, test_acc: 0.8286, best: 0.8333, time: 0:00:39
 Epoch: 261, lr: 4.0e-04, train_loss: 0.1476, train_acc: 0.9496 test_loss: 0.7121, test_acc: 0.8275, best: 0.8333, time: 0:00:39
 Epoch: 262, lr: 4.0e-04, train_loss: 0.1407, train_acc: 0.9520 test_loss: 0.7384, test_acc: 0.8293, best: 0.8333, time: 0:00:40
 Epoch: 263, lr: 4.0e-04, train_loss: 0.1507, train_acc: 0.9492 test_loss: 0.7255, test_acc: 0.8276, best: 0.8333, time: 0:00:39
 Epoch: 264, lr: 4.0e-04, train_loss: 0.1304, train_acc: 0.9556 test_loss: 0.7461, test_acc: 0.8261, best: 0.8333, time: 0:00:40
 Epoch: 265, lr: 4.0e-04, train_loss: 0.1439, train_acc: 0.9494 test_loss: 0.7134, test_acc: 0.8300, best: 0.8333, time: 0:00:40
 Epoch: 266, lr: 4.0e-04, train_loss: 0.1424, train_acc: 0.9526 test_loss: 0.7156, test_acc: 0.8256, best: 0.8333, time: 0:00:40
 Epoch: 267, lr: 4.0e-04, train_loss: 0.1416, train_acc: 0.9540 test_loss: 0.7324, test_acc: 0.8291, best: 0.8333, time: 0:00:40
 Epoch: 268, lr: 4.0e-04, train_loss: 0.1524, train_acc: 0.9474 test_loss: 0.7209, test_acc: 0.8304, best: 0.8333, time: 0:00:39
 Epoch: 269, lr: 4.0e-04, train_loss: 0.1324, train_acc: 0.9556 test_loss: 0.7516, test_acc: 0.8265, best: 0.8333, time: 0:00:40
 Epoch: 270, lr: 8.0e-05, train_loss: 0.1393, train_acc: 0.9506 test_loss: 0.7300, test_acc: 0.8271, best: 0.8333, time: 0:00:40
 Epoch: 271, lr: 8.0e-05, train_loss: 0.1583, train_acc: 0.9470 test_loss: 0.7454, test_acc: 0.8297, best: 0.8333, time: 0:00:40
 Epoch: 272, lr: 8.0e-05, train_loss: 0.1474, train_acc: 0.9490 test_loss: 0.7259, test_acc: 0.8319, best: 0.8333, time: 0:00:40
 Epoch: 273, lr: 8.0e-05, train_loss: 0.1461, train_acc: 0.9472 test_loss: 0.7617, test_acc: 0.8226, best: 0.8333, time: 0:00:40
 Epoch: 274, lr: 8.0e-05, train_loss: 0.1529, train_acc: 0.9484 test_loss: 0.7589, test_acc: 0.8253, best: 0.8333, time: 0:00:40
 Epoch: 275, lr: 8.0e-05, train_loss: 0.1435, train_acc: 0.9524 test_loss: 0.7312, test_acc: 0.8290, best: 0.8333, time: 0:00:40
 Epoch: 276, lr: 8.0e-05, train_loss: 0.1393, train_acc: 0.9524 test_loss: 0.7200, test_acc: 0.8293, best: 0.8333, time: 0:00:40
 Epoch: 277, lr: 8.0e-05, train_loss: 0.1431, train_acc: 0.9526 test_loss: 0.7228, test_acc: 0.8294, best: 0.8333, time: 0:00:41
 Epoch: 278, lr: 8.0e-05, train_loss: 0.1469, train_acc: 0.9502 test_loss: 0.7216, test_acc: 0.8306, best: 0.8333, time: 0:00:40
 Epoch: 279, lr: 8.0e-05, train_loss: 0.1535, train_acc: 0.9488 test_loss: 0.7435, test_acc: 0.8251, best: 0.8333, time: 0:00:40
 Epoch: 280, lr: 8.0e-05, train_loss: 0.1669, train_acc: 0.9418 test_loss: 0.7092, test_acc: 0.8285, best: 0.8333, time: 0:00:40
 Epoch: 281, lr: 8.0e-05, train_loss: 0.1342, train_acc: 0.9532 test_loss: 0.7212, test_acc: 0.8270, best: 0.8333, time: 0:00:40
 Epoch: 282, lr: 8.0e-05, train_loss: 0.1436, train_acc: 0.9510 test_loss: 0.7296, test_acc: 0.8256, best: 0.8333, time: 0:00:40
 Epoch: 283, lr: 8.0e-05, train_loss: 0.1525, train_acc: 0.9480 test_loss: 0.7241, test_acc: 0.8291, best: 0.8333, time: 0:00:40
 Epoch: 284, lr: 8.0e-05, train_loss: 0.1408, train_acc: 0.9492 test_loss: 0.7203, test_acc: 0.8301, best: 0.8333, time: 0:00:40
 Epoch: 285, lr: 8.0e-05, train_loss: 0.1281, train_acc: 0.9550 test_loss: 0.7291, test_acc: 0.8261, best: 0.8333, time: 0:00:40
 Epoch: 286, lr: 8.0e-05, train_loss: 0.1454, train_acc: 0.9520 test_loss: 0.7140, test_acc: 0.8300, best: 0.8333, time: 0:00:40
 Epoch: 287, lr: 8.0e-05, train_loss: 0.1310, train_acc: 0.9552 test_loss: 0.7381, test_acc: 0.8271, best: 0.8333, time: 0:00:40
 Epoch: 288, lr: 8.0e-05, train_loss: 0.1520, train_acc: 0.9476 test_loss: 0.7160, test_acc: 0.8293, best: 0.8333, time: 0:00:39
 Epoch: 289, lr: 8.0e-05, train_loss: 0.1337, train_acc: 0.9534 test_loss: 0.7423, test_acc: 0.8267, best: 0.8333, time: 0:00:39
 Epoch: 290, lr: 8.0e-05, train_loss: 0.1442, train_acc: 0.9514 test_loss: 0.7342, test_acc: 0.8329, best: 0.8333, time: 0:00:39
 Epoch: 291, lr: 8.0e-05, train_loss: 0.1460, train_acc: 0.9456 test_loss: 0.7367, test_acc: 0.8269, best: 0.8333, time: 0:00:39
 Epoch: 292, lr: 8.0e-05, train_loss: 0.1531, train_acc: 0.9464 test_loss: 0.7347, test_acc: 0.8324, best: 0.8333, time: 0:00:40
 Epoch: 293, lr: 8.0e-05, train_loss: 0.1521, train_acc: 0.9492 test_loss: 0.7162, test_acc: 0.8309, best: 0.8333, time: 0:00:39
 Epoch: 294, lr: 8.0e-05, train_loss: 0.1470, train_acc: 0.9510 test_loss: 0.7256, test_acc: 0.8270, best: 0.8333, time: 0:00:39
 Epoch: 295, lr: 8.0e-05, train_loss: 0.1430, train_acc: 0.9498 test_loss: 0.7219, test_acc: 0.8304, best: 0.8333, time: 0:00:40
 Epoch: 296, lr: 8.0e-05, train_loss: 0.1478, train_acc: 0.9466 test_loss: 0.7382, test_acc: 0.8267, best: 0.8333, time: 0:00:40
 Epoch: 297, lr: 8.0e-05, train_loss: 0.1380, train_acc: 0.9534 test_loss: 0.7314, test_acc: 0.8283, best: 0.8333, time: 0:00:40
 Epoch: 298, lr: 8.0e-05, train_loss: 0.1552, train_acc: 0.9450 test_loss: 0.7460, test_acc: 0.8265, best: 0.8333, time: 0:00:40
 Epoch: 299, lr: 8.0e-05, train_loss: 0.1440, train_acc: 0.9518 test_loss: 0.7280, test_acc: 0.8314, best: 0.8333, time: 0:00:40
 Highest accuracy: 0.8333