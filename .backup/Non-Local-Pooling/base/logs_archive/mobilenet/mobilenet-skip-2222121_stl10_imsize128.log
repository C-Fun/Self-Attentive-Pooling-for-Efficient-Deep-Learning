
 Run on time: 2022-07-03 01:04:18.506272

 Architecture: mobilenet-skip-2222121

 Pool Config: {
    "arch": "mobilenet",
    "conv1": {
        "_conv2d": "norm",
        "pool_cfg": {}
    },
    "layer1": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "skip",
            "_stride": 2,
            "_psize": null,
            "_dim_reduced_ratio": null,
            "_num_heads": null,
            "_conv2d": null,
            "_win_norm": null
        }
    },
    "layer2": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "skip",
            "_stride": 2,
            "_psize": null,
            "_dim_reduced_ratio": null,
            "_num_heads": null,
            "_conv2d": null,
            "_win_norm": null
        }
    },
    "layer3": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "skip",
            "_stride": 2,
            "_psize": null,
            "_dim_reduced_ratio": null,
            "_num_heads": null,
            "_conv2d": null,
            "_win_norm": null
        }
    },
    "layer4": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "skip",
            "_stride": 2,
            "_psize": null,
            "_dim_reduced_ratio": null,
            "_num_heads": null,
            "_conv2d": null,
            "_win_norm": null
        }
    },
    "layer5": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "skip",
            "_stride": 1,
            "_psize": null,
            "_dim_reduced_ratio": null,
            "_num_heads": null,
            "_conv2d": null,
            "_win_norm": null
        }
    },
    "layer6": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "skip",
            "_stride": 2,
            "_psize": null,
            "_dim_reduced_ratio": null,
            "_num_heads": null,
            "_conv2d": null,
            "_win_norm": null
        }
    },
    "layer7": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "skip",
            "_stride": 1,
            "_psize": null,
            "_dim_reduced_ratio": null,
            "_num_heads": null,
            "_conv2d": null,
            "_win_norm": null
        }
    },
    "conv2": {
        "_conv2d": "norm",
        "pool_cfg": {}
    }
}

 Arguments:
	 gpu                  : True
	 log                  : True
	 seed                 : 0
	 momentum             : 0.9
	 amsgrad              : True
	 dataset              : STL10
	 batch_size           : 8
	 architecture         : MOBILENET-SKIP-2222121
	 im_size              : 128
	 relu_threshold       : 4.0
	 learning_rate        : 0.01
	 pretrained_backbone  : 
	 pretrained_ann       : 
	 weight_decay         : 0.0
	 test_only            : False
	 epochs               : 300
	 lr_interval          : [180, 240, 270]
	 lr_reduce            : 5
	 optimizer            : SGD
	 dropout              : 0.2
	 kernel_size          : 3
	 dont_save            : False
	 visualize            : False
	 devices              : 0
 DataParallel(
  (module): Network(
    (net): MobileNetV2(
      (conv1): Sequential(
        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (features): Sequential(
        (0): Sequential(
          (0): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
        (1): Sequential(
          (0): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
              (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
              (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
        (2): Sequential(
          (0): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
              (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
              (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
              (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
        (3): Sequential(
          (0): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
              (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (3): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
        (4): Sequential(
          (0): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
              (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
              (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
        (5): Sequential(
          (0): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)
              (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
              (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (2): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
              (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
        (6): Sequential(
          (0): InvertedResidual(
            (conv): Sequential(
              (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU6(inplace=True)
              (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
              (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU6(inplace=True)
              (6): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (conv2): Sequential(
        (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
      (classifier): Linear(in_features=1280, out_features=10, bias=True)
    )
  )
)
 SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0
)
 Epoch: 1, lr: 1.0e-02, train_loss: 2.5375, train_acc: 0.1638 test_loss: 2.0016, test_acc: 0.2557, best: 0.2557, time: 0:00:58
 Epoch: 2, lr: 1.0e-02, train_loss: 2.0905, train_acc: 0.2114 test_loss: 1.8513, test_acc: 0.2714, best: 0.2714, time: 0:00:57
 Epoch: 3, lr: 1.0e-02, train_loss: 1.9952, train_acc: 0.2416 test_loss: 1.8142, test_acc: 0.3019, best: 0.3019, time: 0:00:58
 Epoch: 4, lr: 1.0e-02, train_loss: 1.9562, train_acc: 0.2480 test_loss: 1.7893, test_acc: 0.3090, best: 0.3090, time: 0:00:57
 Epoch: 5, lr: 1.0e-02, train_loss: 1.9648, train_acc: 0.2426 test_loss: 1.7424, test_acc: 0.3101, best: 0.3101, time: 0:00:58
 Epoch: 6, lr: 1.0e-02, train_loss: 1.9155, train_acc: 0.2720 test_loss: 1.7283, test_acc: 0.3098, best: 0.3101, time: 0:00:57
 Epoch: 7, lr: 1.0e-02, train_loss: 1.9112, train_acc: 0.2744 test_loss: 1.7242, test_acc: 0.3513, best: 0.3513, time: 0:00:58
 Epoch: 8, lr: 1.0e-02, train_loss: 1.8774, train_acc: 0.2826 test_loss: 1.6379, test_acc: 0.3659, best: 0.3659, time: 0:00:58
 Epoch: 9, lr: 1.0e-02, train_loss: 1.8378, train_acc: 0.3002 test_loss: 1.7211, test_acc: 0.3321, best: 0.3659, time: 0:00:58
 Epoch: 10, lr: 1.0e-02, train_loss: 1.8233, train_acc: 0.3046 test_loss: 1.6047, test_acc: 0.3987, best: 0.3987, time: 0:00:58
 Epoch: 11, lr: 1.0e-02, train_loss: 1.8066, train_acc: 0.3090 test_loss: 1.6547, test_acc: 0.3579, best: 0.3987, time: 0:00:58
 Epoch: 12, lr: 1.0e-02, train_loss: 1.8118, train_acc: 0.3156 test_loss: 1.6601, test_acc: 0.3794, best: 0.3987, time: 0:00:58
 Epoch: 13, lr: 1.0e-02, train_loss: 1.8226, train_acc: 0.3072 test_loss: 1.5721, test_acc: 0.3784, best: 0.3987, time: 0:00:57
 Epoch: 14, lr: 1.0e-02, train_loss: 1.8206, train_acc: 0.3110 test_loss: 1.6613, test_acc: 0.3534, best: 0.3987, time: 0:00:57
 Epoch: 15, lr: 1.0e-02, train_loss: 1.8213, train_acc: 0.3078 test_loss: 1.6238, test_acc: 0.3623, best: 0.3987, time: 0:00:58
 Epoch: 16, lr: 1.0e-02, train_loss: 1.7813, train_acc: 0.3178 test_loss: 1.6182, test_acc: 0.3710, best: 0.3987, time: 0:00:58
 Epoch: 17, lr: 1.0e-02, train_loss: 1.7526, train_acc: 0.3320 test_loss: 1.5356, test_acc: 0.4176, best: 0.4176, time: 0:00:58
 Epoch: 18, lr: 1.0e-02, train_loss: 1.7217, train_acc: 0.3372 test_loss: 1.5236, test_acc: 0.4241, best: 0.4241, time: 0:00:57
 Epoch: 19, lr: 1.0e-02, train_loss: 1.7105, train_acc: 0.3360 test_loss: 1.5289, test_acc: 0.4069, best: 0.4241, time: 0:00:57
 Epoch: 20, lr: 1.0e-02, train_loss: 1.7320, train_acc: 0.3482 test_loss: 1.5515, test_acc: 0.4076, best: 0.4241, time: 0:00:57
 Epoch: 21, lr: 1.0e-02, train_loss: 1.7130, train_acc: 0.3436 test_loss: 1.5020, test_acc: 0.4376, best: 0.4376, time: 0:00:57
 Epoch: 22, lr: 1.0e-02, train_loss: 1.7064, train_acc: 0.3448 test_loss: 1.5324, test_acc: 0.4116, best: 0.4376, time: 0:00:56
 Epoch: 23, lr: 1.0e-02, train_loss: 1.6754, train_acc: 0.3612 test_loss: 1.5677, test_acc: 0.4176, best: 0.4376, time: 0:00:57
 Epoch: 24, lr: 1.0e-02, train_loss: 1.6360, train_acc: 0.3840 test_loss: 1.4974, test_acc: 0.4420, best: 0.4420, time: 0:00:56
 Epoch: 25, lr: 1.0e-02, train_loss: 1.6605, train_acc: 0.3750 test_loss: 1.5284, test_acc: 0.4370, best: 0.4420, time: 0:00:57
 Epoch: 26, lr: 1.0e-02, train_loss: 1.6330, train_acc: 0.3842 test_loss: 1.5081, test_acc: 0.4294, best: 0.4420, time: 0:00:57
 Epoch: 27, lr: 1.0e-02, train_loss: 1.6085, train_acc: 0.3874 test_loss: 1.4498, test_acc: 0.4555, best: 0.4555, time: 0:00:57
 Epoch: 28, lr: 1.0e-02, train_loss: 1.5853, train_acc: 0.4050 test_loss: 1.4426, test_acc: 0.4527, best: 0.4555, time: 0:00:56
 Epoch: 29, lr: 1.0e-02, train_loss: 1.6442, train_acc: 0.3818 test_loss: 1.4460, test_acc: 0.4525, best: 0.4555, time: 0:00:56
 Epoch: 30, lr: 1.0e-02, train_loss: 1.6050, train_acc: 0.3980 test_loss: 1.5189, test_acc: 0.4522, best: 0.4555, time: 0:00:56
 Epoch: 31, lr: 1.0e-02, train_loss: 1.5922, train_acc: 0.4080 test_loss: 1.4480, test_acc: 0.4582, best: 0.4582, time: 0:00:57
 Epoch: 32, lr: 1.0e-02, train_loss: 1.5762, train_acc: 0.4138 test_loss: 1.4290, test_acc: 0.4647, best: 0.4647, time: 0:00:57
 Epoch: 33, lr: 1.0e-02, train_loss: 1.5751, train_acc: 0.4126 test_loss: 1.4604, test_acc: 0.4689, best: 0.4689, time: 0:00:57
 Epoch: 34, lr: 1.0e-02, train_loss: 1.5393, train_acc: 0.4196 test_loss: 1.4675, test_acc: 0.4580, best: 0.4689, time: 0:00:56
 Epoch: 35, lr: 1.0e-02, train_loss: 1.5613, train_acc: 0.4048 test_loss: 1.3810, test_acc: 0.4804, best: 0.4804, time: 0:00:57
 Epoch: 36, lr: 1.0e-02, train_loss: 1.5214, train_acc: 0.4388 test_loss: 1.3361, test_acc: 0.5174, best: 0.5174, time: 0:00:57
 Epoch: 37, lr: 1.0e-02, train_loss: 1.5378, train_acc: 0.4236 test_loss: 1.3180, test_acc: 0.5080, best: 0.5174, time: 0:00:56
 Epoch: 38, lr: 1.0e-02, train_loss: 1.5026, train_acc: 0.4284 test_loss: 1.3202, test_acc: 0.5151, best: 0.5174, time: 0:00:57
 Epoch: 39, lr: 1.0e-02, train_loss: 1.4752, train_acc: 0.4482 test_loss: 1.3174, test_acc: 0.5115, best: 0.5174, time: 0:00:57
 Epoch: 40, lr: 1.0e-02, train_loss: 1.5099, train_acc: 0.4440 test_loss: 1.3630, test_acc: 0.5064, best: 0.5174, time: 0:00:57
 Epoch: 41, lr: 1.0e-02, train_loss: 1.4987, train_acc: 0.4460 test_loss: 1.3420, test_acc: 0.5226, best: 0.5226, time: 0:00:57
 Epoch: 42, lr: 1.0e-02, train_loss: 1.4701, train_acc: 0.4500 test_loss: 1.3809, test_acc: 0.4883, best: 0.5226, time: 0:00:56
 Epoch: 43, lr: 1.0e-02, train_loss: 1.4898, train_acc: 0.4406 test_loss: 1.3398, test_acc: 0.4998, best: 0.5226, time: 0:00:56
 Epoch: 44, lr: 1.0e-02, train_loss: 1.4873, train_acc: 0.4540 test_loss: 1.2935, test_acc: 0.5211, best: 0.5226, time: 0:00:56
 Epoch: 45, lr: 1.0e-02, train_loss: 1.4592, train_acc: 0.4636 test_loss: 1.2852, test_acc: 0.5311, best: 0.5311, time: 0:00:56
 Epoch: 46, lr: 1.0e-02, train_loss: 1.4642, train_acc: 0.4548 test_loss: 1.3400, test_acc: 0.5158, best: 0.5311, time: 0:00:56
 Epoch: 47, lr: 1.0e-02, train_loss: 1.4294, train_acc: 0.4710 test_loss: 1.3271, test_acc: 0.5185, best: 0.5311, time: 0:00:56
 Epoch: 48, lr: 1.0e-02, train_loss: 1.4691, train_acc: 0.4630 test_loss: 1.2966, test_acc: 0.5336, best: 0.5336, time: 0:00:57
 Epoch: 49, lr: 1.0e-02, train_loss: 1.4418, train_acc: 0.4676 test_loss: 1.3180, test_acc: 0.5194, best: 0.5336, time: 0:00:57
 Epoch: 50, lr: 1.0e-02, train_loss: 1.4286, train_acc: 0.4704 test_loss: 1.2723, test_acc: 0.5265, best: 0.5336, time: 0:00:56
 Epoch: 51, lr: 1.0e-02, train_loss: 1.4362, train_acc: 0.4708 test_loss: 1.2775, test_acc: 0.5389, best: 0.5389, time: 0:00:57
 Epoch: 52, lr: 1.0e-02, train_loss: 1.4025, train_acc: 0.4782 test_loss: 1.2674, test_acc: 0.5316, best: 0.5389, time: 0:00:57
 Epoch: 53, lr: 1.0e-02, train_loss: 1.4029, train_acc: 0.4770 test_loss: 1.2538, test_acc: 0.5494, best: 0.5494, time: 0:00:56
 Epoch: 54, lr: 1.0e-02, train_loss: 1.3959, train_acc: 0.4920 test_loss: 1.2010, test_acc: 0.5663, best: 0.5663, time: 0:00:56
 Epoch: 55, lr: 1.0e-02, train_loss: 1.3821, train_acc: 0.4930 test_loss: 1.2091, test_acc: 0.5635, best: 0.5663, time: 0:00:56
 Epoch: 56, lr: 1.0e-02, train_loss: 1.3732, train_acc: 0.4958 test_loss: 1.2081, test_acc: 0.5496, best: 0.5663, time: 0:00:56
 Epoch: 57, lr: 1.0e-02, train_loss: 1.3545, train_acc: 0.5022 test_loss: 1.1951, test_acc: 0.5643, best: 0.5663, time: 0:00:56
 Epoch: 58, lr: 1.0e-02, train_loss: 1.3251, train_acc: 0.5140 test_loss: 1.2142, test_acc: 0.5606, best: 0.5663, time: 0:00:56
 Epoch: 59, lr: 1.0e-02, train_loss: 1.3506, train_acc: 0.5130 test_loss: 1.2236, test_acc: 0.5630, best: 0.5663, time: 0:00:57
 Epoch: 60, lr: 1.0e-02, train_loss: 1.3220, train_acc: 0.5126 test_loss: 1.1734, test_acc: 0.5810, best: 0.5810, time: 0:00:56
 Epoch: 61, lr: 1.0e-02, train_loss: 1.3300, train_acc: 0.5234 test_loss: 1.2210, test_acc: 0.5664, best: 0.5810, time: 0:00:56
 Epoch: 62, lr: 1.0e-02, train_loss: 1.3172, train_acc: 0.5154 test_loss: 1.1820, test_acc: 0.5810, best: 0.5810, time: 0:00:56
 Epoch: 63, lr: 1.0e-02, train_loss: 1.3177, train_acc: 0.5146 test_loss: 1.1393, test_acc: 0.5913, best: 0.5913, time: 0:00:57
 Epoch: 64, lr: 1.0e-02, train_loss: 1.3055, train_acc: 0.5192 test_loss: 1.1636, test_acc: 0.5793, best: 0.5913, time: 0:00:56
 Epoch: 65, lr: 1.0e-02, train_loss: 1.2940, train_acc: 0.5238 test_loss: 1.1299, test_acc: 0.5961, best: 0.5961, time: 0:00:56
 Epoch: 66, lr: 1.0e-02, train_loss: 1.2802, train_acc: 0.5358 test_loss: 1.1224, test_acc: 0.5901, best: 0.5961, time: 0:00:56
 Epoch: 67, lr: 1.0e-02, train_loss: 1.3024, train_acc: 0.5216 test_loss: 1.1680, test_acc: 0.5667, best: 0.5961, time: 0:00:56
 Epoch: 68, lr: 1.0e-02, train_loss: 1.2889, train_acc: 0.5296 test_loss: 1.1932, test_acc: 0.5664, best: 0.5961, time: 0:00:56
 Epoch: 69, lr: 1.0e-02, train_loss: 1.3061, train_acc: 0.5204 test_loss: 1.1175, test_acc: 0.5935, best: 0.5961, time: 0:00:56
 Epoch: 70, lr: 1.0e-02, train_loss: 1.2862, train_acc: 0.5274 test_loss: 1.1324, test_acc: 0.5841, best: 0.5961, time: 0:00:57
 Epoch: 71, lr: 1.0e-02, train_loss: 1.2711, train_acc: 0.5342 test_loss: 1.1516, test_acc: 0.5794, best: 0.5961, time: 0:00:56
 Epoch: 72, lr: 1.0e-02, train_loss: 1.2933, train_acc: 0.5240 test_loss: 1.1802, test_acc: 0.5697, best: 0.5961, time: 0:00:57
 Epoch: 73, lr: 1.0e-02, train_loss: 1.2910, train_acc: 0.5272 test_loss: 1.1547, test_acc: 0.5787, best: 0.5961, time: 0:00:57
 Epoch: 74, lr: 1.0e-02, train_loss: 1.2892, train_acc: 0.5236 test_loss: 1.1485, test_acc: 0.5784, best: 0.5961, time: 0:00:57
 Epoch: 75, lr: 1.0e-02, train_loss: 1.2797, train_acc: 0.5336 test_loss: 1.1112, test_acc: 0.5950, best: 0.5961, time: 0:00:56
 Epoch: 76, lr: 1.0e-02, train_loss: 1.2599, train_acc: 0.5400 test_loss: 1.1258, test_acc: 0.5879, best: 0.5961, time: 0:00:56
 Epoch: 77, lr: 1.0e-02, train_loss: 1.2585, train_acc: 0.5474 test_loss: 1.0840, test_acc: 0.6102, best: 0.6102, time: 0:00:56
 Epoch: 78, lr: 1.0e-02, train_loss: 1.2375, train_acc: 0.5550 test_loss: 1.1968, test_acc: 0.5757, best: 0.6102, time: 0:00:56
 Epoch: 79, lr: 1.0e-02, train_loss: 1.2146, train_acc: 0.5540 test_loss: 1.0929, test_acc: 0.6032, best: 0.6102, time: 0:00:57
 Epoch: 80, lr: 1.0e-02, train_loss: 1.2365, train_acc: 0.5446 test_loss: 1.0595, test_acc: 0.6139, best: 0.6139, time: 0:00:57
 Epoch: 81, lr: 1.0e-02, train_loss: 1.2448, train_acc: 0.5430 test_loss: 1.0889, test_acc: 0.6039, best: 0.6139, time: 0:00:57
 Epoch: 82, lr: 1.0e-02, train_loss: 1.2136, train_acc: 0.5598 test_loss: 1.1842, test_acc: 0.5673, best: 0.6139, time: 0:00:56
 Epoch: 83, lr: 1.0e-02, train_loss: 1.2018, train_acc: 0.5622 test_loss: 1.0936, test_acc: 0.5968, best: 0.6139, time: 0:00:57
 Epoch: 84, lr: 1.0e-02, train_loss: 1.1940, train_acc: 0.5632 test_loss: 1.0865, test_acc: 0.6126, best: 0.6139, time: 0:00:57
 Epoch: 85, lr: 1.0e-02, train_loss: 1.2020, train_acc: 0.5692 test_loss: 1.1390, test_acc: 0.5990, best: 0.6139, time: 0:00:57
 Epoch: 86, lr: 1.0e-02, train_loss: 1.1919, train_acc: 0.5700 test_loss: 1.0596, test_acc: 0.6121, best: 0.6139, time: 0:00:57
 Epoch: 87, lr: 1.0e-02, train_loss: 1.1968, train_acc: 0.5642 test_loss: 1.1119, test_acc: 0.6012, best: 0.6139, time: 0:00:57
 Epoch: 88, lr: 1.0e-02, train_loss: 1.1694, train_acc: 0.5722 test_loss: 1.0651, test_acc: 0.6131, best: 0.6139, time: 0:00:57
 Epoch: 89, lr: 1.0e-02, train_loss: 1.1549, train_acc: 0.5856 test_loss: 1.0910, test_acc: 0.6106, best: 0.6139, time: 0:00:57
 Epoch: 90, lr: 1.0e-02, train_loss: 1.1622, train_acc: 0.5766 test_loss: 1.0825, test_acc: 0.6030, best: 0.6139, time: 0:00:56
 Epoch: 91, lr: 1.0e-02, train_loss: 1.1675, train_acc: 0.5752 test_loss: 1.0249, test_acc: 0.6312, best: 0.6312, time: 0:00:56
 Epoch: 92, lr: 1.0e-02, train_loss: 1.1665, train_acc: 0.5732 test_loss: 1.0428, test_acc: 0.6341, best: 0.6341, time: 0:00:56
 Epoch: 93, lr: 1.0e-02, train_loss: 1.1543, train_acc: 0.5808 test_loss: 1.0647, test_acc: 0.6229, best: 0.6341, time: 0:00:56
 Epoch: 94, lr: 1.0e-02, train_loss: 1.1670, train_acc: 0.5850 test_loss: 1.0764, test_acc: 0.6076, best: 0.6341, time: 0:00:56
 Epoch: 95, lr: 1.0e-02, train_loss: 1.1514, train_acc: 0.5816 test_loss: 1.0511, test_acc: 0.6182, best: 0.6341, time: 0:00:56
 Epoch: 96, lr: 1.0e-02, train_loss: 1.1364, train_acc: 0.5926 test_loss: 1.0790, test_acc: 0.6139, best: 0.6341, time: 0:00:56
 Epoch: 97, lr: 1.0e-02, train_loss: 1.1561, train_acc: 0.5802 test_loss: 1.0469, test_acc: 0.6139, best: 0.6341, time: 0:00:56
 Epoch: 98, lr: 1.0e-02, train_loss: 1.1326, train_acc: 0.5920 test_loss: 1.0402, test_acc: 0.6226, best: 0.6341, time: 0:00:56
 Epoch: 99, lr: 1.0e-02, train_loss: 1.1577, train_acc: 0.5782 test_loss: 0.9700, test_acc: 0.6484, best: 0.6484, time: 0:00:56
 Epoch: 100, lr: 1.0e-02, train_loss: 1.1372, train_acc: 0.5882 test_loss: 1.0707, test_acc: 0.6140, best: 0.6484, time: 0:00:56
 Epoch: 101, lr: 1.0e-02, train_loss: 1.1389, train_acc: 0.5874 test_loss: 1.0574, test_acc: 0.6155, best: 0.6484, time: 0:00:56
 Epoch: 102, lr: 1.0e-02, train_loss: 1.1359, train_acc: 0.5854 test_loss: 1.0159, test_acc: 0.6335, best: 0.6484, time: 0:00:56
 Epoch: 103, lr: 1.0e-02, train_loss: 1.1205, train_acc: 0.5980 test_loss: 1.0114, test_acc: 0.6331, best: 0.6484, time: 0:00:56
 Epoch: 104, lr: 1.0e-02, train_loss: 1.1253, train_acc: 0.5872 test_loss: 1.0259, test_acc: 0.6339, best: 0.6484, time: 0:00:56
 Epoch: 105, lr: 1.0e-02, train_loss: 1.1243, train_acc: 0.5950 test_loss: 1.0713, test_acc: 0.6199, best: 0.6484, time: 0:00:56
 Epoch: 106, lr: 1.0e-02, train_loss: 1.1152, train_acc: 0.5972 test_loss: 1.0275, test_acc: 0.6308, best: 0.6484, time: 0:00:56
 Epoch: 107, lr: 1.0e-02, train_loss: 1.0904, train_acc: 0.6004 test_loss: 1.0113, test_acc: 0.6359, best: 0.6484, time: 0:00:56
 Epoch: 108, lr: 1.0e-02, train_loss: 1.0898, train_acc: 0.5928 test_loss: 1.0426, test_acc: 0.6258, best: 0.6484, time: 0:00:56
 Epoch: 109, lr: 1.0e-02, train_loss: 1.0941, train_acc: 0.6100 test_loss: 0.9950, test_acc: 0.6442, best: 0.6484, time: 0:00:56
 Epoch: 110, lr: 1.0e-02, train_loss: 1.0695, train_acc: 0.6114 test_loss: 0.9920, test_acc: 0.6499, best: 0.6499, time: 0:00:56
 Epoch: 111, lr: 1.0e-02, train_loss: 1.0737, train_acc: 0.6126 test_loss: 1.0481, test_acc: 0.6330, best: 0.6499, time: 0:00:56
 Epoch: 112, lr: 1.0e-02, train_loss: 1.0761, train_acc: 0.6094 test_loss: 1.0498, test_acc: 0.6199, best: 0.6499, time: 0:00:56
 Epoch: 113, lr: 1.0e-02, train_loss: 1.0653, train_acc: 0.6236 test_loss: 1.0217, test_acc: 0.6366, best: 0.6499, time: 0:00:56
 Epoch: 114, lr: 1.0e-02, train_loss: 1.0856, train_acc: 0.6076 test_loss: 1.0559, test_acc: 0.6168, best: 0.6499, time: 0:00:55
 Epoch: 115, lr: 1.0e-02, train_loss: 1.0798, train_acc: 0.6040 test_loss: 0.9903, test_acc: 0.6375, best: 0.6499, time: 0:00:56
 Epoch: 116, lr: 1.0e-02, train_loss: 1.0927, train_acc: 0.6044 test_loss: 0.9828, test_acc: 0.6501, best: 0.6501, time: 0:00:55
 Epoch: 117, lr: 1.0e-02, train_loss: 1.0964, train_acc: 0.6048 test_loss: 0.9925, test_acc: 0.6409, best: 0.6501, time: 0:00:55
 Epoch: 118, lr: 1.0e-02, train_loss: 1.0761, train_acc: 0.6164 test_loss: 0.9840, test_acc: 0.6508, best: 0.6508, time: 0:00:56
 Epoch: 119, lr: 1.0e-02, train_loss: 1.0638, train_acc: 0.6094 test_loss: 1.0153, test_acc: 0.6390, best: 0.6508, time: 0:00:55
 Epoch: 120, lr: 1.0e-02, train_loss: 1.0564, train_acc: 0.6136 test_loss: 0.9697, test_acc: 0.6554, best: 0.6554, time: 0:00:55
 Epoch: 121, lr: 1.0e-02, train_loss: 1.0447, train_acc: 0.6242 test_loss: 0.9499, test_acc: 0.6651, best: 0.6651, time: 0:00:56
 Epoch: 122, lr: 1.0e-02, train_loss: 1.0457, train_acc: 0.6154 test_loss: 1.0697, test_acc: 0.6231, best: 0.6651, time: 0:00:55
 Epoch: 123, lr: 1.0e-02, train_loss: 1.0617, train_acc: 0.6122 test_loss: 0.9565, test_acc: 0.6603, best: 0.6651, time: 0:00:55
 Epoch: 124, lr: 1.0e-02, train_loss: 1.0411, train_acc: 0.6134 test_loss: 1.0543, test_acc: 0.6251, best: 0.6651, time: 0:00:56
 Epoch: 125, lr: 1.0e-02, train_loss: 1.0383, train_acc: 0.6212 test_loss: 0.9869, test_acc: 0.6436, best: 0.6651, time: 0:00:55
 Epoch: 126, lr: 1.0e-02, train_loss: 1.0521, train_acc: 0.6198 test_loss: 0.9617, test_acc: 0.6589, best: 0.6651, time: 0:00:55
 Epoch: 127, lr: 1.0e-02, train_loss: 1.0307, train_acc: 0.6314 test_loss: 0.9528, test_acc: 0.6665, best: 0.6665, time: 0:00:55
 Epoch: 128, lr: 1.0e-02, train_loss: 1.0327, train_acc: 0.6332 test_loss: 0.9740, test_acc: 0.6515, best: 0.6665, time: 0:00:55
 Epoch: 129, lr: 1.0e-02, train_loss: 1.0314, train_acc: 0.6316 test_loss: 1.0226, test_acc: 0.6364, best: 0.6665, time: 0:00:55
 Epoch: 130, lr: 1.0e-02, train_loss: 1.0153, train_acc: 0.6410 test_loss: 0.9924, test_acc: 0.6438, best: 0.6665, time: 0:00:55
 Epoch: 131, lr: 1.0e-02, train_loss: 1.0266, train_acc: 0.6266 test_loss: 0.9860, test_acc: 0.6561, best: 0.6665, time: 0:00:56
 Epoch: 132, lr: 1.0e-02, train_loss: 1.0188, train_acc: 0.6330 test_loss: 1.0071, test_acc: 0.6474, best: 0.6665, time: 0:00:55
 Epoch: 133, lr: 1.0e-02, train_loss: 1.0219, train_acc: 0.6328 test_loss: 1.0003, test_acc: 0.6470, best: 0.6665, time: 0:00:55
 Epoch: 134, lr: 1.0e-02, train_loss: 1.0047, train_acc: 0.6392 test_loss: 0.9776, test_acc: 0.6584, best: 0.6665, time: 0:00:55
 Epoch: 135, lr: 1.0e-02, train_loss: 1.0064, train_acc: 0.6414 test_loss: 0.9138, test_acc: 0.6725, best: 0.6725, time: 0:00:56
 Epoch: 136, lr: 1.0e-02, train_loss: 1.0127, train_acc: 0.6316 test_loss: 0.9543, test_acc: 0.6621, best: 0.6725, time: 0:00:55
 Epoch: 137, lr: 1.0e-02, train_loss: 1.0162, train_acc: 0.6300 test_loss: 0.9543, test_acc: 0.6545, best: 0.6725, time: 0:00:56
 Epoch: 138, lr: 1.0e-02, train_loss: 0.9933, train_acc: 0.6404 test_loss: 0.9092, test_acc: 0.6705, best: 0.6725, time: 0:00:56
 Epoch: 139, lr: 1.0e-02, train_loss: 1.0059, train_acc: 0.6334 test_loss: 0.9330, test_acc: 0.6665, best: 0.6725, time: 0:00:56
 Epoch: 140, lr: 1.0e-02, train_loss: 0.9720, train_acc: 0.6480 test_loss: 0.9322, test_acc: 0.6704, best: 0.6725, time: 0:00:56
 Epoch: 141, lr: 1.0e-02, train_loss: 0.9734, train_acc: 0.6450 test_loss: 0.9483, test_acc: 0.6565, best: 0.6725, time: 0:00:55
 Epoch: 142, lr: 1.0e-02, train_loss: 0.9772, train_acc: 0.6484 test_loss: 1.0207, test_acc: 0.6330, best: 0.6725, time: 0:00:56
 Epoch: 143, lr: 1.0e-02, train_loss: 1.0004, train_acc: 0.6412 test_loss: 0.9431, test_acc: 0.6611, best: 0.6725, time: 0:00:56
 Epoch: 144, lr: 1.0e-02, train_loss: 0.9722, train_acc: 0.6536 test_loss: 0.9430, test_acc: 0.6604, best: 0.6725, time: 0:00:56
 Epoch: 145, lr: 1.0e-02, train_loss: 0.9818, train_acc: 0.6512 test_loss: 0.9454, test_acc: 0.6626, best: 0.6725, time: 0:00:55
 Epoch: 146, lr: 1.0e-02, train_loss: 0.9583, train_acc: 0.6544 test_loss: 0.9225, test_acc: 0.6730, best: 0.6730, time: 0:00:56
 Epoch: 147, lr: 1.0e-02, train_loss: 0.9646, train_acc: 0.6558 test_loss: 0.9347, test_acc: 0.6665, best: 0.6730, time: 0:00:56
 Epoch: 148, lr: 1.0e-02, train_loss: 0.9649, train_acc: 0.6480 test_loss: 0.9357, test_acc: 0.6664, best: 0.6730, time: 0:00:55
 Epoch: 149, lr: 1.0e-02, train_loss: 0.9467, train_acc: 0.6600 test_loss: 0.8929, test_acc: 0.6814, best: 0.6814, time: 0:00:55
 Epoch: 150, lr: 1.0e-02, train_loss: 0.9430, train_acc: 0.6630 test_loss: 0.9644, test_acc: 0.6614, best: 0.6814, time: 0:00:55
 Epoch: 151, lr: 1.0e-02, train_loss: 0.9558, train_acc: 0.6578 test_loss: 0.9208, test_acc: 0.6664, best: 0.6814, time: 0:00:55
 Epoch: 152, lr: 1.0e-02, train_loss: 0.9613, train_acc: 0.6596 test_loss: 0.9186, test_acc: 0.6704, best: 0.6814, time: 0:00:56
 Epoch: 153, lr: 1.0e-02, train_loss: 0.9711, train_acc: 0.6514 test_loss: 0.9120, test_acc: 0.6719, best: 0.6814, time: 0:00:55
 Epoch: 154, lr: 1.0e-02, train_loss: 0.9635, train_acc: 0.6562 test_loss: 0.8692, test_acc: 0.6946, best: 0.6946, time: 0:00:55
 Epoch: 155, lr: 1.0e-02, train_loss: 0.9297, train_acc: 0.6666 test_loss: 0.9134, test_acc: 0.6709, best: 0.6946, time: 0:00:55
 Epoch: 156, lr: 1.0e-02, train_loss: 0.9541, train_acc: 0.6612 test_loss: 0.9441, test_acc: 0.6703, best: 0.6946, time: 0:00:56
 Epoch: 157, lr: 1.0e-02, train_loss: 0.9397, train_acc: 0.6632 test_loss: 0.9024, test_acc: 0.6815, best: 0.6946, time: 0:00:56
 Epoch: 158, lr: 1.0e-02, train_loss: 0.9558, train_acc: 0.6616 test_loss: 0.9191, test_acc: 0.6751, best: 0.6946, time: 0:00:56
 Epoch: 159, lr: 1.0e-02, train_loss: 0.9098, train_acc: 0.6714 test_loss: 0.9489, test_acc: 0.6735, best: 0.6946, time: 0:00:55
 Epoch: 160, lr: 1.0e-02, train_loss: 0.9271, train_acc: 0.6648 test_loss: 0.9022, test_acc: 0.6800, best: 0.6946, time: 0:00:55
 Epoch: 161, lr: 1.0e-02, train_loss: 0.9601, train_acc: 0.6548 test_loss: 0.8931, test_acc: 0.6821, best: 0.6946, time: 0:00:56
 Epoch: 162, lr: 1.0e-02, train_loss: 0.9415, train_acc: 0.6610 test_loss: 0.9820, test_acc: 0.6532, best: 0.6946, time: 0:00:56
 Epoch: 163, lr: 1.0e-02, train_loss: 0.9209, train_acc: 0.6644 test_loss: 0.8889, test_acc: 0.6849, best: 0.6946, time: 0:00:56
 Epoch: 164, lr: 1.0e-02, train_loss: 0.9345, train_acc: 0.6676 test_loss: 0.9147, test_acc: 0.6813, best: 0.6946, time: 0:00:56
 Epoch: 165, lr: 1.0e-02, train_loss: 0.9044, train_acc: 0.6766 test_loss: 0.8760, test_acc: 0.6913, best: 0.6946, time: 0:00:55
 Epoch: 166, lr: 1.0e-02, train_loss: 0.9023, train_acc: 0.6730 test_loss: 0.9118, test_acc: 0.6783, best: 0.6946, time: 0:00:55
 Epoch: 167, lr: 1.0e-02, train_loss: 0.9076, train_acc: 0.6774 test_loss: 0.8953, test_acc: 0.6936, best: 0.6946, time: 0:00:55
 Epoch: 168, lr: 1.0e-02, train_loss: 0.9013, train_acc: 0.6834 test_loss: 0.9017, test_acc: 0.6823, best: 0.6946, time: 0:00:56
 Epoch: 169, lr: 1.0e-02, train_loss: 0.9238, train_acc: 0.6678 test_loss: 0.9198, test_acc: 0.6799, best: 0.6946, time: 0:00:55
 Epoch: 170, lr: 1.0e-02, train_loss: 0.8932, train_acc: 0.6704 test_loss: 0.9390, test_acc: 0.6690, best: 0.6946, time: 0:00:56
 Epoch: 171, lr: 1.0e-02, train_loss: 0.8947, train_acc: 0.6790 test_loss: 0.8997, test_acc: 0.6833, best: 0.6946, time: 0:00:56
 Epoch: 172, lr: 1.0e-02, train_loss: 0.8926, train_acc: 0.6818 test_loss: 0.8846, test_acc: 0.6909, best: 0.6946, time: 0:00:55
 Epoch: 173, lr: 1.0e-02, train_loss: 0.8850, train_acc: 0.6794 test_loss: 0.9019, test_acc: 0.6766, best: 0.6946, time: 0:00:55
 Epoch: 174, lr: 1.0e-02, train_loss: 0.9192, train_acc: 0.6722 test_loss: 0.9201, test_acc: 0.6806, best: 0.6946, time: 0:00:56
 Epoch: 175, lr: 1.0e-02, train_loss: 0.8842, train_acc: 0.6808 test_loss: 0.9643, test_acc: 0.6597, best: 0.6946, time: 0:00:55
 Epoch: 176, lr: 1.0e-02, train_loss: 0.9024, train_acc: 0.6828 test_loss: 0.8986, test_acc: 0.6829, best: 0.6946, time: 0:00:55
 Epoch: 177, lr: 1.0e-02, train_loss: 0.8885, train_acc: 0.6780 test_loss: 0.9082, test_acc: 0.6804, best: 0.6946, time: 0:00:55
 Epoch: 178, lr: 1.0e-02, train_loss: 0.8877, train_acc: 0.6854 test_loss: 0.9278, test_acc: 0.6737, best: 0.6946, time: 0:00:55
 Epoch: 179, lr: 1.0e-02, train_loss: 0.8852, train_acc: 0.6780 test_loss: 0.9090, test_acc: 0.6785, best: 0.6946, time: 0:00:56
 Epoch: 180, lr: 2.0e-03, train_loss: 0.8171, train_acc: 0.7132 test_loss: 0.8651, test_acc: 0.6987, best: 0.6987, time: 0:00:56
 Epoch: 181, lr: 2.0e-03, train_loss: 0.7742, train_acc: 0.7276 test_loss: 0.8581, test_acc: 0.7021, best: 0.7021, time: 0:00:55
 Epoch: 182, lr: 2.0e-03, train_loss: 0.7986, train_acc: 0.7094 test_loss: 0.8375, test_acc: 0.7099, best: 0.7099, time: 0:00:55
 Epoch: 183, lr: 2.0e-03, train_loss: 0.7553, train_acc: 0.7278 test_loss: 0.8597, test_acc: 0.7013, best: 0.7099, time: 0:00:55
 Epoch: 184, lr: 2.0e-03, train_loss: 0.7556, train_acc: 0.7326 test_loss: 0.8589, test_acc: 0.6994, best: 0.7099, time: 0:00:55
 Epoch: 185, lr: 2.0e-03, train_loss: 0.7569, train_acc: 0.7380 test_loss: 0.8477, test_acc: 0.7060, best: 0.7099, time: 0:00:55
 Epoch: 186, lr: 2.0e-03, train_loss: 0.7633, train_acc: 0.7316 test_loss: 0.8229, test_acc: 0.7104, best: 0.7104, time: 0:00:55
 Epoch: 187, lr: 2.0e-03, train_loss: 0.7698, train_acc: 0.7286 test_loss: 0.8499, test_acc: 0.7047, best: 0.7104, time: 0:00:55
 Epoch: 188, lr: 2.0e-03, train_loss: 0.7363, train_acc: 0.7426 test_loss: 0.8675, test_acc: 0.6984, best: 0.7104, time: 0:00:55
 Epoch: 189, lr: 2.0e-03, train_loss: 0.7609, train_acc: 0.7244 test_loss: 0.8347, test_acc: 0.7053, best: 0.7104, time: 0:00:55
 Epoch: 190, lr: 2.0e-03, train_loss: 0.7490, train_acc: 0.7332 test_loss: 0.8346, test_acc: 0.7103, best: 0.7104, time: 0:00:55
 Epoch: 191, lr: 2.0e-03, train_loss: 0.7520, train_acc: 0.7296 test_loss: 0.8388, test_acc: 0.7094, best: 0.7104, time: 0:00:55
 Epoch: 192, lr: 2.0e-03, train_loss: 0.7545, train_acc: 0.7270 test_loss: 0.8240, test_acc: 0.7136, best: 0.7136, time: 0:00:56
 Epoch: 193, lr: 2.0e-03, train_loss: 0.7271, train_acc: 0.7388 test_loss: 0.8485, test_acc: 0.7033, best: 0.7136, time: 0:00:55
 Epoch: 194, lr: 2.0e-03, train_loss: 0.7288, train_acc: 0.7358 test_loss: 0.8316, test_acc: 0.7134, best: 0.7136, time: 0:00:55
 Epoch: 195, lr: 2.0e-03, train_loss: 0.7355, train_acc: 0.7348 test_loss: 0.8348, test_acc: 0.7107, best: 0.7136, time: 0:00:56
 Epoch: 196, lr: 2.0e-03, train_loss: 0.7375, train_acc: 0.7402 test_loss: 0.8301, test_acc: 0.7161, best: 0.7161, time: 0:00:56
 Epoch: 197, lr: 2.0e-03, train_loss: 0.7116, train_acc: 0.7460 test_loss: 0.8335, test_acc: 0.7154, best: 0.7161, time: 0:00:55
 Epoch: 198, lr: 2.0e-03, train_loss: 0.7220, train_acc: 0.7458 test_loss: 0.8376, test_acc: 0.7114, best: 0.7161, time: 0:00:55
 Epoch: 199, lr: 2.0e-03, train_loss: 0.7190, train_acc: 0.7388 test_loss: 0.8259, test_acc: 0.7155, best: 0.7161, time: 0:00:55
 Epoch: 200, lr: 2.0e-03, train_loss: 0.7067, train_acc: 0.7422 test_loss: 0.8618, test_acc: 0.7054, best: 0.7161, time: 0:00:55
 Epoch: 201, lr: 2.0e-03, train_loss: 0.7016, train_acc: 0.7470 test_loss: 0.8431, test_acc: 0.7089, best: 0.7161, time: 0:00:55
 Epoch: 202, lr: 2.0e-03, train_loss: 0.7132, train_acc: 0.7500 test_loss: 0.8568, test_acc: 0.7069, best: 0.7161, time: 0:00:55
 Epoch: 203, lr: 2.0e-03, train_loss: 0.7063, train_acc: 0.7498 test_loss: 0.8430, test_acc: 0.7055, best: 0.7161, time: 0:00:55
 Epoch: 204, lr: 2.0e-03, train_loss: 0.6894, train_acc: 0.7542 test_loss: 0.8524, test_acc: 0.7095, best: 0.7161, time: 0:00:55
 Epoch: 205, lr: 2.0e-03, train_loss: 0.6858, train_acc: 0.7544 test_loss: 0.8507, test_acc: 0.7139, best: 0.7161, time: 0:00:55
 Epoch: 206, lr: 2.0e-03, train_loss: 0.7069, train_acc: 0.7516 test_loss: 0.8333, test_acc: 0.7091, best: 0.7161, time: 0:00:55
 Epoch: 207, lr: 2.0e-03, train_loss: 0.6894, train_acc: 0.7550 test_loss: 0.8341, test_acc: 0.7119, best: 0.7161, time: 0:00:56
 Epoch: 208, lr: 2.0e-03, train_loss: 0.7066, train_acc: 0.7582 test_loss: 0.8429, test_acc: 0.7096, best: 0.7161, time: 0:00:56
 Epoch: 209, lr: 2.0e-03, train_loss: 0.7171, train_acc: 0.7474 test_loss: 0.8219, test_acc: 0.7179, best: 0.7179, time: 0:00:56
 Epoch: 210, lr: 2.0e-03, train_loss: 0.7072, train_acc: 0.7434 test_loss: 0.8260, test_acc: 0.7121, best: 0.7179, time: 0:00:55
 Epoch: 211, lr: 2.0e-03, train_loss: 0.7038, train_acc: 0.7526 test_loss: 0.8255, test_acc: 0.7127, best: 0.7179, time: 0:00:56
 Epoch: 212, lr: 2.0e-03, train_loss: 0.6856, train_acc: 0.7628 test_loss: 0.8292, test_acc: 0.7153, best: 0.7179, time: 0:00:55
 Epoch: 213, lr: 2.0e-03, train_loss: 0.6988, train_acc: 0.7526 test_loss: 0.8356, test_acc: 0.7113, best: 0.7179, time: 0:00:56
 Epoch: 214, lr: 2.0e-03, train_loss: 0.6931, train_acc: 0.7562 test_loss: 0.8473, test_acc: 0.7050, best: 0.7179, time: 0:00:55
 Epoch: 215, lr: 2.0e-03, train_loss: 0.6940, train_acc: 0.7556 test_loss: 0.8475, test_acc: 0.7094, best: 0.7179, time: 0:00:55
 Epoch: 216, lr: 2.0e-03, train_loss: 0.7206, train_acc: 0.7420 test_loss: 0.8399, test_acc: 0.7107, best: 0.7179, time: 0:00:55
 Epoch: 217, lr: 2.0e-03, train_loss: 0.6886, train_acc: 0.7532 test_loss: 0.8380, test_acc: 0.7116, best: 0.7179, time: 0:00:55
 Epoch: 218, lr: 2.0e-03, train_loss: 0.6939, train_acc: 0.7486 test_loss: 0.8629, test_acc: 0.7075, best: 0.7179, time: 0:00:55
 Epoch: 219, lr: 2.0e-03, train_loss: 0.7042, train_acc: 0.7430 test_loss: 0.8415, test_acc: 0.7136, best: 0.7179, time: 0:00:56
 Epoch: 220, lr: 2.0e-03, train_loss: 0.6854, train_acc: 0.7500 test_loss: 0.8497, test_acc: 0.7113, best: 0.7179, time: 0:00:55
 Epoch: 221, lr: 2.0e-03, train_loss: 0.6838, train_acc: 0.7628 test_loss: 0.8179, test_acc: 0.7177, best: 0.7179, time: 0:00:55
 Epoch: 222, lr: 2.0e-03, train_loss: 0.6708, train_acc: 0.7652 test_loss: 0.8407, test_acc: 0.7104, best: 0.7179, time: 0:00:56
 Epoch: 223, lr: 2.0e-03, train_loss: 0.6774, train_acc: 0.7606 test_loss: 0.8265, test_acc: 0.7167, best: 0.7179, time: 0:00:55
 Epoch: 224, lr: 2.0e-03, train_loss: 0.6981, train_acc: 0.7582 test_loss: 0.8258, test_acc: 0.7177, best: 0.7179, time: 0:00:55
 Epoch: 225, lr: 2.0e-03, train_loss: 0.6917, train_acc: 0.7620 test_loss: 0.8814, test_acc: 0.7000, best: 0.7179, time: 0:00:55
 Epoch: 226, lr: 2.0e-03, train_loss: 0.7031, train_acc: 0.7524 test_loss: 0.8249, test_acc: 0.7160, best: 0.7179, time: 0:00:55
 Epoch: 227, lr: 2.0e-03, train_loss: 0.6847, train_acc: 0.7542 test_loss: 0.8321, test_acc: 0.7135, best: 0.7179, time: 0:00:55
 Epoch: 228, lr: 2.0e-03, train_loss: 0.6843, train_acc: 0.7624 test_loss: 0.8421, test_acc: 0.7115, best: 0.7179, time: 0:00:55
 Epoch: 229, lr: 2.0e-03, train_loss: 0.6896, train_acc: 0.7502 test_loss: 0.8475, test_acc: 0.7125, best: 0.7179, time: 0:00:55
 Epoch: 230, lr: 2.0e-03, train_loss: 0.6666, train_acc: 0.7630 test_loss: 0.8418, test_acc: 0.7165, best: 0.7179, time: 0:00:56
 Epoch: 231, lr: 2.0e-03, train_loss: 0.6644, train_acc: 0.7674 test_loss: 0.8395, test_acc: 0.7160, best: 0.7179, time: 0:00:56
 Epoch: 232, lr: 2.0e-03, train_loss: 0.6748, train_acc: 0.7596 test_loss: 0.8419, test_acc: 0.7101, best: 0.7179, time: 0:00:56
 Epoch: 233, lr: 2.0e-03, train_loss: 0.6598, train_acc: 0.7662 test_loss: 0.8442, test_acc: 0.7127, best: 0.7179, time: 0:00:55
 Epoch: 234, lr: 2.0e-03, train_loss: 0.6636, train_acc: 0.7598 test_loss: 0.8787, test_acc: 0.7083, best: 0.7179, time: 0:00:56
 Epoch: 235, lr: 2.0e-03, train_loss: 0.6600, train_acc: 0.7654 test_loss: 0.8517, test_acc: 0.7133, best: 0.7179, time: 0:00:56
 Epoch: 236, lr: 2.0e-03, train_loss: 0.6482, train_acc: 0.7658 test_loss: 0.8388, test_acc: 0.7120, best: 0.7179, time: 0:00:55
 Epoch: 237, lr: 2.0e-03, train_loss: 0.6505, train_acc: 0.7712 test_loss: 0.8590, test_acc: 0.7119, best: 0.7179, time: 0:00:56
 Epoch: 238, lr: 2.0e-03, train_loss: 0.6623, train_acc: 0.7638 test_loss: 0.8392, test_acc: 0.7159, best: 0.7179, time: 0:00:55
 Epoch: 239, lr: 2.0e-03, train_loss: 0.6564, train_acc: 0.7652 test_loss: 0.8458, test_acc: 0.7139, best: 0.7179, time: 0:00:56
 Epoch: 240, lr: 4.0e-04, train_loss: 0.6328, train_acc: 0.7774 test_loss: 0.8308, test_acc: 0.7215, best: 0.7215, time: 0:00:56
 Epoch: 241, lr: 4.0e-04, train_loss: 0.6380, train_acc: 0.7690 test_loss: 0.8259, test_acc: 0.7216, best: 0.7216, time: 0:00:56
 Epoch: 242, lr: 4.0e-04, train_loss: 0.6474, train_acc: 0.7720 test_loss: 0.8249, test_acc: 0.7238, best: 0.7238, time: 0:00:56
 Epoch: 243, lr: 4.0e-04, train_loss: 0.6492, train_acc: 0.7688 test_loss: 0.8421, test_acc: 0.7166, best: 0.7238, time: 0:00:55
 Epoch: 244, lr: 4.0e-04, train_loss: 0.6416, train_acc: 0.7760 test_loss: 0.8283, test_acc: 0.7177, best: 0.7238, time: 0:00:55
 Epoch: 245, lr: 4.0e-04, train_loss: 0.6514, train_acc: 0.7726 test_loss: 0.8430, test_acc: 0.7156, best: 0.7238, time: 0:00:55
 Epoch: 246, lr: 4.0e-04, train_loss: 0.6437, train_acc: 0.7682 test_loss: 0.8558, test_acc: 0.7136, best: 0.7238, time: 0:00:55
 Epoch: 247, lr: 4.0e-04, train_loss: 0.6268, train_acc: 0.7794 test_loss: 0.8297, test_acc: 0.7180, best: 0.7238, time: 0:00:55
 Epoch: 248, lr: 4.0e-04, train_loss: 0.6144, train_acc: 0.7852 test_loss: 0.8699, test_acc: 0.7133, best: 0.7238, time: 0:00:55
 Epoch: 249, lr: 4.0e-04, train_loss: 0.6117, train_acc: 0.7796 test_loss: 0.8418, test_acc: 0.7156, best: 0.7238, time: 0:00:56
 Epoch: 250, lr: 4.0e-04, train_loss: 0.6296, train_acc: 0.7726 test_loss: 0.8306, test_acc: 0.7191, best: 0.7238, time: 0:00:56
 Epoch: 251, lr: 4.0e-04, train_loss: 0.6168, train_acc: 0.7782 test_loss: 0.8435, test_acc: 0.7167, best: 0.7238, time: 0:00:56
 Epoch: 252, lr: 4.0e-04, train_loss: 0.6270, train_acc: 0.7762 test_loss: 0.8429, test_acc: 0.7155, best: 0.7238, time: 0:00:56
 Epoch: 253, lr: 4.0e-04, train_loss: 0.6442, train_acc: 0.7704 test_loss: 0.8431, test_acc: 0.7143, best: 0.7238, time: 0:00:58
 Epoch: 254, lr: 4.0e-04, train_loss: 0.6257, train_acc: 0.7790 test_loss: 0.8226, test_acc: 0.7181, best: 0.7238, time: 0:00:58
 Epoch: 255, lr: 4.0e-04, train_loss: 0.6256, train_acc: 0.7808 test_loss: 0.8236, test_acc: 0.7189, best: 0.7238, time: 0:00:57
 Epoch: 256, lr: 4.0e-04, train_loss: 0.6092, train_acc: 0.7836 test_loss: 0.8444, test_acc: 0.7204, best: 0.7238, time: 0:00:57
 Epoch: 257, lr: 4.0e-04, train_loss: 0.6130, train_acc: 0.7902 test_loss: 0.8331, test_acc: 0.7202, best: 0.7238, time: 0:00:57
 Epoch: 258, lr: 4.0e-04, train_loss: 0.6266, train_acc: 0.7800 test_loss: 0.8352, test_acc: 0.7184, best: 0.7238, time: 0:00:57
 Epoch: 259, lr: 4.0e-04, train_loss: 0.6291, train_acc: 0.7820 test_loss: 0.8377, test_acc: 0.7170, best: 0.7238, time: 0:00:57
 Epoch: 260, lr: 4.0e-04, train_loss: 0.6237, train_acc: 0.7782 test_loss: 0.8608, test_acc: 0.7110, best: 0.7238, time: 0:00:57
 Epoch: 261, lr: 4.0e-04, train_loss: 0.6263, train_acc: 0.7758 test_loss: 0.8413, test_acc: 0.7126, best: 0.7238, time: 0:00:57
 Epoch: 262, lr: 4.0e-04, train_loss: 0.6249, train_acc: 0.7770 test_loss: 0.8452, test_acc: 0.7145, best: 0.7238, time: 0:00:57
 Epoch: 263, lr: 4.0e-04, train_loss: 0.6106, train_acc: 0.7912 test_loss: 0.8318, test_acc: 0.7166, best: 0.7238, time: 0:00:57
 Epoch: 264, lr: 4.0e-04, train_loss: 0.6287, train_acc: 0.7702 test_loss: 0.8367, test_acc: 0.7164, best: 0.7238, time: 0:00:57
 Epoch: 265, lr: 4.0e-04, train_loss: 0.6082, train_acc: 0.7862 test_loss: 0.8554, test_acc: 0.7180, best: 0.7238, time: 0:00:57
 Epoch: 266, lr: 4.0e-04, train_loss: 0.6286, train_acc: 0.7762 test_loss: 0.8247, test_acc: 0.7214, best: 0.7238, time: 0:00:57
 Epoch: 267, lr: 4.0e-04, train_loss: 0.6143, train_acc: 0.7872 test_loss: 0.8175, test_acc: 0.7235, best: 0.7238, time: 0:00:57
 Epoch: 268, lr: 4.0e-04, train_loss: 0.6130, train_acc: 0.7804 test_loss: 0.8370, test_acc: 0.7212, best: 0.7238, time: 0:00:57
 Epoch: 269, lr: 4.0e-04, train_loss: 0.6242, train_acc: 0.7770 test_loss: 0.8355, test_acc: 0.7195, best: 0.7238, time: 0:00:57
 Epoch: 270, lr: 8.0e-05, train_loss: 0.6109, train_acc: 0.7880 test_loss: 0.8305, test_acc: 0.7194, best: 0.7238, time: 0:00:58
 Epoch: 271, lr: 8.0e-05, train_loss: 0.6171, train_acc: 0.7762 test_loss: 0.8343, test_acc: 0.7185, best: 0.7238, time: 0:00:58
 Epoch: 272, lr: 8.0e-05, train_loss: 0.6200, train_acc: 0.7788 test_loss: 0.8373, test_acc: 0.7190, best: 0.7238, time: 0:00:57
 Epoch: 273, lr: 8.0e-05, train_loss: 0.5859, train_acc: 0.7906 test_loss: 0.8560, test_acc: 0.7165, best: 0.7238, time: 0:00:57
 Epoch: 274, lr: 8.0e-05, train_loss: 0.6287, train_acc: 0.7804 test_loss: 0.8272, test_acc: 0.7219, best: 0.7238, time: 0:00:57
 Epoch: 275, lr: 8.0e-05, train_loss: 0.6179, train_acc: 0.7804 test_loss: 0.8402, test_acc: 0.7194, best: 0.7238, time: 0:00:58
 Epoch: 276, lr: 8.0e-05, train_loss: 0.6036, train_acc: 0.7878 test_loss: 0.8166, test_acc: 0.7219, best: 0.7238, time: 0:00:56
 Epoch: 277, lr: 8.0e-05, train_loss: 0.6269, train_acc: 0.7786 test_loss: 0.8347, test_acc: 0.7177, best: 0.7238, time: 0:00:55
 Epoch: 278, lr: 8.0e-05, train_loss: 0.6072, train_acc: 0.7840 test_loss: 0.8465, test_acc: 0.7196, best: 0.7238, time: 0:00:56
 Epoch: 279, lr: 8.0e-05, train_loss: 0.6126, train_acc: 0.7816 test_loss: 0.8344, test_acc: 0.7167, best: 0.7238, time: 0:00:55
 Epoch: 280, lr: 8.0e-05, train_loss: 0.6057, train_acc: 0.7858 test_loss: 0.8186, test_acc: 0.7248, best: 0.7248, time: 0:00:56
 Epoch: 281, lr: 8.0e-05, train_loss: 0.6121, train_acc: 0.7828 test_loss: 0.8264, test_acc: 0.7249, best: 0.7249, time: 0:00:56
 Epoch: 282, lr: 8.0e-05, train_loss: 0.6248, train_acc: 0.7836 test_loss: 0.8214, test_acc: 0.7205, best: 0.7249, time: 0:00:56
 Epoch: 283, lr: 8.0e-05, train_loss: 0.5935, train_acc: 0.7936 test_loss: 0.8256, test_acc: 0.7215, best: 0.7249, time: 0:00:55
 Epoch: 284, lr: 8.0e-05, train_loss: 0.6172, train_acc: 0.7836 test_loss: 0.8235, test_acc: 0.7210, best: 0.7249, time: 0:00:55
 Epoch: 285, lr: 8.0e-05, train_loss: 0.6090, train_acc: 0.7890 test_loss: 0.8214, test_acc: 0.7202, best: 0.7249, time: 0:00:56
 Epoch: 286, lr: 8.0e-05, train_loss: 0.5986, train_acc: 0.7818 test_loss: 0.8316, test_acc: 0.7180, best: 0.7249, time: 0:00:56
 Epoch: 287, lr: 8.0e-05, train_loss: 0.6025, train_acc: 0.7830 test_loss: 0.8551, test_acc: 0.7156, best: 0.7249, time: 0:00:55
 Epoch: 288, lr: 8.0e-05, train_loss: 0.6198, train_acc: 0.7830 test_loss: 0.8416, test_acc: 0.7175, best: 0.7249, time: 0:00:55
 Epoch: 289, lr: 8.0e-05, train_loss: 0.5959, train_acc: 0.7888 test_loss: 0.8284, test_acc: 0.7205, best: 0.7249, time: 0:00:55
 Epoch: 290, lr: 8.0e-05, train_loss: 0.6036, train_acc: 0.7858 test_loss: 0.8254, test_acc: 0.7220, best: 0.7249, time: 0:00:55
 Epoch: 291, lr: 8.0e-05, train_loss: 0.6118, train_acc: 0.7800 test_loss: 0.8180, test_acc: 0.7196, best: 0.7249, time: 0:00:55
 Epoch: 292, lr: 8.0e-05, train_loss: 0.6077, train_acc: 0.7876 test_loss: 0.8224, test_acc: 0.7174, best: 0.7249, time: 0:00:55
 Epoch: 293, lr: 8.0e-05, train_loss: 0.6370, train_acc: 0.7714 test_loss: 0.8405, test_acc: 0.7149, best: 0.7249, time: 0:00:56
 Epoch: 294, lr: 8.0e-05, train_loss: 0.6053, train_acc: 0.7900 test_loss: 0.8333, test_acc: 0.7201, best: 0.7249, time: 0:00:55
 Epoch: 295, lr: 8.0e-05, train_loss: 0.6403, train_acc: 0.7684 test_loss: 0.8403, test_acc: 0.7160, best: 0.7249, time: 0:00:55
 Epoch: 296, lr: 8.0e-05, train_loss: 0.6261, train_acc: 0.7828 test_loss: 0.8274, test_acc: 0.7228, best: 0.7249, time: 0:00:56
 Epoch: 297, lr: 8.0e-05, train_loss: 0.6201, train_acc: 0.7786 test_loss: 0.8277, test_acc: 0.7192, best: 0.7249, time: 0:00:55
 Epoch: 298, lr: 8.0e-05, train_loss: 0.6230, train_acc: 0.7786 test_loss: 0.8204, test_acc: 0.7166, best: 0.7249, time: 0:00:55
 Epoch: 299, lr: 8.0e-05, train_loss: 0.5987, train_acc: 0.7874 test_loss: 0.8294, test_acc: 0.7210, best: 0.7249, time: 0:00:55
 Highest accuracy: 0.7249