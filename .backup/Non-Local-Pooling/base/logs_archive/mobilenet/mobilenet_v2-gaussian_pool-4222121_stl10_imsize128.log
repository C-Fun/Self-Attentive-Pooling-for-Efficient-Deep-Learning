
 Run on time: 2022-07-03 01:05:12.079250

 Architecture: mobilenet_v2-gaussian_pool-4222121

 Pool Config: {
    "arch": "mobilenet_v2",
    "conv1": {
        "_conv2d": "norm",
        "pool_cfg": {}
    },
    "layer1": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "gaussian_pool",
            "_stride": 4,
            "_psize": null,
            "_dim_reduced_ratio": null,
            "_num_heads": null,
            "_conv2d": null,
            "_win_norm": null
        }
    },
    "layer2": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "gaussian_pool",
            "_stride": 2,
            "_psize": null,
            "_dim_reduced_ratio": null,
            "_num_heads": null,
            "_conv2d": null,
            "_win_norm": null
        }
    },
    "layer3": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "gaussian_pool",
            "_stride": 2,
            "_psize": null,
            "_dim_reduced_ratio": null,
            "_num_heads": null,
            "_conv2d": null,
            "_win_norm": null
        }
    },
    "layer4": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "gaussian_pool",
            "_stride": 2,
            "_psize": null,
            "_dim_reduced_ratio": null,
            "_num_heads": null,
            "_conv2d": null,
            "_win_norm": null
        }
    },
    "layer5": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "skip",
            "_stride": 1,
            "_psize": null,
            "_dim_reduced_ratio": null,
            "_num_heads": null,
            "_conv2d": null,
            "_win_norm": null
        }
    },
    "layer6": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "gaussian_pool",
            "_stride": 2,
            "_psize": null,
            "_dim_reduced_ratio": null,
            "_num_heads": null,
            "_conv2d": null,
            "_win_norm": null
        }
    },
    "layer7": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "skip",
            "_stride": 1,
            "_psize": null,
            "_dim_reduced_ratio": null,
            "_num_heads": null,
            "_conv2d": null,
            "_win_norm": null
        }
    },
    "conv2": {
        "_conv2d": "norm",
        "pool_cfg": {}
    }
}

 Arguments:
	 gpu                  : True
	 log                  : True
	 seed                 : 0
	 momentum             : 0.9
	 amsgrad              : True
	 dataset              : STL10
	 batch_size           : 8
	 architecture         : MOBILENET_V2-GAUSSIAN_POOL-4222121
	 im_size              : 128
	 relu_threshold       : 4.0
	 learning_rate        : 0.01
	 pretrained_backbone  : 
	 pretrained_ann       : 
	 weight_decay         : 0.0
	 test_only            : False
	 epochs               : 300
	 lr_interval          : [180, 240, 270]
	 lr_reduce            : 5
	 optimizer            : SGD
	 dropout              : 0.2
	 kernel_size          : 3
	 dont_save            : False
	 visualize            : False
	 devices              : 0
 DataParallel(
  (module): Network(
    (net): MobileNetV2(
      (conv1): Sequential(
        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (features): Sequential(
        (0): InvertedResidual(
          (pooling): GaussianPooling2d(
            kernel_size=4, stride=4, padding=0
            (ToHidden): Sequential(
              (0): AdaptiveAvgPool2d(output_size=(1, 1))
              (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
              (2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (3): ReLU()
            )
            (ToMean): Sequential(
              (0): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (ToSigma): Sequential(
              (0): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Sigmoid()
            )
            (activation): Softplus(beta=1, threshold=20)
          )
          (conv): Sequential(
            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): InvertedResidual(
          (pooling): GaussianPooling2d(
            kernel_size=2, stride=2, padding=0
            (ToHidden): Sequential(
              (0): AdaptiveAvgPool2d(output_size=(1, 1))
              (1): Conv2d(24, 12, kernel_size=(1, 1), stride=(1, 1))
              (2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (3): ReLU()
            )
            (ToMean): Sequential(
              (0): Conv2d(12, 24, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (ToSigma): Sequential(
              (0): Conv2d(12, 24, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Sigmoid()
            )
            (activation): Softplus(beta=1, threshold=20)
          )
          (conv): Sequential(
            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (2): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
            (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (3): InvertedResidual(
          (pooling): GaussianPooling2d(
            kernel_size=2, stride=2, padding=0
            (ToHidden): Sequential(
              (0): AdaptiveAvgPool2d(output_size=(1, 1))
              (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))
              (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (3): ReLU()
            )
            (ToMean): Sequential(
              (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (ToSigma): Sequential(
              (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Sigmoid()
            )
            (activation): Softplus(beta=1, threshold=20)
          )
          (conv): Sequential(
            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
            (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (4): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (5): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (6): InvertedResidual(
          (pooling): GaussianPooling2d(
            kernel_size=2, stride=2, padding=0
            (ToHidden): Sequential(
              (0): AdaptiveAvgPool2d(output_size=(1, 1))
              (1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (3): ReLU()
            )
            (ToMean): Sequential(
              (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (ToSigma): Sequential(
              (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Sigmoid()
            )
            (activation): Softplus(beta=1, threshold=20)
          )
          (conv): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (7): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (8): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (9): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (10): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (11): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
            (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (12): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
            (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (13): InvertedResidual(
          (pooling): GaussianPooling2d(
            kernel_size=2, stride=2, padding=0
            (ToHidden): Sequential(
              (0): AdaptiveAvgPool2d(output_size=(1, 1))
              (1): Conv2d(160, 80, kernel_size=(1, 1), stride=(1, 1))
              (2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (3): ReLU()
            )
            (ToMean): Sequential(
              (0): Conv2d(80, 160, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (ToSigma): Sequential(
              (0): Conv2d(80, 160, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Sigmoid()
            )
            (activation): Softplus(beta=1, threshold=20)
          )
          (conv): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
            (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (14): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
            (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (15): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
            (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (16): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
            (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
      (conv2): Sequential(
        (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
      (classifier): Linear(in_features=1280, out_features=10, bias=True)
    )
  )
)
 SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0
)
 Epoch: 1, lr: 1.0e-02, train_loss: 3.3065, train_acc: 0.1282 test_loss: 2.5102, test_acc: 0.1103, best: 0.1103, time: 0:00:57
 Epoch: 2, lr: 1.0e-02, train_loss: 2.4573, train_acc: 0.1382 test_loss: 2.2662, test_acc: 0.1547, best: 0.1547, time: 0:00:57
 Epoch: 3, lr: 1.0e-02, train_loss: 2.3038, train_acc: 0.1434 test_loss: 2.3789, test_acc: 0.1701, best: 0.1701, time: 0:00:57
 Epoch: 4, lr: 1.0e-02, train_loss: 2.2071, train_acc: 0.1728 test_loss: 2.0737, test_acc: 0.1921, best: 0.1921, time: 0:00:57
 Epoch: 5, lr: 1.0e-02, train_loss: 2.1467, train_acc: 0.1952 test_loss: 2.1091, test_acc: 0.2031, best: 0.2031, time: 0:00:56
 Epoch: 6, lr: 1.0e-02, train_loss: 2.1783, train_acc: 0.1854 test_loss: 2.1583, test_acc: 0.1737, best: 0.2031, time: 0:00:57
 Epoch: 7, lr: 1.0e-02, train_loss: 2.1837, train_acc: 0.1730 test_loss: 2.1041, test_acc: 0.2102, best: 0.2102, time: 0:00:57
 Epoch: 8, lr: 1.0e-02, train_loss: 2.1444, train_acc: 0.1748 test_loss: 2.0345, test_acc: 0.1846, best: 0.2102, time: 0:00:56
 Epoch: 9, lr: 1.0e-02, train_loss: 2.0796, train_acc: 0.2032 test_loss: 1.9858, test_acc: 0.2367, best: 0.2367, time: 0:00:57
 Epoch: 10, lr: 1.0e-02, train_loss: 2.0966, train_acc: 0.1930 test_loss: 2.0196, test_acc: 0.2300, best: 0.2367, time: 0:00:56
 Epoch: 11, lr: 1.0e-02, train_loss: 2.0671, train_acc: 0.2048 test_loss: 2.1993, test_acc: 0.1709, best: 0.2367, time: 0:00:56
 Epoch: 12, lr: 1.0e-02, train_loss: 2.2472, train_acc: 0.1644 test_loss: 2.2239, test_acc: 0.1847, best: 0.2367, time: 0:00:56
 Epoch: 13, lr: 1.0e-02, train_loss: 2.2399, train_acc: 0.1700 test_loss: 2.2254, test_acc: 0.1767, best: 0.2367, time: 0:00:56
 Epoch: 14, lr: 1.0e-02, train_loss: 2.2678, train_acc: 0.1326 test_loss: 2.2606, test_acc: 0.1526, best: 0.2367, time: 0:00:56
 Epoch: 15, lr: 1.0e-02, train_loss: 2.2899, train_acc: 0.1188 test_loss: 2.2847, test_acc: 0.1479, best: 0.2367, time: 0:00:57
 Epoch: 16, lr: 1.0e-02, train_loss: 2.2622, train_acc: 0.1382 test_loss: 2.2255, test_acc: 0.1644, best: 0.2367, time: 0:00:57
 Epoch: 17, lr: 1.0e-02, train_loss: 2.2546, train_acc: 0.1412 test_loss: 2.2075, test_acc: 0.1695, best: 0.2367, time: 0:00:56
 Epoch: 18, lr: 1.0e-02, train_loss: 2.2143, train_acc: 0.1756 test_loss: 2.2152, test_acc: 0.1782, best: 0.2367, time: 0:00:56
 Epoch: 19, lr: 1.0e-02, train_loss: 2.2162, train_acc: 0.1684 test_loss: 2.1618, test_acc: 0.2189, best: 0.2367, time: 0:00:56
 Epoch: 20, lr: 1.0e-02, train_loss: 2.2080, train_acc: 0.1818 test_loss: 2.2052, test_acc: 0.1875, best: 0.2367, time: 0:00:56
 Epoch: 21, lr: 1.0e-02, train_loss: 2.2347, train_acc: 0.1578 test_loss: 2.2502, test_acc: 0.1414, best: 0.2367, time: 0:00:56
 Epoch: 22, lr: 1.0e-02, train_loss: 2.2398, train_acc: 0.1612 test_loss: 2.2147, test_acc: 0.1819, best: 0.2367, time: 0:00:56
 Epoch: 23, lr: 1.0e-02, train_loss: 2.2612, train_acc: 0.1362 test_loss: 2.2808, test_acc: 0.1224, best: 0.2367, time: 0:00:57
 Epoch: 24, lr: 1.0e-02, train_loss: 2.2609, train_acc: 0.1440 test_loss: 2.2035, test_acc: 0.1635, best: 0.2367, time: 0:00:56
 Epoch: 25, lr: 1.0e-02, train_loss: 2.2579, train_acc: 0.1462 test_loss: 2.2352, test_acc: 0.1479, best: 0.2367, time: 0:00:57
 Epoch: 26, lr: 1.0e-02, train_loss: 2.2504, train_acc: 0.1512 test_loss: 2.2044, test_acc: 0.2086, best: 0.2367, time: 0:00:57
 Epoch: 27, lr: 1.0e-02, train_loss: 2.2182, train_acc: 0.1726 test_loss: 2.2082, test_acc: 0.1640, best: 0.2367, time: 0:00:56
 Epoch: 28, lr: 1.0e-02, train_loss: 2.2331, train_acc: 0.1494 test_loss: 2.1521, test_acc: 0.2070, best: 0.2367, time: 0:00:57
 Epoch: 29, lr: 1.0e-02, train_loss: 2.2160, train_acc: 0.1752 test_loss: 2.1897, test_acc: 0.2189, best: 0.2367, time: 0:00:57
 Epoch: 30, lr: 1.0e-02, train_loss: 2.2095, train_acc: 0.1610 test_loss: 2.1631, test_acc: 0.1965, best: 0.2367, time: 0:00:56
 Epoch: 31, lr: 1.0e-02, train_loss: 2.1741, train_acc: 0.1806 test_loss: 2.1090, test_acc: 0.2377, best: 0.2377, time: 0:00:57
 Epoch: 32, lr: 1.0e-02, train_loss: 2.1599, train_acc: 0.1818 test_loss: 2.0824, test_acc: 0.2339, best: 0.2377, time: 0:00:57
 Epoch: 33, lr: 1.0e-02, train_loss: 2.1593, train_acc: 0.1890 test_loss: 2.1037, test_acc: 0.2055, best: 0.2377, time: 0:00:57
 Epoch: 34, lr: 1.0e-02, train_loss: 2.1382, train_acc: 0.1970 test_loss: 2.0899, test_acc: 0.2341, best: 0.2377, time: 0:00:57
 Epoch: 35, lr: 1.0e-02, train_loss: 2.1107, train_acc: 0.2044 test_loss: 2.0358, test_acc: 0.2606, best: 0.2606, time: 0:00:57
 Epoch: 36, lr: 1.0e-02, train_loss: 2.0990, train_acc: 0.2038 test_loss: 2.0747, test_acc: 0.2545, best: 0.2606, time: 0:00:57
 Epoch: 37, lr: 1.0e-02, train_loss: 2.0892, train_acc: 0.2116 test_loss: 2.0025, test_acc: 0.2532, best: 0.2606, time: 0:00:56
 Epoch: 38, lr: 1.0e-02, train_loss: 2.0829, train_acc: 0.2176 test_loss: 2.0478, test_acc: 0.2496, best: 0.2606, time: 0:00:56
 Epoch: 39, lr: 1.0e-02, train_loss: 2.1162, train_acc: 0.2032 test_loss: 2.0619, test_acc: 0.2474, best: 0.2606, time: 0:00:56
 Epoch: 40, lr: 1.0e-02, train_loss: 2.1171, train_acc: 0.1968 test_loss: 2.0446, test_acc: 0.2211, best: 0.2606, time: 0:00:56
 Epoch: 41, lr: 1.0e-02, train_loss: 2.1573, train_acc: 0.1828 test_loss: 2.0993, test_acc: 0.2185, best: 0.2606, time: 0:00:56
 Epoch: 42, lr: 1.0e-02, train_loss: 2.1033, train_acc: 0.1922 test_loss: 2.0683, test_acc: 0.2223, best: 0.2606, time: 0:00:56
 Epoch: 43, lr: 1.0e-02, train_loss: 2.0960, train_acc: 0.2090 test_loss: 2.0732, test_acc: 0.2231, best: 0.2606, time: 0:00:57
 Epoch: 44, lr: 1.0e-02, train_loss: 2.0744, train_acc: 0.2130 test_loss: 2.0185, test_acc: 0.2361, best: 0.2606, time: 0:00:56
 Epoch: 45, lr: 1.0e-02, train_loss: 2.0539, train_acc: 0.2146 test_loss: 1.9844, test_acc: 0.2407, best: 0.2606, time: 0:00:56
 Epoch: 46, lr: 1.0e-02, train_loss: 2.0064, train_acc: 0.2254 test_loss: 1.9584, test_acc: 0.2565, best: 0.2606, time: 0:00:56
 Epoch: 47, lr: 1.0e-02, train_loss: 2.0032, train_acc: 0.2312 test_loss: 1.9052, test_acc: 0.2721, best: 0.2721, time: 0:00:56
 Epoch: 48, lr: 1.0e-02, train_loss: 1.9797, train_acc: 0.2376 test_loss: 1.8969, test_acc: 0.2677, best: 0.2721, time: 0:00:56
 Epoch: 49, lr: 1.0e-02, train_loss: 1.9813, train_acc: 0.2330 test_loss: 1.8729, test_acc: 0.2848, best: 0.2848, time: 0:00:56
 Epoch: 50, lr: 1.0e-02, train_loss: 1.9720, train_acc: 0.2374 test_loss: 1.8662, test_acc: 0.2811, best: 0.2848, time: 0:00:57
 Epoch: 51, lr: 1.0e-02, train_loss: 1.9937, train_acc: 0.2404 test_loss: 1.8863, test_acc: 0.2903, best: 0.2903, time: 0:00:57
 Epoch: 52, lr: 1.0e-02, train_loss: 1.9603, train_acc: 0.2424 test_loss: 1.8911, test_acc: 0.3014, best: 0.3014, time: 0:00:57
 Epoch: 53, lr: 1.0e-02, train_loss: 1.9613, train_acc: 0.2508 test_loss: 1.8465, test_acc: 0.3184, best: 0.3184, time: 0:00:56
 Epoch: 54, lr: 1.0e-02, train_loss: 1.9493, train_acc: 0.2470 test_loss: 1.8593, test_acc: 0.2874, best: 0.3184, time: 0:00:56
 Epoch: 55, lr: 1.0e-02, train_loss: 1.9358, train_acc: 0.2578 test_loss: 1.8221, test_acc: 0.3004, best: 0.3184, time: 0:00:56
 Epoch: 56, lr: 1.0e-02, train_loss: 1.9369, train_acc: 0.2626 test_loss: 1.7981, test_acc: 0.3315, best: 0.3315, time: 0:00:56
 Epoch: 57, lr: 1.0e-02, train_loss: 1.9346, train_acc: 0.2606 test_loss: 1.9164, test_acc: 0.2845, best: 0.3315, time: 0:00:56
 Epoch: 58, lr: 1.0e-02, train_loss: 1.9299, train_acc: 0.2538 test_loss: 1.7986, test_acc: 0.3045, best: 0.3315, time: 0:00:55
 Epoch: 59, lr: 1.0e-02, train_loss: 1.9091, train_acc: 0.2560 test_loss: 1.7955, test_acc: 0.3003, best: 0.3315, time: 0:00:56
 Epoch: 60, lr: 1.0e-02, train_loss: 1.8953, train_acc: 0.2670 test_loss: 1.7870, test_acc: 0.3266, best: 0.3315, time: 0:00:56
 Epoch: 61, lr: 1.0e-02, train_loss: 1.9430, train_acc: 0.2538 test_loss: 1.8348, test_acc: 0.3023, best: 0.3315, time: 0:00:56
 Epoch: 62, lr: 1.0e-02, train_loss: 1.9273, train_acc: 0.2490 test_loss: 1.8527, test_acc: 0.2792, best: 0.3315, time: 0:00:56
 Epoch: 63, lr: 1.0e-02, train_loss: 1.9511, train_acc: 0.2486 test_loss: 1.9341, test_acc: 0.2609, best: 0.3315, time: 0:00:55
 Epoch: 64, lr: 1.0e-02, train_loss: 1.9558, train_acc: 0.2482 test_loss: 1.8803, test_acc: 0.2955, best: 0.3315, time: 0:00:55
 Epoch: 65, lr: 1.0e-02, train_loss: 1.9741, train_acc: 0.2400 test_loss: 1.9468, test_acc: 0.2661, best: 0.3315, time: 0:00:55
 Epoch: 66, lr: 1.0e-02, train_loss: 1.9657, train_acc: 0.2470 test_loss: 1.8550, test_acc: 0.3140, best: 0.3315, time: 0:00:55
 Epoch: 67, lr: 1.0e-02, train_loss: 1.9395, train_acc: 0.2608 test_loss: 1.8805, test_acc: 0.2896, best: 0.3315, time: 0:00:55
 Epoch: 68, lr: 1.0e-02, train_loss: 1.9291, train_acc: 0.2584 test_loss: 1.8912, test_acc: 0.2496, best: 0.3315, time: 0:00:55
 Epoch: 69, lr: 1.0e-02, train_loss: 1.9295, train_acc: 0.2592 test_loss: 1.8190, test_acc: 0.3296, best: 0.3315, time: 0:00:56
 Epoch: 70, lr: 1.0e-02, train_loss: 1.9081, train_acc: 0.2668 test_loss: 1.8529, test_acc: 0.3126, best: 0.3315, time: 0:00:55
 Epoch: 71, lr: 1.0e-02, train_loss: 1.9357, train_acc: 0.2608 test_loss: 1.8378, test_acc: 0.3045, best: 0.3315, time: 0:00:55
 Epoch: 72, lr: 1.0e-02, train_loss: 1.9333, train_acc: 0.2682 test_loss: 1.9906, test_acc: 0.2731, best: 0.3315, time: 0:00:55
 Epoch: 73, lr: 1.0e-02, train_loss: 2.1206, train_acc: 0.2048 test_loss: 2.1591, test_acc: 0.1708, best: 0.3315, time: 0:00:55
 Epoch: 74, lr: 1.0e-02, train_loss: 2.1127, train_acc: 0.1996 test_loss: 2.0302, test_acc: 0.2376, best: 0.3315, time: 0:00:55
 Epoch: 75, lr: 1.0e-02, train_loss: 2.0783, train_acc: 0.2146 test_loss: 2.0178, test_acc: 0.1983, best: 0.3315, time: 0:00:55
 Epoch: 76, lr: 1.0e-02, train_loss: 2.0845, train_acc: 0.2104 test_loss: 2.2005, test_acc: 0.1596, best: 0.3315, time: 0:00:55
 Epoch: 77, lr: 1.0e-02, train_loss: 2.1106, train_acc: 0.2014 test_loss: 1.9409, test_acc: 0.2779, best: 0.3315, time: 0:00:55
 Epoch: 78, lr: 1.0e-02, train_loss: 2.0734, train_acc: 0.2256 test_loss: 1.9901, test_acc: 0.2376, best: 0.3315, time: 0:00:55
 Epoch: 79, lr: 1.0e-02, train_loss: 2.0905, train_acc: 0.2098 test_loss: 1.9498, test_acc: 0.2756, best: 0.3315, time: 0:00:54
 Epoch: 80, lr: 1.0e-02, train_loss: 2.0548, train_acc: 0.2080 test_loss: 1.9589, test_acc: 0.2509, best: 0.3315, time: 0:00:54
 Epoch: 81, lr: 1.0e-02, train_loss: 2.0366, train_acc: 0.2242 test_loss: 1.9537, test_acc: 0.2731, best: 0.3315, time: 0:00:55
 Epoch: 82, lr: 1.0e-02, train_loss: 2.0666, train_acc: 0.2210 test_loss: 1.9757, test_acc: 0.2586, best: 0.3315, time: 0:00:55
 Epoch: 83, lr: 1.0e-02, train_loss: 2.0384, train_acc: 0.2266 test_loss: 2.0628, test_acc: 0.2384, best: 0.3315, time: 0:00:55
 Epoch: 84, lr: 1.0e-02, train_loss: 2.0706, train_acc: 0.2146 test_loss: 1.9631, test_acc: 0.2711, best: 0.3315, time: 0:00:55
 Epoch: 85, lr: 1.0e-02, train_loss: 2.0955, train_acc: 0.2188 test_loss: 2.1136, test_acc: 0.1943, best: 0.3315, time: 0:00:55
 Epoch: 86, lr: 1.0e-02, train_loss: 2.1296, train_acc: 0.1908 test_loss: 2.0669, test_acc: 0.2171, best: 0.3315, time: 0:00:54
 Epoch: 87, lr: 1.0e-02, train_loss: 2.0671, train_acc: 0.2208 test_loss: 1.9653, test_acc: 0.2541, best: 0.3315, time: 0:00:54
 Epoch: 88, lr: 1.0e-02, train_loss: 2.0684, train_acc: 0.2156 test_loss: 1.9949, test_acc: 0.2590, best: 0.3315, time: 0:00:54
 Epoch: 89, lr: 1.0e-02, train_loss: 2.0338, train_acc: 0.2158 test_loss: 1.9141, test_acc: 0.2775, best: 0.3315, time: 0:00:55
 Epoch: 90, lr: 1.0e-02, train_loss: 2.0141, train_acc: 0.2336 test_loss: 1.9362, test_acc: 0.2707, best: 0.3315, time: 0:00:55
 Epoch: 91, lr: 1.0e-02, train_loss: 2.0237, train_acc: 0.2288 test_loss: 2.0387, test_acc: 0.2746, best: 0.3315, time: 0:00:55
 Epoch: 92, lr: 1.0e-02, train_loss: 2.0539, train_acc: 0.2174 test_loss: 1.9330, test_acc: 0.2819, best: 0.3315, time: 0:00:54
 Epoch: 93, lr: 1.0e-02, train_loss: 2.0382, train_acc: 0.2232 test_loss: 1.9300, test_acc: 0.2797, best: 0.3315, time: 0:00:54
 Epoch: 94, lr: 1.0e-02, train_loss: 2.0049, train_acc: 0.2354 test_loss: 1.9505, test_acc: 0.2624, best: 0.3315, time: 0:00:54
 Epoch: 95, lr: 1.0e-02, train_loss: 2.0498, train_acc: 0.2246 test_loss: 2.0120, test_acc: 0.2289, best: 0.3315, time: 0:00:54
 Epoch: 96, lr: 1.0e-02, train_loss: 2.0247, train_acc: 0.2248 test_loss: 1.9568, test_acc: 0.2676, best: 0.3315, time: 0:00:54
 Epoch: 97, lr: 1.0e-02, train_loss: 2.0205, train_acc: 0.2300 test_loss: 1.9999, test_acc: 0.2569, best: 0.3315, time: 0:00:54
 Epoch: 98, lr: 1.0e-02, train_loss: 1.9941, train_acc: 0.2182 test_loss: 1.8953, test_acc: 0.2675, best: 0.3315, time: 0:00:55
 Epoch: 99, lr: 1.0e-02, train_loss: 1.9974, train_acc: 0.2318 test_loss: 2.1318, test_acc: 0.2221, best: 0.3315, time: 0:00:54
 Epoch: 100, lr: 1.0e-02, train_loss: 2.1557, train_acc: 0.2058 test_loss: 2.0841, test_acc: 0.2223, best: 0.3315, time: 0:00:54
 Epoch: 101, lr: 1.0e-02, train_loss: 2.1705, train_acc: 0.1896 test_loss: 2.1054, test_acc: 0.2059, best: 0.3315, time: 0:00:54
 Epoch: 102, lr: 1.0e-02, train_loss: 2.2190, train_acc: 0.1514 test_loss: 2.0983, test_acc: 0.2095, best: 0.3315, time: 0:00:54
 Epoch: 103, lr: 1.0e-02, train_loss: 2.1730, train_acc: 0.1758 test_loss: 2.0702, test_acc: 0.2286, best: 0.3315, time: 0:00:54
 Epoch: 104, lr: 1.0e-02, train_loss: 2.1038, train_acc: 0.1962 test_loss: 2.0143, test_acc: 0.2258, best: 0.3315, time: 0:00:54
 Epoch: 105, lr: 1.0e-02, train_loss: 2.1184, train_acc: 0.2084 test_loss: 2.0636, test_acc: 0.2137, best: 0.3315, time: 0:00:54
 Epoch: 106, lr: 1.0e-02, train_loss: 2.0953, train_acc: 0.2048 test_loss: 2.0186, test_acc: 0.2461, best: 0.3315, time: 0:00:54
 Epoch: 107, lr: 1.0e-02, train_loss: 2.1352, train_acc: 0.1956 test_loss: 2.0788, test_acc: 0.2111, best: 0.3315, time: 0:00:54
 Epoch: 108, lr: 1.0e-02, train_loss: 2.1184, train_acc: 0.1936 test_loss: 2.0477, test_acc: 0.2451, best: 0.3315, time: 0:00:54
 Epoch: 109, lr: 1.0e-02, train_loss: 2.1105, train_acc: 0.2044 test_loss: 2.1300, test_acc: 0.1928, best: 0.3315, time: 0:00:54
 Epoch: 110, lr: 1.0e-02, train_loss: 2.0983, train_acc: 0.2068 test_loss: 2.0164, test_acc: 0.2730, best: 0.3315, time: 0:00:54
 Epoch: 111, lr: 1.0e-02, train_loss: 2.0935, train_acc: 0.2024 test_loss: 2.0797, test_acc: 0.2090, best: 0.3315, time: 0:00:54
 Epoch: 112, lr: 1.0e-02, train_loss: 2.0868, train_acc: 0.2090 test_loss: 2.0517, test_acc: 0.2470, best: 0.3315, time: 0:00:54
 Epoch: 113, lr: 1.0e-02, train_loss: 2.0463, train_acc: 0.2062 test_loss: 1.9570, test_acc: 0.2821, best: 0.3315, time: 0:00:54
 Epoch: 114, lr: 1.0e-02, train_loss: 2.0616, train_acc: 0.2212 test_loss: 2.0042, test_acc: 0.2515, best: 0.3315, time: 0:00:54
 Epoch: 115, lr: 1.0e-02, train_loss: 2.0518, train_acc: 0.2294 test_loss: 1.9495, test_acc: 0.2921, best: 0.3315, time: 0:00:55
 Epoch: 116, lr: 1.0e-02, train_loss: 2.0134, train_acc: 0.2440 test_loss: 1.9233, test_acc: 0.3019, best: 0.3315, time: 0:00:54
 Epoch: 117, lr: 1.0e-02, train_loss: 2.0090, train_acc: 0.2402 test_loss: 1.8837, test_acc: 0.2951, best: 0.3315, time: 0:00:54
 Epoch: 118, lr: 1.0e-02, train_loss: 1.9930, train_acc: 0.2378 test_loss: 1.8710, test_acc: 0.3070, best: 0.3315, time: 0:00:54
 Epoch: 119, lr: 1.0e-02, train_loss: 1.9686, train_acc: 0.2494 test_loss: 1.8654, test_acc: 0.3179, best: 0.3315, time: 0:00:54
 Epoch: 120, lr: 1.0e-02, train_loss: 1.9489, train_acc: 0.2564 test_loss: 1.8371, test_acc: 0.3292, best: 0.3315, time: 0:00:55
 Epoch: 121, lr: 1.0e-02, train_loss: 1.9444, train_acc: 0.2612 test_loss: 1.8671, test_acc: 0.3100, best: 0.3315, time: 0:00:54
 Epoch: 122, lr: 1.0e-02, train_loss: 1.9439, train_acc: 0.2684 test_loss: 1.8476, test_acc: 0.3049, best: 0.3315, time: 0:00:54
 Epoch: 123, lr: 1.0e-02, train_loss: 1.9451, train_acc: 0.2584 test_loss: 1.8228, test_acc: 0.3265, best: 0.3315, time: 0:00:54
 Epoch: 124, lr: 1.0e-02, train_loss: 2.0549, train_acc: 0.2238 test_loss: 1.9659, test_acc: 0.2869, best: 0.3315, time: 0:00:54
 Epoch: 125, lr: 1.0e-02, train_loss: 2.0204, train_acc: 0.2258 test_loss: 1.9510, test_acc: 0.2582, best: 0.3315, time: 0:00:54
 Epoch: 126, lr: 1.0e-02, train_loss: 1.9846, train_acc: 0.2454 test_loss: 1.8912, test_acc: 0.3103, best: 0.3315, time: 0:00:55
 Epoch: 127, lr: 1.0e-02, train_loss: 1.9679, train_acc: 0.2410 test_loss: 1.8723, test_acc: 0.3099, best: 0.3315, time: 0:00:54
 Epoch: 128, lr: 1.0e-02, train_loss: 1.9607, train_acc: 0.2600 test_loss: 1.8482, test_acc: 0.3205, best: 0.3315, time: 0:00:54
 Epoch: 129, lr: 1.0e-02, train_loss: 1.9629, train_acc: 0.2486 test_loss: 1.8407, test_acc: 0.3306, best: 0.3315, time: 0:00:54
 Epoch: 130, lr: 1.0e-02, train_loss: 2.0107, train_acc: 0.2404 test_loss: 2.1399, test_acc: 0.2360, best: 0.3315, time: 0:00:54
 Epoch: 131, lr: 1.0e-02, train_loss: 2.1252, train_acc: 0.1944 test_loss: 2.0734, test_acc: 0.2215, best: 0.3315, time: 0:00:54
 Epoch: 132, lr: 1.0e-02, train_loss: 2.1198, train_acc: 0.1986 test_loss: 2.1109, test_acc: 0.2069, best: 0.3315, time: 0:00:54
 Epoch: 133, lr: 1.0e-02, train_loss: 2.0839, train_acc: 0.2114 test_loss: 2.0171, test_acc: 0.2411, best: 0.3315, time: 0:00:54
 Epoch: 134, lr: 1.0e-02, train_loss: 2.0779, train_acc: 0.2176 test_loss: 2.0196, test_acc: 0.2439, best: 0.3315, time: 0:00:55
 Epoch: 135, lr: 1.0e-02, train_loss: 2.0387, train_acc: 0.2258 test_loss: 1.9351, test_acc: 0.2732, best: 0.3315, time: 0:00:54
 Epoch: 136, lr: 1.0e-02, train_loss: 2.0375, train_acc: 0.2306 test_loss: 1.9725, test_acc: 0.2820, best: 0.3315, time: 0:00:55
 Epoch: 137, lr: 1.0e-02, train_loss: 2.0246, train_acc: 0.2358 test_loss: 2.3785, test_acc: 0.1177, best: 0.3315, time: 0:00:54
 Epoch: 138, lr: 1.0e-02, train_loss: 2.1674, train_acc: 0.1924 test_loss: 2.0704, test_acc: 0.2205, best: 0.3315, time: 0:00:54
 Epoch: 139, lr: 1.0e-02, train_loss: 2.2044, train_acc: 0.1734 test_loss: 2.2449, test_acc: 0.1664, best: 0.3315, time: 0:00:54
 Epoch: 140, lr: 1.0e-02, train_loss: 2.2447, train_acc: 0.1460 test_loss: 2.1662, test_acc: 0.1737, best: 0.3315, time: 0:00:54
 Epoch: 141, lr: 1.0e-02, train_loss: 2.2250, train_acc: 0.1524 test_loss: 2.2532, test_acc: 0.1430, best: 0.3315, time: 0:00:54
 Epoch: 142, lr: 1.0e-02, train_loss: 2.2538, train_acc: 0.1392 test_loss: 2.2523, test_acc: 0.1354, best: 0.3315, time: 0:00:54
 Epoch: 143, lr: 1.0e-02, train_loss: 2.2191, train_acc: 0.1600 test_loss: 2.1738, test_acc: 0.1956, best: 0.3315, time: 0:00:54
 Epoch: 144, lr: 1.0e-02, train_loss: 2.2566, train_acc: 0.1426 test_loss: 2.3082, test_acc: 0.1025, best: 0.3315, time: 0:00:54
 Epoch: 145, lr: 1.0e-02, train_loss: 2.2861, train_acc: 0.1206 test_loss: 2.2634, test_acc: 0.1321, best: 0.3315, time: 0:00:54
 Epoch: 146, lr: 1.0e-02, train_loss: 2.2617, train_acc: 0.1380 test_loss: 2.2236, test_acc: 0.1593, best: 0.3315, time: 0:00:54
 Epoch: 147, lr: 1.0e-02, train_loss: 2.2468, train_acc: 0.1400 test_loss: 2.2808, test_acc: 0.1225, best: 0.3315, time: 0:00:54
 Epoch: 148, lr: 1.0e-02, train_loss: 2.2492, train_acc: 0.1376 test_loss: 2.2233, test_acc: 0.1414, best: 0.3315, time: 0:00:55
 Epoch: 149, lr: 1.0e-02, train_loss: 2.2368, train_acc: 0.1474 test_loss: 2.2271, test_acc: 0.1388, best: 0.3315, time: 0:00:55
 Epoch: 150, lr: 1.0e-02, train_loss: 2.2487, train_acc: 0.1428 test_loss: 2.2112, test_acc: 0.1474, best: 0.3315, time: 0:00:55
 Epoch: 151, lr: 1.0e-02, train_loss: 2.2369, train_acc: 0.1480 test_loss: 2.2141, test_acc: 0.1645, best: 0.3315, time: 0:00:55
 Epoch: 152, lr: 1.0e-02, train_loss: 2.2343, train_acc: 0.1598 test_loss: 2.2212, test_acc: 0.1600, best: 0.3315, time: 0:00:54
 Epoch: 153, lr: 1.0e-02, train_loss: 2.2238, train_acc: 0.1564 test_loss: 2.2124, test_acc: 0.1811, best: 0.3315, time: 0:00:54
 Epoch: 154, lr: 1.0e-02, train_loss: 2.2041, train_acc: 0.1656 test_loss: 2.1773, test_acc: 0.1713, best: 0.3315, time: 0:00:54
 Epoch: 155, lr: 1.0e-02, train_loss: 2.2063, train_acc: 0.1652 test_loss: 2.2342, test_acc: 0.1259, best: 0.3315, time: 0:00:54
 Epoch: 156, lr: 1.0e-02, train_loss: 2.2180, train_acc: 0.1514 test_loss: 2.1633, test_acc: 0.1812, best: 0.3315, time: 0:00:54
 Epoch: 157, lr: 1.0e-02, train_loss: 2.1917, train_acc: 0.1672 test_loss: 2.1780, test_acc: 0.1966, best: 0.3315, time: 0:00:54
 Epoch: 158, lr: 1.0e-02, train_loss: 2.1632, train_acc: 0.1728 test_loss: 2.1281, test_acc: 0.1956, best: 0.3315, time: 0:00:55
 Epoch: 159, lr: 1.0e-02, train_loss: 2.1422, train_acc: 0.1850 test_loss: 2.1515, test_acc: 0.1864, best: 0.3315, time: 0:00:54
 Epoch: 160, lr: 1.0e-02, train_loss: 2.1475, train_acc: 0.1924 test_loss: 2.1067, test_acc: 0.2023, best: 0.3315, time: 0:00:54
 Epoch: 161, lr: 1.0e-02, train_loss: 2.1556, train_acc: 0.1790 test_loss: 2.1419, test_acc: 0.2091, best: 0.3315, time: 0:00:54
 Epoch: 162, lr: 1.0e-02, train_loss: 2.1857, train_acc: 0.1626 test_loss: 2.1427, test_acc: 0.1864, best: 0.3315, time: 0:00:54
 Epoch: 163, lr: 1.0e-02, train_loss: 2.1713, train_acc: 0.1742 test_loss: 2.1550, test_acc: 0.1959, best: 0.3315, time: 0:00:54
 Epoch: 164, lr: 1.0e-02, train_loss: 2.1870, train_acc: 0.1742 test_loss: 2.1242, test_acc: 0.1929, best: 0.3315, time: 0:00:55
 Epoch: 165, lr: 1.0e-02, train_loss: 2.1452, train_acc: 0.1938 test_loss: 2.0783, test_acc: 0.2291, best: 0.3315, time: 0:00:54
 Epoch: 166, lr: 1.0e-02, train_loss: 2.1362, train_acc: 0.1950 test_loss: 2.0679, test_acc: 0.2304, best: 0.3315, time: 0:00:54
 Epoch: 167, lr: 1.0e-02, train_loss: 2.1445, train_acc: 0.1860 test_loss: 2.1078, test_acc: 0.2024, best: 0.3315, time: 0:00:54
 Epoch: 168, lr: 1.0e-02, train_loss: 2.1585, train_acc: 0.1794 test_loss: 2.1304, test_acc: 0.2117, best: 0.3315, time: 0:00:54
 Epoch: 169, lr: 1.0e-02, train_loss: 2.1352, train_acc: 0.1932 test_loss: 2.1052, test_acc: 0.2034, best: 0.3315, time: 0:00:54
 Epoch: 170, lr: 1.0e-02, train_loss: 2.1178, train_acc: 0.1980 test_loss: 2.0548, test_acc: 0.2245, best: 0.3315, time: 0:00:54
 Epoch: 171, lr: 1.0e-02, train_loss: 2.1197, train_acc: 0.1986 test_loss: 2.0470, test_acc: 0.2451, best: 0.3315, time: 0:00:54
 Epoch: 172, lr: 1.0e-02, train_loss: 2.1312, train_acc: 0.1914 test_loss: 2.0920, test_acc: 0.2219, best: 0.3315, time: 0:00:55
 Epoch: 173, lr: 1.0e-02, train_loss: 2.1189, train_acc: 0.1984 test_loss: 2.0821, test_acc: 0.2220, best: 0.3315, time: 0:00:54
 Epoch: 174, lr: 1.0e-02, train_loss: 2.1251, train_acc: 0.1910 test_loss: 2.0912, test_acc: 0.2343, best: 0.3315, time: 0:00:55
 Epoch: 175, lr: 1.0e-02, train_loss: 2.1073, train_acc: 0.2040 test_loss: 2.0379, test_acc: 0.2579, best: 0.3315, time: 0:00:54
 Epoch: 176, lr: 1.0e-02, train_loss: 2.1057, train_acc: 0.2072 test_loss: 2.0165, test_acc: 0.2544, best: 0.3315, time: 0:00:54
 Epoch: 177, lr: 1.0e-02, train_loss: 2.1329, train_acc: 0.1998 test_loss: 2.0773, test_acc: 0.2520, best: 0.3315, time: 0:00:54
 Epoch: 178, lr: 1.0e-02, train_loss: 2.1164, train_acc: 0.2104 test_loss: 2.0334, test_acc: 0.2559, best: 0.3315, time: 0:00:55
 Epoch: 179, lr: 1.0e-02, train_loss: 2.0966, train_acc: 0.2150 test_loss: 2.0063, test_acc: 0.2779, best: 0.3315, time: 0:00:55
 Epoch: 180, lr: 2.0e-03, train_loss: 2.0714, train_acc: 0.2268 test_loss: 1.9608, test_acc: 0.2970, best: 0.3315, time: 0:00:54
 Epoch: 181, lr: 2.0e-03, train_loss: 2.0532, train_acc: 0.2296 test_loss: 1.9398, test_acc: 0.2969, best: 0.3315, time: 0:00:55
 Epoch: 182, lr: 2.0e-03, train_loss: 2.0615, train_acc: 0.2296 test_loss: 1.9667, test_acc: 0.2981, best: 0.3315, time: 0:00:54
 Epoch: 183, lr: 2.0e-03, train_loss: 2.0982, train_acc: 0.2114 test_loss: 2.0176, test_acc: 0.2651, best: 0.3315, time: 0:00:54
 Epoch: 184, lr: 2.0e-03, train_loss: 2.1069, train_acc: 0.2110 test_loss: 2.0396, test_acc: 0.2532, best: 0.3315, time: 0:00:55
 Epoch: 185, lr: 2.0e-03, train_loss: 2.0962, train_acc: 0.2164 test_loss: 2.0153, test_acc: 0.2652, best: 0.3315, time: 0:00:55
 Epoch: 186, lr: 2.0e-03, train_loss: 2.0966, train_acc: 0.2132 test_loss: 1.9993, test_acc: 0.2700, best: 0.3315, time: 0:00:54
 Epoch: 187, lr: 2.0e-03, train_loss: 2.0827, train_acc: 0.2226 test_loss: 2.0356, test_acc: 0.2522, best: 0.3315, time: 0:00:54
 Epoch: 188, lr: 2.0e-03, train_loss: 2.1030, train_acc: 0.2112 test_loss: 2.0212, test_acc: 0.2566, best: 0.3315, time: 0:00:54
 Epoch: 189, lr: 2.0e-03, train_loss: 2.0943, train_acc: 0.2078 test_loss: 2.0335, test_acc: 0.2560, best: 0.3315, time: 0:00:54
 Epoch: 190, lr: 2.0e-03, train_loss: 2.0678, train_acc: 0.2162 test_loss: 2.0018, test_acc: 0.2684, best: 0.3315, time: 0:00:54
 Epoch: 191, lr: 2.0e-03, train_loss: 2.0693, train_acc: 0.2278 test_loss: 2.0150, test_acc: 0.2606, best: 0.3315, time: 0:00:54
 Epoch: 192, lr: 2.0e-03, train_loss: 2.0645, train_acc: 0.2270 test_loss: 2.0104, test_acc: 0.2614, best: 0.3315, time: 0:00:54
 Epoch: 193, lr: 2.0e-03, train_loss: 2.0610, train_acc: 0.2238 test_loss: 1.9959, test_acc: 0.2705, best: 0.3315, time: 0:00:55
 Epoch: 194, lr: 2.0e-03, train_loss: 2.0966, train_acc: 0.2140 test_loss: 2.0303, test_acc: 0.2456, best: 0.3315, time: 0:00:54
 Epoch: 195, lr: 2.0e-03, train_loss: 2.0713, train_acc: 0.2264 test_loss: 1.9910, test_acc: 0.2732, best: 0.3315, time: 0:00:54
 Epoch: 196, lr: 2.0e-03, train_loss: 2.0689, train_acc: 0.2176 test_loss: 2.0088, test_acc: 0.2689, best: 0.3315, time: 0:00:54
 Epoch: 197, lr: 2.0e-03, train_loss: 2.0731, train_acc: 0.2174 test_loss: 1.9990, test_acc: 0.2627, best: 0.3315, time: 0:00:55
 Epoch: 198, lr: 2.0e-03, train_loss: 2.0401, train_acc: 0.2326 test_loss: 1.9787, test_acc: 0.2741, best: 0.3315, time: 0:00:54
 Epoch: 199, lr: 2.0e-03, train_loss: 2.0453, train_acc: 0.2380 test_loss: 1.9690, test_acc: 0.2750, best: 0.3315, time: 0:00:54
 Epoch: 200, lr: 2.0e-03, train_loss: 2.0528, train_acc: 0.2190 test_loss: 1.9564, test_acc: 0.2876, best: 0.3315, time: 0:00:54
 Epoch: 201, lr: 2.0e-03, train_loss: 2.0603, train_acc: 0.2244 test_loss: 1.9905, test_acc: 0.2804, best: 0.3315, time: 0:00:55
 Epoch: 202, lr: 2.0e-03, train_loss: 2.0381, train_acc: 0.2334 test_loss: 1.9714, test_acc: 0.2626, best: 0.3315, time: 0:00:55
 Epoch: 203, lr: 2.0e-03, train_loss: 2.0348, train_acc: 0.2334 test_loss: 1.9428, test_acc: 0.2814, best: 0.3315, time: 0:00:54
 Epoch: 204, lr: 2.0e-03, train_loss: 2.0453, train_acc: 0.2292 test_loss: 1.9682, test_acc: 0.2756, best: 0.3315, time: 0:00:54
 Epoch: 205, lr: 2.0e-03, train_loss: 2.0477, train_acc: 0.2306 test_loss: 1.9605, test_acc: 0.2805, best: 0.3315, time: 0:00:54
 Epoch: 206, lr: 2.0e-03, train_loss: 2.0549, train_acc: 0.2182 test_loss: 1.9613, test_acc: 0.2804, best: 0.3315, time: 0:00:54
 Epoch: 207, lr: 2.0e-03, train_loss: 2.0581, train_acc: 0.2220 test_loss: 1.9623, test_acc: 0.2823, best: 0.3315, time: 0:00:54
 Epoch: 208, lr: 2.0e-03, train_loss: 2.0468, train_acc: 0.2322 test_loss: 1.9420, test_acc: 0.2870, best: 0.3315, time: 0:00:55
 Epoch: 209, lr: 2.0e-03, train_loss: 2.0447, train_acc: 0.2232 test_loss: 1.9399, test_acc: 0.2951, best: 0.3315, time: 0:00:54
 Epoch: 210, lr: 2.0e-03, train_loss: 2.0285, train_acc: 0.2366 test_loss: 1.9448, test_acc: 0.2934, best: 0.3315, time: 0:00:54
 Epoch: 211, lr: 2.0e-03, train_loss: 2.0303, train_acc: 0.2428 test_loss: 1.9191, test_acc: 0.2985, best: 0.3315, time: 0:00:54
 Epoch: 212, lr: 2.0e-03, train_loss: 2.0233, train_acc: 0.2466 test_loss: 1.9322, test_acc: 0.3054, best: 0.3315, time: 0:00:54
 Epoch: 213, lr: 2.0e-03, train_loss: 2.0385, train_acc: 0.2310 test_loss: 1.9136, test_acc: 0.3049, best: 0.3315, time: 0:00:54
 Epoch: 214, lr: 2.0e-03, train_loss: 2.0268, train_acc: 0.2396 test_loss: 1.9156, test_acc: 0.3088, best: 0.3315, time: 0:00:55
 Epoch: 215, lr: 2.0e-03, train_loss: 2.0185, train_acc: 0.2450 test_loss: 1.9259, test_acc: 0.3079, best: 0.3315, time: 0:00:55
 Epoch: 216, lr: 2.0e-03, train_loss: 2.0172, train_acc: 0.2434 test_loss: 1.9123, test_acc: 0.3006, best: 0.3315, time: 0:00:54
 Epoch: 217, lr: 2.0e-03, train_loss: 2.0239, train_acc: 0.2436 test_loss: 1.9304, test_acc: 0.3068, best: 0.3315, time: 0:00:55
 Epoch: 218, lr: 2.0e-03, train_loss: 2.0154, train_acc: 0.2454 test_loss: 1.9301, test_acc: 0.2966, best: 0.3315, time: 0:00:54
 Epoch: 219, lr: 2.0e-03, train_loss: 2.0141, train_acc: 0.2394 test_loss: 1.9158, test_acc: 0.3099, best: 0.3315, time: 0:00:54
 Epoch: 220, lr: 2.0e-03, train_loss: 2.0139, train_acc: 0.2466 test_loss: 1.9189, test_acc: 0.3069, best: 0.3315, time: 0:00:54
 Epoch: 221, lr: 2.0e-03, train_loss: 2.0173, train_acc: 0.2464 test_loss: 1.9253, test_acc: 0.3121, best: 0.3315, time: 0:00:54
 Epoch: 222, lr: 2.0e-03, train_loss: 2.0224, train_acc: 0.2406 test_loss: 1.9067, test_acc: 0.3090, best: 0.3315, time: 0:00:54
 Epoch: 223, lr: 2.0e-03, train_loss: 2.0136, train_acc: 0.2370 test_loss: 1.9165, test_acc: 0.3054, best: 0.3315, time: 0:00:55
 Epoch: 224, lr: 2.0e-03, train_loss: 2.0114, train_acc: 0.2460 test_loss: 1.9087, test_acc: 0.3064, best: 0.3315, time: 0:00:54
 Epoch: 225, lr: 2.0e-03, train_loss: 2.0102, train_acc: 0.2464 test_loss: 1.8933, test_acc: 0.3110, best: 0.3315, time: 0:00:54
 Epoch: 226, lr: 2.0e-03, train_loss: 2.0060, train_acc: 0.2474 test_loss: 1.9076, test_acc: 0.3144, best: 0.3315, time: 0:00:55
 Epoch: 227, lr: 2.0e-03, train_loss: 2.0189, train_acc: 0.2470 test_loss: 1.9083, test_acc: 0.3051, best: 0.3315, time: 0:00:54
 Epoch: 228, lr: 2.0e-03, train_loss: 2.0011, train_acc: 0.2476 test_loss: 1.8924, test_acc: 0.3157, best: 0.3315, time: 0:00:55
 Epoch: 229, lr: 2.0e-03, train_loss: 2.0241, train_acc: 0.2498 test_loss: 1.9142, test_acc: 0.3114, best: 0.3315, time: 0:00:54
 Epoch: 230, lr: 2.0e-03, train_loss: 2.0131, train_acc: 0.2396 test_loss: 1.8994, test_acc: 0.3100, best: 0.3315, time: 0:00:54
 Epoch: 231, lr: 2.0e-03, train_loss: 2.0109, train_acc: 0.2430 test_loss: 1.9159, test_acc: 0.3108, best: 0.3315, time: 0:00:54
 Epoch: 232, lr: 2.0e-03, train_loss: 2.0039, train_acc: 0.2532 test_loss: 1.8931, test_acc: 0.3061, best: 0.3315, time: 0:00:54
 Epoch: 233, lr: 2.0e-03, train_loss: 2.0043, train_acc: 0.2424 test_loss: 1.9059, test_acc: 0.2988, best: 0.3315, time: 0:00:54
 Epoch: 234, lr: 2.0e-03, train_loss: 1.9989, train_acc: 0.2454 test_loss: 1.9036, test_acc: 0.3127, best: 0.3315, time: 0:00:54
 Epoch: 235, lr: 2.0e-03, train_loss: 1.9827, train_acc: 0.2574 test_loss: 1.8885, test_acc: 0.3071, best: 0.3315, time: 0:00:54
 Epoch: 236, lr: 2.0e-03, train_loss: 1.9910, train_acc: 0.2638 test_loss: 1.8956, test_acc: 0.3108, best: 0.3315, time: 0:00:54
 Epoch: 237, lr: 2.0e-03, train_loss: 1.9748, train_acc: 0.2644 test_loss: 1.8930, test_acc: 0.3134, best: 0.3315, time: 0:00:54
 Epoch: 238, lr: 2.0e-03, train_loss: 1.9836, train_acc: 0.2598 test_loss: 1.8731, test_acc: 0.3216, best: 0.3315, time: 0:00:54
 Epoch: 239, lr: 2.0e-03, train_loss: 1.9812, train_acc: 0.2574 test_loss: 1.8831, test_acc: 0.3167, best: 0.3315, time: 0:00:54
 Epoch: 240, lr: 4.0e-04, train_loss: 1.9845, train_acc: 0.2622 test_loss: 1.8710, test_acc: 0.3210, best: 0.3315, time: 0:00:54
 Epoch: 241, lr: 4.0e-04, train_loss: 1.9829, train_acc: 0.2586 test_loss: 1.8726, test_acc: 0.3139, best: 0.3315, time: 0:00:55
 Epoch: 242, lr: 4.0e-04, train_loss: 1.9754, train_acc: 0.2674 test_loss: 1.8607, test_acc: 0.3240, best: 0.3315, time: 0:00:54
 Epoch: 243, lr: 4.0e-04, train_loss: 1.9751, train_acc: 0.2638 test_loss: 1.8550, test_acc: 0.3270, best: 0.3315, time: 0:00:55
 Epoch: 244, lr: 4.0e-04, train_loss: 1.9801, train_acc: 0.2602 test_loss: 1.8604, test_acc: 0.3245, best: 0.3315, time: 0:00:54
 Epoch: 245, lr: 4.0e-04, train_loss: 1.9678, train_acc: 0.2598 test_loss: 1.8507, test_acc: 0.3236, best: 0.3315, time: 0:00:55
 Epoch: 246, lr: 4.0e-04, train_loss: 1.9682, train_acc: 0.2674 test_loss: 1.8694, test_acc: 0.3236, best: 0.3315, time: 0:00:54
 Epoch: 247, lr: 4.0e-04, train_loss: 1.9775, train_acc: 0.2514 test_loss: 1.8721, test_acc: 0.3206, best: 0.3315, time: 0:00:54
 Epoch: 248, lr: 4.0e-04, train_loss: 1.9664, train_acc: 0.2584 test_loss: 1.8552, test_acc: 0.3236, best: 0.3315, time: 0:00:54
 Epoch: 249, lr: 4.0e-04, train_loss: 1.9800, train_acc: 0.2606 test_loss: 1.8601, test_acc: 0.3201, best: 0.3315, time: 0:00:54
 Epoch: 250, lr: 4.0e-04, train_loss: 1.9803, train_acc: 0.2626 test_loss: 1.8751, test_acc: 0.3200, best: 0.3315, time: 0:00:54
 Epoch: 251, lr: 4.0e-04, train_loss: 1.9788, train_acc: 0.2606 test_loss: 1.8705, test_acc: 0.3212, best: 0.3315, time: 0:00:55
 Epoch: 252, lr: 4.0e-04, train_loss: 1.9798, train_acc: 0.2530 test_loss: 1.8530, test_acc: 0.3224, best: 0.3315, time: 0:00:54
 Epoch: 253, lr: 4.0e-04, train_loss: 1.9766, train_acc: 0.2616 test_loss: 1.8577, test_acc: 0.3219, best: 0.3315, time: 0:00:55
 Epoch: 254, lr: 4.0e-04, train_loss: 1.9642, train_acc: 0.2618 test_loss: 1.8438, test_acc: 0.3291, best: 0.3315, time: 0:00:55
 Epoch: 255, lr: 4.0e-04, train_loss: 1.9715, train_acc: 0.2672 test_loss: 1.8371, test_acc: 0.3314, best: 0.3315, time: 0:00:54
 Epoch: 256, lr: 4.0e-04, train_loss: 1.9773, train_acc: 0.2592 test_loss: 1.8513, test_acc: 0.3265, best: 0.3315, time: 0:00:54
 Epoch: 257, lr: 4.0e-04, train_loss: 1.9895, train_acc: 0.2540 test_loss: 1.8476, test_acc: 0.3285, best: 0.3315, time: 0:00:54
 Epoch: 258, lr: 4.0e-04, train_loss: 1.9629, train_acc: 0.2638 test_loss: 1.8534, test_acc: 0.3234, best: 0.3315, time: 0:00:54
 Epoch: 259, lr: 4.0e-04, train_loss: 1.9689, train_acc: 0.2572 test_loss: 1.8573, test_acc: 0.3175, best: 0.3315, time: 0:00:54
 Epoch: 260, lr: 4.0e-04, train_loss: 1.9643, train_acc: 0.2574 test_loss: 1.8318, test_acc: 0.3299, best: 0.3315, time: 0:00:55
 Epoch: 261, lr: 4.0e-04, train_loss: 1.9703, train_acc: 0.2604 test_loss: 1.8460, test_acc: 0.3244, best: 0.3315, time: 0:00:54
 Epoch: 262, lr: 4.0e-04, train_loss: 1.9617, train_acc: 0.2616 test_loss: 1.8400, test_acc: 0.3267, best: 0.3315, time: 0:00:54
 Epoch: 263, lr: 4.0e-04, train_loss: 1.9590, train_acc: 0.2642 test_loss: 1.8369, test_acc: 0.3252, best: 0.3315, time: 0:00:54
 Epoch: 264, lr: 4.0e-04, train_loss: 1.9643, train_acc: 0.2586 test_loss: 1.8405, test_acc: 0.3251, best: 0.3315, time: 0:00:54
 Epoch: 265, lr: 4.0e-04, train_loss: 1.9700, train_acc: 0.2636 test_loss: 1.8378, test_acc: 0.3290, best: 0.3315, time: 0:00:54
 Epoch: 266, lr: 4.0e-04, train_loss: 1.9698, train_acc: 0.2552 test_loss: 1.8351, test_acc: 0.3256, best: 0.3315, time: 0:00:54
 Epoch: 267, lr: 4.0e-04, train_loss: 1.9574, train_acc: 0.2612 test_loss: 1.8453, test_acc: 0.3261, best: 0.3315, time: 0:00:54
 Epoch: 268, lr: 4.0e-04, train_loss: 1.9687, train_acc: 0.2546 test_loss: 1.8432, test_acc: 0.3261, best: 0.3315, time: 0:00:54
 Epoch: 269, lr: 4.0e-04, train_loss: 1.9618, train_acc: 0.2612 test_loss: 1.8454, test_acc: 0.3260, best: 0.3315, time: 0:00:54
 Epoch: 270, lr: 8.0e-05, train_loss: 1.9696, train_acc: 0.2638 test_loss: 1.8412, test_acc: 0.3225, best: 0.3315, time: 0:00:54
 Epoch: 271, lr: 8.0e-05, train_loss: 1.9715, train_acc: 0.2596 test_loss: 1.8313, test_acc: 0.3221, best: 0.3315, time: 0:00:54
 Epoch: 272, lr: 8.0e-05, train_loss: 1.9623, train_acc: 0.2684 test_loss: 1.8330, test_acc: 0.3302, best: 0.3315, time: 0:00:54
 Epoch: 273, lr: 8.0e-05, train_loss: 1.9548, train_acc: 0.2676 test_loss: 1.8353, test_acc: 0.3307, best: 0.3315, time: 0:00:54
 Epoch: 274, lr: 8.0e-05, train_loss: 1.9647, train_acc: 0.2600 test_loss: 1.8432, test_acc: 0.3260, best: 0.3315, time: 0:00:54
 Epoch: 275, lr: 8.0e-05, train_loss: 1.9699, train_acc: 0.2658 test_loss: 1.8387, test_acc: 0.3292, best: 0.3315, time: 0:00:55
 Epoch: 276, lr: 8.0e-05, train_loss: 1.9655, train_acc: 0.2562 test_loss: 1.8341, test_acc: 0.3234, best: 0.3315, time: 0:00:55
 Epoch: 277, lr: 8.0e-05, train_loss: 1.9713, train_acc: 0.2574 test_loss: 1.8383, test_acc: 0.3319, best: 0.3319, time: 0:00:54
 Epoch: 278, lr: 8.0e-05, train_loss: 1.9689, train_acc: 0.2624 test_loss: 1.8365, test_acc: 0.3252, best: 0.3319, time: 0:00:54
 Epoch: 279, lr: 8.0e-05, train_loss: 1.9646, train_acc: 0.2530 test_loss: 1.8427, test_acc: 0.3210, best: 0.3319, time: 0:00:54
 Epoch: 280, lr: 8.0e-05, train_loss: 1.9573, train_acc: 0.2584 test_loss: 1.8296, test_acc: 0.3252, best: 0.3319, time: 0:00:54
 Epoch: 281, lr: 8.0e-05, train_loss: 1.9720, train_acc: 0.2582 test_loss: 1.8321, test_acc: 0.3306, best: 0.3319, time: 0:00:54
 Epoch: 282, lr: 8.0e-05, train_loss: 1.9656, train_acc: 0.2620 test_loss: 1.8291, test_acc: 0.3267, best: 0.3319, time: 0:00:54
 Epoch: 283, lr: 8.0e-05, train_loss: 1.9638, train_acc: 0.2688 test_loss: 1.8322, test_acc: 0.3280, best: 0.3319, time: 0:00:54
 Epoch: 284, lr: 8.0e-05, train_loss: 1.9649, train_acc: 0.2578 test_loss: 1.8284, test_acc: 0.3242, best: 0.3319, time: 0:00:54
 Epoch: 285, lr: 8.0e-05, train_loss: 1.9650, train_acc: 0.2614 test_loss: 1.8429, test_acc: 0.3171, best: 0.3319, time: 0:00:54
 Epoch: 286, lr: 8.0e-05, train_loss: 1.9607, train_acc: 0.2652 test_loss: 1.8413, test_acc: 0.3327, best: 0.3327, time: 0:00:55
 Epoch: 287, lr: 8.0e-05, train_loss: 1.9561, train_acc: 0.2652 test_loss: 1.8340, test_acc: 0.3306, best: 0.3327, time: 0:00:54
 Epoch: 288, lr: 8.0e-05, train_loss: 1.9592, train_acc: 0.2648 test_loss: 1.8327, test_acc: 0.3250, best: 0.3327, time: 0:00:54
 Epoch: 289, lr: 8.0e-05, train_loss: 1.9635, train_acc: 0.2704 test_loss: 1.8371, test_acc: 0.3235, best: 0.3327, time: 0:00:54
 Epoch: 290, lr: 8.0e-05, train_loss: 1.9656, train_acc: 0.2668 test_loss: 1.8428, test_acc: 0.3209, best: 0.3327, time: 0:00:55
 Epoch: 291, lr: 8.0e-05, train_loss: 1.9669, train_acc: 0.2630 test_loss: 1.8321, test_acc: 0.3270, best: 0.3327, time: 0:00:54
 Epoch: 292, lr: 8.0e-05, train_loss: 1.9522, train_acc: 0.2630 test_loss: 1.8294, test_acc: 0.3331, best: 0.3331, time: 0:00:55
 Epoch: 293, lr: 8.0e-05, train_loss: 1.9655, train_acc: 0.2584 test_loss: 1.8338, test_acc: 0.3265, best: 0.3331, time: 0:00:54
 Epoch: 294, lr: 8.0e-05, train_loss: 1.9624, train_acc: 0.2658 test_loss: 1.8316, test_acc: 0.3256, best: 0.3331, time: 0:00:54
 Epoch: 295, lr: 8.0e-05, train_loss: 1.9656, train_acc: 0.2670 test_loss: 1.8235, test_acc: 0.3305, best: 0.3331, time: 0:00:54
 Epoch: 296, lr: 8.0e-05, train_loss: 1.9805, train_acc: 0.2558 test_loss: 1.8309, test_acc: 0.3319, best: 0.3331, time: 0:00:54
 Epoch: 297, lr: 8.0e-05, train_loss: 1.9556, train_acc: 0.2744 test_loss: 1.8241, test_acc: 0.3287, best: 0.3331, time: 0:00:55
 Epoch: 298, lr: 8.0e-05, train_loss: 1.9661, train_acc: 0.2648 test_loss: 1.8385, test_acc: 0.3309, best: 0.3331, time: 0:00:54
 Epoch: 299, lr: 8.0e-05, train_loss: 1.9548, train_acc: 0.2632 test_loss: 1.8439, test_acc: 0.3216, best: 0.3331, time: 0:00:54
 Highest accuracy: 0.3331