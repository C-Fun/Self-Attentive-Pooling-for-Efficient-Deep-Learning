
 Run on time: 2022-07-03 16:40:16.411816

 Architecture: mobilenet_v2-nlp-1222121

 Pool Config: {
    "arch": "mobilenet_v2",
    "conv1": {
        "_conv2d": "norm",
        "pool_cfg": {}
    },
    "layer1": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "skip",
            "_stride": 1,
            "_psize": null,
            "_dim_reduced_ratio": null,
            "_num_heads": null,
            "_conv2d": null,
            "_win_norm": null
        }
    },
    "layer2": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "nlp",
            "_stride": 2,
            "_psize": 1,
            "_dim_reduced_ratio": 1,
            "_num_heads": 4,
            "_conv2d": "norm",
            "_win_norm": true
        }
    },
    "layer3": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "nlp",
            "_stride": 2,
            "_psize": 1,
            "_dim_reduced_ratio": 1,
            "_num_heads": 8,
            "_conv2d": "norm",
            "_win_norm": true
        }
    },
    "layer4": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "nlp",
            "_stride": 2,
            "_psize": 1,
            "_dim_reduced_ratio": 1,
            "_num_heads": 16,
            "_conv2d": "norm",
            "_win_norm": true
        }
    },
    "layer5": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "skip",
            "_stride": 1,
            "_psize": null,
            "_dim_reduced_ratio": null,
            "_num_heads": null,
            "_conv2d": null,
            "_win_norm": null
        }
    },
    "layer6": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "nlp",
            "_stride": 2,
            "_psize": 1,
            "_dim_reduced_ratio": 1,
            "_num_heads": 64,
            "_conv2d": "norm",
            "_win_norm": true
        }
    },
    "layer7": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "skip",
            "_stride": 1,
            "_psize": null,
            "_dim_reduced_ratio": null,
            "_num_heads": null,
            "_conv2d": null,
            "_win_norm": null
        }
    },
    "conv2": {
        "_conv2d": "norm",
        "pool_cfg": {}
    }
}

 Arguments:
	 gpu                  : True
	 log                  : True
	 seed                 : 0
	 momentum             : 0.9
	 amsgrad              : True
	 dataset              : STL10
	 batch_size           : 8
	 architecture         : MOBILENET_V2-NLP-1222121
	 im_size              : 128
	 relu_threshold       : 4.0
	 learning_rate        : 0.01
	 pretrained_backbone  : 
	 pretrained_ann       : 
	 weight_decay         : 0.0
	 test_only            : False
	 epochs               : 300
	 lr_interval          : [180, 240, 270]
	 lr_reduce            : 5
	 optimizer            : SGD
	 dropout              : 0.2
	 kernel_size          : 3
	 dont_save            : False
	 visualize            : False
	 devices              : 0
 DataParallel(
  (module): Network(
    (net): MobileNetV2(
      (conv1): Sequential(
        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (features): Sequential(
        (0): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): InvertedResidual(
          (pooling): Pool2d(
            (logit): Sequential(
              (pool_weight): NLP_BASE(
                (downsample): Sequential(
                  (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1))
                  (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (2): ReLU(inplace=True)
                )
                (multihead_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=24, out_features=24, bias=True)
                )
                (restore): Sequential(
                  (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1))
                  (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (2): Sigmoid()
                )
                (pos_embed): PositionEmbeddingLearned(
                  (row_embed): Embedding(256, 12)
                  (col_embed): Embedding(256, 12)
                )
              )
            )
            (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
          )
          (conv): Sequential(
            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (2): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
            (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (3): InvertedResidual(
          (pooling): Pool2d(
            (logit): Sequential(
              (pool_weight): NLP_BASE(
                (downsample): Sequential(
                  (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
                  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (2): ReLU(inplace=True)
                )
                (multihead_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
                )
                (restore): Sequential(
                  (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
                  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (2): Sigmoid()
                )
                (pos_embed): PositionEmbeddingLearned(
                  (row_embed): Embedding(256, 16)
                  (col_embed): Embedding(256, 16)
                )
              )
            )
            (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
          )
          (conv): Sequential(
            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
            (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (4): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (5): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (6): InvertedResidual(
          (pooling): Pool2d(
            (logit): Sequential(
              (pool_weight): NLP_BASE(
                (downsample): Sequential(
                  (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
                  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (2): ReLU(inplace=True)
                )
                (multihead_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
                )
                (restore): Sequential(
                  (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
                  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (2): Sigmoid()
                )
                (pos_embed): PositionEmbeddingLearned(
                  (row_embed): Embedding(256, 32)
                  (col_embed): Embedding(256, 32)
                )
              )
            )
            (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
          )
          (conv): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (7): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (8): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (9): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (10): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (11): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
            (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (12): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
            (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (13): InvertedResidual(
          (pooling): Pool2d(
            (logit): Sequential(
              (pool_weight): NLP_BASE(
                (downsample): Sequential(
                  (0): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1))
                  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (2): ReLU(inplace=True)
                )
                (multihead_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
                )
                (restore): Sequential(
                  (0): Conv2d(128, 160, kernel_size=(1, 1), stride=(1, 1))
                  (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (2): Sigmoid()
                )
                (pos_embed): PositionEmbeddingLearned(
                  (row_embed): Embedding(256, 64)
                  (col_embed): Embedding(256, 64)
                )
              )
            )
            (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
          )
          (conv): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
            (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (14): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
            (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (15): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
            (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (16): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
            (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
      (conv2): Sequential(
        (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
      (classifier): Linear(in_features=1280, out_features=10, bias=True)
    )
  )
)
 SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0
)
 Epoch: 1, lr: 1.0e-02, train_loss: 2.4892, train_acc: 0.1702 test_loss: 1.9836, test_acc: 0.2677, best: 0.2677, time: 0:03:54
 Epoch: 2, lr: 1.0e-02, train_loss: 2.0030, train_acc: 0.2390 test_loss: 1.8294, test_acc: 0.2749, best: 0.2749, time: 0:03:53
 Epoch: 3, lr: 1.0e-02, train_loss: 1.9276, train_acc: 0.2694 test_loss: 1.6802, test_acc: 0.3508, best: 0.3508, time: 0:03:53
 Epoch: 4, lr: 1.0e-02, train_loss: 1.8677, train_acc: 0.2900 test_loss: 1.7685, test_acc: 0.3409, best: 0.3508, time: 0:03:53
 Epoch: 5, lr: 1.0e-02, train_loss: 1.8332, train_acc: 0.3038 test_loss: 1.6342, test_acc: 0.3523, best: 0.3523, time: 0:03:53
 Epoch: 6, lr: 1.0e-02, train_loss: 1.7891, train_acc: 0.3210 test_loss: 1.6033, test_acc: 0.3987, best: 0.3987, time: 0:03:53
 Epoch: 7, lr: 1.0e-02, train_loss: 1.7704, train_acc: 0.3398 test_loss: 1.5677, test_acc: 0.3892, best: 0.3987, time: 0:03:53
 Epoch: 8, lr: 1.0e-02, train_loss: 1.7106, train_acc: 0.3636 test_loss: 1.5283, test_acc: 0.4198, best: 0.4198, time: 0:03:53
 Epoch: 9, lr: 1.0e-02, train_loss: 1.6738, train_acc: 0.3788 test_loss: 1.4951, test_acc: 0.4359, best: 0.4359, time: 0:03:53
 Epoch: 10, lr: 1.0e-02, train_loss: 1.6430, train_acc: 0.3896 test_loss: 1.3680, test_acc: 0.4780, best: 0.4780, time: 0:03:53
 Epoch: 11, lr: 1.0e-02, train_loss: 1.6012, train_acc: 0.4060 test_loss: 1.4469, test_acc: 0.4587, best: 0.4780, time: 0:03:53
 Epoch: 12, lr: 1.0e-02, train_loss: 1.5919, train_acc: 0.4058 test_loss: 1.3320, test_acc: 0.5008, best: 0.5008, time: 0:03:53
 Epoch: 13, lr: 1.0e-02, train_loss: 1.5402, train_acc: 0.4262 test_loss: 1.2896, test_acc: 0.5414, best: 0.5414, time: 0:03:53
 Epoch: 14, lr: 1.0e-02, train_loss: 1.5040, train_acc: 0.4454 test_loss: 1.3267, test_acc: 0.5138, best: 0.5414, time: 0:03:53
 Epoch: 15, lr: 1.0e-02, train_loss: 1.4835, train_acc: 0.4514 test_loss: 1.3586, test_acc: 0.4824, best: 0.5414, time: 0:03:53
 Epoch: 16, lr: 1.0e-02, train_loss: 1.4574, train_acc: 0.4622 test_loss: 1.1978, test_acc: 0.5656, best: 0.5656, time: 0:03:53
 Epoch: 17, lr: 1.0e-02, train_loss: 1.4034, train_acc: 0.4812 test_loss: 1.2303, test_acc: 0.5467, best: 0.5656, time: 0:03:53
 Epoch: 18, lr: 1.0e-02, train_loss: 1.3952, train_acc: 0.4972 test_loss: 1.1974, test_acc: 0.5664, best: 0.5664, time: 0:03:53
 Epoch: 19, lr: 1.0e-02, train_loss: 1.3863, train_acc: 0.4936 test_loss: 1.2265, test_acc: 0.5566, best: 0.5664, time: 0:03:53
 Epoch: 20, lr: 1.0e-02, train_loss: 1.3817, train_acc: 0.4936 test_loss: 1.1011, test_acc: 0.5969, best: 0.5969, time: 0:03:53
 Epoch: 21, lr: 1.0e-02, train_loss: 1.3427, train_acc: 0.5124 test_loss: 1.1042, test_acc: 0.5887, best: 0.5969, time: 0:03:53
 Epoch: 22, lr: 1.0e-02, train_loss: 1.3131, train_acc: 0.5268 test_loss: 1.1609, test_acc: 0.5907, best: 0.5969, time: 0:03:53
 Epoch: 23, lr: 1.0e-02, train_loss: 1.2974, train_acc: 0.5260 test_loss: 1.1142, test_acc: 0.5950, best: 0.5969, time: 0:03:53
 Epoch: 24, lr: 1.0e-02, train_loss: 1.2669, train_acc: 0.5300 test_loss: 1.1455, test_acc: 0.5870, best: 0.5969, time: 0:03:53
 Epoch: 25, lr: 1.0e-02, train_loss: 1.2796, train_acc: 0.5416 test_loss: 1.0436, test_acc: 0.6221, best: 0.6221, time: 0:03:53
 Epoch: 26, lr: 1.0e-02, train_loss: 1.2429, train_acc: 0.5530 test_loss: 1.0149, test_acc: 0.6240, best: 0.6240, time: 0:03:53
 Epoch: 27, lr: 1.0e-02, train_loss: 1.2452, train_acc: 0.5426 test_loss: 1.0601, test_acc: 0.6114, best: 0.6240, time: 0:03:53
 Epoch: 28, lr: 1.0e-02, train_loss: 1.2052, train_acc: 0.5648 test_loss: 1.0830, test_acc: 0.6090, best: 0.6240, time: 0:03:53
 Epoch: 29, lr: 1.0e-02, train_loss: 1.1892, train_acc: 0.5632 test_loss: 0.9815, test_acc: 0.6450, best: 0.6450, time: 0:03:53
 Epoch: 30, lr: 1.0e-02, train_loss: 1.1834, train_acc: 0.5742 test_loss: 1.0373, test_acc: 0.6234, best: 0.6450, time: 0:03:53
 Epoch: 31, lr: 1.0e-02, train_loss: 1.1687, train_acc: 0.5766 test_loss: 1.0012, test_acc: 0.6401, best: 0.6450, time: 0:03:53
 Epoch: 32, lr: 1.0e-02, train_loss: 1.1557, train_acc: 0.5824 test_loss: 0.9828, test_acc: 0.6428, best: 0.6450, time: 0:03:53
 Epoch: 33, lr: 1.0e-02, train_loss: 1.1413, train_acc: 0.5866 test_loss: 1.0019, test_acc: 0.6384, best: 0.6450, time: 0:03:53
 Epoch: 34, lr: 1.0e-02, train_loss: 1.1307, train_acc: 0.5910 test_loss: 0.9731, test_acc: 0.6519, best: 0.6519, time: 0:03:53
 Epoch: 35, lr: 1.0e-02, train_loss: 1.1150, train_acc: 0.6032 test_loss: 0.9757, test_acc: 0.6482, best: 0.6519, time: 0:03:53
 Epoch: 36, lr: 1.0e-02, train_loss: 1.0959, train_acc: 0.6078 test_loss: 0.9988, test_acc: 0.6374, best: 0.6519, time: 0:03:53
 Epoch: 37, lr: 1.0e-02, train_loss: 1.1006, train_acc: 0.6104 test_loss: 1.0139, test_acc: 0.6302, best: 0.6519, time: 0:03:53
 Epoch: 38, lr: 1.0e-02, train_loss: 1.0764, train_acc: 0.6134 test_loss: 0.9750, test_acc: 0.6450, best: 0.6519, time: 0:03:53
 Epoch: 39, lr: 1.0e-02, train_loss: 1.0768, train_acc: 0.6082 test_loss: 1.0710, test_acc: 0.6160, best: 0.6519, time: 0:03:53
 Epoch: 40, lr: 1.0e-02, train_loss: 1.0751, train_acc: 0.6088 test_loss: 0.8842, test_acc: 0.6834, best: 0.6834, time: 0:03:53
 Epoch: 41, lr: 1.0e-02, train_loss: 1.0622, train_acc: 0.6218 test_loss: 0.8996, test_acc: 0.6805, best: 0.6834, time: 0:03:53
 Epoch: 42, lr: 1.0e-02, train_loss: 1.0246, train_acc: 0.6278 test_loss: 0.8890, test_acc: 0.6751, best: 0.6834, time: 0:03:53
 Epoch: 43, lr: 1.0e-02, train_loss: 1.0353, train_acc: 0.6346 test_loss: 0.9058, test_acc: 0.6720, best: 0.6834, time: 0:03:53
 Epoch: 44, lr: 1.0e-02, train_loss: 1.0056, train_acc: 0.6360 test_loss: 0.8840, test_acc: 0.6861, best: 0.6861, time: 0:03:53
 Epoch: 45, lr: 1.0e-02, train_loss: 0.9916, train_acc: 0.6432 test_loss: 0.8912, test_acc: 0.6789, best: 0.6861, time: 0:03:53
 Epoch: 46, lr: 1.0e-02, train_loss: 0.9819, train_acc: 0.6416 test_loss: 1.0243, test_acc: 0.6315, best: 0.6861, time: 0:03:53
 Epoch: 47, lr: 1.0e-02, train_loss: 1.0093, train_acc: 0.6354 test_loss: 0.8446, test_acc: 0.7001, best: 0.7001, time: 0:03:53
 Epoch: 48, lr: 1.0e-02, train_loss: 0.9770, train_acc: 0.6442 test_loss: 0.8172, test_acc: 0.7083, best: 0.7083, time: 0:03:53
 Epoch: 49, lr: 1.0e-02, train_loss: 0.9638, train_acc: 0.6552 test_loss: 0.8471, test_acc: 0.6983, best: 0.7083, time: 0:03:53
 Epoch: 50, lr: 1.0e-02, train_loss: 0.9621, train_acc: 0.6508 test_loss: 0.8405, test_acc: 0.7067, best: 0.7083, time: 0:03:53
 Epoch: 51, lr: 1.0e-02, train_loss: 0.9210, train_acc: 0.6686 test_loss: 0.8475, test_acc: 0.7023, best: 0.7083, time: 0:03:53
 Epoch: 52, lr: 1.0e-02, train_loss: 0.9576, train_acc: 0.6484 test_loss: 0.8259, test_acc: 0.7036, best: 0.7083, time: 0:03:53
 Epoch: 53, lr: 1.0e-02, train_loss: 0.9385, train_acc: 0.6616 test_loss: 0.8297, test_acc: 0.7077, best: 0.7083, time: 0:03:53
 Epoch: 54, lr: 1.0e-02, train_loss: 0.9357, train_acc: 0.6690 test_loss: 0.8427, test_acc: 0.6976, best: 0.7083, time: 0:03:53
 Epoch: 55, lr: 1.0e-02, train_loss: 0.9159, train_acc: 0.6714 test_loss: 0.8449, test_acc: 0.6927, best: 0.7083, time: 0:03:53
 Epoch: 56, lr: 1.0e-02, train_loss: 0.9313, train_acc: 0.6702 test_loss: 0.8328, test_acc: 0.7080, best: 0.7083, time: 0:03:53
 Epoch: 57, lr: 1.0e-02, train_loss: 0.8994, train_acc: 0.6712 test_loss: 0.8195, test_acc: 0.7093, best: 0.7093, time: 0:03:53
 Epoch: 58, lr: 1.0e-02, train_loss: 0.8954, train_acc: 0.6840 test_loss: 0.8464, test_acc: 0.6966, best: 0.7093, time: 0:03:53
 Epoch: 59, lr: 1.0e-02, train_loss: 0.8749, train_acc: 0.6854 test_loss: 0.8102, test_acc: 0.7119, best: 0.7119, time: 0:03:53
 Epoch: 60, lr: 1.0e-02, train_loss: 0.8887, train_acc: 0.6842 test_loss: 0.8738, test_acc: 0.6879, best: 0.7119, time: 0:03:53
 Epoch: 61, lr: 1.0e-02, train_loss: 0.8794, train_acc: 0.6816 test_loss: 0.8062, test_acc: 0.7183, best: 0.7183, time: 0:03:53
 Epoch: 62, lr: 1.0e-02, train_loss: 0.8703, train_acc: 0.6878 test_loss: 0.8386, test_acc: 0.7065, best: 0.7183, time: 0:03:53
 Epoch: 63, lr: 1.0e-02, train_loss: 0.8694, train_acc: 0.6948 test_loss: 0.8397, test_acc: 0.7091, best: 0.7183, time: 0:03:53
 Epoch: 64, lr: 1.0e-02, train_loss: 0.8446, train_acc: 0.6974 test_loss: 0.8633, test_acc: 0.6997, best: 0.7183, time: 0:03:53
 Epoch: 65, lr: 1.0e-02, train_loss: 0.8502, train_acc: 0.6876 test_loss: 0.8159, test_acc: 0.7179, best: 0.7183, time: 0:03:53
 Epoch: 66, lr: 1.0e-02, train_loss: 0.8254, train_acc: 0.7006 test_loss: 0.8007, test_acc: 0.7211, best: 0.7211, time: 0:03:53
 Epoch: 67, lr: 1.0e-02, train_loss: 0.8475, train_acc: 0.6952 test_loss: 0.7712, test_acc: 0.7329, best: 0.7329, time: 0:03:53
 Epoch: 68, lr: 1.0e-02, train_loss: 0.8081, train_acc: 0.7112 test_loss: 0.7648, test_acc: 0.7311, best: 0.7329, time: 0:03:53
 Epoch: 69, lr: 1.0e-02, train_loss: 0.8248, train_acc: 0.7080 test_loss: 0.7882, test_acc: 0.7312, best: 0.7329, time: 0:03:53
 Epoch: 70, lr: 1.0e-02, train_loss: 0.8154, train_acc: 0.7132 test_loss: 0.7706, test_acc: 0.7345, best: 0.7345, time: 0:03:53
 Epoch: 71, lr: 1.0e-02, train_loss: 0.8102, train_acc: 0.7120 test_loss: 0.8177, test_acc: 0.7232, best: 0.7345, time: 0:03:53
 Epoch: 72, lr: 1.0e-02, train_loss: 0.8051, train_acc: 0.7178 test_loss: 0.7514, test_acc: 0.7369, best: 0.7369, time: 0:03:53
 Epoch: 73, lr: 1.0e-02, train_loss: 0.8022, train_acc: 0.7148 test_loss: 0.8147, test_acc: 0.7184, best: 0.7369, time: 0:03:53
 Epoch: 74, lr: 1.0e-02, train_loss: 0.7996, train_acc: 0.7104 test_loss: 0.7771, test_acc: 0.7239, best: 0.7369, time: 0:03:53
 Epoch: 75, lr: 1.0e-02, train_loss: 0.7681, train_acc: 0.7288 test_loss: 0.7416, test_acc: 0.7412, best: 0.7412, time: 0:03:54
 Epoch: 76, lr: 1.0e-02, train_loss: 0.7670, train_acc: 0.7226 test_loss: 0.7378, test_acc: 0.7504, best: 0.7504, time: 0:03:53
 Epoch: 77, lr: 1.0e-02, train_loss: 0.7909, train_acc: 0.7144 test_loss: 0.7596, test_acc: 0.7324, best: 0.7504, time: 0:03:53
 Epoch: 78, lr: 1.0e-02, train_loss: 0.7557, train_acc: 0.7224 test_loss: 0.7821, test_acc: 0.7341, best: 0.7504, time: 0:03:53
 Epoch: 79, lr: 1.0e-02, train_loss: 0.7454, train_acc: 0.7326 test_loss: 0.7473, test_acc: 0.7458, best: 0.7504, time: 0:03:53
 Epoch: 80, lr: 1.0e-02, train_loss: 0.7451, train_acc: 0.7266 test_loss: 0.8217, test_acc: 0.7163, best: 0.7504, time: 0:03:53
 Epoch: 81, lr: 1.0e-02, train_loss: 0.7613, train_acc: 0.7266 test_loss: 0.7321, test_acc: 0.7426, best: 0.7504, time: 0:03:53
 Epoch: 82, lr: 1.0e-02, train_loss: 0.7506, train_acc: 0.7428 test_loss: 0.8644, test_acc: 0.7120, best: 0.7504, time: 0:03:53
 Epoch: 83, lr: 1.0e-02, train_loss: 0.7613, train_acc: 0.7290 test_loss: 0.8320, test_acc: 0.7220, best: 0.7504, time: 0:03:53
 Epoch: 84, lr: 1.0e-02, train_loss: 0.7252, train_acc: 0.7394 test_loss: 0.7655, test_acc: 0.7322, best: 0.7504, time: 0:03:53
 Epoch: 85, lr: 1.0e-02, train_loss: 0.7229, train_acc: 0.7430 test_loss: 0.7312, test_acc: 0.7455, best: 0.7504, time: 0:03:53
 Epoch: 86, lr: 1.0e-02, train_loss: 0.7123, train_acc: 0.7492 test_loss: 0.7135, test_acc: 0.7504, best: 0.7504, time: 0:03:53
 Epoch: 87, lr: 1.0e-02, train_loss: 0.7051, train_acc: 0.7510 test_loss: 0.7660, test_acc: 0.7411, best: 0.7504, time: 0:03:53
 Epoch: 88, lr: 1.0e-02, train_loss: 0.7107, train_acc: 0.7452 test_loss: 0.8037, test_acc: 0.7328, best: 0.7504, time: 0:03:53
 Epoch: 89, lr: 1.0e-02, train_loss: 0.7103, train_acc: 0.7486 test_loss: 0.7129, test_acc: 0.7586, best: 0.7586, time: 0:03:53
 Epoch: 90, lr: 1.0e-02, train_loss: 0.6904, train_acc: 0.7552 test_loss: 0.7707, test_acc: 0.7352, best: 0.7586, time: 0:03:53
 Epoch: 91, lr: 1.0e-02, train_loss: 0.7099, train_acc: 0.7530 test_loss: 0.7352, test_acc: 0.7538, best: 0.7586, time: 0:03:53
 Epoch: 92, lr: 1.0e-02, train_loss: 0.6859, train_acc: 0.7538 test_loss: 0.7691, test_acc: 0.7399, best: 0.7586, time: 0:03:53
 Epoch: 93, lr: 1.0e-02, train_loss: 0.7045, train_acc: 0.7450 test_loss: 0.7583, test_acc: 0.7475, best: 0.7586, time: 0:03:53
 Epoch: 94, lr: 1.0e-02, train_loss: 0.6861, train_acc: 0.7554 test_loss: 0.7591, test_acc: 0.7499, best: 0.7586, time: 0:03:53
 Epoch: 95, lr: 1.0e-02, train_loss: 0.6732, train_acc: 0.7576 test_loss: 0.7758, test_acc: 0.7462, best: 0.7586, time: 0:03:53
 Epoch: 96, lr: 1.0e-02, train_loss: 0.6782, train_acc: 0.7556 test_loss: 0.7743, test_acc: 0.7484, best: 0.7586, time: 0:03:53
 Epoch: 97, lr: 1.0e-02, train_loss: 0.6722, train_acc: 0.7628 test_loss: 0.7307, test_acc: 0.7545, best: 0.7586, time: 0:03:53
 Epoch: 98, lr: 1.0e-02, train_loss: 0.6804, train_acc: 0.7594 test_loss: 0.7054, test_acc: 0.7595, best: 0.7595, time: 0:03:53
 Epoch: 99, lr: 1.0e-02, train_loss: 0.6713, train_acc: 0.7604 test_loss: 0.7239, test_acc: 0.7604, best: 0.7604, time: 0:03:53
 Epoch: 100, lr: 1.0e-02, train_loss: 0.6510, train_acc: 0.7720 test_loss: 0.7596, test_acc: 0.7519, best: 0.7604, time: 0:03:53
 Epoch: 101, lr: 1.0e-02, train_loss: 0.6578, train_acc: 0.7676 test_loss: 0.7113, test_acc: 0.7651, best: 0.7651, time: 0:03:53
 Epoch: 102, lr: 1.0e-02, train_loss: 0.6545, train_acc: 0.7616 test_loss: 0.7730, test_acc: 0.7514, best: 0.7651, time: 0:03:53
 Epoch: 103, lr: 1.0e-02, train_loss: 0.6383, train_acc: 0.7798 test_loss: 0.8235, test_acc: 0.7374, best: 0.7651, time: 0:03:53
 Epoch: 104, lr: 1.0e-02, train_loss: 0.6644, train_acc: 0.7654 test_loss: 0.7495, test_acc: 0.7548, best: 0.7651, time: 0:03:53
 Epoch: 105, lr: 1.0e-02, train_loss: 0.6417, train_acc: 0.7724 test_loss: 0.7088, test_acc: 0.7690, best: 0.7690, time: 0:03:53
 Epoch: 106, lr: 1.0e-02, train_loss: 0.6459, train_acc: 0.7684 test_loss: 0.7079, test_acc: 0.7645, best: 0.7690, time: 0:03:53
 Epoch: 107, lr: 1.0e-02, train_loss: 0.6331, train_acc: 0.7742 test_loss: 0.7057, test_acc: 0.7672, best: 0.7690, time: 0:03:53
 Epoch: 108, lr: 1.0e-02, train_loss: 0.6272, train_acc: 0.7770 test_loss: 0.7402, test_acc: 0.7585, best: 0.7690, time: 0:03:53
 Epoch: 109, lr: 1.0e-02, train_loss: 0.6169, train_acc: 0.7838 test_loss: 0.7434, test_acc: 0.7640, best: 0.7690, time: 0:03:53
 Epoch: 110, lr: 1.0e-02, train_loss: 0.6118, train_acc: 0.7824 test_loss: 0.7349, test_acc: 0.7684, best: 0.7690, time: 0:03:53
 Epoch: 111, lr: 1.0e-02, train_loss: 0.6202, train_acc: 0.7828 test_loss: 0.7056, test_acc: 0.7701, best: 0.7701, time: 0:03:53
 Epoch: 112, lr: 1.0e-02, train_loss: 0.6214, train_acc: 0.7790 test_loss: 0.7824, test_acc: 0.7525, best: 0.7701, time: 0:03:53
 Epoch: 113, lr: 1.0e-02, train_loss: 0.5935, train_acc: 0.7894 test_loss: 0.7475, test_acc: 0.7591, best: 0.7701, time: 0:03:53
 Epoch: 114, lr: 1.0e-02, train_loss: 0.5962, train_acc: 0.7900 test_loss: 0.7099, test_acc: 0.7676, best: 0.7701, time: 0:03:53
 Epoch: 115, lr: 1.0e-02, train_loss: 0.6156, train_acc: 0.7780 test_loss: 0.6948, test_acc: 0.7631, best: 0.7701, time: 0:03:53
 Epoch: 116, lr: 1.0e-02, train_loss: 0.6159, train_acc: 0.7812 test_loss: 0.7405, test_acc: 0.7636, best: 0.7701, time: 0:03:53
 Epoch: 117, lr: 1.0e-02, train_loss: 0.6157, train_acc: 0.7772 test_loss: 0.7572, test_acc: 0.7618, best: 0.7701, time: 0:03:53
 Epoch: 118, lr: 1.0e-02, train_loss: 0.5983, train_acc: 0.7850 test_loss: 0.7351, test_acc: 0.7652, best: 0.7701, time: 0:03:53
 Epoch: 119, lr: 1.0e-02, train_loss: 0.6074, train_acc: 0.7858 test_loss: 0.7897, test_acc: 0.7421, best: 0.7701, time: 0:03:53
 Epoch: 120, lr: 1.0e-02, train_loss: 0.6059, train_acc: 0.7868 test_loss: 0.7588, test_acc: 0.7478, best: 0.7701, time: 0:03:53
 Epoch: 121, lr: 1.0e-02, train_loss: 0.5776, train_acc: 0.8008 test_loss: 0.7268, test_acc: 0.7595, best: 0.7701, time: 0:03:53
 Epoch: 122, lr: 1.0e-02, train_loss: 0.5957, train_acc: 0.7924 test_loss: 0.7457, test_acc: 0.7631, best: 0.7701, time: 0:03:53
 Epoch: 123, lr: 1.0e-02, train_loss: 0.5767, train_acc: 0.7928 test_loss: 0.7605, test_acc: 0.7542, best: 0.7701, time: 0:03:53
 Epoch: 124, lr: 1.0e-02, train_loss: 0.5771, train_acc: 0.7982 test_loss: 0.7325, test_acc: 0.7698, best: 0.7701, time: 0:03:53
 Epoch: 125, lr: 1.0e-02, train_loss: 0.5636, train_acc: 0.7978 test_loss: 0.7621, test_acc: 0.7689, best: 0.7701, time: 0:03:53
 Epoch: 126, lr: 1.0e-02, train_loss: 0.5874, train_acc: 0.7968 test_loss: 0.6701, test_acc: 0.7780, best: 0.7780, time: 0:03:53
 Epoch: 127, lr: 1.0e-02, train_loss: 0.5702, train_acc: 0.8032 test_loss: 0.7319, test_acc: 0.7725, best: 0.7780, time: 0:03:53
 Epoch: 128, lr: 1.0e-02, train_loss: 0.5638, train_acc: 0.8030 test_loss: 0.7079, test_acc: 0.7792, best: 0.7792, time: 0:03:53
 Epoch: 129, lr: 1.0e-02, train_loss: 0.5579, train_acc: 0.8000 test_loss: 0.8038, test_acc: 0.7569, best: 0.7792, time: 0:03:53
 Epoch: 130, lr: 1.0e-02, train_loss: 0.5684, train_acc: 0.8010 test_loss: 0.7673, test_acc: 0.7674, best: 0.7792, time: 0:03:53
 Epoch: 131, lr: 1.0e-02, train_loss: 0.5529, train_acc: 0.8050 test_loss: 0.7388, test_acc: 0.7766, best: 0.7792, time: 0:03:53
 Epoch: 132, lr: 1.0e-02, train_loss: 0.5631, train_acc: 0.7986 test_loss: 0.7159, test_acc: 0.7655, best: 0.7792, time: 0:03:53
 Epoch: 133, lr: 1.0e-02, train_loss: 0.5615, train_acc: 0.7982 test_loss: 0.6970, test_acc: 0.7771, best: 0.7792, time: 0:03:53
 Epoch: 134, lr: 1.0e-02, train_loss: 0.5333, train_acc: 0.8082 test_loss: 0.7263, test_acc: 0.7750, best: 0.7792, time: 0:03:53
 Epoch: 135, lr: 1.0e-02, train_loss: 0.5296, train_acc: 0.8172 test_loss: 0.7182, test_acc: 0.7781, best: 0.7792, time: 0:03:53
 Epoch: 136, lr: 1.0e-02, train_loss: 0.5430, train_acc: 0.8164 test_loss: 0.7366, test_acc: 0.7701, best: 0.7792, time: 0:03:53
 Epoch: 137, lr: 1.0e-02, train_loss: 0.5359, train_acc: 0.8134 test_loss: 0.7320, test_acc: 0.7748, best: 0.7792, time: 0:03:53
 Epoch: 138, lr: 1.0e-02, train_loss: 0.5573, train_acc: 0.8064 test_loss: 0.7160, test_acc: 0.7780, best: 0.7792, time: 0:03:53
 Epoch: 139, lr: 1.0e-02, train_loss: 0.5521, train_acc: 0.8036 test_loss: 0.7567, test_acc: 0.7578, best: 0.7792, time: 0:03:53
 Epoch: 140, lr: 1.0e-02, train_loss: 0.5349, train_acc: 0.8088 test_loss: 0.7569, test_acc: 0.7711, best: 0.7792, time: 0:03:53
 Epoch: 141, lr: 1.0e-02, train_loss: 0.5280, train_acc: 0.8116 test_loss: 0.7550, test_acc: 0.7694, best: 0.7792, time: 0:03:53
 Epoch: 142, lr: 1.0e-02, train_loss: 0.5146, train_acc: 0.8100 test_loss: 0.7529, test_acc: 0.7718, best: 0.7792, time: 0:03:53
 Epoch: 143, lr: 1.0e-02, train_loss: 0.5311, train_acc: 0.8126 test_loss: 0.7132, test_acc: 0.7814, best: 0.7814, time: 0:03:53
 Epoch: 144, lr: 1.0e-02, train_loss: 0.5197, train_acc: 0.8164 test_loss: 0.7271, test_acc: 0.7758, best: 0.7814, time: 0:03:53
 Epoch: 145, lr: 1.0e-02, train_loss: 0.5329, train_acc: 0.8126 test_loss: 0.7850, test_acc: 0.7694, best: 0.7814, time: 0:03:53
 Epoch: 146, lr: 1.0e-02, train_loss: 0.4962, train_acc: 0.8228 test_loss: 0.7506, test_acc: 0.7698, best: 0.7814, time: 0:03:53
 Epoch: 147, lr: 1.0e-02, train_loss: 0.5175, train_acc: 0.8134 test_loss: 0.7364, test_acc: 0.7801, best: 0.7814, time: 0:03:53
 Epoch: 148, lr: 1.0e-02, train_loss: 0.5265, train_acc: 0.8080 test_loss: 0.7863, test_acc: 0.7588, best: 0.7814, time: 0:03:53
 Epoch: 149, lr: 1.0e-02, train_loss: 0.5356, train_acc: 0.8122 test_loss: 0.6980, test_acc: 0.7847, best: 0.7847, time: 0:03:53
 Epoch: 150, lr: 1.0e-02, train_loss: 0.5140, train_acc: 0.8166 test_loss: 0.7198, test_acc: 0.7756, best: 0.7847, time: 0:03:53
 Epoch: 151, lr: 1.0e-02, train_loss: 0.5239, train_acc: 0.8166 test_loss: 0.7044, test_acc: 0.7853, best: 0.7853, time: 0:03:53
 Epoch: 152, lr: 1.0e-02, train_loss: 0.4818, train_acc: 0.8278 test_loss: 0.7170, test_acc: 0.7731, best: 0.7853, time: 0:03:53
 Epoch: 153, lr: 1.0e-02, train_loss: 0.4957, train_acc: 0.8184 test_loss: 0.7296, test_acc: 0.7859, best: 0.7859, time: 0:03:53
 Epoch: 154, lr: 1.0e-02, train_loss: 0.4976, train_acc: 0.8224 test_loss: 0.7093, test_acc: 0.7843, best: 0.7859, time: 0:03:53
 Epoch: 155, lr: 1.0e-02, train_loss: 0.4980, train_acc: 0.8320 test_loss: 0.6782, test_acc: 0.7889, best: 0.7889, time: 0:03:53
 Epoch: 156, lr: 1.0e-02, train_loss: 0.4855, train_acc: 0.8260 test_loss: 0.6673, test_acc: 0.7895, best: 0.7895, time: 0:03:53
 Epoch: 157, lr: 1.0e-02, train_loss: 0.4759, train_acc: 0.8344 test_loss: 0.7520, test_acc: 0.7806, best: 0.7895, time: 0:03:53
 Epoch: 158, lr: 1.0e-02, train_loss: 0.4916, train_acc: 0.8258 test_loss: 0.6987, test_acc: 0.7770, best: 0.7895, time: 0:03:53
 Epoch: 159, lr: 1.0e-02, train_loss: 0.4687, train_acc: 0.8336 test_loss: 0.7592, test_acc: 0.7778, best: 0.7895, time: 0:03:53
 Epoch: 160, lr: 1.0e-02, train_loss: 0.4682, train_acc: 0.8322 test_loss: 0.7603, test_acc: 0.7833, best: 0.7895, time: 0:03:53
 Epoch: 161, lr: 1.0e-02, train_loss: 0.5102, train_acc: 0.8174 test_loss: 0.6809, test_acc: 0.7983, best: 0.7983, time: 0:03:53
 Epoch: 162, lr: 1.0e-02, train_loss: 0.4877, train_acc: 0.8270 test_loss: 0.8117, test_acc: 0.7708, best: 0.7983, time: 0:03:53
 Epoch: 163, lr: 1.0e-02, train_loss: 0.4933, train_acc: 0.8308 test_loss: 0.7313, test_acc: 0.7782, best: 0.7983, time: 0:03:53
 Epoch: 164, lr: 1.0e-02, train_loss: 0.4746, train_acc: 0.8358 test_loss: 0.7098, test_acc: 0.7865, best: 0.7983, time: 0:03:53
 Epoch: 165, lr: 1.0e-02, train_loss: 0.4764, train_acc: 0.8276 test_loss: 0.7966, test_acc: 0.7796, best: 0.7983, time: 0:03:53
 Epoch: 166, lr: 1.0e-02, train_loss: 0.4850, train_acc: 0.8238 test_loss: 0.7565, test_acc: 0.7771, best: 0.7983, time: 0:03:53
 Epoch: 167, lr: 1.0e-02, train_loss: 0.4642, train_acc: 0.8366 test_loss: 0.7828, test_acc: 0.7796, best: 0.7983, time: 0:03:53
 Epoch: 168, lr: 1.0e-02, train_loss: 0.4698, train_acc: 0.8340 test_loss: 0.7533, test_acc: 0.7802, best: 0.7983, time: 0:03:53
 Epoch: 169, lr: 1.0e-02, train_loss: 0.4786, train_acc: 0.8278 test_loss: 0.8483, test_acc: 0.7685, best: 0.7983, time: 0:03:53
 Epoch: 170, lr: 1.0e-02, train_loss: 0.4698, train_acc: 0.8312 test_loss: 0.7561, test_acc: 0.7795, best: 0.7983, time: 0:03:53
 Epoch: 171, lr: 1.0e-02, train_loss: 0.4447, train_acc: 0.8420 test_loss: 0.7489, test_acc: 0.7864, best: 0.7983, time: 0:03:53
 Epoch: 172, lr: 1.0e-02, train_loss: 0.4453, train_acc: 0.8438 test_loss: 0.8793, test_acc: 0.7552, best: 0.7983, time: 0:03:53
 Epoch: 173, lr: 1.0e-02, train_loss: 0.4556, train_acc: 0.8380 test_loss: 0.7434, test_acc: 0.7883, best: 0.7983, time: 0:03:53
 Epoch: 174, lr: 1.0e-02, train_loss: 0.4388, train_acc: 0.8468 test_loss: 0.7127, test_acc: 0.7929, best: 0.7983, time: 0:03:53
 Epoch: 175, lr: 1.0e-02, train_loss: 0.4510, train_acc: 0.8440 test_loss: 0.7660, test_acc: 0.7804, best: 0.7983, time: 0:03:53
 Epoch: 176, lr: 1.0e-02, train_loss: 0.4501, train_acc: 0.8440 test_loss: 0.8188, test_acc: 0.7689, best: 0.7983, time: 0:03:53
 Epoch: 177, lr: 1.0e-02, train_loss: 0.4394, train_acc: 0.8438 test_loss: 0.7793, test_acc: 0.7741, best: 0.7983, time: 0:03:53
 Epoch: 178, lr: 1.0e-02, train_loss: 0.4587, train_acc: 0.8358 test_loss: 0.7088, test_acc: 0.7950, best: 0.7983, time: 0:03:53
 Epoch: 179, lr: 1.0e-02, train_loss: 0.4418, train_acc: 0.8458 test_loss: 0.7493, test_acc: 0.7765, best: 0.7983, time: 0:03:53
 Epoch: 180, lr: 2.0e-03, train_loss: 0.3929, train_acc: 0.8598 test_loss: 0.6876, test_acc: 0.8057, best: 0.8057, time: 0:03:53
 Epoch: 181, lr: 2.0e-03, train_loss: 0.3591, train_acc: 0.8798 test_loss: 0.6939, test_acc: 0.8109, best: 0.8109, time: 0:03:53
 Epoch: 182, lr: 2.0e-03, train_loss: 0.3614, train_acc: 0.8740 test_loss: 0.6817, test_acc: 0.8130, best: 0.8130, time: 0:03:53
 Epoch: 183, lr: 2.0e-03, train_loss: 0.3281, train_acc: 0.8878 test_loss: 0.7246, test_acc: 0.8019, best: 0.8130, time: 0:03:53
 Epoch: 184, lr: 2.0e-03, train_loss: 0.3260, train_acc: 0.8862 test_loss: 0.7086, test_acc: 0.8083, best: 0.8130, time: 0:03:53
 Epoch: 185, lr: 2.0e-03, train_loss: 0.3373, train_acc: 0.8816 test_loss: 0.7288, test_acc: 0.8027, best: 0.8130, time: 0:03:53
 Epoch: 186, lr: 2.0e-03, train_loss: 0.3272, train_acc: 0.8846 test_loss: 0.7139, test_acc: 0.8121, best: 0.8130, time: 0:03:53
 Epoch: 187, lr: 2.0e-03, train_loss: 0.3130, train_acc: 0.8876 test_loss: 0.7327, test_acc: 0.8010, best: 0.8130, time: 0:03:53
 Epoch: 188, lr: 2.0e-03, train_loss: 0.3258, train_acc: 0.8884 test_loss: 0.7543, test_acc: 0.7999, best: 0.8130, time: 0:03:53
 Epoch: 189, lr: 2.0e-03, train_loss: 0.3251, train_acc: 0.8834 test_loss: 0.7241, test_acc: 0.8049, best: 0.8130, time: 0:03:53
 Epoch: 190, lr: 2.0e-03, train_loss: 0.3366, train_acc: 0.8782 test_loss: 0.7277, test_acc: 0.8019, best: 0.8130, time: 0:03:53
 Epoch: 191, lr: 2.0e-03, train_loss: 0.3095, train_acc: 0.8932 test_loss: 0.7447, test_acc: 0.8040, best: 0.8130, time: 0:03:52
 Epoch: 192, lr: 2.0e-03, train_loss: 0.3102, train_acc: 0.8912 test_loss: 0.7121, test_acc: 0.8095, best: 0.8130, time: 0:03:52
 Epoch: 193, lr: 2.0e-03, train_loss: 0.2976, train_acc: 0.8928 test_loss: 0.7133, test_acc: 0.8089, best: 0.8130, time: 0:03:52
 Epoch: 194, lr: 2.0e-03, train_loss: 0.2963, train_acc: 0.8962 test_loss: 0.7039, test_acc: 0.8133, best: 0.8133, time: 0:03:53
 Epoch: 195, lr: 2.0e-03, train_loss: 0.2983, train_acc: 0.8950 test_loss: 0.7355, test_acc: 0.8085, best: 0.8133, time: 0:03:52
 Epoch: 196, lr: 2.0e-03, train_loss: 0.3370, train_acc: 0.8802 test_loss: 0.6982, test_acc: 0.8107, best: 0.8133, time: 0:03:52
 Epoch: 197, lr: 2.0e-03, train_loss: 0.3015, train_acc: 0.8954 test_loss: 0.7545, test_acc: 0.8021, best: 0.8133, time: 0:03:51
 Epoch: 198, lr: 2.0e-03, train_loss: 0.3007, train_acc: 0.8954 test_loss: 0.7141, test_acc: 0.8076, best: 0.8133, time: 0:03:51
 Epoch: 199, lr: 2.0e-03, train_loss: 0.3029, train_acc: 0.8942 test_loss: 0.7233, test_acc: 0.8057, best: 0.8133, time: 0:03:51
 Epoch: 200, lr: 2.0e-03, train_loss: 0.3235, train_acc: 0.8888 test_loss: 0.7276, test_acc: 0.8121, best: 0.8133, time: 0:03:51
 Epoch: 201, lr: 2.0e-03, train_loss: 0.2936, train_acc: 0.9030 test_loss: 0.7245, test_acc: 0.8117, best: 0.8133, time: 0:03:51
 Epoch: 202, lr: 2.0e-03, train_loss: 0.3086, train_acc: 0.8920 test_loss: 0.7439, test_acc: 0.8084, best: 0.8133, time: 0:03:51
 Epoch: 203, lr: 2.0e-03, train_loss: 0.3083, train_acc: 0.8916 test_loss: 0.7452, test_acc: 0.8096, best: 0.8133, time: 0:03:51
 Epoch: 204, lr: 2.0e-03, train_loss: 0.3126, train_acc: 0.8930 test_loss: 0.7273, test_acc: 0.8045, best: 0.8133, time: 0:03:51
 Epoch: 205, lr: 2.0e-03, train_loss: 0.2941, train_acc: 0.8970 test_loss: 0.7403, test_acc: 0.8103, best: 0.8133, time: 0:03:51
 Epoch: 206, lr: 2.0e-03, train_loss: 0.3030, train_acc: 0.8980 test_loss: 0.7643, test_acc: 0.8047, best: 0.8133, time: 0:03:51
 Epoch: 207, lr: 2.0e-03, train_loss: 0.2806, train_acc: 0.9028 test_loss: 0.7493, test_acc: 0.8057, best: 0.8133, time: 0:03:51
 Epoch: 208, lr: 2.0e-03, train_loss: 0.2926, train_acc: 0.8966 test_loss: 0.7757, test_acc: 0.8051, best: 0.8133, time: 0:03:51
 Epoch: 209, lr: 2.0e-03, train_loss: 0.2853, train_acc: 0.9006 test_loss: 0.7556, test_acc: 0.8071, best: 0.8133, time: 0:03:51
 Epoch: 210, lr: 2.0e-03, train_loss: 0.2942, train_acc: 0.8982 test_loss: 0.7247, test_acc: 0.8123, best: 0.8133, time: 0:03:51
 Epoch: 211, lr: 2.0e-03, train_loss: 0.2981, train_acc: 0.8978 test_loss: 0.7226, test_acc: 0.8067, best: 0.8133, time: 0:03:51
 Epoch: 212, lr: 2.0e-03, train_loss: 0.3080, train_acc: 0.8914 test_loss: 0.7385, test_acc: 0.8091, best: 0.8133, time: 0:03:51
 Epoch: 213, lr: 2.0e-03, train_loss: 0.2825, train_acc: 0.8994 test_loss: 0.7572, test_acc: 0.8066, best: 0.8133, time: 0:03:51
 Epoch: 214, lr: 2.0e-03, train_loss: 0.2916, train_acc: 0.8966 test_loss: 0.7663, test_acc: 0.8043, best: 0.8133, time: 0:03:51
 Epoch: 215, lr: 2.0e-03, train_loss: 0.3061, train_acc: 0.8918 test_loss: 0.7351, test_acc: 0.8083, best: 0.8133, time: 0:03:51
 Epoch: 216, lr: 2.0e-03, train_loss: 0.2987, train_acc: 0.8980 test_loss: 0.7604, test_acc: 0.8021, best: 0.8133, time: 0:03:51
 Epoch: 217, lr: 2.0e-03, train_loss: 0.2950, train_acc: 0.8990 test_loss: 0.7382, test_acc: 0.8115, best: 0.8133, time: 0:03:51
 Epoch: 218, lr: 2.0e-03, train_loss: 0.2779, train_acc: 0.8994 test_loss: 0.7475, test_acc: 0.8115, best: 0.8133, time: 0:03:51
 Epoch: 219, lr: 2.0e-03, train_loss: 0.2786, train_acc: 0.9034 test_loss: 0.7724, test_acc: 0.8059, best: 0.8133, time: 0:03:51
 Epoch: 220, lr: 2.0e-03, train_loss: 0.2846, train_acc: 0.8980 test_loss: 0.7774, test_acc: 0.8023, best: 0.8133, time: 0:03:51
 Epoch: 221, lr: 2.0e-03, train_loss: 0.2954, train_acc: 0.8956 test_loss: 0.7713, test_acc: 0.8050, best: 0.8133, time: 0:03:51
 Epoch: 222, lr: 2.0e-03, train_loss: 0.2931, train_acc: 0.8976 test_loss: 0.7324, test_acc: 0.8126, best: 0.8133, time: 0:03:51
 Epoch: 223, lr: 2.0e-03, train_loss: 0.2833, train_acc: 0.9000 test_loss: 0.7432, test_acc: 0.8117, best: 0.8133, time: 0:03:51
 Epoch: 224, lr: 2.0e-03, train_loss: 0.2804, train_acc: 0.9062 test_loss: 0.7601, test_acc: 0.8080, best: 0.8133, time: 0:03:51
 Epoch: 225, lr: 2.0e-03, train_loss: 0.2842, train_acc: 0.8982 test_loss: 0.7514, test_acc: 0.8097, best: 0.8133, time: 0:03:51
 Epoch: 226, lr: 2.0e-03, train_loss: 0.2800, train_acc: 0.9028 test_loss: 0.7630, test_acc: 0.8081, best: 0.8133, time: 0:03:51
 Epoch: 227, lr: 2.0e-03, train_loss: 0.2697, train_acc: 0.9020 test_loss: 0.7951, test_acc: 0.8050, best: 0.8133, time: 0:03:51
 Epoch: 228, lr: 2.0e-03, train_loss: 0.2808, train_acc: 0.9020 test_loss: 0.7611, test_acc: 0.8083, best: 0.8133, time: 0:03:51
 Epoch: 229, lr: 2.0e-03, train_loss: 0.2691, train_acc: 0.9078 test_loss: 0.7786, test_acc: 0.8099, best: 0.8133, time: 0:03:51
 Epoch: 230, lr: 2.0e-03, train_loss: 0.2800, train_acc: 0.9038 test_loss: 0.7728, test_acc: 0.8070, best: 0.8133, time: 0:03:51
 Epoch: 231, lr: 2.0e-03, train_loss: 0.2774, train_acc: 0.9082 test_loss: 0.7740, test_acc: 0.8011, best: 0.8133, time: 0:03:51
 Epoch: 232, lr: 2.0e-03, train_loss: 0.2998, train_acc: 0.8956 test_loss: 0.7355, test_acc: 0.8097, best: 0.8133, time: 0:03:51
 Epoch: 233, lr: 2.0e-03, train_loss: 0.2906, train_acc: 0.8982 test_loss: 0.7690, test_acc: 0.8054, best: 0.8133, time: 0:03:51
 Epoch: 234, lr: 2.0e-03, train_loss: 0.2872, train_acc: 0.9050 test_loss: 0.7479, test_acc: 0.8049, best: 0.8133, time: 0:03:51
 Epoch: 235, lr: 2.0e-03, train_loss: 0.2742, train_acc: 0.9030 test_loss: 0.7513, test_acc: 0.8094, best: 0.8133, time: 0:03:51
 Epoch: 236, lr: 2.0e-03, train_loss: 0.2747, train_acc: 0.9094 test_loss: 0.7600, test_acc: 0.8056, best: 0.8133, time: 0:03:51
 Epoch: 237, lr: 2.0e-03, train_loss: 0.2942, train_acc: 0.8966 test_loss: 0.7497, test_acc: 0.8109, best: 0.8133, time: 0:03:51
 Epoch: 238, lr: 2.0e-03, train_loss: 0.2559, train_acc: 0.9086 test_loss: 0.7497, test_acc: 0.8091, best: 0.8133, time: 0:03:51
 Epoch: 239, lr: 2.0e-03, train_loss: 0.2688, train_acc: 0.9064 test_loss: 0.7708, test_acc: 0.8009, best: 0.8133, time: 0:03:51
 Epoch: 240, lr: 4.0e-04, train_loss: 0.2687, train_acc: 0.9042 test_loss: 0.7291, test_acc: 0.8086, best: 0.8133, time: 0:03:51
 Epoch: 241, lr: 4.0e-04, train_loss: 0.2752, train_acc: 0.9056 test_loss: 0.7363, test_acc: 0.8121, best: 0.8133, time: 0:03:51
 Epoch: 242, lr: 4.0e-04, train_loss: 0.2580, train_acc: 0.9088 test_loss: 0.7473, test_acc: 0.8105, best: 0.8133, time: 0:03:51
 Epoch: 243, lr: 4.0e-04, train_loss: 0.2516, train_acc: 0.9122 test_loss: 0.7261, test_acc: 0.8139, best: 0.8139, time: 0:03:51
 Epoch: 244, lr: 4.0e-04, train_loss: 0.2776, train_acc: 0.9040 test_loss: 0.7297, test_acc: 0.8093, best: 0.8139, time: 0:03:51
 Epoch: 245, lr: 4.0e-04, train_loss: 0.2589, train_acc: 0.9126 test_loss: 0.7408, test_acc: 0.8111, best: 0.8139, time: 0:03:51
 Epoch: 246, lr: 4.0e-04, train_loss: 0.2653, train_acc: 0.9096 test_loss: 0.7256, test_acc: 0.8104, best: 0.8139, time: 0:03:51
 Epoch: 247, lr: 4.0e-04, train_loss: 0.2478, train_acc: 0.9098 test_loss: 0.7424, test_acc: 0.8107, best: 0.8139, time: 0:03:51
 Epoch: 248, lr: 4.0e-04, train_loss: 0.2498, train_acc: 0.9104 test_loss: 0.7456, test_acc: 0.8104, best: 0.8139, time: 0:03:51
 Epoch: 249, lr: 4.0e-04, train_loss: 0.2487, train_acc: 0.9132 test_loss: 0.7544, test_acc: 0.8060, best: 0.8139, time: 0:03:51
 Epoch: 250, lr: 4.0e-04, train_loss: 0.2580, train_acc: 0.9088 test_loss: 0.7565, test_acc: 0.8123, best: 0.8139, time: 0:03:51
 Epoch: 251, lr: 4.0e-04, train_loss: 0.2483, train_acc: 0.9172 test_loss: 0.7348, test_acc: 0.8141, best: 0.8141, time: 0:03:51
 Epoch: 252, lr: 4.0e-04, train_loss: 0.2520, train_acc: 0.9100 test_loss: 0.7625, test_acc: 0.8123, best: 0.8141, time: 0:03:51
 Epoch: 253, lr: 4.0e-04, train_loss: 0.2693, train_acc: 0.9098 test_loss: 0.7588, test_acc: 0.8124, best: 0.8141, time: 0:03:51
 Epoch: 254, lr: 4.0e-04, train_loss: 0.2531, train_acc: 0.9128 test_loss: 0.7419, test_acc: 0.8133, best: 0.8141, time: 0:03:51
 Epoch: 255, lr: 4.0e-04, train_loss: 0.2337, train_acc: 0.9186 test_loss: 0.7342, test_acc: 0.8107, best: 0.8141, time: 0:03:51
 Epoch: 256, lr: 4.0e-04, train_loss: 0.2534, train_acc: 0.9176 test_loss: 0.7530, test_acc: 0.8105, best: 0.8141, time: 0:03:51
 Epoch: 257, lr: 4.0e-04, train_loss: 0.2579, train_acc: 0.9102 test_loss: 0.7249, test_acc: 0.8130, best: 0.8141, time: 0:03:51
 Epoch: 258, lr: 4.0e-04, train_loss: 0.2610, train_acc: 0.9124 test_loss: 0.7398, test_acc: 0.8117, best: 0.8141, time: 0:03:51
 Epoch: 259, lr: 4.0e-04, train_loss: 0.2706, train_acc: 0.9078 test_loss: 0.7620, test_acc: 0.8103, best: 0.8141, time: 0:03:51
 Epoch: 260, lr: 4.0e-04, train_loss: 0.2334, train_acc: 0.9200 test_loss: 0.7486, test_acc: 0.8101, best: 0.8141, time: 0:03:52
 Epoch: 261, lr: 4.0e-04, train_loss: 0.2700, train_acc: 0.9078 test_loss: 0.7708, test_acc: 0.8087, best: 0.8141, time: 0:03:51
 Epoch: 262, lr: 4.0e-04, train_loss: 0.2510, train_acc: 0.9144 test_loss: 0.7389, test_acc: 0.8116, best: 0.8141, time: 0:03:52
 Epoch: 263, lr: 4.0e-04, train_loss: 0.2488, train_acc: 0.9150 test_loss: 0.7380, test_acc: 0.8094, best: 0.8141, time: 0:03:51
 Epoch: 264, lr: 4.0e-04, train_loss: 0.2757, train_acc: 0.9032 test_loss: 0.7581, test_acc: 0.8105, best: 0.8141, time: 0:03:51
 Epoch: 265, lr: 4.0e-04, train_loss: 0.2549, train_acc: 0.9126 test_loss: 0.7452, test_acc: 0.8074, best: 0.8141, time: 0:03:51
 Epoch: 266, lr: 4.0e-04, train_loss: 0.2547, train_acc: 0.9108 test_loss: 0.7436, test_acc: 0.8109, best: 0.8141, time: 0:03:51
 Epoch: 267, lr: 4.0e-04, train_loss: 0.2570, train_acc: 0.9134 test_loss: 0.7483, test_acc: 0.8095, best: 0.8141, time: 0:03:51
 Epoch: 268, lr: 4.0e-04, train_loss: 0.2425, train_acc: 0.9160 test_loss: 0.7510, test_acc: 0.8081, best: 0.8141, time: 0:03:51
 Epoch: 269, lr: 4.0e-04, train_loss: 0.2499, train_acc: 0.9084 test_loss: 0.7361, test_acc: 0.8129, best: 0.8141, time: 0:03:51
 Epoch: 270, lr: 8.0e-05, train_loss: 0.2504, train_acc: 0.9098 test_loss: 0.7620, test_acc: 0.8113, best: 0.8141, time: 0:03:51
 Epoch: 271, lr: 8.0e-05, train_loss: 0.2514, train_acc: 0.9136 test_loss: 0.7365, test_acc: 0.8129, best: 0.8141, time: 0:03:51
 Epoch: 272, lr: 8.0e-05, train_loss: 0.2612, train_acc: 0.9076 test_loss: 0.7365, test_acc: 0.8134, best: 0.8141, time: 0:03:51
 Epoch: 273, lr: 8.0e-05, train_loss: 0.2416, train_acc: 0.9176 test_loss: 0.7617, test_acc: 0.8134, best: 0.8141, time: 0:03:51
 Epoch: 274, lr: 8.0e-05, train_loss: 0.2355, train_acc: 0.9182 test_loss: 0.7410, test_acc: 0.8124, best: 0.8141, time: 0:03:51
 Epoch: 275, lr: 8.0e-05, train_loss: 0.2489, train_acc: 0.9112 test_loss: 0.7463, test_acc: 0.8113, best: 0.8141, time: 0:03:51
 Epoch: 276, lr: 8.0e-05, train_loss: 0.2513, train_acc: 0.9148 test_loss: 0.7413, test_acc: 0.8136, best: 0.8141, time: 0:03:51
 Epoch: 277, lr: 8.0e-05, train_loss: 0.2512, train_acc: 0.9100 test_loss: 0.7479, test_acc: 0.8123, best: 0.8141, time: 0:03:51
 Epoch: 278, lr: 8.0e-05, train_loss: 0.2476, train_acc: 0.9148 test_loss: 0.7642, test_acc: 0.8113, best: 0.8141, time: 0:03:51
 Epoch: 279, lr: 8.0e-05, train_loss: 0.2533, train_acc: 0.9150 test_loss: 0.7429, test_acc: 0.8124, best: 0.8141, time: 0:03:51
 Epoch: 280, lr: 8.0e-05, train_loss: 0.2586, train_acc: 0.9084 test_loss: 0.7378, test_acc: 0.8103, best: 0.8141, time: 0:03:51
 Epoch: 281, lr: 8.0e-05, train_loss: 0.2323, train_acc: 0.9162 test_loss: 0.7338, test_acc: 0.8127, best: 0.8141, time: 0:03:51
 Epoch: 282, lr: 8.0e-05, train_loss: 0.2559, train_acc: 0.9100 test_loss: 0.7369, test_acc: 0.8117, best: 0.8141, time: 0:03:51
 Epoch: 283, lr: 8.0e-05, train_loss: 0.2582, train_acc: 0.9108 test_loss: 0.7407, test_acc: 0.8113, best: 0.8141, time: 0:03:51
 Epoch: 284, lr: 8.0e-05, train_loss: 0.2479, train_acc: 0.9170 test_loss: 0.7432, test_acc: 0.8145, best: 0.8145, time: 0:03:51
 Epoch: 285, lr: 8.0e-05, train_loss: 0.2366, train_acc: 0.9220 test_loss: 0.7616, test_acc: 0.8106, best: 0.8145, time: 0:03:51
 Epoch: 286, lr: 8.0e-05, train_loss: 0.2562, train_acc: 0.9110 test_loss: 0.7441, test_acc: 0.8103, best: 0.8145, time: 0:03:51
 Epoch: 287, lr: 8.0e-05, train_loss: 0.2660, train_acc: 0.9070 test_loss: 0.7364, test_acc: 0.8126, best: 0.8145, time: 0:03:51
 Epoch: 288, lr: 8.0e-05, train_loss: 0.2272, train_acc: 0.9200 test_loss: 0.7516, test_acc: 0.8144, best: 0.8145, time: 0:03:51
 Epoch: 289, lr: 8.0e-05, train_loss: 0.2500, train_acc: 0.9140 test_loss: 0.7441, test_acc: 0.8113, best: 0.8145, time: 0:03:51
 Epoch: 290, lr: 8.0e-05, train_loss: 0.2539, train_acc: 0.9130 test_loss: 0.7489, test_acc: 0.8145, best: 0.8145, time: 0:03:51
 Epoch: 291, lr: 8.0e-05, train_loss: 0.2461, train_acc: 0.9184 test_loss: 0.7502, test_acc: 0.8100, best: 0.8145, time: 0:03:51
 Epoch: 292, lr: 8.0e-05, train_loss: 0.2556, train_acc: 0.9108 test_loss: 0.7486, test_acc: 0.8084, best: 0.8145, time: 0:03:51
 Epoch: 293, lr: 8.0e-05, train_loss: 0.2365, train_acc: 0.9114 test_loss: 0.7444, test_acc: 0.8126, best: 0.8145, time: 0:03:51
 Epoch: 294, lr: 8.0e-05, train_loss: 0.2525, train_acc: 0.9084 test_loss: 0.7418, test_acc: 0.8086, best: 0.8145, time: 0:03:51
 Epoch: 295, lr: 8.0e-05, train_loss: 0.2486, train_acc: 0.9136 test_loss: 0.7317, test_acc: 0.8136, best: 0.8145, time: 0:03:51
 Epoch: 296, lr: 8.0e-05, train_loss: 0.2466, train_acc: 0.9142 test_loss: 0.7364, test_acc: 0.8125, best: 0.8145, time: 0:03:51
 Epoch: 297, lr: 8.0e-05, train_loss: 0.2476, train_acc: 0.9120 test_loss: 0.7692, test_acc: 0.8086, best: 0.8145, time: 0:03:50
 Epoch: 298, lr: 8.0e-05, train_loss: 0.2310, train_acc: 0.9152 test_loss: 0.7563, test_acc: 0.8103, best: 0.8145, time: 0:03:50
 Epoch: 299, lr: 8.0e-05, train_loss: 0.2422, train_acc: 0.9166 test_loss: 0.7487, test_acc: 0.8107, best: 0.8145, time: 0:03:50
 Highest accuracy: 0.8145