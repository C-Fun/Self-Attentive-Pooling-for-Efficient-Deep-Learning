
 Run on time: 2022-07-03 16:42:15.100059

 Architecture: mobilenet_v2-nlp_headfix2-1222121

 Pool Config: {
    "arch": "mobilenet_v2",
    "conv1": {
        "_conv2d": "norm",
        "pool_cfg": {}
    },
    "layer1": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "skip",
            "_stride": 1,
            "_psize": null,
            "_dim_reduced_ratio": null,
            "_num_heads": 2,
            "_conv2d": null,
            "_win_norm": null
        }
    },
    "layer2": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "nlp",
            "_stride": 2,
            "_psize": 1,
            "_dim_reduced_ratio": 1,
            "_num_heads": 2,
            "_conv2d": "norm",
            "_win_norm": true
        }
    },
    "layer3": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "nlp",
            "_stride": 2,
            "_psize": 1,
            "_dim_reduced_ratio": 1,
            "_num_heads": 2,
            "_conv2d": "norm",
            "_win_norm": true
        }
    },
    "layer4": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "nlp",
            "_stride": 2,
            "_psize": 1,
            "_dim_reduced_ratio": 1,
            "_num_heads": 2,
            "_conv2d": "norm",
            "_win_norm": true
        }
    },
    "layer5": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "skip",
            "_stride": 1,
            "_psize": null,
            "_dim_reduced_ratio": null,
            "_num_heads": 2,
            "_conv2d": null,
            "_win_norm": null
        }
    },
    "layer6": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "nlp",
            "_stride": 2,
            "_psize": 1,
            "_dim_reduced_ratio": 1,
            "_num_heads": 2,
            "_conv2d": "norm",
            "_win_norm": true
        }
    },
    "layer7": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "skip",
            "_stride": 1,
            "_psize": null,
            "_dim_reduced_ratio": null,
            "_num_heads": 2,
            "_conv2d": null,
            "_win_norm": null
        }
    },
    "conv2": {
        "_conv2d": "norm",
        "pool_cfg": {}
    }
}

 Arguments:
	 gpu                  : True
	 log                  : True
	 seed                 : 0
	 momentum             : 0.9
	 amsgrad              : True
	 dataset              : STL10
	 batch_size           : 8
	 architecture         : MOBILENET_V2-NLP_HEADFIX2-1222121
	 im_size              : 128
	 relu_threshold       : 4.0
	 learning_rate        : 0.01
	 pretrained_backbone  : 
	 pretrained_ann       : 
	 weight_decay         : 0.0
	 test_only            : False
	 epochs               : 300
	 lr_interval          : [180, 240, 270]
	 lr_reduce            : 5
	 optimizer            : SGD
	 dropout              : 0.2
	 kernel_size          : 3
	 dont_save            : False
	 visualize            : False
	 devices              : 0
 DataParallel(
  (module): Network(
    (net): MobileNetV2(
      (conv1): Sequential(
        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (features): Sequential(
        (0): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): InvertedResidual(
          (pooling): Pool2d(
            (logit): Sequential(
              (pool_weight): NLP_BASE(
                (downsample): Sequential(
                  (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1))
                  (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (2): ReLU(inplace=True)
                )
                (multihead_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=24, out_features=24, bias=True)
                )
                (restore): Sequential(
                  (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1))
                  (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (2): Sigmoid()
                )
                (pos_embed): PositionEmbeddingLearned(
                  (row_embed): Embedding(256, 12)
                  (col_embed): Embedding(256, 12)
                )
              )
            )
            (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
          )
          (conv): Sequential(
            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (2): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
            (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (3): InvertedResidual(
          (pooling): Pool2d(
            (logit): Sequential(
              (pool_weight): NLP_BASE(
                (downsample): Sequential(
                  (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
                  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (2): ReLU(inplace=True)
                )
                (multihead_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
                )
                (restore): Sequential(
                  (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
                  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (2): Sigmoid()
                )
                (pos_embed): PositionEmbeddingLearned(
                  (row_embed): Embedding(256, 16)
                  (col_embed): Embedding(256, 16)
                )
              )
            )
            (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
          )
          (conv): Sequential(
            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
            (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (4): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (5): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (6): InvertedResidual(
          (pooling): Pool2d(
            (logit): Sequential(
              (pool_weight): NLP_BASE(
                (downsample): Sequential(
                  (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
                  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (2): ReLU(inplace=True)
                )
                (multihead_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
                )
                (restore): Sequential(
                  (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
                  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (2): Sigmoid()
                )
                (pos_embed): PositionEmbeddingLearned(
                  (row_embed): Embedding(256, 32)
                  (col_embed): Embedding(256, 32)
                )
              )
            )
            (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
          )
          (conv): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (7): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (8): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (9): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (10): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (11): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
            (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (12): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
            (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (13): InvertedResidual(
          (pooling): Pool2d(
            (logit): Sequential(
              (pool_weight): NLP_BASE(
                (downsample): Sequential(
                  (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))
                  (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (2): ReLU(inplace=True)
                )
                (multihead_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=160, out_features=160, bias=True)
                )
                (restore): Sequential(
                  (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))
                  (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (2): Sigmoid()
                )
                (pos_embed): PositionEmbeddingLearned(
                  (row_embed): Embedding(256, 80)
                  (col_embed): Embedding(256, 80)
                )
              )
            )
            (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
          )
          (conv): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
            (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (14): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
            (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (15): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
            (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (16): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
            (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
      (conv2): Sequential(
        (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
      (classifier): Linear(in_features=1280, out_features=10, bias=True)
    )
  )
)
 SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0
)
 Epoch: 1, lr: 1.0e-02, train_loss: 2.4638, train_acc: 0.1746 test_loss: 1.9271, test_acc: 0.2500, best: 0.2500, time: 0:02:27
 Epoch: 2, lr: 1.0e-02, train_loss: 2.0189, train_acc: 0.2354 test_loss: 1.7813, test_acc: 0.2686, best: 0.2686, time: 0:02:32
 Epoch: 3, lr: 1.0e-02, train_loss: 1.9267, train_acc: 0.2648 test_loss: 1.6338, test_acc: 0.3693, best: 0.3693, time: 0:02:32
 Epoch: 4, lr: 1.0e-02, train_loss: 1.8573, train_acc: 0.3028 test_loss: 1.5871, test_acc: 0.3867, best: 0.3867, time: 0:02:32
 Epoch: 5, lr: 1.0e-02, train_loss: 1.8181, train_acc: 0.3158 test_loss: 1.5750, test_acc: 0.4052, best: 0.4052, time: 0:02:32
 Epoch: 6, lr: 1.0e-02, train_loss: 1.7655, train_acc: 0.3414 test_loss: 1.5324, test_acc: 0.4260, best: 0.4260, time: 0:02:32
 Epoch: 7, lr: 1.0e-02, train_loss: 1.7351, train_acc: 0.3532 test_loss: 1.4656, test_acc: 0.4470, best: 0.4470, time: 0:02:32
 Epoch: 8, lr: 1.0e-02, train_loss: 1.6870, train_acc: 0.3696 test_loss: 1.5268, test_acc: 0.4294, best: 0.4470, time: 0:02:32
 Epoch: 9, lr: 1.0e-02, train_loss: 1.6441, train_acc: 0.3892 test_loss: 1.4702, test_acc: 0.4605, best: 0.4605, time: 0:02:32
 Epoch: 10, lr: 1.0e-02, train_loss: 1.6077, train_acc: 0.4060 test_loss: 1.3687, test_acc: 0.4980, best: 0.4980, time: 0:02:32
 Epoch: 11, lr: 1.0e-02, train_loss: 1.5595, train_acc: 0.4232 test_loss: 1.3636, test_acc: 0.4849, best: 0.4980, time: 0:02:32
 Epoch: 12, lr: 1.0e-02, train_loss: 1.5317, train_acc: 0.4368 test_loss: 1.3083, test_acc: 0.5146, best: 0.5146, time: 0:02:32
 Epoch: 13, lr: 1.0e-02, train_loss: 1.5084, train_acc: 0.4470 test_loss: 1.2835, test_acc: 0.5202, best: 0.5202, time: 0:02:32
 Epoch: 14, lr: 1.0e-02, train_loss: 1.4648, train_acc: 0.4660 test_loss: 1.2558, test_acc: 0.5340, best: 0.5340, time: 0:02:32
 Epoch: 15, lr: 1.0e-02, train_loss: 1.4710, train_acc: 0.4610 test_loss: 1.2915, test_acc: 0.5340, best: 0.5340, time: 0:02:32
 Epoch: 16, lr: 1.0e-02, train_loss: 1.4391, train_acc: 0.4794 test_loss: 1.2800, test_acc: 0.5333, best: 0.5340, time: 0:02:32
 Epoch: 17, lr: 1.0e-02, train_loss: 1.3867, train_acc: 0.4910 test_loss: 1.1909, test_acc: 0.5493, best: 0.5493, time: 0:02:32
 Epoch: 18, lr: 1.0e-02, train_loss: 1.3647, train_acc: 0.5056 test_loss: 1.1293, test_acc: 0.5930, best: 0.5930, time: 0:02:32
 Epoch: 19, lr: 1.0e-02, train_loss: 1.3522, train_acc: 0.5064 test_loss: 1.1505, test_acc: 0.5684, best: 0.5930, time: 0:02:32
 Epoch: 20, lr: 1.0e-02, train_loss: 1.3041, train_acc: 0.5250 test_loss: 1.1535, test_acc: 0.5794, best: 0.5930, time: 0:02:32
 Epoch: 21, lr: 1.0e-02, train_loss: 1.3164, train_acc: 0.5154 test_loss: 1.1706, test_acc: 0.5811, best: 0.5930, time: 0:02:32
 Epoch: 22, lr: 1.0e-02, train_loss: 1.2935, train_acc: 0.5296 test_loss: 1.1624, test_acc: 0.5791, best: 0.5930, time: 0:02:32
 Epoch: 23, lr: 1.0e-02, train_loss: 1.2840, train_acc: 0.5316 test_loss: 1.1267, test_acc: 0.5851, best: 0.5930, time: 0:02:33
 Epoch: 24, lr: 1.0e-02, train_loss: 1.2534, train_acc: 0.5520 test_loss: 1.1370, test_acc: 0.5731, best: 0.5930, time: 0:02:33
 Epoch: 25, lr: 1.0e-02, train_loss: 1.2443, train_acc: 0.5532 test_loss: 1.2084, test_acc: 0.5646, best: 0.5930, time: 0:02:33
 Epoch: 26, lr: 1.0e-02, train_loss: 1.2062, train_acc: 0.5590 test_loss: 1.0972, test_acc: 0.5959, best: 0.5959, time: 0:02:33
 Epoch: 27, lr: 1.0e-02, train_loss: 1.2117, train_acc: 0.5662 test_loss: 0.9905, test_acc: 0.6431, best: 0.6431, time: 0:02:33
 Epoch: 28, lr: 1.0e-02, train_loss: 1.1849, train_acc: 0.5722 test_loss: 1.0578, test_acc: 0.6178, best: 0.6431, time: 0:02:33
 Epoch: 29, lr: 1.0e-02, train_loss: 1.1939, train_acc: 0.5694 test_loss: 0.9869, test_acc: 0.6454, best: 0.6454, time: 0:02:33
 Epoch: 30, lr: 1.0e-02, train_loss: 1.1534, train_acc: 0.5854 test_loss: 0.9965, test_acc: 0.6418, best: 0.6454, time: 0:02:33
 Epoch: 31, lr: 1.0e-02, train_loss: 1.1667, train_acc: 0.5794 test_loss: 1.0689, test_acc: 0.6198, best: 0.6454, time: 0:02:33
 Epoch: 32, lr: 1.0e-02, train_loss: 1.1253, train_acc: 0.5854 test_loss: 0.9717, test_acc: 0.6464, best: 0.6464, time: 0:02:33
 Epoch: 33, lr: 1.0e-02, train_loss: 1.1191, train_acc: 0.5946 test_loss: 1.0646, test_acc: 0.6192, best: 0.6464, time: 0:02:33
 Epoch: 34, lr: 1.0e-02, train_loss: 1.1058, train_acc: 0.5974 test_loss: 0.9802, test_acc: 0.6536, best: 0.6536, time: 0:02:33
 Epoch: 35, lr: 1.0e-02, train_loss: 1.0936, train_acc: 0.6090 test_loss: 1.0694, test_acc: 0.6202, best: 0.6536, time: 0:02:33
 Epoch: 36, lr: 1.0e-02, train_loss: 1.1023, train_acc: 0.6000 test_loss: 0.9594, test_acc: 0.6593, best: 0.6593, time: 0:02:33
 Epoch: 37, lr: 1.0e-02, train_loss: 1.0786, train_acc: 0.6146 test_loss: 0.9309, test_acc: 0.6666, best: 0.6666, time: 0:02:33
 Epoch: 38, lr: 1.0e-02, train_loss: 1.0564, train_acc: 0.6210 test_loss: 0.8791, test_acc: 0.6873, best: 0.6873, time: 0:02:33
 Epoch: 39, lr: 1.0e-02, train_loss: 1.0435, train_acc: 0.6194 test_loss: 0.9215, test_acc: 0.6733, best: 0.6873, time: 0:02:33
 Epoch: 40, lr: 1.0e-02, train_loss: 1.0481, train_acc: 0.6106 test_loss: 0.9201, test_acc: 0.6664, best: 0.6873, time: 0:02:33
 Epoch: 41, lr: 1.0e-02, train_loss: 1.0432, train_acc: 0.6182 test_loss: 0.9257, test_acc: 0.6694, best: 0.6873, time: 0:02:33
 Epoch: 42, lr: 1.0e-02, train_loss: 1.0172, train_acc: 0.6338 test_loss: 0.9305, test_acc: 0.6723, best: 0.6873, time: 0:02:33
 Epoch: 43, lr: 1.0e-02, train_loss: 1.0023, train_acc: 0.6376 test_loss: 0.8956, test_acc: 0.6786, best: 0.6873, time: 0:02:32
 Epoch: 44, lr: 1.0e-02, train_loss: 0.9895, train_acc: 0.6428 test_loss: 0.8759, test_acc: 0.6854, best: 0.6873, time: 0:02:33
 Epoch: 45, lr: 1.0e-02, train_loss: 0.9795, train_acc: 0.6438 test_loss: 0.8816, test_acc: 0.6954, best: 0.6954, time: 0:02:33
 Epoch: 46, lr: 1.0e-02, train_loss: 0.9862, train_acc: 0.6400 test_loss: 0.8613, test_acc: 0.6991, best: 0.6991, time: 0:02:33
 Epoch: 47, lr: 1.0e-02, train_loss: 0.9684, train_acc: 0.6446 test_loss: 0.8939, test_acc: 0.6876, best: 0.6991, time: 0:02:33
 Epoch: 48, lr: 1.0e-02, train_loss: 0.9544, train_acc: 0.6596 test_loss: 0.9602, test_acc: 0.6707, best: 0.6991, time: 0:02:33
 Epoch: 49, lr: 1.0e-02, train_loss: 0.9808, train_acc: 0.6546 test_loss: 0.8695, test_acc: 0.6921, best: 0.6991, time: 0:02:33
 Epoch: 50, lr: 1.0e-02, train_loss: 0.9269, train_acc: 0.6656 test_loss: 0.8536, test_acc: 0.7019, best: 0.7019, time: 0:02:33
 Epoch: 51, lr: 1.0e-02, train_loss: 0.9289, train_acc: 0.6594 test_loss: 0.8854, test_acc: 0.7000, best: 0.7019, time: 0:02:32
 Epoch: 52, lr: 1.0e-02, train_loss: 0.9197, train_acc: 0.6744 test_loss: 0.8871, test_acc: 0.6894, best: 0.7019, time: 0:02:33
 Epoch: 53, lr: 1.0e-02, train_loss: 0.9298, train_acc: 0.6636 test_loss: 0.9418, test_acc: 0.6770, best: 0.7019, time: 0:02:32
 Epoch: 54, lr: 1.0e-02, train_loss: 0.9062, train_acc: 0.6818 test_loss: 0.8183, test_acc: 0.7086, best: 0.7086, time: 0:02:33
 Epoch: 55, lr: 1.0e-02, train_loss: 0.9265, train_acc: 0.6720 test_loss: 0.8508, test_acc: 0.7051, best: 0.7086, time: 0:02:33
 Epoch: 56, lr: 1.0e-02, train_loss: 0.9008, train_acc: 0.6778 test_loss: 0.8402, test_acc: 0.7060, best: 0.7086, time: 0:02:33
 Epoch: 57, lr: 1.0e-02, train_loss: 0.8954, train_acc: 0.6780 test_loss: 0.8200, test_acc: 0.7074, best: 0.7086, time: 0:02:33
 Epoch: 58, lr: 1.0e-02, train_loss: 0.8817, train_acc: 0.6840 test_loss: 0.8274, test_acc: 0.7150, best: 0.7150, time: 0:02:33
 Epoch: 59, lr: 1.0e-02, train_loss: 0.8601, train_acc: 0.6856 test_loss: 0.7915, test_acc: 0.7222, best: 0.7222, time: 0:02:33
 Epoch: 60, lr: 1.0e-02, train_loss: 0.8708, train_acc: 0.6926 test_loss: 0.8095, test_acc: 0.7188, best: 0.7222, time: 0:02:33
 Epoch: 61, lr: 1.0e-02, train_loss: 0.8408, train_acc: 0.7020 test_loss: 0.9266, test_acc: 0.6763, best: 0.7222, time: 0:02:33
 Epoch: 62, lr: 1.0e-02, train_loss: 0.8790, train_acc: 0.6898 test_loss: 0.8654, test_acc: 0.6980, best: 0.7222, time: 0:02:32
 Epoch: 63, lr: 1.0e-02, train_loss: 0.8443, train_acc: 0.6982 test_loss: 0.8204, test_acc: 0.7179, best: 0.7222, time: 0:02:33
 Epoch: 64, lr: 1.0e-02, train_loss: 0.8386, train_acc: 0.6990 test_loss: 0.8224, test_acc: 0.7248, best: 0.7248, time: 0:02:33
 Epoch: 65, lr: 1.0e-02, train_loss: 0.8383, train_acc: 0.6956 test_loss: 0.7875, test_acc: 0.7300, best: 0.7300, time: 0:02:33
 Epoch: 66, lr: 1.0e-02, train_loss: 0.8312, train_acc: 0.6976 test_loss: 0.7833, test_acc: 0.7312, best: 0.7312, time: 0:02:33
 Epoch: 67, lr: 1.0e-02, train_loss: 0.8174, train_acc: 0.7126 test_loss: 0.7621, test_acc: 0.7356, best: 0.7356, time: 0:02:33
 Epoch: 68, lr: 1.0e-02, train_loss: 0.8041, train_acc: 0.7142 test_loss: 0.8035, test_acc: 0.7208, best: 0.7356, time: 0:02:33
 Epoch: 69, lr: 1.0e-02, train_loss: 0.8269, train_acc: 0.7060 test_loss: 0.8154, test_acc: 0.7200, best: 0.7356, time: 0:02:33
 Epoch: 70, lr: 1.0e-02, train_loss: 0.8119, train_acc: 0.7140 test_loss: 0.7993, test_acc: 0.7264, best: 0.7356, time: 0:02:33
 Epoch: 71, lr: 1.0e-02, train_loss: 0.8014, train_acc: 0.7168 test_loss: 0.8092, test_acc: 0.7235, best: 0.7356, time: 0:02:33
 Epoch: 72, lr: 1.0e-02, train_loss: 0.7858, train_acc: 0.7210 test_loss: 0.7540, test_acc: 0.7371, best: 0.7371, time: 0:02:33
 Epoch: 73, lr: 1.0e-02, train_loss: 0.7874, train_acc: 0.7232 test_loss: 0.8272, test_acc: 0.7181, best: 0.7371, time: 0:02:32
 Epoch: 74, lr: 1.0e-02, train_loss: 0.7899, train_acc: 0.7208 test_loss: 0.7955, test_acc: 0.7301, best: 0.7371, time: 0:02:33
 Epoch: 75, lr: 1.0e-02, train_loss: 0.7807, train_acc: 0.7190 test_loss: 0.7362, test_acc: 0.7426, best: 0.7426, time: 0:02:33
 Epoch: 76, lr: 1.0e-02, train_loss: 0.7638, train_acc: 0.7328 test_loss: 0.7519, test_acc: 0.7396, best: 0.7426, time: 0:02:33
 Epoch: 77, lr: 1.0e-02, train_loss: 0.7638, train_acc: 0.7298 test_loss: 0.7509, test_acc: 0.7389, best: 0.7426, time: 0:02:33
 Epoch: 78, lr: 1.0e-02, train_loss: 0.7658, train_acc: 0.7306 test_loss: 0.7664, test_acc: 0.7330, best: 0.7426, time: 0:02:33
 Epoch: 79, lr: 1.0e-02, train_loss: 0.7473, train_acc: 0.7346 test_loss: 0.8153, test_acc: 0.7232, best: 0.7426, time: 0:02:33
 Epoch: 80, lr: 1.0e-02, train_loss: 0.7760, train_acc: 0.7268 test_loss: 0.7812, test_acc: 0.7339, best: 0.7426, time: 0:02:33
 Epoch: 81, lr: 1.0e-02, train_loss: 0.7245, train_acc: 0.7400 test_loss: 0.7610, test_acc: 0.7488, best: 0.7488, time: 0:02:33
 Epoch: 82, lr: 1.0e-02, train_loss: 0.7374, train_acc: 0.7410 test_loss: 0.7768, test_acc: 0.7435, best: 0.7488, time: 0:02:33
 Epoch: 83, lr: 1.0e-02, train_loss: 0.7394, train_acc: 0.7400 test_loss: 0.7774, test_acc: 0.7385, best: 0.7488, time: 0:02:33
 Epoch: 84, lr: 1.0e-02, train_loss: 0.7366, train_acc: 0.7402 test_loss: 0.7983, test_acc: 0.7340, best: 0.7488, time: 0:02:33
 Epoch: 85, lr: 1.0e-02, train_loss: 0.7288, train_acc: 0.7426 test_loss: 0.7058, test_acc: 0.7576, best: 0.7576, time: 0:02:33
 Epoch: 86, lr: 1.0e-02, train_loss: 0.7111, train_acc: 0.7518 test_loss: 0.7585, test_acc: 0.7504, best: 0.7576, time: 0:02:33
 Epoch: 87, lr: 1.0e-02, train_loss: 0.7177, train_acc: 0.7456 test_loss: 0.8086, test_acc: 0.7266, best: 0.7576, time: 0:02:33
 Epoch: 88, lr: 1.0e-02, train_loss: 0.7049, train_acc: 0.7520 test_loss: 0.7252, test_acc: 0.7566, best: 0.7576, time: 0:02:33
 Epoch: 89, lr: 1.0e-02, train_loss: 0.7152, train_acc: 0.7484 test_loss: 0.7569, test_acc: 0.7446, best: 0.7576, time: 0:02:33
 Epoch: 90, lr: 1.0e-02, train_loss: 0.7256, train_acc: 0.7426 test_loss: 0.7781, test_acc: 0.7390, best: 0.7576, time: 0:02:32
 Epoch: 91, lr: 1.0e-02, train_loss: 0.6842, train_acc: 0.7512 test_loss: 0.7426, test_acc: 0.7514, best: 0.7576, time: 0:02:32
 Epoch: 92, lr: 1.0e-02, train_loss: 0.6839, train_acc: 0.7550 test_loss: 0.7959, test_acc: 0.7350, best: 0.7576, time: 0:02:32
 Epoch: 93, lr: 1.0e-02, train_loss: 0.6865, train_acc: 0.7514 test_loss: 0.7384, test_acc: 0.7561, best: 0.7576, time: 0:02:32
 Epoch: 94, lr: 1.0e-02, train_loss: 0.6856, train_acc: 0.7638 test_loss: 0.7256, test_acc: 0.7584, best: 0.7584, time: 0:02:33
 Epoch: 95, lr: 1.0e-02, train_loss: 0.6711, train_acc: 0.7642 test_loss: 0.7623, test_acc: 0.7506, best: 0.7584, time: 0:02:33
 Epoch: 96, lr: 1.0e-02, train_loss: 0.6574, train_acc: 0.7658 test_loss: 0.7741, test_acc: 0.7469, best: 0.7584, time: 0:02:33
 Epoch: 97, lr: 1.0e-02, train_loss: 0.6704, train_acc: 0.7610 test_loss: 0.7478, test_acc: 0.7490, best: 0.7584, time: 0:02:33
 Epoch: 98, lr: 1.0e-02, train_loss: 0.6607, train_acc: 0.7640 test_loss: 0.7915, test_acc: 0.7416, best: 0.7584, time: 0:02:32
 Epoch: 99, lr: 1.0e-02, train_loss: 0.6605, train_acc: 0.7612 test_loss: 0.7930, test_acc: 0.7339, best: 0.7584, time: 0:02:33
 Epoch: 100, lr: 1.0e-02, train_loss: 0.6700, train_acc: 0.7624 test_loss: 0.8924, test_acc: 0.7230, best: 0.7584, time: 0:02:33
 Epoch: 101, lr: 1.0e-02, train_loss: 0.6620, train_acc: 0.7686 test_loss: 0.7636, test_acc: 0.7495, best: 0.7584, time: 0:02:33
 Epoch: 102, lr: 1.0e-02, train_loss: 0.6487, train_acc: 0.7708 test_loss: 0.8261, test_acc: 0.7411, best: 0.7584, time: 0:02:32
 Epoch: 103, lr: 1.0e-02, train_loss: 0.6567, train_acc: 0.7664 test_loss: 0.7819, test_acc: 0.7435, best: 0.7584, time: 0:02:33
 Epoch: 104, lr: 1.0e-02, train_loss: 0.6360, train_acc: 0.7712 test_loss: 0.7094, test_acc: 0.7661, best: 0.7661, time: 0:02:32
 Epoch: 105, lr: 1.0e-02, train_loss: 0.6418, train_acc: 0.7706 test_loss: 0.6936, test_acc: 0.7708, best: 0.7708, time: 0:02:33
 Epoch: 106, lr: 1.0e-02, train_loss: 0.6357, train_acc: 0.7776 test_loss: 0.7248, test_acc: 0.7674, best: 0.7708, time: 0:02:32
 Epoch: 107, lr: 1.0e-02, train_loss: 0.6171, train_acc: 0.7820 test_loss: 0.7782, test_acc: 0.7518, best: 0.7708, time: 0:02:32
 Epoch: 108, lr: 1.0e-02, train_loss: 0.6058, train_acc: 0.7858 test_loss: 0.8452, test_acc: 0.7442, best: 0.7708, time: 0:02:32
 Epoch: 109, lr: 1.0e-02, train_loss: 0.6146, train_acc: 0.7810 test_loss: 0.7565, test_acc: 0.7610, best: 0.7708, time: 0:02:32
 Epoch: 110, lr: 1.0e-02, train_loss: 0.6214, train_acc: 0.7872 test_loss: 0.7327, test_acc: 0.7666, best: 0.7708, time: 0:02:32
 Epoch: 111, lr: 1.0e-02, train_loss: 0.6136, train_acc: 0.7806 test_loss: 0.7785, test_acc: 0.7482, best: 0.7708, time: 0:02:32
 Epoch: 112, lr: 1.0e-02, train_loss: 0.6022, train_acc: 0.7866 test_loss: 0.7528, test_acc: 0.7580, best: 0.7708, time: 0:02:32
 Epoch: 113, lr: 1.0e-02, train_loss: 0.6270, train_acc: 0.7784 test_loss: 0.7888, test_acc: 0.7479, best: 0.7708, time: 0:02:32
 Epoch: 114, lr: 1.0e-02, train_loss: 0.6050, train_acc: 0.7832 test_loss: 0.8003, test_acc: 0.7522, best: 0.7708, time: 0:02:32
 Epoch: 115, lr: 1.0e-02, train_loss: 0.6024, train_acc: 0.7786 test_loss: 0.7260, test_acc: 0.7666, best: 0.7708, time: 0:02:32
 Epoch: 116, lr: 1.0e-02, train_loss: 0.5888, train_acc: 0.7878 test_loss: 0.6946, test_acc: 0.7692, best: 0.7708, time: 0:02:32
 Epoch: 117, lr: 1.0e-02, train_loss: 0.6028, train_acc: 0.7838 test_loss: 0.7486, test_acc: 0.7632, best: 0.7708, time: 0:02:32
 Epoch: 118, lr: 1.0e-02, train_loss: 0.5896, train_acc: 0.7908 test_loss: 0.7412, test_acc: 0.7630, best: 0.7708, time: 0:02:32
 Epoch: 119, lr: 1.0e-02, train_loss: 0.5980, train_acc: 0.7884 test_loss: 0.6877, test_acc: 0.7825, best: 0.7825, time: 0:02:32
 Epoch: 120, lr: 1.0e-02, train_loss: 0.5840, train_acc: 0.7956 test_loss: 0.7794, test_acc: 0.7515, best: 0.7825, time: 0:02:32
 Epoch: 121, lr: 1.0e-02, train_loss: 0.5919, train_acc: 0.7908 test_loss: 0.7626, test_acc: 0.7604, best: 0.7825, time: 0:02:32
 Epoch: 122, lr: 1.0e-02, train_loss: 0.5693, train_acc: 0.8080 test_loss: 0.6982, test_acc: 0.7770, best: 0.7825, time: 0:02:32
 Epoch: 123, lr: 1.0e-02, train_loss: 0.5657, train_acc: 0.7974 test_loss: 0.7509, test_acc: 0.7720, best: 0.7825, time: 0:02:32
 Epoch: 124, lr: 1.0e-02, train_loss: 0.5730, train_acc: 0.8004 test_loss: 0.7358, test_acc: 0.7662, best: 0.7825, time: 0:02:32
 Epoch: 125, lr: 1.0e-02, train_loss: 0.5654, train_acc: 0.7994 test_loss: 0.7323, test_acc: 0.7716, best: 0.7825, time: 0:02:32
 Epoch: 126, lr: 1.0e-02, train_loss: 0.5710, train_acc: 0.8008 test_loss: 0.7657, test_acc: 0.7624, best: 0.7825, time: 0:02:32
 Epoch: 127, lr: 1.0e-02, train_loss: 0.5621, train_acc: 0.8026 test_loss: 0.7091, test_acc: 0.7722, best: 0.7825, time: 0:02:32
 Epoch: 128, lr: 1.0e-02, train_loss: 0.5757, train_acc: 0.8008 test_loss: 0.7339, test_acc: 0.7698, best: 0.7825, time: 0:02:32
 Epoch: 129, lr: 1.0e-02, train_loss: 0.5391, train_acc: 0.8132 test_loss: 0.7266, test_acc: 0.7756, best: 0.7825, time: 0:02:32
 Epoch: 130, lr: 1.0e-02, train_loss: 0.5360, train_acc: 0.8066 test_loss: 0.8112, test_acc: 0.7586, best: 0.7825, time: 0:02:32
 Epoch: 131, lr: 1.0e-02, train_loss: 0.5793, train_acc: 0.7974 test_loss: 0.7572, test_acc: 0.7618, best: 0.7825, time: 0:02:32
 Epoch: 132, lr: 1.0e-02, train_loss: 0.5266, train_acc: 0.8144 test_loss: 0.8239, test_acc: 0.7558, best: 0.7825, time: 0:02:32
 Epoch: 133, lr: 1.0e-02, train_loss: 0.5519, train_acc: 0.8056 test_loss: 0.7869, test_acc: 0.7565, best: 0.7825, time: 0:02:32
 Epoch: 134, lr: 1.0e-02, train_loss: 0.5461, train_acc: 0.8074 test_loss: 0.7570, test_acc: 0.7742, best: 0.7825, time: 0:02:32
 Epoch: 135, lr: 1.0e-02, train_loss: 0.5414, train_acc: 0.8064 test_loss: 0.7348, test_acc: 0.7732, best: 0.7825, time: 0:02:32
 Epoch: 136, lr: 1.0e-02, train_loss: 0.5387, train_acc: 0.8110 test_loss: 0.8701, test_acc: 0.7490, best: 0.7825, time: 0:02:32
 Epoch: 137, lr: 1.0e-02, train_loss: 0.5384, train_acc: 0.8096 test_loss: 0.7925, test_acc: 0.7674, best: 0.7825, time: 0:02:32
 Epoch: 138, lr: 1.0e-02, train_loss: 0.5379, train_acc: 0.8050 test_loss: 0.8329, test_acc: 0.7551, best: 0.7825, time: 0:02:32
 Epoch: 139, lr: 1.0e-02, train_loss: 0.5153, train_acc: 0.8202 test_loss: 0.7793, test_acc: 0.7724, best: 0.7825, time: 0:02:32
 Epoch: 140, lr: 1.0e-02, train_loss: 0.5206, train_acc: 0.8168 test_loss: 0.8080, test_acc: 0.7709, best: 0.7825, time: 0:02:32
 Epoch: 141, lr: 1.0e-02, train_loss: 0.5082, train_acc: 0.8194 test_loss: 0.6808, test_acc: 0.7881, best: 0.7881, time: 0:02:32
 Epoch: 142, lr: 1.0e-02, train_loss: 0.5273, train_acc: 0.8154 test_loss: 0.7382, test_acc: 0.7711, best: 0.7881, time: 0:02:32
 Epoch: 143, lr: 1.0e-02, train_loss: 0.5107, train_acc: 0.8192 test_loss: 0.7476, test_acc: 0.7824, best: 0.7881, time: 0:02:32
 Epoch: 144, lr: 1.0e-02, train_loss: 0.5257, train_acc: 0.8168 test_loss: 0.9016, test_acc: 0.7489, best: 0.7881, time: 0:02:32
 Epoch: 145, lr: 1.0e-02, train_loss: 0.5144, train_acc: 0.8148 test_loss: 0.7436, test_acc: 0.7730, best: 0.7881, time: 0:02:32
 Epoch: 146, lr: 1.0e-02, train_loss: 0.5135, train_acc: 0.8174 test_loss: 0.7469, test_acc: 0.7808, best: 0.7881, time: 0:02:32
 Epoch: 147, lr: 1.0e-02, train_loss: 0.5199, train_acc: 0.8170 test_loss: 0.8014, test_acc: 0.7705, best: 0.7881, time: 0:02:32
 Epoch: 148, lr: 1.0e-02, train_loss: 0.5200, train_acc: 0.8154 test_loss: 0.6905, test_acc: 0.7870, best: 0.7881, time: 0:02:32
 Epoch: 149, lr: 1.0e-02, train_loss: 0.5107, train_acc: 0.8260 test_loss: 0.6870, test_acc: 0.7844, best: 0.7881, time: 0:02:32
 Epoch: 150, lr: 1.0e-02, train_loss: 0.4989, train_acc: 0.8210 test_loss: 0.7414, test_acc: 0.7815, best: 0.7881, time: 0:02:32
 Epoch: 151, lr: 1.0e-02, train_loss: 0.5046, train_acc: 0.8274 test_loss: 0.7067, test_acc: 0.7750, best: 0.7881, time: 0:02:32
 Epoch: 152, lr: 1.0e-02, train_loss: 0.5030, train_acc: 0.8212 test_loss: 0.7431, test_acc: 0.7715, best: 0.7881, time: 0:02:32
 Epoch: 153, lr: 1.0e-02, train_loss: 0.5080, train_acc: 0.8174 test_loss: 0.7172, test_acc: 0.7800, best: 0.7881, time: 0:02:32
 Epoch: 154, lr: 1.0e-02, train_loss: 0.4943, train_acc: 0.8304 test_loss: 0.7945, test_acc: 0.7665, best: 0.7881, time: 0:02:32
 Epoch: 155, lr: 1.0e-02, train_loss: 0.5002, train_acc: 0.8202 test_loss: 0.7665, test_acc: 0.7704, best: 0.7881, time: 0:02:32
 Epoch: 156, lr: 1.0e-02, train_loss: 0.4838, train_acc: 0.8294 test_loss: 0.7579, test_acc: 0.7784, best: 0.7881, time: 0:02:32
 Epoch: 157, lr: 1.0e-02, train_loss: 0.4786, train_acc: 0.8312 test_loss: 0.7708, test_acc: 0.7762, best: 0.7881, time: 0:02:32
 Epoch: 158, lr: 1.0e-02, train_loss: 0.4730, train_acc: 0.8388 test_loss: 0.7638, test_acc: 0.7666, best: 0.7881, time: 0:02:32
 Epoch: 159, lr: 1.0e-02, train_loss: 0.4748, train_acc: 0.8324 test_loss: 0.7786, test_acc: 0.7728, best: 0.7881, time: 0:02:32
 Epoch: 160, lr: 1.0e-02, train_loss: 0.4599, train_acc: 0.8372 test_loss: 0.7432, test_acc: 0.7816, best: 0.7881, time: 0:02:32
 Epoch: 161, lr: 1.0e-02, train_loss: 0.4708, train_acc: 0.8328 test_loss: 0.6991, test_acc: 0.7871, best: 0.7881, time: 0:02:32
 Epoch: 162, lr: 1.0e-02, train_loss: 0.4823, train_acc: 0.8284 test_loss: 0.8165, test_acc: 0.7725, best: 0.7881, time: 0:02:32
 Epoch: 163, lr: 1.0e-02, train_loss: 0.4847, train_acc: 0.8332 test_loss: 0.7513, test_acc: 0.7856, best: 0.7881, time: 0:02:32
 Epoch: 164, lr: 1.0e-02, train_loss: 0.4562, train_acc: 0.8376 test_loss: 0.7791, test_acc: 0.7796, best: 0.7881, time: 0:02:32
 Epoch: 165, lr: 1.0e-02, train_loss: 0.4885, train_acc: 0.8282 test_loss: 0.7369, test_acc: 0.7762, best: 0.7881, time: 0:02:32
 Epoch: 166, lr: 1.0e-02, train_loss: 0.4770, train_acc: 0.8296 test_loss: 0.7170, test_acc: 0.7925, best: 0.7925, time: 0:02:32
 Epoch: 167, lr: 1.0e-02, train_loss: 0.4451, train_acc: 0.8408 test_loss: 0.7768, test_acc: 0.7782, best: 0.7925, time: 0:02:32
 Epoch: 168, lr: 1.0e-02, train_loss: 0.4653, train_acc: 0.8382 test_loss: 0.7174, test_acc: 0.7900, best: 0.7925, time: 0:02:32
 Epoch: 169, lr: 1.0e-02, train_loss: 0.4594, train_acc: 0.8404 test_loss: 0.7713, test_acc: 0.7761, best: 0.7925, time: 0:02:32
 Epoch: 170, lr: 1.0e-02, train_loss: 0.4646, train_acc: 0.8426 test_loss: 0.7687, test_acc: 0.7829, best: 0.7925, time: 0:02:32
 Epoch: 171, lr: 1.0e-02, train_loss: 0.4690, train_acc: 0.8344 test_loss: 0.7398, test_acc: 0.7830, best: 0.7925, time: 0:02:32
 Epoch: 172, lr: 1.0e-02, train_loss: 0.4572, train_acc: 0.8382 test_loss: 0.7557, test_acc: 0.7770, best: 0.7925, time: 0:02:32
 Epoch: 173, lr: 1.0e-02, train_loss: 0.4532, train_acc: 0.8390 test_loss: 0.7143, test_acc: 0.7880, best: 0.7925, time: 0:02:32
 Epoch: 174, lr: 1.0e-02, train_loss: 0.4580, train_acc: 0.8430 test_loss: 0.7310, test_acc: 0.7786, best: 0.7925, time: 0:02:32
 Epoch: 175, lr: 1.0e-02, train_loss: 0.4484, train_acc: 0.8408 test_loss: 0.7506, test_acc: 0.7854, best: 0.7925, time: 0:02:32
 Epoch: 176, lr: 1.0e-02, train_loss: 0.4716, train_acc: 0.8368 test_loss: 0.7687, test_acc: 0.7811, best: 0.7925, time: 0:02:32
 Epoch: 177, lr: 1.0e-02, train_loss: 0.4610, train_acc: 0.8422 test_loss: 0.7015, test_acc: 0.7869, best: 0.7925, time: 0:02:32
 Epoch: 178, lr: 1.0e-02, train_loss: 0.4593, train_acc: 0.8370 test_loss: 0.7332, test_acc: 0.7820, best: 0.7925, time: 0:02:32
 Epoch: 179, lr: 1.0e-02, train_loss: 0.4281, train_acc: 0.8512 test_loss: 0.7907, test_acc: 0.7760, best: 0.7925, time: 0:02:32
 Epoch: 180, lr: 2.0e-03, train_loss: 0.3888, train_acc: 0.8658 test_loss: 0.7012, test_acc: 0.8015, best: 0.8015, time: 0:02:32
 Epoch: 181, lr: 2.0e-03, train_loss: 0.3613, train_acc: 0.8734 test_loss: 0.6996, test_acc: 0.8016, best: 0.8016, time: 0:02:32
 Epoch: 182, lr: 2.0e-03, train_loss: 0.3414, train_acc: 0.8834 test_loss: 0.7018, test_acc: 0.8033, best: 0.8033, time: 0:02:32
 Epoch: 183, lr: 2.0e-03, train_loss: 0.3401, train_acc: 0.8816 test_loss: 0.7169, test_acc: 0.8025, best: 0.8033, time: 0:02:32
 Epoch: 184, lr: 2.0e-03, train_loss: 0.3368, train_acc: 0.8826 test_loss: 0.7098, test_acc: 0.8064, best: 0.8064, time: 0:02:32
 Epoch: 185, lr: 2.0e-03, train_loss: 0.3126, train_acc: 0.8962 test_loss: 0.7435, test_acc: 0.8019, best: 0.8064, time: 0:02:32
 Epoch: 186, lr: 2.0e-03, train_loss: 0.3295, train_acc: 0.8852 test_loss: 0.7714, test_acc: 0.7989, best: 0.8064, time: 0:02:32
 Epoch: 187, lr: 2.0e-03, train_loss: 0.3181, train_acc: 0.8900 test_loss: 0.7510, test_acc: 0.8029, best: 0.8064, time: 0:02:32
 Epoch: 188, lr: 2.0e-03, train_loss: 0.3315, train_acc: 0.8876 test_loss: 0.7339, test_acc: 0.7993, best: 0.8064, time: 0:02:32
 Epoch: 189, lr: 2.0e-03, train_loss: 0.3244, train_acc: 0.8844 test_loss: 0.7279, test_acc: 0.8037, best: 0.8064, time: 0:02:32
 Epoch: 190, lr: 2.0e-03, train_loss: 0.3010, train_acc: 0.8954 test_loss: 0.7535, test_acc: 0.8030, best: 0.8064, time: 0:02:32
 Epoch: 191, lr: 2.0e-03, train_loss: 0.3007, train_acc: 0.8994 test_loss: 0.7160, test_acc: 0.8076, best: 0.8076, time: 0:02:32
 Epoch: 192, lr: 2.0e-03, train_loss: 0.3077, train_acc: 0.8888 test_loss: 0.7501, test_acc: 0.8066, best: 0.8076, time: 0:02:32
 Epoch: 193, lr: 2.0e-03, train_loss: 0.3202, train_acc: 0.8894 test_loss: 0.7229, test_acc: 0.8066, best: 0.8076, time: 0:02:32
 Epoch: 194, lr: 2.0e-03, train_loss: 0.3060, train_acc: 0.8946 test_loss: 0.7387, test_acc: 0.8065, best: 0.8076, time: 0:02:32
 Epoch: 195, lr: 2.0e-03, train_loss: 0.3236, train_acc: 0.8894 test_loss: 0.7242, test_acc: 0.8049, best: 0.8076, time: 0:02:32
 Epoch: 196, lr: 2.0e-03, train_loss: 0.2821, train_acc: 0.8992 test_loss: 0.7492, test_acc: 0.8007, best: 0.8076, time: 0:02:32
 Epoch: 197, lr: 2.0e-03, train_loss: 0.2960, train_acc: 0.8914 test_loss: 0.7467, test_acc: 0.8056, best: 0.8076, time: 0:02:32
 Epoch: 198, lr: 2.0e-03, train_loss: 0.3005, train_acc: 0.8946 test_loss: 0.7404, test_acc: 0.8070, best: 0.8076, time: 0:02:32
 Epoch: 199, lr: 2.0e-03, train_loss: 0.2803, train_acc: 0.9038 test_loss: 0.7675, test_acc: 0.8064, best: 0.8076, time: 0:02:32
 Epoch: 200, lr: 2.0e-03, train_loss: 0.2997, train_acc: 0.8972 test_loss: 0.7682, test_acc: 0.7985, best: 0.8076, time: 0:02:32
 Epoch: 201, lr: 2.0e-03, train_loss: 0.2849, train_acc: 0.9006 test_loss: 0.7428, test_acc: 0.8064, best: 0.8076, time: 0:02:32
 Epoch: 202, lr: 2.0e-03, train_loss: 0.2902, train_acc: 0.9004 test_loss: 0.7631, test_acc: 0.8000, best: 0.8076, time: 0:02:32
 Epoch: 203, lr: 2.0e-03, train_loss: 0.3031, train_acc: 0.8938 test_loss: 0.7814, test_acc: 0.7985, best: 0.8076, time: 0:02:32
 Epoch: 204, lr: 2.0e-03, train_loss: 0.2759, train_acc: 0.9034 test_loss: 0.7417, test_acc: 0.8023, best: 0.8076, time: 0:02:32
 Epoch: 205, lr: 2.0e-03, train_loss: 0.2894, train_acc: 0.8970 test_loss: 0.7607, test_acc: 0.8030, best: 0.8076, time: 0:02:32
 Epoch: 206, lr: 2.0e-03, train_loss: 0.2838, train_acc: 0.9052 test_loss: 0.7394, test_acc: 0.8074, best: 0.8076, time: 0:02:32
 Epoch: 207, lr: 2.0e-03, train_loss: 0.2827, train_acc: 0.8994 test_loss: 0.7725, test_acc: 0.8067, best: 0.8076, time: 0:02:32
 Epoch: 208, lr: 2.0e-03, train_loss: 0.2842, train_acc: 0.9010 test_loss: 0.7429, test_acc: 0.8099, best: 0.8099, time: 0:02:32
 Epoch: 209, lr: 2.0e-03, train_loss: 0.2972, train_acc: 0.8994 test_loss: 0.7545, test_acc: 0.8066, best: 0.8099, time: 0:02:32
 Epoch: 210, lr: 2.0e-03, train_loss: 0.2900, train_acc: 0.8988 test_loss: 0.8079, test_acc: 0.7986, best: 0.8099, time: 0:02:32
 Epoch: 211, lr: 2.0e-03, train_loss: 0.2954, train_acc: 0.8932 test_loss: 0.7765, test_acc: 0.8046, best: 0.8099, time: 0:02:32
 Epoch: 212, lr: 2.0e-03, train_loss: 0.3045, train_acc: 0.8948 test_loss: 0.7773, test_acc: 0.8047, best: 0.8099, time: 0:02:32
 Epoch: 213, lr: 2.0e-03, train_loss: 0.2916, train_acc: 0.8976 test_loss: 0.7938, test_acc: 0.8007, best: 0.8099, time: 0:02:32
 Epoch: 214, lr: 2.0e-03, train_loss: 0.2781, train_acc: 0.9078 test_loss: 0.7554, test_acc: 0.8045, best: 0.8099, time: 0:02:32
 Epoch: 215, lr: 2.0e-03, train_loss: 0.2805, train_acc: 0.9020 test_loss: 0.7744, test_acc: 0.8051, best: 0.8099, time: 0:02:32
 Epoch: 216, lr: 2.0e-03, train_loss: 0.2817, train_acc: 0.9004 test_loss: 0.7698, test_acc: 0.8061, best: 0.8099, time: 0:02:32
 Epoch: 217, lr: 2.0e-03, train_loss: 0.2785, train_acc: 0.9050 test_loss: 0.7686, test_acc: 0.8045, best: 0.8099, time: 0:02:32
 Epoch: 218, lr: 2.0e-03, train_loss: 0.2965, train_acc: 0.8986 test_loss: 0.7626, test_acc: 0.8004, best: 0.8099, time: 0:02:32
 Epoch: 219, lr: 2.0e-03, train_loss: 0.2873, train_acc: 0.8976 test_loss: 0.7437, test_acc: 0.8020, best: 0.8099, time: 0:02:32
 Epoch: 220, lr: 2.0e-03, train_loss: 0.2959, train_acc: 0.8964 test_loss: 0.7598, test_acc: 0.8056, best: 0.8099, time: 0:02:32
 Epoch: 221, lr: 2.0e-03, train_loss: 0.2981, train_acc: 0.8990 test_loss: 0.7342, test_acc: 0.8050, best: 0.8099, time: 0:02:32
 Epoch: 222, lr: 2.0e-03, train_loss: 0.2627, train_acc: 0.9064 test_loss: 0.7646, test_acc: 0.8036, best: 0.8099, time: 0:02:32
 Epoch: 223, lr: 2.0e-03, train_loss: 0.2693, train_acc: 0.9120 test_loss: 0.7585, test_acc: 0.8061, best: 0.8099, time: 0:02:32
 Epoch: 224, lr: 2.0e-03, train_loss: 0.2747, train_acc: 0.9036 test_loss: 0.7862, test_acc: 0.8041, best: 0.8099, time: 0:02:32
 Epoch: 225, lr: 2.0e-03, train_loss: 0.2946, train_acc: 0.9002 test_loss: 0.7648, test_acc: 0.8073, best: 0.8099, time: 0:02:32
 Epoch: 226, lr: 2.0e-03, train_loss: 0.2663, train_acc: 0.9084 test_loss: 0.7826, test_acc: 0.8015, best: 0.8099, time: 0:02:32
 Epoch: 227, lr: 2.0e-03, train_loss: 0.2678, train_acc: 0.9078 test_loss: 0.7702, test_acc: 0.8011, best: 0.8099, time: 0:02:32
 Epoch: 228, lr: 2.0e-03, train_loss: 0.2899, train_acc: 0.9012 test_loss: 0.8033, test_acc: 0.7999, best: 0.8099, time: 0:02:32
 Epoch: 229, lr: 2.0e-03, train_loss: 0.2814, train_acc: 0.9016 test_loss: 0.8187, test_acc: 0.8003, best: 0.8099, time: 0:02:32
 Epoch: 230, lr: 2.0e-03, train_loss: 0.3015, train_acc: 0.8928 test_loss: 0.8041, test_acc: 0.7997, best: 0.8099, time: 0:02:32
 Epoch: 231, lr: 2.0e-03, train_loss: 0.2802, train_acc: 0.8982 test_loss: 0.7574, test_acc: 0.8065, best: 0.8099, time: 0:02:32
 Epoch: 232, lr: 2.0e-03, train_loss: 0.2668, train_acc: 0.9070 test_loss: 0.7630, test_acc: 0.8107, best: 0.8107, time: 0:02:32
 Epoch: 233, lr: 2.0e-03, train_loss: 0.2799, train_acc: 0.9036 test_loss: 0.8088, test_acc: 0.8033, best: 0.8107, time: 0:02:32
 Epoch: 234, lr: 2.0e-03, train_loss: 0.2695, train_acc: 0.9066 test_loss: 0.7650, test_acc: 0.8055, best: 0.8107, time: 0:02:32
 Epoch: 235, lr: 2.0e-03, train_loss: 0.2758, train_acc: 0.9022 test_loss: 0.7717, test_acc: 0.8061, best: 0.8107, time: 0:02:32
 Epoch: 236, lr: 2.0e-03, train_loss: 0.2577, train_acc: 0.9050 test_loss: 0.7965, test_acc: 0.8059, best: 0.8107, time: 0:02:32
 Epoch: 237, lr: 2.0e-03, train_loss: 0.2653, train_acc: 0.9106 test_loss: 0.7744, test_acc: 0.8040, best: 0.8107, time: 0:02:32
 Epoch: 238, lr: 2.0e-03, train_loss: 0.2736, train_acc: 0.9078 test_loss: 0.7854, test_acc: 0.8015, best: 0.8107, time: 0:02:32
 Epoch: 239, lr: 2.0e-03, train_loss: 0.2710, train_acc: 0.9066 test_loss: 0.7872, test_acc: 0.8063, best: 0.8107, time: 0:02:32
 Epoch: 240, lr: 4.0e-04, train_loss: 0.2665, train_acc: 0.9098 test_loss: 0.7834, test_acc: 0.8060, best: 0.8107, time: 0:02:32
 Epoch: 241, lr: 4.0e-04, train_loss: 0.2750, train_acc: 0.9052 test_loss: 0.7535, test_acc: 0.8085, best: 0.8107, time: 0:02:32
 Epoch: 242, lr: 4.0e-04, train_loss: 0.2756, train_acc: 0.9056 test_loss: 0.7615, test_acc: 0.8057, best: 0.8107, time: 0:02:32
 Epoch: 243, lr: 4.0e-04, train_loss: 0.2549, train_acc: 0.9128 test_loss: 0.7374, test_acc: 0.8100, best: 0.8107, time: 0:02:32
 Epoch: 244, lr: 4.0e-04, train_loss: 0.2628, train_acc: 0.9112 test_loss: 0.7586, test_acc: 0.8096, best: 0.8107, time: 0:02:32
 Epoch: 245, lr: 4.0e-04, train_loss: 0.2566, train_acc: 0.9082 test_loss: 0.7567, test_acc: 0.8064, best: 0.8107, time: 0:02:32
 Epoch: 246, lr: 4.0e-04, train_loss: 0.2559, train_acc: 0.9116 test_loss: 0.7864, test_acc: 0.8063, best: 0.8107, time: 0:02:32
 Epoch: 247, lr: 4.0e-04, train_loss: 0.2363, train_acc: 0.9174 test_loss: 0.7479, test_acc: 0.8126, best: 0.8126, time: 0:02:32
 Epoch: 248, lr: 4.0e-04, train_loss: 0.2491, train_acc: 0.9130 test_loss: 0.7614, test_acc: 0.8085, best: 0.8126, time: 0:02:32
 Epoch: 249, lr: 4.0e-04, train_loss: 0.2454, train_acc: 0.9164 test_loss: 0.7729, test_acc: 0.8057, best: 0.8126, time: 0:02:32
 Epoch: 250, lr: 4.0e-04, train_loss: 0.2604, train_acc: 0.9076 test_loss: 0.7612, test_acc: 0.8083, best: 0.8126, time: 0:02:32
 Epoch: 251, lr: 4.0e-04, train_loss: 0.2513, train_acc: 0.9126 test_loss: 0.7694, test_acc: 0.8045, best: 0.8126, time: 0:02:32
 Epoch: 252, lr: 4.0e-04, train_loss: 0.2405, train_acc: 0.9198 test_loss: 0.7849, test_acc: 0.8049, best: 0.8126, time: 0:02:32
 Epoch: 253, lr: 4.0e-04, train_loss: 0.2487, train_acc: 0.9134 test_loss: 0.7764, test_acc: 0.8046, best: 0.8126, time: 0:02:32
 Epoch: 254, lr: 4.0e-04, train_loss: 0.2324, train_acc: 0.9172 test_loss: 0.7680, test_acc: 0.8034, best: 0.8126, time: 0:02:32
 Epoch: 255, lr: 4.0e-04, train_loss: 0.2501, train_acc: 0.9100 test_loss: 0.7656, test_acc: 0.8065, best: 0.8126, time: 0:02:32
 Epoch: 256, lr: 4.0e-04, train_loss: 0.2530, train_acc: 0.9142 test_loss: 0.7840, test_acc: 0.8020, best: 0.8126, time: 0:02:32
 Epoch: 257, lr: 4.0e-04, train_loss: 0.2475, train_acc: 0.9186 test_loss: 0.7748, test_acc: 0.8044, best: 0.8126, time: 0:02:32
 Epoch: 258, lr: 4.0e-04, train_loss: 0.2623, train_acc: 0.9058 test_loss: 0.7730, test_acc: 0.8037, best: 0.8126, time: 0:02:32
 Epoch: 259, lr: 4.0e-04, train_loss: 0.2506, train_acc: 0.9148 test_loss: 0.7786, test_acc: 0.8040, best: 0.8126, time: 0:02:32
 Epoch: 260, lr: 4.0e-04, train_loss: 0.2517, train_acc: 0.9096 test_loss: 0.7564, test_acc: 0.8066, best: 0.8126, time: 0:02:32
 Epoch: 261, lr: 4.0e-04, train_loss: 0.2458, train_acc: 0.9134 test_loss: 0.7604, test_acc: 0.8069, best: 0.8126, time: 0:02:32
 Epoch: 262, lr: 4.0e-04, train_loss: 0.2517, train_acc: 0.9118 test_loss: 0.7735, test_acc: 0.8050, best: 0.8126, time: 0:02:31
 Epoch: 263, lr: 4.0e-04, train_loss: 0.2433, train_acc: 0.9154 test_loss: 0.7595, test_acc: 0.8087, best: 0.8126, time: 0:02:31
 Epoch: 264, lr: 4.0e-04, train_loss: 0.2370, train_acc: 0.9160 test_loss: 0.7592, test_acc: 0.8119, best: 0.8126, time: 0:02:31
 Epoch: 265, lr: 4.0e-04, train_loss: 0.2250, train_acc: 0.9214 test_loss: 0.7773, test_acc: 0.8056, best: 0.8126, time: 0:02:31
 Epoch: 266, lr: 4.0e-04, train_loss: 0.2645, train_acc: 0.9058 test_loss: 0.7703, test_acc: 0.8071, best: 0.8126, time: 0:02:31
 Epoch: 267, lr: 4.0e-04, train_loss: 0.2392, train_acc: 0.9200 test_loss: 0.7586, test_acc: 0.8083, best: 0.8126, time: 0:02:31
 Epoch: 268, lr: 4.0e-04, train_loss: 0.2476, train_acc: 0.9108 test_loss: 0.7827, test_acc: 0.8057, best: 0.8126, time: 0:02:31
 Epoch: 269, lr: 4.0e-04, train_loss: 0.2547, train_acc: 0.9110 test_loss: 0.7619, test_acc: 0.8043, best: 0.8126, time: 0:02:31
 Epoch: 270, lr: 8.0e-05, train_loss: 0.2437, train_acc: 0.9196 test_loss: 0.7541, test_acc: 0.8024, best: 0.8126, time: 0:02:31
 Epoch: 271, lr: 8.0e-05, train_loss: 0.2441, train_acc: 0.9148 test_loss: 0.7649, test_acc: 0.8066, best: 0.8126, time: 0:02:32
 Epoch: 272, lr: 8.0e-05, train_loss: 0.2413, train_acc: 0.9180 test_loss: 0.7717, test_acc: 0.8054, best: 0.8126, time: 0:02:32
 Epoch: 273, lr: 8.0e-05, train_loss: 0.2409, train_acc: 0.9158 test_loss: 0.7652, test_acc: 0.8054, best: 0.8126, time: 0:02:32
 Epoch: 274, lr: 8.0e-05, train_loss: 0.2425, train_acc: 0.9162 test_loss: 0.7750, test_acc: 0.8045, best: 0.8126, time: 0:02:32
 Epoch: 275, lr: 8.0e-05, train_loss: 0.2288, train_acc: 0.9238 test_loss: 0.7693, test_acc: 0.8035, best: 0.8126, time: 0:02:32
 Epoch: 276, lr: 8.0e-05, train_loss: 0.2450, train_acc: 0.9106 test_loss: 0.7598, test_acc: 0.8070, best: 0.8126, time: 0:02:32
 Epoch: 277, lr: 8.0e-05, train_loss: 0.2531, train_acc: 0.9126 test_loss: 0.7687, test_acc: 0.8081, best: 0.8126, time: 0:02:32
 Epoch: 278, lr: 8.0e-05, train_loss: 0.2377, train_acc: 0.9204 test_loss: 0.7641, test_acc: 0.8076, best: 0.8126, time: 0:02:32
 Epoch: 279, lr: 8.0e-05, train_loss: 0.2331, train_acc: 0.9156 test_loss: 0.7694, test_acc: 0.8060, best: 0.8126, time: 0:02:32
 Epoch: 280, lr: 8.0e-05, train_loss: 0.2572, train_acc: 0.9142 test_loss: 0.7496, test_acc: 0.8083, best: 0.8126, time: 0:02:32
 Epoch: 281, lr: 8.0e-05, train_loss: 0.2532, train_acc: 0.9112 test_loss: 0.7614, test_acc: 0.8061, best: 0.8126, time: 0:02:32
 Epoch: 282, lr: 8.0e-05, train_loss: 0.2328, train_acc: 0.9184 test_loss: 0.7598, test_acc: 0.8069, best: 0.8126, time: 0:02:32
 Epoch: 283, lr: 8.0e-05, train_loss: 0.2374, train_acc: 0.9156 test_loss: 0.7531, test_acc: 0.8090, best: 0.8126, time: 0:02:32
 Epoch: 284, lr: 8.0e-05, train_loss: 0.2343, train_acc: 0.9168 test_loss: 0.7755, test_acc: 0.8083, best: 0.8126, time: 0:02:32
 Epoch: 285, lr: 8.0e-05, train_loss: 0.2421, train_acc: 0.9128 test_loss: 0.7747, test_acc: 0.8083, best: 0.8126, time: 0:02:32
 Epoch: 286, lr: 8.0e-05, train_loss: 0.2620, train_acc: 0.9114 test_loss: 0.7579, test_acc: 0.8087, best: 0.8126, time: 0:02:32
 Epoch: 287, lr: 8.0e-05, train_loss: 0.2418, train_acc: 0.9144 test_loss: 0.7527, test_acc: 0.8087, best: 0.8126, time: 0:02:32
 Epoch: 288, lr: 8.0e-05, train_loss: 0.2443, train_acc: 0.9170 test_loss: 0.7525, test_acc: 0.8084, best: 0.8126, time: 0:02:32
 Epoch: 289, lr: 8.0e-05, train_loss: 0.2410, train_acc: 0.9176 test_loss: 0.7692, test_acc: 0.8037, best: 0.8126, time: 0:02:32
 Epoch: 290, lr: 8.0e-05, train_loss: 0.2320, train_acc: 0.9168 test_loss: 0.7808, test_acc: 0.8053, best: 0.8126, time: 0:02:32
 Epoch: 291, lr: 8.0e-05, train_loss: 0.2362, train_acc: 0.9186 test_loss: 0.7562, test_acc: 0.8074, best: 0.8126, time: 0:02:32
 Epoch: 292, lr: 8.0e-05, train_loss: 0.2571, train_acc: 0.9086 test_loss: 0.7792, test_acc: 0.8059, best: 0.8126, time: 0:02:32
 Epoch: 293, lr: 8.0e-05, train_loss: 0.2336, train_acc: 0.9182 test_loss: 0.7808, test_acc: 0.8063, best: 0.8126, time: 0:02:32
 Epoch: 294, lr: 8.0e-05, train_loss: 0.2420, train_acc: 0.9138 test_loss: 0.7839, test_acc: 0.8073, best: 0.8126, time: 0:02:32
 Epoch: 295, lr: 8.0e-05, train_loss: 0.2314, train_acc: 0.9176 test_loss: 0.7584, test_acc: 0.8091, best: 0.8126, time: 0:02:32
 Epoch: 296, lr: 8.0e-05, train_loss: 0.2358, train_acc: 0.9204 test_loss: 0.7637, test_acc: 0.8083, best: 0.8126, time: 0:02:32
 Epoch: 297, lr: 8.0e-05, train_loss: 0.2455, train_acc: 0.9132 test_loss: 0.7795, test_acc: 0.8074, best: 0.8126, time: 0:02:32
 Epoch: 298, lr: 8.0e-05, train_loss: 0.2272, train_acc: 0.9252 test_loss: 0.7607, test_acc: 0.8059, best: 0.8126, time: 0:02:32
 Epoch: 299, lr: 8.0e-05, train_loss: 0.2403, train_acc: 0.9146 test_loss: 0.7722, test_acc: 0.8075, best: 0.8126, time: 0:02:32
 Highest accuracy: 0.8126