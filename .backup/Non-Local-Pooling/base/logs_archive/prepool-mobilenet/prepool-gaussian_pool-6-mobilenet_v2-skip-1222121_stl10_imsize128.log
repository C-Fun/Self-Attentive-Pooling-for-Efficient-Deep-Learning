
 Run on time: 2022-07-04 20:54:31.277690

 Architecture: prepool-gaussian_pool-6-mobilenet_v2-skip-1222121

 Pool Config: {
    "arch": "mobilenet_v2",
    "conv1": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "gaussian_pool",
            "_stride": 6,
            "_psize": null,
            "_dim_reduced_ratio": null,
            "_num_heads": null,
            "_conv2d": null,
            "_win_norm": null
        }
    },
    "layer1": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "skip",
            "_stride": 1,
            "_psize": null,
            "_dim_reduced_ratio": null,
            "_num_heads": null,
            "_conv2d": null,
            "_win_norm": null
        }
    },
    "layer2": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "skip",
            "_stride": 2,
            "_psize": null,
            "_dim_reduced_ratio": null,
            "_num_heads": null,
            "_conv2d": null,
            "_win_norm": null
        }
    },
    "layer3": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "skip",
            "_stride": 2,
            "_psize": null,
            "_dim_reduced_ratio": null,
            "_num_heads": null,
            "_conv2d": null,
            "_win_norm": null
        }
    },
    "layer4": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "skip",
            "_stride": 2,
            "_psize": null,
            "_dim_reduced_ratio": null,
            "_num_heads": null,
            "_conv2d": null,
            "_win_norm": null
        }
    },
    "layer5": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "skip",
            "_stride": 1,
            "_psize": null,
            "_dim_reduced_ratio": null,
            "_num_heads": null,
            "_conv2d": null,
            "_win_norm": null
        }
    },
    "layer6": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "skip",
            "_stride": 2,
            "_psize": null,
            "_dim_reduced_ratio": null,
            "_num_heads": null,
            "_conv2d": null,
            "_win_norm": null
        }
    },
    "layer7": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "skip",
            "_stride": 1,
            "_psize": null,
            "_dim_reduced_ratio": null,
            "_num_heads": null,
            "_conv2d": null,
            "_win_norm": null
        }
    },
    "conv2": {
        "_conv2d": "norm",
        "pool_cfg": {}
    }
}

 Arguments:
	 gpu                  : True
	 log                  : True
	 seed                 : 0
	 momentum             : 0.9
	 amsgrad              : True
	 dataset              : STL10
	 batch_size           : 8
	 architecture         : prepool-gaussian_pool-6-mobilenet_v2-skip-1222121
	 im_size              : 128
	 relu_threshold       : 4.0
	 learning_rate        : 0.01
	 pretrained_backbone  : 
	 pretrained_ann       : 
	 weight_decay         : 0.0
	 test_only            : False
	 epochs               : 300
	 lr_interval          : [180, 240, 270]
	 lr_reduce            : 5
	 optimizer            : SGD
	 dropout              : 0.2
	 kernel_size          : 3
	 dont_save            : False
	 visualize            : False
	 devices              : 0
 DataParallel(
  (module): Network(
    (net): MobileNetV2(
      (conv1): Sequential(
        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (conv1_pool): GaussianPooling2d(
        kernel_size=6, stride=6, padding=0
        (ToHidden): Sequential(
          (0): AdaptiveAvgPool2d(output_size=(1, 1))
          (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))
          (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (3): ReLU()
        )
        (ToMean): Sequential(
          (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (ToSigma): Sequential(
          (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Sigmoid()
        )
        (activation): Softplus(beta=1, threshold=20)
      )
      (features): Sequential(
        (0): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (2): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
            (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (3): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
            (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (4): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (5): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (6): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
            (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (7): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (8): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (9): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (10): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (11): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
            (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (12): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
            (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (13): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)
            (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (14): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
            (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (15): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
            (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (16): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
            (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU6(inplace=True)
            (6): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
      (conv2): Sequential(
        (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
      (classifier): Linear(in_features=1280, out_features=10, bias=True)
    )
  )
)
 SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0
)
 Epoch: 1, lr: 1.0e-02, train_loss: 2.6169, train_acc: 0.1430 test_loss: 2.1246, test_acc: 0.1943, best: 0.1943, time: 0:00:52
 Epoch: 2, lr: 1.0e-02, train_loss: 2.1715, train_acc: 0.1896 test_loss: 1.9972, test_acc: 0.2065, best: 0.2065, time: 0:00:49
 Epoch: 3, lr: 1.0e-02, train_loss: 2.1198, train_acc: 0.1936 test_loss: 2.1023, test_acc: 0.2180, best: 0.2180, time: 0:00:47
 Epoch: 4, lr: 1.0e-02, train_loss: 2.1077, train_acc: 0.2032 test_loss: 1.8778, test_acc: 0.2665, best: 0.2665, time: 0:00:47
 Epoch: 5, lr: 1.0e-02, train_loss: 2.0687, train_acc: 0.2082 test_loss: 1.8459, test_acc: 0.2831, best: 0.2831, time: 0:00:47
 Epoch: 6, lr: 1.0e-02, train_loss: 2.0541, train_acc: 0.2226 test_loss: 1.9425, test_acc: 0.2416, best: 0.2831, time: 0:00:46
 Epoch: 7, lr: 1.0e-02, train_loss: 2.0249, train_acc: 0.2292 test_loss: 1.8028, test_acc: 0.3035, best: 0.3035, time: 0:00:46
 Epoch: 8, lr: 1.0e-02, train_loss: 2.0064, train_acc: 0.2410 test_loss: 1.7956, test_acc: 0.3144, best: 0.3144, time: 0:00:46
 Epoch: 9, lr: 1.0e-02, train_loss: 1.9642, train_acc: 0.2568 test_loss: 1.8169, test_acc: 0.3090, best: 0.3144, time: 0:00:51
 Epoch: 10, lr: 1.0e-02, train_loss: 1.9355, train_acc: 0.2564 test_loss: 1.7668, test_acc: 0.3416, best: 0.3416, time: 0:00:51
 Epoch: 11, lr: 1.0e-02, train_loss: 1.9752, train_acc: 0.2682 test_loss: 1.8250, test_acc: 0.3150, best: 0.3416, time: 0:00:52
 Epoch: 12, lr: 1.0e-02, train_loss: 1.9904, train_acc: 0.2500 test_loss: 1.7842, test_acc: 0.3386, best: 0.3416, time: 0:00:52
 Epoch: 13, lr: 1.0e-02, train_loss: 1.9792, train_acc: 0.2494 test_loss: 1.8140, test_acc: 0.3182, best: 0.3416, time: 0:00:53
 Epoch: 14, lr: 1.0e-02, train_loss: 1.9630, train_acc: 0.2562 test_loss: 1.7400, test_acc: 0.3186, best: 0.3416, time: 0:00:52
 Epoch: 15, lr: 1.0e-02, train_loss: 1.9418, train_acc: 0.2626 test_loss: 1.7892, test_acc: 0.3325, best: 0.3416, time: 0:00:52
 Epoch: 16, lr: 1.0e-02, train_loss: 1.9200, train_acc: 0.2646 test_loss: 1.7150, test_acc: 0.3556, best: 0.3556, time: 0:00:52
 Epoch: 17, lr: 1.0e-02, train_loss: 1.9442, train_acc: 0.2524 test_loss: 1.7148, test_acc: 0.3409, best: 0.3556, time: 0:00:52
 Epoch: 18, lr: 1.0e-02, train_loss: 1.9009, train_acc: 0.2782 test_loss: 1.6965, test_acc: 0.3621, best: 0.3621, time: 0:00:51
 Epoch: 19, lr: 1.0e-02, train_loss: 1.9205, train_acc: 0.2880 test_loss: 1.7619, test_acc: 0.3251, best: 0.3621, time: 0:00:51
 Epoch: 20, lr: 1.0e-02, train_loss: 1.8933, train_acc: 0.2860 test_loss: 1.6714, test_acc: 0.3805, best: 0.3805, time: 0:00:51
 Epoch: 21, lr: 1.0e-02, train_loss: 1.8699, train_acc: 0.2894 test_loss: 1.6919, test_acc: 0.3611, best: 0.3805, time: 0:00:51
 Epoch: 22, lr: 1.0e-02, train_loss: 1.8654, train_acc: 0.2908 test_loss: 1.6495, test_acc: 0.3887, best: 0.3887, time: 0:00:51
 Epoch: 23, lr: 1.0e-02, train_loss: 1.8702, train_acc: 0.2948 test_loss: 1.6971, test_acc: 0.3576, best: 0.3887, time: 0:00:51
 Epoch: 24, lr: 1.0e-02, train_loss: 1.8303, train_acc: 0.3108 test_loss: 1.6304, test_acc: 0.3997, best: 0.3997, time: 0:00:51
 Epoch: 25, lr: 1.0e-02, train_loss: 1.8445, train_acc: 0.2956 test_loss: 1.6675, test_acc: 0.3464, best: 0.3997, time: 0:00:51
 Epoch: 26, lr: 1.0e-02, train_loss: 1.8235, train_acc: 0.3074 test_loss: 1.5940, test_acc: 0.3795, best: 0.3997, time: 0:00:51
 Epoch: 27, lr: 1.0e-02, train_loss: 1.8127, train_acc: 0.3128 test_loss: 1.5909, test_acc: 0.3932, best: 0.3997, time: 0:00:51
 Epoch: 28, lr: 1.0e-02, train_loss: 1.8296, train_acc: 0.3086 test_loss: 1.7532, test_acc: 0.3297, best: 0.3997, time: 0:00:51
 Epoch: 29, lr: 1.0e-02, train_loss: 1.8568, train_acc: 0.2984 test_loss: 1.7000, test_acc: 0.3262, best: 0.3997, time: 0:00:51
 Epoch: 30, lr: 1.0e-02, train_loss: 1.8428, train_acc: 0.2914 test_loss: 1.6974, test_acc: 0.3362, best: 0.3997, time: 0:00:51
 Epoch: 31, lr: 1.0e-02, train_loss: 1.8388, train_acc: 0.3018 test_loss: 1.6692, test_acc: 0.3690, best: 0.3997, time: 0:00:51
 Epoch: 32, lr: 1.0e-02, train_loss: 1.8218, train_acc: 0.3126 test_loss: 1.6590, test_acc: 0.3566, best: 0.3997, time: 0:00:50
 Epoch: 33, lr: 1.0e-02, train_loss: 1.8531, train_acc: 0.2966 test_loss: 1.6884, test_acc: 0.3481, best: 0.3997, time: 0:00:51
 Epoch: 34, lr: 1.0e-02, train_loss: 1.8080, train_acc: 0.3144 test_loss: 1.6066, test_acc: 0.3995, best: 0.3997, time: 0:00:51
 Epoch: 35, lr: 1.0e-02, train_loss: 1.7949, train_acc: 0.3090 test_loss: 1.6189, test_acc: 0.3811, best: 0.3997, time: 0:00:51
 Epoch: 36, lr: 1.0e-02, train_loss: 1.7963, train_acc: 0.3168 test_loss: 1.6268, test_acc: 0.3805, best: 0.3997, time: 0:00:50
 Epoch: 37, lr: 1.0e-02, train_loss: 1.7639, train_acc: 0.3198 test_loss: 1.5677, test_acc: 0.4070, best: 0.4070, time: 0:00:51
 Epoch: 38, lr: 1.0e-02, train_loss: 1.7512, train_acc: 0.3328 test_loss: 1.5490, test_acc: 0.4124, best: 0.4124, time: 0:00:51
 Epoch: 39, lr: 1.0e-02, train_loss: 1.7432, train_acc: 0.3360 test_loss: 1.5397, test_acc: 0.3980, best: 0.4124, time: 0:00:54
 Epoch: 40, lr: 1.0e-02, train_loss: 1.7377, train_acc: 0.3320 test_loss: 1.5933, test_acc: 0.4027, best: 0.4124, time: 0:00:52
 Epoch: 41, lr: 1.0e-02, train_loss: 1.7262, train_acc: 0.3398 test_loss: 1.5697, test_acc: 0.3937, best: 0.4124, time: 0:00:52
 Epoch: 42, lr: 1.0e-02, train_loss: 1.7303, train_acc: 0.3504 test_loss: 1.5536, test_acc: 0.4103, best: 0.4124, time: 0:00:52
 Epoch: 43, lr: 1.0e-02, train_loss: 1.7173, train_acc: 0.3444 test_loss: 1.5382, test_acc: 0.4189, best: 0.4189, time: 0:00:51
 Epoch: 44, lr: 1.0e-02, train_loss: 1.7037, train_acc: 0.3476 test_loss: 1.5312, test_acc: 0.4213, best: 0.4213, time: 0:00:51
 Epoch: 45, lr: 1.0e-02, train_loss: 1.6741, train_acc: 0.3606 test_loss: 1.5262, test_acc: 0.4191, best: 0.4213, time: 0:00:51
 Epoch: 46, lr: 1.0e-02, train_loss: 1.6946, train_acc: 0.3592 test_loss: 1.4734, test_acc: 0.4380, best: 0.4380, time: 0:00:54
 Epoch: 47, lr: 1.0e-02, train_loss: 1.6960, train_acc: 0.3508 test_loss: 1.4999, test_acc: 0.4305, best: 0.4380, time: 0:00:52
 Epoch: 48, lr: 1.0e-02, train_loss: 1.6896, train_acc: 0.3622 test_loss: 1.4996, test_acc: 0.4323, best: 0.4380, time: 0:00:50
 Epoch: 49, lr: 1.0e-02, train_loss: 1.6972, train_acc: 0.3556 test_loss: 1.5106, test_acc: 0.4265, best: 0.4380, time: 0:00:50
 Epoch: 50, lr: 1.0e-02, train_loss: 1.6672, train_acc: 0.3616 test_loss: 1.4848, test_acc: 0.4382, best: 0.4382, time: 0:00:51
 Epoch: 51, lr: 1.0e-02, train_loss: 1.6570, train_acc: 0.3602 test_loss: 1.4868, test_acc: 0.4465, best: 0.4465, time: 0:00:50
 Epoch: 52, lr: 1.0e-02, train_loss: 1.6820, train_acc: 0.3622 test_loss: 1.4918, test_acc: 0.4326, best: 0.4465, time: 0:00:52
 Epoch: 53, lr: 1.0e-02, train_loss: 1.6865, train_acc: 0.3622 test_loss: 1.4882, test_acc: 0.4447, best: 0.4465, time: 0:00:52
 Epoch: 54, lr: 1.0e-02, train_loss: 1.6836, train_acc: 0.3644 test_loss: 1.5097, test_acc: 0.4380, best: 0.4465, time: 0:00:53
 Epoch: 55, lr: 1.0e-02, train_loss: 1.6364, train_acc: 0.3754 test_loss: 1.5339, test_acc: 0.4274, best: 0.4465, time: 0:00:52
 Epoch: 56, lr: 1.0e-02, train_loss: 1.6760, train_acc: 0.3672 test_loss: 1.4770, test_acc: 0.4363, best: 0.4465, time: 0:00:50
 Epoch: 57, lr: 1.0e-02, train_loss: 1.6161, train_acc: 0.3864 test_loss: 1.4157, test_acc: 0.4650, best: 0.4650, time: 0:00:50
 Epoch: 58, lr: 1.0e-02, train_loss: 1.6999, train_acc: 0.3596 test_loss: 1.5466, test_acc: 0.4124, best: 0.4650, time: 0:00:50
 Epoch: 59, lr: 1.0e-02, train_loss: 1.6581, train_acc: 0.3608 test_loss: 1.4935, test_acc: 0.4226, best: 0.4650, time: 0:00:50
 Epoch: 60, lr: 1.0e-02, train_loss: 1.6608, train_acc: 0.3678 test_loss: 1.4576, test_acc: 0.4615, best: 0.4650, time: 0:00:50
 Epoch: 61, lr: 1.0e-02, train_loss: 1.6421, train_acc: 0.3782 test_loss: 1.4540, test_acc: 0.4462, best: 0.4650, time: 0:00:50
 Epoch: 62, lr: 1.0e-02, train_loss: 1.6159, train_acc: 0.3836 test_loss: 1.4642, test_acc: 0.4519, best: 0.4650, time: 0:00:50
 Epoch: 63, lr: 1.0e-02, train_loss: 1.6324, train_acc: 0.3790 test_loss: 1.4411, test_acc: 0.4626, best: 0.4650, time: 0:00:51
 Epoch: 64, lr: 1.0e-02, train_loss: 1.6092, train_acc: 0.3894 test_loss: 1.4411, test_acc: 0.4521, best: 0.4650, time: 0:00:50
 Epoch: 65, lr: 1.0e-02, train_loss: 1.6347, train_acc: 0.3806 test_loss: 1.5089, test_acc: 0.4259, best: 0.4650, time: 0:00:51
 Epoch: 66, lr: 1.0e-02, train_loss: 1.6238, train_acc: 0.3888 test_loss: 1.4333, test_acc: 0.4526, best: 0.4650, time: 0:00:52
 Epoch: 67, lr: 1.0e-02, train_loss: 1.6280, train_acc: 0.3904 test_loss: 1.4327, test_acc: 0.4537, best: 0.4650, time: 0:00:51
 Epoch: 68, lr: 1.0e-02, train_loss: 1.6196, train_acc: 0.3812 test_loss: 1.4041, test_acc: 0.4582, best: 0.4650, time: 0:00:50
 Epoch: 69, lr: 1.0e-02, train_loss: 1.6002, train_acc: 0.3932 test_loss: 1.4110, test_acc: 0.4759, best: 0.4759, time: 0:00:51
 Epoch: 70, lr: 1.0e-02, train_loss: 1.5964, train_acc: 0.3952 test_loss: 1.3846, test_acc: 0.4778, best: 0.4778, time: 0:00:51
 Epoch: 71, lr: 1.0e-02, train_loss: 1.5709, train_acc: 0.4108 test_loss: 1.4088, test_acc: 0.4718, best: 0.4778, time: 0:00:53
 Epoch: 72, lr: 1.0e-02, train_loss: 1.5892, train_acc: 0.4026 test_loss: 1.4061, test_acc: 0.4793, best: 0.4793, time: 0:00:52
 Epoch: 73, lr: 1.0e-02, train_loss: 1.5555, train_acc: 0.4076 test_loss: 1.3809, test_acc: 0.4713, best: 0.4793, time: 0:00:51
 Epoch: 74, lr: 1.0e-02, train_loss: 1.5652, train_acc: 0.4048 test_loss: 1.3881, test_acc: 0.4773, best: 0.4793, time: 0:00:52
 Epoch: 75, lr: 1.0e-02, train_loss: 1.5639, train_acc: 0.4080 test_loss: 1.3800, test_acc: 0.4886, best: 0.4886, time: 0:00:54
 Epoch: 76, lr: 1.0e-02, train_loss: 1.5379, train_acc: 0.4220 test_loss: 1.3311, test_acc: 0.5025, best: 0.5025, time: 0:00:53
 Epoch: 77, lr: 1.0e-02, train_loss: 1.5453, train_acc: 0.4330 test_loss: 1.3905, test_acc: 0.4928, best: 0.5025, time: 0:00:50
 Epoch: 78, lr: 1.0e-02, train_loss: 1.5271, train_acc: 0.4198 test_loss: 1.4043, test_acc: 0.4610, best: 0.5025, time: 0:00:53
 Epoch: 79, lr: 1.0e-02, train_loss: 1.5273, train_acc: 0.4276 test_loss: 1.3263, test_acc: 0.5061, best: 0.5061, time: 0:00:54
 Epoch: 80, lr: 1.0e-02, train_loss: 1.5262, train_acc: 0.4336 test_loss: 1.3681, test_acc: 0.4889, best: 0.5061, time: 0:00:54
 Epoch: 81, lr: 1.0e-02, train_loss: 1.5127, train_acc: 0.4200 test_loss: 1.3324, test_acc: 0.5004, best: 0.5061, time: 0:00:54
 Epoch: 82, lr: 1.0e-02, train_loss: 1.5187, train_acc: 0.4292 test_loss: 1.3179, test_acc: 0.5140, best: 0.5140, time: 0:00:54
 Epoch: 83, lr: 1.0e-02, train_loss: 1.5153, train_acc: 0.4312 test_loss: 1.3372, test_acc: 0.5025, best: 0.5140, time: 0:00:54
 Epoch: 84, lr: 1.0e-02, train_loss: 1.5118, train_acc: 0.4278 test_loss: 1.4282, test_acc: 0.4656, best: 0.5140, time: 0:00:54
 Epoch: 85, lr: 1.0e-02, train_loss: 1.5073, train_acc: 0.4514 test_loss: 1.3222, test_acc: 0.5076, best: 0.5140, time: 0:00:54
 Epoch: 86, lr: 1.0e-02, train_loss: 1.4924, train_acc: 0.4462 test_loss: 1.3364, test_acc: 0.5016, best: 0.5140, time: 0:00:54
 Epoch: 87, lr: 1.0e-02, train_loss: 1.5253, train_acc: 0.4250 test_loss: 1.3784, test_acc: 0.4890, best: 0.5140, time: 0:00:54
 Epoch: 88, lr: 1.0e-02, train_loss: 1.5297, train_acc: 0.4310 test_loss: 1.3538, test_acc: 0.4936, best: 0.5140, time: 0:00:54
 Epoch: 89, lr: 1.0e-02, train_loss: 1.5081, train_acc: 0.4352 test_loss: 1.3102, test_acc: 0.5118, best: 0.5140, time: 0:00:55
 Epoch: 90, lr: 1.0e-02, train_loss: 1.4894, train_acc: 0.4452 test_loss: 1.3216, test_acc: 0.5101, best: 0.5140, time: 0:00:55
 Epoch: 91, lr: 1.0e-02, train_loss: 1.4765, train_acc: 0.4594 test_loss: 1.2991, test_acc: 0.5241, best: 0.5241, time: 0:00:56
 Epoch: 92, lr: 1.0e-02, train_loss: 1.4673, train_acc: 0.4560 test_loss: 1.2970, test_acc: 0.5240, best: 0.5241, time: 0:00:56
 Epoch: 93, lr: 1.0e-02, train_loss: 1.4454, train_acc: 0.4626 test_loss: 1.2438, test_acc: 0.5437, best: 0.5437, time: 0:00:55
 Epoch: 94, lr: 1.0e-02, train_loss: 1.4273, train_acc: 0.4706 test_loss: 1.2673, test_acc: 0.5305, best: 0.5437, time: 0:00:55
 Epoch: 95, lr: 1.0e-02, train_loss: 1.4975, train_acc: 0.4392 test_loss: 1.3341, test_acc: 0.5134, best: 0.5437, time: 0:00:55
 Epoch: 96, lr: 1.0e-02, train_loss: 1.4787, train_acc: 0.4520 test_loss: 1.3179, test_acc: 0.5206, best: 0.5437, time: 0:00:55
 Epoch: 97, lr: 1.0e-02, train_loss: 1.4614, train_acc: 0.4574 test_loss: 1.2805, test_acc: 0.5240, best: 0.5437, time: 0:00:56
 Epoch: 98, lr: 1.0e-02, train_loss: 1.4297, train_acc: 0.4610 test_loss: 1.2720, test_acc: 0.5319, best: 0.5437, time: 0:00:55
 Epoch: 99, lr: 1.0e-02, train_loss: 1.4370, train_acc: 0.4668 test_loss: 1.2703, test_acc: 0.5326, best: 0.5437, time: 0:00:55
 Epoch: 100, lr: 1.0e-02, train_loss: 1.6195, train_acc: 0.4014 test_loss: 1.3979, test_acc: 0.4831, best: 0.5437, time: 0:00:55
 Epoch: 101, lr: 1.0e-02, train_loss: 1.5536, train_acc: 0.4212 test_loss: 1.3352, test_acc: 0.4956, best: 0.5437, time: 0:00:55
 Epoch: 102, lr: 1.0e-02, train_loss: 1.4972, train_acc: 0.4444 test_loss: 1.3386, test_acc: 0.5124, best: 0.5437, time: 0:00:54
 Epoch: 103, lr: 1.0e-02, train_loss: 1.4491, train_acc: 0.4572 test_loss: 1.3160, test_acc: 0.5185, best: 0.5437, time: 0:00:54
 Epoch: 104, lr: 1.0e-02, train_loss: 1.4516, train_acc: 0.4632 test_loss: 1.2573, test_acc: 0.5407, best: 0.5437, time: 0:00:54
 Epoch: 105, lr: 1.0e-02, train_loss: 1.4455, train_acc: 0.4738 test_loss: 1.2928, test_acc: 0.5306, best: 0.5437, time: 0:00:54
 Epoch: 106, lr: 1.0e-02, train_loss: 1.4289, train_acc: 0.4674 test_loss: 1.2884, test_acc: 0.5162, best: 0.5437, time: 0:00:55
 Epoch: 107, lr: 1.0e-02, train_loss: 1.4053, train_acc: 0.4740 test_loss: 1.2501, test_acc: 0.5393, best: 0.5437, time: 0:00:55
 Epoch: 108, lr: 1.0e-02, train_loss: 1.4361, train_acc: 0.4716 test_loss: 1.2537, test_acc: 0.5429, best: 0.5437, time: 0:00:55
 Epoch: 109, lr: 1.0e-02, train_loss: 1.4010, train_acc: 0.4838 test_loss: 1.2325, test_acc: 0.5350, best: 0.5437, time: 0:00:56
 Epoch: 110, lr: 1.0e-02, train_loss: 1.3896, train_acc: 0.4868 test_loss: 1.2333, test_acc: 0.5485, best: 0.5485, time: 0:00:55
 Epoch: 111, lr: 1.0e-02, train_loss: 1.3890, train_acc: 0.4926 test_loss: 1.2131, test_acc: 0.5607, best: 0.5607, time: 0:00:55
 Epoch: 112, lr: 1.0e-02, train_loss: 1.3994, train_acc: 0.4832 test_loss: 1.2368, test_acc: 0.5504, best: 0.5607, time: 0:00:55
 Epoch: 113, lr: 1.0e-02, train_loss: 1.3783, train_acc: 0.4874 test_loss: 1.2285, test_acc: 0.5525, best: 0.5607, time: 0:00:56
 Epoch: 114, lr: 1.0e-02, train_loss: 1.3564, train_acc: 0.5044 test_loss: 1.2633, test_acc: 0.5394, best: 0.5607, time: 0:00:55
 Epoch: 115, lr: 1.0e-02, train_loss: 1.3769, train_acc: 0.4954 test_loss: 1.2295, test_acc: 0.5507, best: 0.5607, time: 0:00:55
 Epoch: 116, lr: 1.0e-02, train_loss: 1.3608, train_acc: 0.5008 test_loss: 1.2045, test_acc: 0.5604, best: 0.5607, time: 0:00:55
 Epoch: 117, lr: 1.0e-02, train_loss: 1.4180, train_acc: 0.4792 test_loss: 1.2508, test_acc: 0.5310, best: 0.5607, time: 0:00:55
 Epoch: 118, lr: 1.0e-02, train_loss: 1.4662, train_acc: 0.4520 test_loss: 1.3278, test_acc: 0.5074, best: 0.5607, time: 0:00:55
 Epoch: 119, lr: 1.0e-02, train_loss: 1.4166, train_acc: 0.4790 test_loss: 1.2105, test_acc: 0.5551, best: 0.5607, time: 0:00:54
 Epoch: 120, lr: 1.0e-02, train_loss: 1.3798, train_acc: 0.4862 test_loss: 1.2548, test_acc: 0.5333, best: 0.5607, time: 0:00:54
 Epoch: 121, lr: 1.0e-02, train_loss: 1.3522, train_acc: 0.4988 test_loss: 1.2001, test_acc: 0.5600, best: 0.5607, time: 0:00:54
 Epoch: 122, lr: 1.0e-02, train_loss: 1.3635, train_acc: 0.4968 test_loss: 1.1704, test_acc: 0.5713, best: 0.5713, time: 0:00:54
 Epoch: 123, lr: 1.0e-02, train_loss: 1.3349, train_acc: 0.5058 test_loss: 1.1652, test_acc: 0.5817, best: 0.5817, time: 0:00:55
 Epoch: 124, lr: 1.0e-02, train_loss: 1.3306, train_acc: 0.5112 test_loss: 1.1522, test_acc: 0.5855, best: 0.5855, time: 0:00:55
 Epoch: 125, lr: 1.0e-02, train_loss: 1.3036, train_acc: 0.5180 test_loss: 1.1641, test_acc: 0.5727, best: 0.5855, time: 0:00:55
 Epoch: 126, lr: 1.0e-02, train_loss: 1.2931, train_acc: 0.5256 test_loss: 1.1482, test_acc: 0.5746, best: 0.5855, time: 0:00:55
 Epoch: 127, lr: 1.0e-02, train_loss: 1.3190, train_acc: 0.5146 test_loss: 1.1849, test_acc: 0.5703, best: 0.5855, time: 0:00:54
 Epoch: 128, lr: 1.0e-02, train_loss: 1.3066, train_acc: 0.5178 test_loss: 1.1811, test_acc: 0.5723, best: 0.5855, time: 0:00:54
 Epoch: 129, lr: 1.0e-02, train_loss: 1.3595, train_acc: 0.4938 test_loss: 1.1748, test_acc: 0.5697, best: 0.5855, time: 0:00:54
 Epoch: 130, lr: 1.0e-02, train_loss: 1.3284, train_acc: 0.5180 test_loss: 1.1744, test_acc: 0.5755, best: 0.5855, time: 0:00:54
 Epoch: 131, lr: 1.0e-02, train_loss: 1.3312, train_acc: 0.5102 test_loss: 1.1534, test_acc: 0.5843, best: 0.5855, time: 0:00:54
 Epoch: 132, lr: 1.0e-02, train_loss: 1.3068, train_acc: 0.5144 test_loss: 1.1849, test_acc: 0.5663, best: 0.5855, time: 0:00:54
 Epoch: 133, lr: 1.0e-02, train_loss: 1.3236, train_acc: 0.5136 test_loss: 1.1946, test_acc: 0.5611, best: 0.5855, time: 0:00:54
 Epoch: 134, lr: 1.0e-02, train_loss: 1.2923, train_acc: 0.5210 test_loss: 1.1959, test_acc: 0.5585, best: 0.5855, time: 0:00:54
 Epoch: 135, lr: 1.0e-02, train_loss: 1.2751, train_acc: 0.5302 test_loss: 1.1387, test_acc: 0.5815, best: 0.5855, time: 0:00:54
 Epoch: 136, lr: 1.0e-02, train_loss: 1.2758, train_acc: 0.5292 test_loss: 1.1244, test_acc: 0.5933, best: 0.5933, time: 0:00:55
 Epoch: 137, lr: 1.0e-02, train_loss: 1.2728, train_acc: 0.5190 test_loss: 1.1490, test_acc: 0.5763, best: 0.5933, time: 0:00:54
 Epoch: 138, lr: 1.0e-02, train_loss: 1.2697, train_acc: 0.5372 test_loss: 1.1171, test_acc: 0.5989, best: 0.5989, time: 0:00:54
 Epoch: 139, lr: 1.0e-02, train_loss: 1.2654, train_acc: 0.5466 test_loss: 1.1411, test_acc: 0.5791, best: 0.5989, time: 0:00:54
 Epoch: 140, lr: 1.0e-02, train_loss: 1.2440, train_acc: 0.5426 test_loss: 1.1367, test_acc: 0.5861, best: 0.5989, time: 0:00:55
 Epoch: 141, lr: 1.0e-02, train_loss: 1.2629, train_acc: 0.5372 test_loss: 1.1654, test_acc: 0.5701, best: 0.5989, time: 0:00:55
 Epoch: 142, lr: 1.0e-02, train_loss: 1.2587, train_acc: 0.5386 test_loss: 1.1576, test_acc: 0.5770, best: 0.5989, time: 0:00:54
 Epoch: 143, lr: 1.0e-02, train_loss: 1.2933, train_acc: 0.5258 test_loss: 1.1588, test_acc: 0.5819, best: 0.5989, time: 0:00:54
 Epoch: 144, lr: 1.0e-02, train_loss: 1.2636, train_acc: 0.5426 test_loss: 1.1286, test_acc: 0.5900, best: 0.5989, time: 0:00:55
 Epoch: 145, lr: 1.0e-02, train_loss: 1.2501, train_acc: 0.5478 test_loss: 1.1683, test_acc: 0.5706, best: 0.5989, time: 0:00:54
 Epoch: 146, lr: 1.0e-02, train_loss: 1.2377, train_acc: 0.5436 test_loss: 1.1316, test_acc: 0.5860, best: 0.5989, time: 0:00:55
 Epoch: 147, lr: 1.0e-02, train_loss: 1.2353, train_acc: 0.5468 test_loss: 1.1073, test_acc: 0.5988, best: 0.5989, time: 0:00:54
 Epoch: 148, lr: 1.0e-02, train_loss: 1.2033, train_acc: 0.5596 test_loss: 1.1572, test_acc: 0.5735, best: 0.5989, time: 0:00:54
 Epoch: 149, lr: 1.0e-02, train_loss: 1.2290, train_acc: 0.5534 test_loss: 1.0906, test_acc: 0.6012, best: 0.6012, time: 0:00:54
 Epoch: 150, lr: 1.0e-02, train_loss: 1.2376, train_acc: 0.5480 test_loss: 1.0834, test_acc: 0.6044, best: 0.6044, time: 0:00:54
 Epoch: 151, lr: 1.0e-02, train_loss: 1.2450, train_acc: 0.5420 test_loss: 1.1171, test_acc: 0.5958, best: 0.6044, time: 0:00:54
 Epoch: 152, lr: 1.0e-02, train_loss: 1.2340, train_acc: 0.5490 test_loss: 1.0832, test_acc: 0.6049, best: 0.6049, time: 0:00:54
 Epoch: 153, lr: 1.0e-02, train_loss: 1.2147, train_acc: 0.5436 test_loss: 1.0919, test_acc: 0.5919, best: 0.6049, time: 0:00:54
 Epoch: 154, lr: 1.0e-02, train_loss: 1.2089, train_acc: 0.5610 test_loss: 1.1471, test_acc: 0.5791, best: 0.6049, time: 0:00:53
 Epoch: 155, lr: 1.0e-02, train_loss: 1.1916, train_acc: 0.5674 test_loss: 1.1050, test_acc: 0.5942, best: 0.6049, time: 0:00:52
 Epoch: 156, lr: 1.0e-02, train_loss: 1.1953, train_acc: 0.5664 test_loss: 1.0925, test_acc: 0.6030, best: 0.6049, time: 0:00:53
 Epoch: 157, lr: 1.0e-02, train_loss: 1.1883, train_acc: 0.5662 test_loss: 1.1006, test_acc: 0.6018, best: 0.6049, time: 0:00:53
 Epoch: 158, lr: 1.0e-02, train_loss: 1.2074, train_acc: 0.5592 test_loss: 1.0788, test_acc: 0.6050, best: 0.6050, time: 0:00:54
 Epoch: 159, lr: 1.0e-02, train_loss: 1.1806, train_acc: 0.5682 test_loss: 1.0504, test_acc: 0.6192, best: 0.6192, time: 0:00:54
 Epoch: 160, lr: 1.0e-02, train_loss: 1.1816, train_acc: 0.5712 test_loss: 1.0509, test_acc: 0.6178, best: 0.6192, time: 0:00:54
 Epoch: 161, lr: 1.0e-02, train_loss: 1.2131, train_acc: 0.5574 test_loss: 1.0831, test_acc: 0.6119, best: 0.6192, time: 0:00:54
 Epoch: 162, lr: 1.0e-02, train_loss: 1.1832, train_acc: 0.5704 test_loss: 1.0991, test_acc: 0.6001, best: 0.6192, time: 0:00:54
 Epoch: 163, lr: 1.0e-02, train_loss: 1.1924, train_acc: 0.5620 test_loss: 1.1458, test_acc: 0.5803, best: 0.6192, time: 0:00:54
 Epoch: 164, lr: 1.0e-02, train_loss: 1.1585, train_acc: 0.5744 test_loss: 1.0958, test_acc: 0.6048, best: 0.6192, time: 0:00:54
 Epoch: 165, lr: 1.0e-02, train_loss: 1.1702, train_acc: 0.5676 test_loss: 1.0722, test_acc: 0.6099, best: 0.6192, time: 0:00:54
 Epoch: 166, lr: 1.0e-02, train_loss: 1.1775, train_acc: 0.5716 test_loss: 1.0769, test_acc: 0.6039, best: 0.6192, time: 0:00:54
 Epoch: 167, lr: 1.0e-02, train_loss: 1.1392, train_acc: 0.5814 test_loss: 1.1070, test_acc: 0.5982, best: 0.6192, time: 0:00:54
 Epoch: 168, lr: 1.0e-02, train_loss: 1.1724, train_acc: 0.5708 test_loss: 1.0962, test_acc: 0.5985, best: 0.6192, time: 0:00:54
 Epoch: 169, lr: 1.0e-02, train_loss: 1.1612, train_acc: 0.5806 test_loss: 1.0996, test_acc: 0.5998, best: 0.6192, time: 0:00:54
 Epoch: 170, lr: 1.0e-02, train_loss: 1.1711, train_acc: 0.5734 test_loss: 1.0637, test_acc: 0.6165, best: 0.6192, time: 0:00:54
 Epoch: 171, lr: 1.0e-02, train_loss: 1.1796, train_acc: 0.5718 test_loss: 1.0504, test_acc: 0.6154, best: 0.6192, time: 0:00:54
 Epoch: 172, lr: 1.0e-02, train_loss: 1.1582, train_acc: 0.5734 test_loss: 1.0484, test_acc: 0.6200, best: 0.6200, time: 0:00:54
 Epoch: 173, lr: 1.0e-02, train_loss: 1.1531, train_acc: 0.5728 test_loss: 1.1438, test_acc: 0.5896, best: 0.6200, time: 0:00:54
 Epoch: 174, lr: 1.0e-02, train_loss: 1.1881, train_acc: 0.5696 test_loss: 1.1079, test_acc: 0.5986, best: 0.6200, time: 0:00:54
 Epoch: 175, lr: 1.0e-02, train_loss: 1.1608, train_acc: 0.5782 test_loss: 1.0572, test_acc: 0.6202, best: 0.6202, time: 0:00:54
 Epoch: 176, lr: 1.0e-02, train_loss: 1.1654, train_acc: 0.5648 test_loss: 1.1103, test_acc: 0.6032, best: 0.6202, time: 0:00:54
 Epoch: 177, lr: 1.0e-02, train_loss: 1.1762, train_acc: 0.5716 test_loss: 1.0722, test_acc: 0.6055, best: 0.6202, time: 0:00:54
 Epoch: 178, lr: 1.0e-02, train_loss: 1.1878, train_acc: 0.5738 test_loss: 1.0874, test_acc: 0.6095, best: 0.6202, time: 0:00:54
 Epoch: 179, lr: 1.0e-02, train_loss: 1.1572, train_acc: 0.5756 test_loss: 1.0697, test_acc: 0.6115, best: 0.6202, time: 0:00:54
 Epoch: 180, lr: 2.0e-03, train_loss: 1.0988, train_acc: 0.5990 test_loss: 1.0097, test_acc: 0.6355, best: 0.6355, time: 0:00:54
 Epoch: 181, lr: 2.0e-03, train_loss: 1.0907, train_acc: 0.6038 test_loss: 1.0115, test_acc: 0.6315, best: 0.6355, time: 0:00:54
 Epoch: 182, lr: 2.0e-03, train_loss: 1.0646, train_acc: 0.6124 test_loss: 1.0066, test_acc: 0.6375, best: 0.6375, time: 0:00:55
 Epoch: 183, lr: 2.0e-03, train_loss: 1.0621, train_acc: 0.6214 test_loss: 0.9945, test_acc: 0.6429, best: 0.6429, time: 0:00:54
 Epoch: 184, lr: 2.0e-03, train_loss: 1.0430, train_acc: 0.6210 test_loss: 0.9890, test_acc: 0.6438, best: 0.6438, time: 0:00:55
 Epoch: 185, lr: 2.0e-03, train_loss: 1.0389, train_acc: 0.6216 test_loss: 0.9775, test_acc: 0.6490, best: 0.6490, time: 0:00:55
 Epoch: 186, lr: 2.0e-03, train_loss: 1.0320, train_acc: 0.6264 test_loss: 0.9734, test_acc: 0.6491, best: 0.6491, time: 0:00:55
 Epoch: 187, lr: 2.0e-03, train_loss: 1.0207, train_acc: 0.6268 test_loss: 0.9738, test_acc: 0.6502, best: 0.6502, time: 0:00:55
 Epoch: 188, lr: 2.0e-03, train_loss: 1.0364, train_acc: 0.6238 test_loss: 0.9723, test_acc: 0.6511, best: 0.6511, time: 0:00:56
 Epoch: 189, lr: 2.0e-03, train_loss: 1.0247, train_acc: 0.6282 test_loss: 1.0079, test_acc: 0.6389, best: 0.6511, time: 0:00:55
 Epoch: 190, lr: 2.0e-03, train_loss: 1.0234, train_acc: 0.6246 test_loss: 0.9933, test_acc: 0.6421, best: 0.6511, time: 0:00:54
 Epoch: 191, lr: 2.0e-03, train_loss: 1.0154, train_acc: 0.6304 test_loss: 0.9844, test_acc: 0.6461, best: 0.6511, time: 0:00:54
 Epoch: 192, lr: 2.0e-03, train_loss: 1.0142, train_acc: 0.6292 test_loss: 0.9805, test_acc: 0.6502, best: 0.6511, time: 0:00:54
 Epoch: 193, lr: 2.0e-03, train_loss: 1.0021, train_acc: 0.6326 test_loss: 0.9608, test_acc: 0.6594, best: 0.6594, time: 0:00:54
 Epoch: 194, lr: 2.0e-03, train_loss: 0.9950, train_acc: 0.6404 test_loss: 0.9787, test_acc: 0.6512, best: 0.6594, time: 0:00:54
 Epoch: 195, lr: 2.0e-03, train_loss: 1.0136, train_acc: 0.6296 test_loss: 0.9813, test_acc: 0.6484, best: 0.6594, time: 0:00:54
 Epoch: 196, lr: 2.0e-03, train_loss: 1.0208, train_acc: 0.6322 test_loss: 0.9878, test_acc: 0.6436, best: 0.6594, time: 0:00:54
 Epoch: 197, lr: 2.0e-03, train_loss: 0.9878, train_acc: 0.6454 test_loss: 0.9943, test_acc: 0.6475, best: 0.6594, time: 0:00:54
 Epoch: 198, lr: 2.0e-03, train_loss: 0.9921, train_acc: 0.6366 test_loss: 0.9826, test_acc: 0.6489, best: 0.6594, time: 0:00:54
 Epoch: 199, lr: 2.0e-03, train_loss: 0.9838, train_acc: 0.6386 test_loss: 0.9756, test_acc: 0.6514, best: 0.6594, time: 0:00:54
 Epoch: 200, lr: 2.0e-03, train_loss: 0.9817, train_acc: 0.6398 test_loss: 0.9731, test_acc: 0.6505, best: 0.6594, time: 0:00:54
 Epoch: 201, lr: 2.0e-03, train_loss: 0.9914, train_acc: 0.6418 test_loss: 0.9796, test_acc: 0.6505, best: 0.6594, time: 0:00:55
 Epoch: 202, lr: 2.0e-03, train_loss: 0.9977, train_acc: 0.6356 test_loss: 0.9987, test_acc: 0.6409, best: 0.6594, time: 0:00:55
 Epoch: 203, lr: 2.0e-03, train_loss: 0.9898, train_acc: 0.6410 test_loss: 0.9684, test_acc: 0.6484, best: 0.6594, time: 0:00:55
 Epoch: 204, lr: 2.0e-03, train_loss: 0.9851, train_acc: 0.6404 test_loss: 0.9761, test_acc: 0.6486, best: 0.6594, time: 0:00:55
 Epoch: 205, lr: 2.0e-03, train_loss: 0.9872, train_acc: 0.6418 test_loss: 0.9671, test_acc: 0.6536, best: 0.6594, time: 0:00:55
 Epoch: 206, lr: 2.0e-03, train_loss: 0.9890, train_acc: 0.6384 test_loss: 0.9760, test_acc: 0.6492, best: 0.6594, time: 0:00:54
 Epoch: 207, lr: 2.0e-03, train_loss: 0.9638, train_acc: 0.6412 test_loss: 0.9932, test_acc: 0.6404, best: 0.6594, time: 0:00:54
 Epoch: 208, lr: 2.0e-03, train_loss: 0.9650, train_acc: 0.6410 test_loss: 0.9782, test_acc: 0.6479, best: 0.6594, time: 0:00:54
 Epoch: 209, lr: 2.0e-03, train_loss: 1.0011, train_acc: 0.6430 test_loss: 0.9627, test_acc: 0.6550, best: 0.6594, time: 0:00:54
 Epoch: 210, lr: 2.0e-03, train_loss: 0.9731, train_acc: 0.6530 test_loss: 0.9581, test_acc: 0.6558, best: 0.6594, time: 0:00:54
 Epoch: 211, lr: 2.0e-03, train_loss: 0.9793, train_acc: 0.6476 test_loss: 0.9654, test_acc: 0.6530, best: 0.6594, time: 0:00:55
 Epoch: 212, lr: 2.0e-03, train_loss: 0.9745, train_acc: 0.6416 test_loss: 0.9714, test_acc: 0.6492, best: 0.6594, time: 0:00:55
 Epoch: 213, lr: 2.0e-03, train_loss: 0.9825, train_acc: 0.6502 test_loss: 0.9735, test_acc: 0.6504, best: 0.6594, time: 0:00:54
 Epoch: 214, lr: 2.0e-03, train_loss: 0.9674, train_acc: 0.6518 test_loss: 0.9820, test_acc: 0.6434, best: 0.6594, time: 0:00:54
 Epoch: 215, lr: 2.0e-03, train_loss: 0.9888, train_acc: 0.6316 test_loss: 0.9831, test_acc: 0.6450, best: 0.6594, time: 0:00:55
 Epoch: 216, lr: 2.0e-03, train_loss: 0.9665, train_acc: 0.6376 test_loss: 1.0030, test_acc: 0.6389, best: 0.6594, time: 0:00:55
 Epoch: 217, lr: 2.0e-03, train_loss: 0.9764, train_acc: 0.6494 test_loss: 0.9753, test_acc: 0.6458, best: 0.6594, time: 0:00:55
 Epoch: 218, lr: 2.0e-03, train_loss: 0.9710, train_acc: 0.6444 test_loss: 0.9947, test_acc: 0.6422, best: 0.6594, time: 0:00:54
 Epoch: 219, lr: 2.0e-03, train_loss: 0.9578, train_acc: 0.6524 test_loss: 0.9738, test_acc: 0.6510, best: 0.6594, time: 0:00:54
 Epoch: 220, lr: 2.0e-03, train_loss: 0.9613, train_acc: 0.6584 test_loss: 0.9758, test_acc: 0.6499, best: 0.6594, time: 0:00:55
 Epoch: 221, lr: 2.0e-03, train_loss: 0.9745, train_acc: 0.6408 test_loss: 0.9707, test_acc: 0.6536, best: 0.6594, time: 0:00:55
 Epoch: 222, lr: 2.0e-03, train_loss: 0.9483, train_acc: 0.6546 test_loss: 0.9760, test_acc: 0.6494, best: 0.6594, time: 0:00:55
 Epoch: 223, lr: 2.0e-03, train_loss: 0.9507, train_acc: 0.6538 test_loss: 0.9794, test_acc: 0.6509, best: 0.6594, time: 0:00:54
 Epoch: 224, lr: 2.0e-03, train_loss: 0.9645, train_acc: 0.6500 test_loss: 0.9759, test_acc: 0.6545, best: 0.6594, time: 0:00:53
 Epoch: 225, lr: 2.0e-03, train_loss: 0.9736, train_acc: 0.6514 test_loss: 0.9900, test_acc: 0.6446, best: 0.6594, time: 0:00:53
 Epoch: 226, lr: 2.0e-03, train_loss: 0.9564, train_acc: 0.6572 test_loss: 0.9965, test_acc: 0.6449, best: 0.6594, time: 0:00:53
 Epoch: 227, lr: 2.0e-03, train_loss: 0.9725, train_acc: 0.6474 test_loss: 0.9593, test_acc: 0.6544, best: 0.6594, time: 0:00:54
 Epoch: 228, lr: 2.0e-03, train_loss: 0.9469, train_acc: 0.6584 test_loss: 0.9552, test_acc: 0.6600, best: 0.6600, time: 0:00:54
 Epoch: 229, lr: 2.0e-03, train_loss: 0.9536, train_acc: 0.6598 test_loss: 0.9590, test_acc: 0.6564, best: 0.6600, time: 0:00:54
 Epoch: 230, lr: 2.0e-03, train_loss: 0.9326, train_acc: 0.6618 test_loss: 0.9984, test_acc: 0.6402, best: 0.6600, time: 0:00:54
 Epoch: 231, lr: 2.0e-03, train_loss: 0.9484, train_acc: 0.6506 test_loss: 0.9646, test_acc: 0.6509, best: 0.6600, time: 0:00:54
 Epoch: 232, lr: 2.0e-03, train_loss: 0.9309, train_acc: 0.6628 test_loss: 0.9949, test_acc: 0.6509, best: 0.6600, time: 0:00:54
 Epoch: 233, lr: 2.0e-03, train_loss: 0.9241, train_acc: 0.6604 test_loss: 0.9881, test_acc: 0.6456, best: 0.6600, time: 0:00:54
 Epoch: 234, lr: 2.0e-03, train_loss: 0.9662, train_acc: 0.6470 test_loss: 0.9533, test_acc: 0.6586, best: 0.6600, time: 0:00:54
 Epoch: 235, lr: 2.0e-03, train_loss: 0.9361, train_acc: 0.6670 test_loss: 0.9922, test_acc: 0.6485, best: 0.6600, time: 0:00:54
 Epoch: 236, lr: 2.0e-03, train_loss: 0.9306, train_acc: 0.6666 test_loss: 0.9787, test_acc: 0.6482, best: 0.6600, time: 0:00:54
 Epoch: 237, lr: 2.0e-03, train_loss: 0.9307, train_acc: 0.6660 test_loss: 0.9844, test_acc: 0.6458, best: 0.6600, time: 0:00:54
 Epoch: 238, lr: 2.0e-03, train_loss: 0.9255, train_acc: 0.6688 test_loss: 0.9876, test_acc: 0.6410, best: 0.6600, time: 0:00:54
 Epoch: 239, lr: 2.0e-03, train_loss: 0.9415, train_acc: 0.6558 test_loss: 0.9709, test_acc: 0.6570, best: 0.6600, time: 0:00:54
 Epoch: 240, lr: 4.0e-04, train_loss: 0.9004, train_acc: 0.6712 test_loss: 0.9477, test_acc: 0.6594, best: 0.6600, time: 0:00:54
 Epoch: 241, lr: 4.0e-04, train_loss: 0.9063, train_acc: 0.6730 test_loss: 0.9539, test_acc: 0.6571, best: 0.6600, time: 0:00:54
 Epoch: 242, lr: 4.0e-04, train_loss: 0.8863, train_acc: 0.6794 test_loss: 0.9595, test_acc: 0.6573, best: 0.6600, time: 0:00:54
 Epoch: 243, lr: 4.0e-04, train_loss: 0.9310, train_acc: 0.6638 test_loss: 0.9710, test_acc: 0.6560, best: 0.6600, time: 0:00:54
 Epoch: 244, lr: 4.0e-04, train_loss: 0.8899, train_acc: 0.6796 test_loss: 0.9633, test_acc: 0.6555, best: 0.6600, time: 0:00:55
 Epoch: 245, lr: 4.0e-04, train_loss: 0.9053, train_acc: 0.6706 test_loss: 0.9475, test_acc: 0.6606, best: 0.6606, time: 0:00:55
 Epoch: 246, lr: 4.0e-04, train_loss: 0.9202, train_acc: 0.6724 test_loss: 0.9743, test_acc: 0.6551, best: 0.6606, time: 0:00:54
 Epoch: 247, lr: 4.0e-04, train_loss: 0.8879, train_acc: 0.6788 test_loss: 0.9640, test_acc: 0.6549, best: 0.6606, time: 0:00:54
 Epoch: 248, lr: 4.0e-04, train_loss: 0.8942, train_acc: 0.6796 test_loss: 0.9699, test_acc: 0.6550, best: 0.6606, time: 0:00:54
 Epoch: 249, lr: 4.0e-04, train_loss: 0.8929, train_acc: 0.6740 test_loss: 0.9554, test_acc: 0.6587, best: 0.6606, time: 0:00:54
 Epoch: 250, lr: 4.0e-04, train_loss: 0.8969, train_acc: 0.6712 test_loss: 0.9529, test_acc: 0.6617, best: 0.6617, time: 0:00:54
 Epoch: 251, lr: 4.0e-04, train_loss: 0.8967, train_acc: 0.6770 test_loss: 0.9515, test_acc: 0.6609, best: 0.6617, time: 0:00:54
 Epoch: 252, lr: 4.0e-04, train_loss: 0.9140, train_acc: 0.6670 test_loss: 0.9579, test_acc: 0.6566, best: 0.6617, time: 0:00:54
 Epoch: 253, lr: 4.0e-04, train_loss: 0.8977, train_acc: 0.6736 test_loss: 0.9546, test_acc: 0.6605, best: 0.6617, time: 0:00:54
 Epoch: 254, lr: 4.0e-04, train_loss: 0.8934, train_acc: 0.6746 test_loss: 0.9712, test_acc: 0.6534, best: 0.6617, time: 0:00:54
 Epoch: 255, lr: 4.0e-04, train_loss: 0.8973, train_acc: 0.6776 test_loss: 0.9657, test_acc: 0.6520, best: 0.6617, time: 0:00:54
 Epoch: 256, lr: 4.0e-04, train_loss: 0.8906, train_acc: 0.6760 test_loss: 0.9592, test_acc: 0.6583, best: 0.6617, time: 0:00:54
 Epoch: 257, lr: 4.0e-04, train_loss: 0.8783, train_acc: 0.6870 test_loss: 0.9550, test_acc: 0.6599, best: 0.6617, time: 0:00:54
 Epoch: 258, lr: 4.0e-04, train_loss: 0.8808, train_acc: 0.6828 test_loss: 0.9532, test_acc: 0.6599, best: 0.6617, time: 0:00:54
 Epoch: 259, lr: 4.0e-04, train_loss: 0.8943, train_acc: 0.6800 test_loss: 0.9689, test_acc: 0.6529, best: 0.6617, time: 0:00:54
 Epoch: 260, lr: 4.0e-04, train_loss: 0.8886, train_acc: 0.6818 test_loss: 0.9717, test_acc: 0.6490, best: 0.6617, time: 0:00:54
 Epoch: 261, lr: 4.0e-04, train_loss: 0.8987, train_acc: 0.6762 test_loss: 0.9546, test_acc: 0.6583, best: 0.6617, time: 0:00:54
 Epoch: 262, lr: 4.0e-04, train_loss: 0.8922, train_acc: 0.6768 test_loss: 0.9495, test_acc: 0.6626, best: 0.6626, time: 0:00:54
 Epoch: 263, lr: 4.0e-04, train_loss: 0.8936, train_acc: 0.6798 test_loss: 0.9684, test_acc: 0.6541, best: 0.6626, time: 0:00:54
 Epoch: 264, lr: 4.0e-04, train_loss: 0.8775, train_acc: 0.6860 test_loss: 0.9689, test_acc: 0.6539, best: 0.6626, time: 0:00:54
 Epoch: 265, lr: 4.0e-04, train_loss: 0.8856, train_acc: 0.6756 test_loss: 0.9595, test_acc: 0.6571, best: 0.6626, time: 0:00:54
 Epoch: 266, lr: 4.0e-04, train_loss: 0.8922, train_acc: 0.6840 test_loss: 0.9532, test_acc: 0.6613, best: 0.6626, time: 0:00:54
 Epoch: 267, lr: 4.0e-04, train_loss: 0.8675, train_acc: 0.6986 test_loss: 0.9636, test_acc: 0.6624, best: 0.6626, time: 0:00:54
 Epoch: 268, lr: 4.0e-04, train_loss: 0.8482, train_acc: 0.7002 test_loss: 0.9630, test_acc: 0.6610, best: 0.6626, time: 0:00:54
 Epoch: 269, lr: 4.0e-04, train_loss: 0.9007, train_acc: 0.6790 test_loss: 0.9527, test_acc: 0.6624, best: 0.6626, time: 0:00:54
 Epoch: 270, lr: 8.0e-05, train_loss: 0.8724, train_acc: 0.6832 test_loss: 0.9641, test_acc: 0.6576, best: 0.6626, time: 0:00:54
 Epoch: 271, lr: 8.0e-05, train_loss: 0.8754, train_acc: 0.6898 test_loss: 0.9564, test_acc: 0.6570, best: 0.6626, time: 0:00:54
 Epoch: 272, lr: 8.0e-05, train_loss: 0.8709, train_acc: 0.6818 test_loss: 0.9554, test_acc: 0.6587, best: 0.6626, time: 0:00:54
 Epoch: 273, lr: 8.0e-05, train_loss: 0.8684, train_acc: 0.6872 test_loss: 0.9656, test_acc: 0.6554, best: 0.6626, time: 0:00:54
 Epoch: 274, lr: 8.0e-05, train_loss: 0.8968, train_acc: 0.6790 test_loss: 0.9545, test_acc: 0.6620, best: 0.6626, time: 0:00:54
 Epoch: 275, lr: 8.0e-05, train_loss: 0.8736, train_acc: 0.6872 test_loss: 0.9457, test_acc: 0.6629, best: 0.6629, time: 0:00:54
 Epoch: 276, lr: 8.0e-05, train_loss: 0.8863, train_acc: 0.6842 test_loss: 0.9621, test_acc: 0.6606, best: 0.6629, time: 0:00:54
 Epoch: 277, lr: 8.0e-05, train_loss: 0.8839, train_acc: 0.6858 test_loss: 0.9651, test_acc: 0.6577, best: 0.6629, time: 0:00:54
 Epoch: 278, lr: 8.0e-05, train_loss: 0.9100, train_acc: 0.6662 test_loss: 0.9739, test_acc: 0.6610, best: 0.6629, time: 0:00:54
 Epoch: 279, lr: 8.0e-05, train_loss: 0.8907, train_acc: 0.6758 test_loss: 0.9621, test_acc: 0.6597, best: 0.6629, time: 0:00:54
 Epoch: 280, lr: 8.0e-05, train_loss: 0.8537, train_acc: 0.6846 test_loss: 0.9542, test_acc: 0.6606, best: 0.6629, time: 0:00:54
 Epoch: 281, lr: 8.0e-05, train_loss: 0.8836, train_acc: 0.6730 test_loss: 0.9513, test_acc: 0.6611, best: 0.6629, time: 0:00:54
 Epoch: 282, lr: 8.0e-05, train_loss: 0.8502, train_acc: 0.6960 test_loss: 0.9579, test_acc: 0.6596, best: 0.6629, time: 0:00:54
 Epoch: 283, lr: 8.0e-05, train_loss: 0.8657, train_acc: 0.6862 test_loss: 0.9598, test_acc: 0.6607, best: 0.6629, time: 0:00:53
 Epoch: 284, lr: 8.0e-05, train_loss: 0.8736, train_acc: 0.6880 test_loss: 0.9600, test_acc: 0.6630, best: 0.6630, time: 0:00:55
 Epoch: 285, lr: 8.0e-05, train_loss: 0.8777, train_acc: 0.6800 test_loss: 0.9457, test_acc: 0.6645, best: 0.6645, time: 0:00:55
 Epoch: 286, lr: 8.0e-05, train_loss: 0.8911, train_acc: 0.6788 test_loss: 0.9587, test_acc: 0.6603, best: 0.6645, time: 0:00:55
 Epoch: 287, lr: 8.0e-05, train_loss: 0.8841, train_acc: 0.6856 test_loss: 0.9524, test_acc: 0.6619, best: 0.6645, time: 0:00:54
 Epoch: 288, lr: 8.0e-05, train_loss: 0.8770, train_acc: 0.6802 test_loss: 0.9628, test_acc: 0.6571, best: 0.6645, time: 0:00:54
 Epoch: 289, lr: 8.0e-05, train_loss: 0.8693, train_acc: 0.6798 test_loss: 0.9559, test_acc: 0.6575, best: 0.6645, time: 0:00:54
 Epoch: 290, lr: 8.0e-05, train_loss: 0.8780, train_acc: 0.6804 test_loss: 0.9529, test_acc: 0.6621, best: 0.6645, time: 0:00:55
 Epoch: 291, lr: 8.0e-05, train_loss: 0.8670, train_acc: 0.6852 test_loss: 0.9582, test_acc: 0.6607, best: 0.6645, time: 0:00:54
 Epoch: 292, lr: 8.0e-05, train_loss: 0.8809, train_acc: 0.6792 test_loss: 0.9579, test_acc: 0.6605, best: 0.6645, time: 0:00:54
 Epoch: 293, lr: 8.0e-05, train_loss: 0.8759, train_acc: 0.6836 test_loss: 0.9559, test_acc: 0.6615, best: 0.6645, time: 0:00:54
 Epoch: 294, lr: 8.0e-05, train_loss: 0.8558, train_acc: 0.6892 test_loss: 0.9486, test_acc: 0.6626, best: 0.6645, time: 0:00:53
 Epoch: 295, lr: 8.0e-05, train_loss: 0.8671, train_acc: 0.6876 test_loss: 0.9484, test_acc: 0.6617, best: 0.6645, time: 0:00:52
 Epoch: 296, lr: 8.0e-05, train_loss: 0.8658, train_acc: 0.6878 test_loss: 0.9587, test_acc: 0.6609, best: 0.6645, time: 0:00:53
 Epoch: 297, lr: 8.0e-05, train_loss: 0.8659, train_acc: 0.6806 test_loss: 0.9463, test_acc: 0.6645, best: 0.6645, time: 0:00:54
 Epoch: 298, lr: 8.0e-05, train_loss: 0.8693, train_acc: 0.6900 test_loss: 0.9511, test_acc: 0.6634, best: 0.6645, time: 0:00:55
 Epoch: 299, lr: 8.0e-05, train_loss: 0.8698, train_acc: 0.6778 test_loss: 0.9519, test_acc: 0.6666, best: 0.6666, time: 0:00:55
 Highest accuracy: 0.6666