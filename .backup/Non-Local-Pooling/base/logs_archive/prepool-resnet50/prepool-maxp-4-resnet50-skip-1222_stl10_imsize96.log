
 Run on time: 2022-07-01 13:44:26.565826

 Architecture: prepool-maxp-4-resnet50-skip-1222

 Pool Config: {
    "arch": "resnet50",
    "conv1": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "skip",
            "_stride": 1,
            "_psize": null,
            "_dim_reduced_ratio": null,
            "_num_heads": null,
            "_conv2d": null,
            "_win_norm": null
        }
    },
    "pool": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "maxp",
            "_stride": 4,
            "_psize": null,
            "_dim_reduced_ratio": null,
            "_num_heads": null,
            "_conv2d": null,
            "_win_norm": null
        }
    },
    "layer1": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "skip",
            "_stride": 1,
            "_psize": null,
            "_dim_reduced_ratio": null,
            "_num_heads": null,
            "_conv2d": null,
            "_win_norm": null
        }
    },
    "layer2": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "skip",
            "_stride": 2,
            "_psize": null,
            "_dim_reduced_ratio": null,
            "_num_heads": null,
            "_conv2d": null,
            "_win_norm": null
        }
    },
    "layer3": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "skip",
            "_stride": 2,
            "_psize": null,
            "_dim_reduced_ratio": null,
            "_num_heads": null,
            "_conv2d": null,
            "_win_norm": null
        }
    },
    "layer4": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "skip",
            "_stride": 2,
            "_psize": null,
            "_dim_reduced_ratio": null,
            "_num_heads": null,
            "_conv2d": null,
            "_win_norm": null
        }
    }
}

 Arguments:
	 gpu                  : True
	 log                  : True
	 seed                 : 0
	 momentum             : 0.9
	 amsgrad              : True
	 dataset              : STL10
	 batch_size           : 8
	 architecture         : PREPOOL-MAXP-4-RESNET50-SKIP-1222
	 im_size              : None
	 relu_threshold       : 4.0
	 learning_rate        : 0.01
	 pretrained_backbone  : 
	 pretrained_ann       : 
	 weight_decay         : 0.0
	 test_only            : False
	 epochs               : 300
	 lr_interval          : [180, 240, 270]
	 lr_reduce            : 5
	 optimizer            : SGD
	 dropout              : 0.2
	 kernel_size          : 3
	 dont_save            : False
	 visualize            : False
	 devices              : 0
 DataParallel(
  (module): Network(
    (net): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (pool): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer4): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
      (fc): Linear(in_features=2048, out_features=10, bias=True)
    )
  )
)
 SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0
)
 Epoch: 1, lr: 1.0e-02, train_loss: 3.3461, train_acc: 0.1468 test_loss: 3.6965, test_acc: 0.1924, best: 0.1924, time: 0:01:52
 Epoch: 2, lr: 1.0e-02, train_loss: 2.1198, train_acc: 0.1984 test_loss: 3.2044, test_acc: 0.2300, best: 0.2300, time: 0:01:50
 Epoch: 3, lr: 1.0e-02, train_loss: 1.9822, train_acc: 0.2366 test_loss: 3.2812, test_acc: 0.2964, best: 0.2964, time: 0:01:51
 Epoch: 4, lr: 1.0e-02, train_loss: 1.9175, train_acc: 0.2592 test_loss: 2.5658, test_acc: 0.3355, best: 0.3355, time: 0:01:52
 Epoch: 5, lr: 1.0e-02, train_loss: 1.8670, train_acc: 0.2896 test_loss: 2.3158, test_acc: 0.3354, best: 0.3355, time: 0:01:48
 Epoch: 6, lr: 1.0e-02, train_loss: 1.8660, train_acc: 0.2974 test_loss: 2.2112, test_acc: 0.3610, best: 0.3610, time: 0:01:51
 Epoch: 7, lr: 1.0e-02, train_loss: 1.8196, train_acc: 0.3086 test_loss: 3.4240, test_acc: 0.3311, best: 0.3610, time: 0:01:50
 Epoch: 8, lr: 1.0e-02, train_loss: 1.8279, train_acc: 0.3088 test_loss: 1.5978, test_acc: 0.3967, best: 0.3967, time: 0:01:51
 Epoch: 9, lr: 1.0e-02, train_loss: 1.8245, train_acc: 0.3078 test_loss: 7.5736, test_acc: 0.3324, best: 0.3967, time: 0:01:50
 Epoch: 10, lr: 1.0e-02, train_loss: 1.7956, train_acc: 0.3188 test_loss: 1.7060, test_acc: 0.4022, best: 0.4022, time: 0:01:49
 Epoch: 11, lr: 1.0e-02, train_loss: 1.7383, train_acc: 0.3386 test_loss: 1.5750, test_acc: 0.4218, best: 0.4218, time: 0:01:46
 Epoch: 12, lr: 1.0e-02, train_loss: 1.7030, train_acc: 0.3526 test_loss: 1.7052, test_acc: 0.4020, best: 0.4218, time: 0:01:51
 Epoch: 13, lr: 1.0e-02, train_loss: 1.6982, train_acc: 0.3542 test_loss: 1.8297, test_acc: 0.3945, best: 0.4218, time: 0:01:51
 Epoch: 14, lr: 1.0e-02, train_loss: 1.6618, train_acc: 0.3918 test_loss: 2.0735, test_acc: 0.4000, best: 0.4218, time: 0:01:49
 Epoch: 15, lr: 1.0e-02, train_loss: 1.6445, train_acc: 0.3930 test_loss: 2.1385, test_acc: 0.4239, best: 0.4239, time: 0:01:51
 Epoch: 16, lr: 1.0e-02, train_loss: 1.6219, train_acc: 0.3956 test_loss: 2.3554, test_acc: 0.4336, best: 0.4336, time: 0:01:51
 Epoch: 17, lr: 1.0e-02, train_loss: 1.5917, train_acc: 0.4112 test_loss: 2.2921, test_acc: 0.4126, best: 0.4336, time: 0:01:50
 Epoch: 18, lr: 1.0e-02, train_loss: 1.5581, train_acc: 0.4248 test_loss: 3.3839, test_acc: 0.4396, best: 0.4396, time: 0:01:51
 Epoch: 19, lr: 1.0e-02, train_loss: 1.5742, train_acc: 0.4058 test_loss: 1.9211, test_acc: 0.4440, best: 0.4440, time: 0:01:51
 Epoch: 20, lr: 1.0e-02, train_loss: 1.5320, train_acc: 0.4380 test_loss: 1.7172, test_acc: 0.4636, best: 0.4636, time: 0:01:51
 Epoch: 21, lr: 1.0e-02, train_loss: 1.5676, train_acc: 0.4226 test_loss: 1.5654, test_acc: 0.4509, best: 0.4636, time: 0:01:49
 Epoch: 22, lr: 1.0e-02, train_loss: 1.5110, train_acc: 0.4398 test_loss: 2.7433, test_acc: 0.4677, best: 0.4677, time: 0:01:49
 Epoch: 23, lr: 1.0e-02, train_loss: 1.5089, train_acc: 0.4416 test_loss: 1.3965, test_acc: 0.4888, best: 0.4888, time: 0:01:51
 Epoch: 24, lr: 1.0e-02, train_loss: 1.4839, train_acc: 0.4554 test_loss: 2.0985, test_acc: 0.4515, best: 0.4888, time: 0:01:50
 Epoch: 25, lr: 1.0e-02, train_loss: 1.4648, train_acc: 0.4668 test_loss: 2.9821, test_acc: 0.4718, best: 0.4888, time: 0:01:49
 Epoch: 26, lr: 1.0e-02, train_loss: 1.4368, train_acc: 0.4768 test_loss: 1.4294, test_acc: 0.5175, best: 0.5175, time: 0:01:50
 Epoch: 27, lr: 1.0e-02, train_loss: 1.4222, train_acc: 0.4796 test_loss: 2.6288, test_acc: 0.4766, best: 0.5175, time: 0:01:50
 Epoch: 28, lr: 1.0e-02, train_loss: 1.4236, train_acc: 0.4754 test_loss: 2.4094, test_acc: 0.5060, best: 0.5175, time: 0:01:51
 Epoch: 29, lr: 1.0e-02, train_loss: 1.4229, train_acc: 0.4812 test_loss: 2.1045, test_acc: 0.5252, best: 0.5252, time: 0:01:51
 Epoch: 30, lr: 1.0e-02, train_loss: 1.3922, train_acc: 0.4922 test_loss: 1.7008, test_acc: 0.5029, best: 0.5252, time: 0:01:50
 Epoch: 31, lr: 1.0e-02, train_loss: 1.3739, train_acc: 0.4986 test_loss: 1.4689, test_acc: 0.5361, best: 0.5361, time: 0:01:51
 Epoch: 32, lr: 1.0e-02, train_loss: 1.3338, train_acc: 0.5068 test_loss: 5.0717, test_acc: 0.5295, best: 0.5361, time: 0:01:50
 Epoch: 33, lr: 1.0e-02, train_loss: 1.3237, train_acc: 0.5172 test_loss: 1.4399, test_acc: 0.5535, best: 0.5535, time: 0:01:49
 Epoch: 34, lr: 1.0e-02, train_loss: 1.3180, train_acc: 0.5180 test_loss: 1.7815, test_acc: 0.5409, best: 0.5535, time: 0:01:50
 Epoch: 35, lr: 1.0e-02, train_loss: 1.3757, train_acc: 0.5058 test_loss: 1.7870, test_acc: 0.5429, best: 0.5535, time: 0:01:51
 Epoch: 36, lr: 1.0e-02, train_loss: 1.2919, train_acc: 0.5300 test_loss: 2.4793, test_acc: 0.5469, best: 0.5535, time: 0:01:49
 Epoch: 37, lr: 1.0e-02, train_loss: 1.2906, train_acc: 0.5224 test_loss: 2.9808, test_acc: 0.5190, best: 0.5535, time: 0:01:49
 Epoch: 38, lr: 1.0e-02, train_loss: 1.2563, train_acc: 0.5384 test_loss: 1.7865, test_acc: 0.5296, best: 0.5535, time: 0:01:51
 Epoch: 39, lr: 1.0e-02, train_loss: 1.2479, train_acc: 0.5418 test_loss: 1.6383, test_acc: 0.5674, best: 0.5674, time: 0:01:52
 Epoch: 40, lr: 1.0e-02, train_loss: 1.2278, train_acc: 0.5530 test_loss: 2.2484, test_acc: 0.5476, best: 0.5674, time: 0:01:52
 Epoch: 41, lr: 1.0e-02, train_loss: 1.2204, train_acc: 0.5612 test_loss: 2.0392, test_acc: 0.5674, best: 0.5674, time: 0:01:50
 Epoch: 42, lr: 1.0e-02, train_loss: 1.2054, train_acc: 0.5564 test_loss: 1.8457, test_acc: 0.5695, best: 0.5695, time: 0:01:51
 Epoch: 43, lr: 1.0e-02, train_loss: 1.1845, train_acc: 0.5736 test_loss: 1.6505, test_acc: 0.5992, best: 0.5992, time: 0:01:51
 Epoch: 44, lr: 1.0e-02, train_loss: 1.1779, train_acc: 0.5642 test_loss: 3.0231, test_acc: 0.5966, best: 0.5992, time: 0:01:45
 Epoch: 45, lr: 1.0e-02, train_loss: 1.1502, train_acc: 0.5774 test_loss: 2.1121, test_acc: 0.5939, best: 0.5992, time: 0:01:50
 Epoch: 46, lr: 1.0e-02, train_loss: 1.1617, train_acc: 0.5776 test_loss: 2.2320, test_acc: 0.5820, best: 0.5992, time: 0:01:52
 Epoch: 47, lr: 1.0e-02, train_loss: 1.1367, train_acc: 0.5900 test_loss: 1.2079, test_acc: 0.6044, best: 0.6044, time: 0:01:51
 Epoch: 48, lr: 1.0e-02, train_loss: 1.1378, train_acc: 0.5834 test_loss: 1.2797, test_acc: 0.5984, best: 0.6044, time: 0:01:49
 Epoch: 49, lr: 1.0e-02, train_loss: 1.1259, train_acc: 0.5894 test_loss: 2.3841, test_acc: 0.5694, best: 0.6044, time: 0:01:50
 Epoch: 50, lr: 1.0e-02, train_loss: 1.1167, train_acc: 0.6004 test_loss: 2.1092, test_acc: 0.6016, best: 0.6044, time: 0:01:51
 Epoch: 51, lr: 1.0e-02, train_loss: 1.1055, train_acc: 0.6090 test_loss: 1.5348, test_acc: 0.6278, best: 0.6278, time: 0:01:52
 Epoch: 52, lr: 1.0e-02, train_loss: 1.1393, train_acc: 0.5880 test_loss: 1.3942, test_acc: 0.5935, best: 0.6278, time: 0:01:50
 Epoch: 53, lr: 1.0e-02, train_loss: 1.1231, train_acc: 0.5932 test_loss: 1.4865, test_acc: 0.5716, best: 0.6278, time: 0:01:50
 Epoch: 54, lr: 1.0e-02, train_loss: 1.1180, train_acc: 0.5968 test_loss: 1.1069, test_acc: 0.6159, best: 0.6278, time: 0:01:51
 Epoch: 55, lr: 1.0e-02, train_loss: 1.1256, train_acc: 0.5896 test_loss: 1.2410, test_acc: 0.6058, best: 0.6278, time: 0:01:50
 Epoch: 56, lr: 1.0e-02, train_loss: 1.1081, train_acc: 0.5956 test_loss: 1.3441, test_acc: 0.5765, best: 0.6278, time: 0:01:50
 Epoch: 57, lr: 1.0e-02, train_loss: 1.0684, train_acc: 0.6142 test_loss: 1.1648, test_acc: 0.5979, best: 0.6278, time: 0:01:49
 Epoch: 58, lr: 1.0e-02, train_loss: 1.0420, train_acc: 0.6242 test_loss: 1.1082, test_acc: 0.6315, best: 0.6315, time: 0:01:51
 Epoch: 59, lr: 1.0e-02, train_loss: 1.0290, train_acc: 0.6304 test_loss: 1.2411, test_acc: 0.6225, best: 0.6315, time: 0:01:50
 Epoch: 60, lr: 1.0e-02, train_loss: 1.0395, train_acc: 0.6238 test_loss: 1.4466, test_acc: 0.6095, best: 0.6315, time: 0:01:48
 Epoch: 61, lr: 1.0e-02, train_loss: 1.0144, train_acc: 0.6320 test_loss: 1.3116, test_acc: 0.6369, best: 0.6369, time: 0:01:52
 Epoch: 62, lr: 1.0e-02, train_loss: 0.9816, train_acc: 0.6444 test_loss: 5.0229, test_acc: 0.5981, best: 0.6369, time: 0:01:51
 Epoch: 63, lr: 1.0e-02, train_loss: 0.9949, train_acc: 0.6424 test_loss: 1.0913, test_acc: 0.6535, best: 0.6535, time: 0:01:51
 Epoch: 64, lr: 1.0e-02, train_loss: 0.9806, train_acc: 0.6500 test_loss: 1.0865, test_acc: 0.6599, best: 0.6599, time: 0:01:50
 Epoch: 65, lr: 1.0e-02, train_loss: 0.9637, train_acc: 0.6514 test_loss: 1.5726, test_acc: 0.6130, best: 0.6599, time: 0:01:51
 Epoch: 66, lr: 1.0e-02, train_loss: 0.9595, train_acc: 0.6542 test_loss: 1.1708, test_acc: 0.6284, best: 0.6599, time: 0:01:49
 Epoch: 67, lr: 1.0e-02, train_loss: 0.9686, train_acc: 0.6564 test_loss: 3.0748, test_acc: 0.5819, best: 0.6599, time: 0:01:49
 Epoch: 68, lr: 1.0e-02, train_loss: 0.9515, train_acc: 0.6568 test_loss: 1.9577, test_acc: 0.6286, best: 0.6599, time: 0:01:50
 Epoch: 69, lr: 1.0e-02, train_loss: 0.9698, train_acc: 0.6530 test_loss: 2.0428, test_acc: 0.6440, best: 0.6599, time: 0:01:49
 Epoch: 70, lr: 1.0e-02, train_loss: 0.9559, train_acc: 0.6502 test_loss: 4.1135, test_acc: 0.5880, best: 0.6599, time: 0:01:50
 Epoch: 71, lr: 1.0e-02, train_loss: 0.9150, train_acc: 0.6764 test_loss: 4.3765, test_acc: 0.6026, best: 0.6599, time: 0:01:50
 Epoch: 72, lr: 1.0e-02, train_loss: 0.9303, train_acc: 0.6640 test_loss: 6.5318, test_acc: 0.6058, best: 0.6599, time: 0:01:49
 Epoch: 73, lr: 1.0e-02, train_loss: 0.9355, train_acc: 0.6692 test_loss: 1.2418, test_acc: 0.6504, best: 0.6599, time: 0:01:50
 Epoch: 74, lr: 1.0e-02, train_loss: 0.9244, train_acc: 0.6728 test_loss: 1.1358, test_acc: 0.6562, best: 0.6599, time: 0:01:50
 Epoch: 75, lr: 1.0e-02, train_loss: 0.9041, train_acc: 0.6700 test_loss: 3.4030, test_acc: 0.6082, best: 0.6599, time: 0:01:48
 Epoch: 76, lr: 1.0e-02, train_loss: 0.9114, train_acc: 0.6672 test_loss: 2.2103, test_acc: 0.6055, best: 0.6599, time: 0:01:50
 Epoch: 77, lr: 1.0e-02, train_loss: 0.9116, train_acc: 0.6660 test_loss: 1.9758, test_acc: 0.6238, best: 0.6599, time: 0:01:50
 Epoch: 78, lr: 1.0e-02, train_loss: 0.9448, train_acc: 0.6610 test_loss: 1.7214, test_acc: 0.6392, best: 0.6599, time: 0:01:51
 Epoch: 79, lr: 1.0e-02, train_loss: 0.9035, train_acc: 0.6720 test_loss: 1.1989, test_acc: 0.6521, best: 0.6599, time: 0:01:50
 Epoch: 80, lr: 1.0e-02, train_loss: 0.8777, train_acc: 0.6826 test_loss: 1.1875, test_acc: 0.6254, best: 0.6599, time: 0:01:51
 Epoch: 81, lr: 1.0e-02, train_loss: 0.8673, train_acc: 0.6898 test_loss: 4.9112, test_acc: 0.6108, best: 0.6599, time: 0:01:49
 Epoch: 82, lr: 1.0e-02, train_loss: 0.8679, train_acc: 0.6878 test_loss: 1.8999, test_acc: 0.6196, best: 0.6599, time: 0:01:49
 Epoch: 83, lr: 1.0e-02, train_loss: 0.8477, train_acc: 0.6942 test_loss: 2.8487, test_acc: 0.6146, best: 0.6599, time: 0:01:50
 Epoch: 84, lr: 1.0e-02, train_loss: 0.8606, train_acc: 0.7038 test_loss: 1.6800, test_acc: 0.6462, best: 0.6599, time: 0:01:50
 Epoch: 85, lr: 1.0e-02, train_loss: 0.8513, train_acc: 0.6956 test_loss: 6.1569, test_acc: 0.6364, best: 0.6599, time: 0:01:50
 Epoch: 86, lr: 1.0e-02, train_loss: 0.8210, train_acc: 0.6994 test_loss: 1.3328, test_acc: 0.6770, best: 0.6770, time: 0:01:51
 Epoch: 87, lr: 1.0e-02, train_loss: 0.8239, train_acc: 0.7028 test_loss: 2.8140, test_acc: 0.6509, best: 0.6770, time: 0:01:52
 Epoch: 88, lr: 1.0e-02, train_loss: 0.8436, train_acc: 0.7042 test_loss: 1.1277, test_acc: 0.6539, best: 0.6770, time: 0:01:50
 Epoch: 89, lr: 1.0e-02, train_loss: 0.8435, train_acc: 0.6988 test_loss: 1.5171, test_acc: 0.6534, best: 0.6770, time: 0:01:53
 Epoch: 90, lr: 1.0e-02, train_loss: 0.8393, train_acc: 0.7052 test_loss: 1.2266, test_acc: 0.6549, best: 0.6770, time: 0:01:51
 Epoch: 91, lr: 1.0e-02, train_loss: 0.8473, train_acc: 0.7026 test_loss: 1.4513, test_acc: 0.6835, best: 0.6835, time: 0:01:52
 Epoch: 92, lr: 1.0e-02, train_loss: 0.8721, train_acc: 0.6876 test_loss: 1.0365, test_acc: 0.6699, best: 0.6835, time: 0:01:51
 Epoch: 93, lr: 1.0e-02, train_loss: 0.8898, train_acc: 0.6812 test_loss: 1.1189, test_acc: 0.6430, best: 0.6835, time: 0:01:49
 Epoch: 94, lr: 1.0e-02, train_loss: 0.8471, train_acc: 0.6982 test_loss: 0.9705, test_acc: 0.6814, best: 0.6835, time: 0:01:51
 Epoch: 95, lr: 1.0e-02, train_loss: 0.8157, train_acc: 0.7046 test_loss: 1.3928, test_acc: 0.6784, best: 0.6835, time: 0:01:48
 Epoch: 96, lr: 1.0e-02, train_loss: 0.7616, train_acc: 0.7276 test_loss: 2.7314, test_acc: 0.6525, best: 0.6835, time: 0:01:48
 Epoch: 97, lr: 1.0e-02, train_loss: 0.7924, train_acc: 0.7218 test_loss: 5.3369, test_acc: 0.5729, best: 0.6835, time: 0:01:51
 Epoch: 98, lr: 1.0e-02, train_loss: 0.7912, train_acc: 0.7162 test_loss: 1.8070, test_acc: 0.6376, best: 0.6835, time: 0:01:51
 Epoch: 99, lr: 1.0e-02, train_loss: 0.8153, train_acc: 0.7034 test_loss: 1.5093, test_acc: 0.6599, best: 0.6835, time: 0:01:52
 Epoch: 100, lr: 1.0e-02, train_loss: 0.8373, train_acc: 0.7048 test_loss: 1.2590, test_acc: 0.6633, best: 0.6835, time: 0:01:51
 Epoch: 101, lr: 1.0e-02, train_loss: 0.8210, train_acc: 0.7088 test_loss: 3.3908, test_acc: 0.6428, best: 0.6835, time: 0:01:50
 Epoch: 102, lr: 1.0e-02, train_loss: 0.8103, train_acc: 0.7114 test_loss: 1.0072, test_acc: 0.6964, best: 0.6964, time: 0:01:52
 Epoch: 103, lr: 1.0e-02, train_loss: 0.7765, train_acc: 0.7208 test_loss: 1.0992, test_acc: 0.6524, best: 0.6964, time: 0:01:50
 Epoch: 104, lr: 1.0e-02, train_loss: 0.7430, train_acc: 0.7310 test_loss: 1.5990, test_acc: 0.6579, best: 0.6964, time: 0:01:51
 Epoch: 105, lr: 1.0e-02, train_loss: 0.7430, train_acc: 0.7350 test_loss: 1.1765, test_acc: 0.6819, best: 0.6964, time: 0:01:51
 Epoch: 106, lr: 1.0e-02, train_loss: 0.7522, train_acc: 0.7296 test_loss: 1.6027, test_acc: 0.6703, best: 0.6964, time: 0:01:50
 Epoch: 107, lr: 1.0e-02, train_loss: 0.7539, train_acc: 0.7298 test_loss: 1.0842, test_acc: 0.6893, best: 0.6964, time: 0:01:50
 Epoch: 108, lr: 1.0e-02, train_loss: 0.7391, train_acc: 0.7358 test_loss: 2.2667, test_acc: 0.6366, best: 0.6964, time: 0:01:49
 Epoch: 109, lr: 1.0e-02, train_loss: 0.7565, train_acc: 0.7354 test_loss: 1.7273, test_acc: 0.6558, best: 0.6964, time: 0:01:47
 Epoch: 110, lr: 1.0e-02, train_loss: 0.7225, train_acc: 0.7462 test_loss: 4.0197, test_acc: 0.5976, best: 0.6964, time: 0:01:49
 Epoch: 111, lr: 1.0e-02, train_loss: 0.7334, train_acc: 0.7386 test_loss: 2.2035, test_acc: 0.6185, best: 0.6964, time: 0:01:51
 Epoch: 112, lr: 1.0e-02, train_loss: 0.7075, train_acc: 0.7498 test_loss: 2.6105, test_acc: 0.6070, best: 0.6964, time: 0:01:50
 Epoch: 113, lr: 1.0e-02, train_loss: 0.6964, train_acc: 0.7556 test_loss: 2.3217, test_acc: 0.6321, best: 0.6964, time: 0:01:50
 Epoch: 114, lr: 1.0e-02, train_loss: 0.7166, train_acc: 0.7462 test_loss: 1.5572, test_acc: 0.6715, best: 0.6964, time: 0:01:52
 Epoch: 115, lr: 1.0e-02, train_loss: 0.7417, train_acc: 0.7378 test_loss: 4.6538, test_acc: 0.6441, best: 0.6964, time: 0:01:51
 Epoch: 116, lr: 1.0e-02, train_loss: 0.7343, train_acc: 0.7410 test_loss: 2.5381, test_acc: 0.6352, best: 0.6964, time: 0:01:51
 Epoch: 117, lr: 1.0e-02, train_loss: 0.6966, train_acc: 0.7486 test_loss: 2.0764, test_acc: 0.6723, best: 0.6964, time: 0:01:51
 Epoch: 118, lr: 1.0e-02, train_loss: 0.7670, train_acc: 0.7338 test_loss: 1.0247, test_acc: 0.6817, best: 0.6964, time: 0:01:51
 Epoch: 119, lr: 1.0e-02, train_loss: 0.7056, train_acc: 0.7502 test_loss: 1.2825, test_acc: 0.6587, best: 0.6964, time: 0:01:49
 Epoch: 120, lr: 1.0e-02, train_loss: 0.6964, train_acc: 0.7486 test_loss: 1.6434, test_acc: 0.6671, best: 0.6964, time: 0:01:51
 Epoch: 121, lr: 1.0e-02, train_loss: 0.6925, train_acc: 0.7612 test_loss: 3.8988, test_acc: 0.6058, best: 0.6964, time: 0:01:50
 Epoch: 122, lr: 1.0e-02, train_loss: 0.6920, train_acc: 0.7594 test_loss: 1.7528, test_acc: 0.6461, best: 0.6964, time: 0:01:51
 Epoch: 123, lr: 1.0e-02, train_loss: 0.6965, train_acc: 0.7466 test_loss: 2.8360, test_acc: 0.6651, best: 0.6964, time: 0:01:50
 Epoch: 124, lr: 1.0e-02, train_loss: 0.7103, train_acc: 0.7468 test_loss: 2.6500, test_acc: 0.6691, best: 0.6964, time: 0:01:48
 Epoch: 125, lr: 1.0e-02, train_loss: 0.6664, train_acc: 0.7694 test_loss: 1.3783, test_acc: 0.6663, best: 0.6964, time: 0:01:43
 Epoch: 126, lr: 1.0e-02, train_loss: 0.6594, train_acc: 0.7660 test_loss: 3.5246, test_acc: 0.6597, best: 0.6964, time: 0:01:51
 Epoch: 127, lr: 1.0e-02, train_loss: 0.6753, train_acc: 0.7670 test_loss: 1.1711, test_acc: 0.6837, best: 0.6964, time: 0:01:48
 Epoch: 128, lr: 1.0e-02, train_loss: 0.6650, train_acc: 0.7680 test_loss: 1.6706, test_acc: 0.6686, best: 0.6964, time: 0:01:42
 Epoch: 129, lr: 1.0e-02, train_loss: 0.6629, train_acc: 0.7660 test_loss: 1.4935, test_acc: 0.6587, best: 0.6964, time: 0:01:52
 Epoch: 130, lr: 1.0e-02, train_loss: 0.6449, train_acc: 0.7724 test_loss: 1.3176, test_acc: 0.6959, best: 0.6964, time: 0:01:51
 Epoch: 131, lr: 1.0e-02, train_loss: 0.6383, train_acc: 0.7780 test_loss: 1.1614, test_acc: 0.6853, best: 0.6964, time: 0:01:48
 Epoch: 132, lr: 1.0e-02, train_loss: 0.6433, train_acc: 0.7750 test_loss: 2.9283, test_acc: 0.6385, best: 0.6964, time: 0:01:51
 Epoch: 133, lr: 1.0e-02, train_loss: 0.6441, train_acc: 0.7718 test_loss: 1.1475, test_acc: 0.6925, best: 0.6964, time: 0:01:49
 Epoch: 134, lr: 1.0e-02, train_loss: 0.6197, train_acc: 0.7826 test_loss: 3.3694, test_acc: 0.6498, best: 0.6964, time: 0:01:51
 Epoch: 135, lr: 1.0e-02, train_loss: 0.6365, train_acc: 0.7732 test_loss: 1.8054, test_acc: 0.6703, best: 0.6964, time: 0:01:51
 Epoch: 136, lr: 1.0e-02, train_loss: 0.6489, train_acc: 0.7722 test_loss: 1.7805, test_acc: 0.6847, best: 0.6964, time: 0:01:51
 Epoch: 137, lr: 1.0e-02, train_loss: 0.6035, train_acc: 0.7902 test_loss: 1.3191, test_acc: 0.6891, best: 0.6964, time: 0:01:50
 Epoch: 138, lr: 1.0e-02, train_loss: 0.6068, train_acc: 0.7810 test_loss: 1.7733, test_acc: 0.6965, best: 0.6965, time: 0:01:51
 Epoch: 139, lr: 1.0e-02, train_loss: 0.5987, train_acc: 0.7862 test_loss: 1.5510, test_acc: 0.6697, best: 0.6965, time: 0:01:51
 Epoch: 140, lr: 1.0e-02, train_loss: 0.6204, train_acc: 0.7828 test_loss: 1.1419, test_acc: 0.6897, best: 0.6965, time: 0:01:51
 Epoch: 141, lr: 1.0e-02, train_loss: 0.6032, train_acc: 0.7868 test_loss: 4.0601, test_acc: 0.6589, best: 0.6965, time: 0:01:51
 Epoch: 142, lr: 1.0e-02, train_loss: 0.6057, train_acc: 0.7890 test_loss: 2.4487, test_acc: 0.6794, best: 0.6965, time: 0:01:51
 Epoch: 143, lr: 1.0e-02, train_loss: 0.5859, train_acc: 0.7890 test_loss: 2.3225, test_acc: 0.6786, best: 0.6965, time: 0:01:51
 Epoch: 144, lr: 1.0e-02, train_loss: 0.6259, train_acc: 0.7830 test_loss: 3.6820, test_acc: 0.6710, best: 0.6965, time: 0:01:52
 Epoch: 145, lr: 1.0e-02, train_loss: 0.5826, train_acc: 0.7922 test_loss: 3.6219, test_acc: 0.6739, best: 0.6965, time: 0:01:52
 Epoch: 146, lr: 1.0e-02, train_loss: 0.5766, train_acc: 0.7952 test_loss: 7.0628, test_acc: 0.6719, best: 0.6965, time: 0:01:50
 Epoch: 147, lr: 1.0e-02, train_loss: 0.5886, train_acc: 0.7934 test_loss: 1.1446, test_acc: 0.7051, best: 0.7051, time: 0:01:52
 Epoch: 148, lr: 1.0e-02, train_loss: 0.5856, train_acc: 0.8002 test_loss: 1.0818, test_acc: 0.7057, best: 0.7057, time: 0:01:52
 Epoch: 149, lr: 1.0e-02, train_loss: 0.5703, train_acc: 0.8002 test_loss: 1.0268, test_acc: 0.7047, best: 0.7057, time: 0:01:51
 Epoch: 150, lr: 1.0e-02, train_loss: 0.5696, train_acc: 0.7980 test_loss: 1.8590, test_acc: 0.6856, best: 0.7057, time: 0:01:50
 Epoch: 151, lr: 1.0e-02, train_loss: 0.5651, train_acc: 0.7986 test_loss: 1.2162, test_acc: 0.6825, best: 0.7057, time: 0:01:50
 Epoch: 152, lr: 1.0e-02, train_loss: 0.5548, train_acc: 0.8090 test_loss: 1.0313, test_acc: 0.7050, best: 0.7057, time: 0:01:51
 Epoch: 153, lr: 1.0e-02, train_loss: 0.5560, train_acc: 0.8030 test_loss: 2.3512, test_acc: 0.6855, best: 0.7057, time: 0:01:50
 Epoch: 154, lr: 1.0e-02, train_loss: 0.5522, train_acc: 0.8052 test_loss: 1.5150, test_acc: 0.6705, best: 0.7057, time: 0:01:50
 Epoch: 155, lr: 1.0e-02, train_loss: 0.5676, train_acc: 0.8032 test_loss: 2.3045, test_acc: 0.6781, best: 0.7057, time: 0:01:51
 Epoch: 156, lr: 1.0e-02, train_loss: 0.5690, train_acc: 0.8016 test_loss: 1.4622, test_acc: 0.7200, best: 0.7200, time: 0:01:51
 Epoch: 157, lr: 1.0e-02, train_loss: 0.5690, train_acc: 0.8008 test_loss: 1.4879, test_acc: 0.7013, best: 0.7200, time: 0:01:50
 Epoch: 158, lr: 1.0e-02, train_loss: 0.5407, train_acc: 0.8102 test_loss: 4.9730, test_acc: 0.6665, best: 0.7200, time: 0:01:50
 Epoch: 159, lr: 1.0e-02, train_loss: 0.5618, train_acc: 0.8106 test_loss: 3.2867, test_acc: 0.6764, best: 0.7200, time: 0:01:51
 Epoch: 160, lr: 1.0e-02, train_loss: 0.5513, train_acc: 0.8136 test_loss: 4.1281, test_acc: 0.6751, best: 0.7200, time: 0:01:51
 Epoch: 161, lr: 1.0e-02, train_loss: 0.5317, train_acc: 0.8180 test_loss: 4.7373, test_acc: 0.6626, best: 0.7200, time: 0:01:50
 Epoch: 162, lr: 1.0e-02, train_loss: 0.5356, train_acc: 0.8152 test_loss: 1.9622, test_acc: 0.6980, best: 0.7200, time: 0:01:51
 Epoch: 163, lr: 1.0e-02, train_loss: 0.5161, train_acc: 0.8272 test_loss: 1.4463, test_acc: 0.7020, best: 0.7200, time: 0:01:50
 Epoch: 164, lr: 1.0e-02, train_loss: 0.5159, train_acc: 0.8208 test_loss: 1.3684, test_acc: 0.7065, best: 0.7200, time: 0:01:50
 Epoch: 165, lr: 1.0e-02, train_loss: 0.5461, train_acc: 0.8134 test_loss: 1.8638, test_acc: 0.6963, best: 0.7200, time: 0:01:51
 Epoch: 166, lr: 1.0e-02, train_loss: 0.5216, train_acc: 0.8158 test_loss: 1.0876, test_acc: 0.7134, best: 0.7200, time: 0:01:49
 Epoch: 167, lr: 1.0e-02, train_loss: 0.5238, train_acc: 0.8110 test_loss: 1.1340, test_acc: 0.7139, best: 0.7200, time: 0:01:50
 Epoch: 168, lr: 1.0e-02, train_loss: 0.5150, train_acc: 0.8254 test_loss: 1.1091, test_acc: 0.6973, best: 0.7200, time: 0:01:50
 Epoch: 169, lr: 1.0e-02, train_loss: 0.5104, train_acc: 0.8262 test_loss: 1.1722, test_acc: 0.6924, best: 0.7200, time: 0:01:51
 Epoch: 170, lr: 1.0e-02, train_loss: 0.5135, train_acc: 0.8192 test_loss: 1.5147, test_acc: 0.6432, best: 0.7200, time: 0:01:51
 Epoch: 171, lr: 1.0e-02, train_loss: 0.4853, train_acc: 0.8336 test_loss: 1.2694, test_acc: 0.6905, best: 0.7200, time: 0:01:51
 Epoch: 172, lr: 1.0e-02, train_loss: 0.4996, train_acc: 0.8284 test_loss: 1.6705, test_acc: 0.6736, best: 0.7200, time: 0:01:50
 Epoch: 173, lr: 1.0e-02, train_loss: 0.4910, train_acc: 0.8222 test_loss: 1.2021, test_acc: 0.6831, best: 0.7200, time: 0:01:50
 Epoch: 174, lr: 1.0e-02, train_loss: 0.5022, train_acc: 0.8246 test_loss: 1.1918, test_acc: 0.6980, best: 0.7200, time: 0:01:51
 Epoch: 175, lr: 1.0e-02, train_loss: 0.4924, train_acc: 0.8290 test_loss: 1.2443, test_acc: 0.6915, best: 0.7200, time: 0:01:51
 Epoch: 176, lr: 1.0e-02, train_loss: 0.4936, train_acc: 0.8258 test_loss: 1.4385, test_acc: 0.6894, best: 0.7200, time: 0:01:50
 Epoch: 177, lr: 1.0e-02, train_loss: 0.4992, train_acc: 0.8294 test_loss: 2.2642, test_acc: 0.6378, best: 0.7200, time: 0:01:50
 Epoch: 178, lr: 1.0e-02, train_loss: 0.4848, train_acc: 0.8344 test_loss: 1.2724, test_acc: 0.6981, best: 0.7200, time: 0:01:50
 Epoch: 179, lr: 1.0e-02, train_loss: 0.4858, train_acc: 0.8322 test_loss: 2.2365, test_acc: 0.6723, best: 0.7200, time: 0:01:49
 Epoch: 180, lr: 2.0e-03, train_loss: 0.4159, train_acc: 0.8566 test_loss: 1.6325, test_acc: 0.6931, best: 0.7200, time: 0:01:51
 Epoch: 181, lr: 2.0e-03, train_loss: 0.3700, train_acc: 0.8694 test_loss: 2.5158, test_acc: 0.6891, best: 0.7200, time: 0:01:50
 Epoch: 182, lr: 2.0e-03, train_loss: 0.3724, train_acc: 0.8726 test_loss: 1.5561, test_acc: 0.7190, best: 0.7200, time: 0:01:50
 Epoch: 183, lr: 2.0e-03, train_loss: 0.3655, train_acc: 0.8760 test_loss: 1.3621, test_acc: 0.7286, best: 0.7286, time: 0:01:52
 Epoch: 184, lr: 2.0e-03, train_loss: 0.3720, train_acc: 0.8734 test_loss: 1.3749, test_acc: 0.7070, best: 0.7286, time: 0:01:49
 Epoch: 185, lr: 2.0e-03, train_loss: 0.3392, train_acc: 0.8868 test_loss: 1.4711, test_acc: 0.7206, best: 0.7286, time: 0:01:51
 Epoch: 186, lr: 2.0e-03, train_loss: 0.3689, train_acc: 0.8726 test_loss: 1.5503, test_acc: 0.7166, best: 0.7286, time: 0:01:51
 Epoch: 187, lr: 2.0e-03, train_loss: 0.3437, train_acc: 0.8814 test_loss: 1.2875, test_acc: 0.7210, best: 0.7286, time: 0:01:50
 Epoch: 188, lr: 2.0e-03, train_loss: 0.3201, train_acc: 0.8880 test_loss: 1.6649, test_acc: 0.7077, best: 0.7286, time: 0:01:51
 Epoch: 189, lr: 2.0e-03, train_loss: 0.3290, train_acc: 0.8810 test_loss: 1.9968, test_acc: 0.7059, best: 0.7286, time: 0:01:48
 Epoch: 190, lr: 2.0e-03, train_loss: 0.3638, train_acc: 0.8778 test_loss: 2.1582, test_acc: 0.6951, best: 0.7286, time: 0:01:51
 Epoch: 191, lr: 2.0e-03, train_loss: 0.3577, train_acc: 0.8790 test_loss: 1.7484, test_acc: 0.6991, best: 0.7286, time: 0:01:50
 Epoch: 192, lr: 2.0e-03, train_loss: 0.3424, train_acc: 0.8816 test_loss: 1.3607, test_acc: 0.7256, best: 0.7286, time: 0:01:49
 Epoch: 193, lr: 2.0e-03, train_loss: 0.3351, train_acc: 0.8820 test_loss: 1.4126, test_acc: 0.7301, best: 0.7301, time: 0:01:52
 Epoch: 194, lr: 2.0e-03, train_loss: 0.3364, train_acc: 0.8866 test_loss: 1.9523, test_acc: 0.7281, best: 0.7301, time: 0:01:50
 Epoch: 195, lr: 2.0e-03, train_loss: 0.3482, train_acc: 0.8864 test_loss: 1.6183, test_acc: 0.7255, best: 0.7301, time: 0:01:50
 Epoch: 196, lr: 2.0e-03, train_loss: 0.3328, train_acc: 0.8840 test_loss: 1.5574, test_acc: 0.7270, best: 0.7301, time: 0:01:51
 Epoch: 197, lr: 2.0e-03, train_loss: 0.3270, train_acc: 0.8878 test_loss: 2.8314, test_acc: 0.7123, best: 0.7301, time: 0:01:52
 Epoch: 198, lr: 2.0e-03, train_loss: 0.3248, train_acc: 0.8880 test_loss: 1.3856, test_acc: 0.7318, best: 0.7318, time: 0:01:51
 Epoch: 199, lr: 2.0e-03, train_loss: 0.3472, train_acc: 0.8810 test_loss: 1.6567, test_acc: 0.7296, best: 0.7318, time: 0:01:51
 Epoch: 200, lr: 2.0e-03, train_loss: 0.3325, train_acc: 0.8852 test_loss: 1.5876, test_acc: 0.7299, best: 0.7318, time: 0:01:52
 Epoch: 201, lr: 2.0e-03, train_loss: 0.3185, train_acc: 0.8976 test_loss: 2.1047, test_acc: 0.7147, best: 0.7318, time: 0:01:50
 Epoch: 202, lr: 2.0e-03, train_loss: 0.3206, train_acc: 0.8938 test_loss: 1.5823, test_acc: 0.7175, best: 0.7318, time: 0:01:52
 Epoch: 203, lr: 2.0e-03, train_loss: 0.3357, train_acc: 0.8866 test_loss: 2.0724, test_acc: 0.7133, best: 0.7318, time: 0:01:51
 Epoch: 204, lr: 2.0e-03, train_loss: 0.3406, train_acc: 0.8868 test_loss: 2.5843, test_acc: 0.7096, best: 0.7318, time: 0:01:51
 Epoch: 205, lr: 2.0e-03, train_loss: 0.2863, train_acc: 0.8966 test_loss: 2.5725, test_acc: 0.7150, best: 0.7318, time: 0:01:51
 Epoch: 206, lr: 2.0e-03, train_loss: 0.3117, train_acc: 0.8958 test_loss: 1.7705, test_acc: 0.7221, best: 0.7318, time: 0:01:49
 Epoch: 207, lr: 2.0e-03, train_loss: 0.3283, train_acc: 0.8876 test_loss: 1.9135, test_acc: 0.7014, best: 0.7318, time: 0:01:47
 Epoch: 208, lr: 2.0e-03, train_loss: 0.3109, train_acc: 0.8990 test_loss: 2.4201, test_acc: 0.7206, best: 0.7318, time: 0:01:51
 Epoch: 209, lr: 2.0e-03, train_loss: 0.3088, train_acc: 0.8938 test_loss: 2.6630, test_acc: 0.6965, best: 0.7318, time: 0:01:51
 Epoch: 210, lr: 2.0e-03, train_loss: 0.3045, train_acc: 0.8994 test_loss: 2.3037, test_acc: 0.7105, best: 0.7318, time: 0:01:52
 Epoch: 211, lr: 2.0e-03, train_loss: 0.3171, train_acc: 0.8886 test_loss: 1.5583, test_acc: 0.7191, best: 0.7318, time: 0:01:51
 Epoch: 212, lr: 2.0e-03, train_loss: 0.2993, train_acc: 0.8998 test_loss: 1.9012, test_acc: 0.7206, best: 0.7318, time: 0:01:51
 Epoch: 213, lr: 2.0e-03, train_loss: 0.3036, train_acc: 0.8978 test_loss: 1.2891, test_acc: 0.7320, best: 0.7320, time: 0:01:51
 Epoch: 214, lr: 2.0e-03, train_loss: 0.3127, train_acc: 0.8940 test_loss: 1.4773, test_acc: 0.7161, best: 0.7320, time: 0:01:50
 Epoch: 215, lr: 2.0e-03, train_loss: 0.3034, train_acc: 0.8936 test_loss: 2.5823, test_acc: 0.7090, best: 0.7320, time: 0:01:50
 Epoch: 216, lr: 2.0e-03, train_loss: 0.3011, train_acc: 0.8970 test_loss: 1.8841, test_acc: 0.7005, best: 0.7320, time: 0:01:51
 Epoch: 217, lr: 2.0e-03, train_loss: 0.3195, train_acc: 0.8888 test_loss: 1.7883, test_acc: 0.7085, best: 0.7320, time: 0:01:51
 Epoch: 218, lr: 2.0e-03, train_loss: 0.3029, train_acc: 0.9002 test_loss: 2.8940, test_acc: 0.6851, best: 0.7320, time: 0:01:51
 Epoch: 219, lr: 2.0e-03, train_loss: 0.3221, train_acc: 0.8910 test_loss: 1.5839, test_acc: 0.7315, best: 0.7320, time: 0:01:51
 Epoch: 220, lr: 2.0e-03, train_loss: 0.3096, train_acc: 0.8936 test_loss: 2.7715, test_acc: 0.7096, best: 0.7320, time: 0:01:51
 Epoch: 221, lr: 2.0e-03, train_loss: 0.3185, train_acc: 0.8858 test_loss: 2.0565, test_acc: 0.7301, best: 0.7320, time: 0:01:51
 Epoch: 222, lr: 2.0e-03, train_loss: 0.3096, train_acc: 0.8984 test_loss: 3.8929, test_acc: 0.6777, best: 0.7320, time: 0:01:49
 Epoch: 223, lr: 2.0e-03, train_loss: 0.2992, train_acc: 0.9008 test_loss: 1.2808, test_acc: 0.7396, best: 0.7396, time: 0:01:51
 Epoch: 224, lr: 2.0e-03, train_loss: 0.2955, train_acc: 0.9016 test_loss: 1.5597, test_acc: 0.7188, best: 0.7396, time: 0:01:49
 Epoch: 225, lr: 2.0e-03, train_loss: 0.3193, train_acc: 0.8908 test_loss: 1.1962, test_acc: 0.7278, best: 0.7396, time: 0:01:50
 Epoch: 226, lr: 2.0e-03, train_loss: 0.3089, train_acc: 0.8990 test_loss: 2.4641, test_acc: 0.6747, best: 0.7396, time: 0:01:51
 Epoch: 227, lr: 2.0e-03, train_loss: 0.3040, train_acc: 0.8980 test_loss: 1.4567, test_acc: 0.7280, best: 0.7396, time: 0:01:49
 Epoch: 228, lr: 2.0e-03, train_loss: 0.2825, train_acc: 0.9022 test_loss: 1.3593, test_acc: 0.7276, best: 0.7396, time: 0:01:50
 Epoch: 229, lr: 2.0e-03, train_loss: 0.2823, train_acc: 0.9056 test_loss: 1.7945, test_acc: 0.7285, best: 0.7396, time: 0:01:50
 Epoch: 230, lr: 2.0e-03, train_loss: 0.2770, train_acc: 0.9014 test_loss: 2.1829, test_acc: 0.7035, best: 0.7396, time: 0:01:50
 Epoch: 231, lr: 2.0e-03, train_loss: 0.2877, train_acc: 0.9052 test_loss: 1.3103, test_acc: 0.7346, best: 0.7396, time: 0:01:51
 Epoch: 232, lr: 2.0e-03, train_loss: 0.2973, train_acc: 0.8990 test_loss: 1.4360, test_acc: 0.7311, best: 0.7396, time: 0:01:51
 Epoch: 233, lr: 2.0e-03, train_loss: 0.3042, train_acc: 0.8942 test_loss: 1.5497, test_acc: 0.7309, best: 0.7396, time: 0:01:50
 Epoch: 234, lr: 2.0e-03, train_loss: 0.3010, train_acc: 0.8968 test_loss: 1.6169, test_acc: 0.7189, best: 0.7396, time: 0:01:51
 Epoch: 235, lr: 2.0e-03, train_loss: 0.2971, train_acc: 0.8990 test_loss: 1.2384, test_acc: 0.7398, best: 0.7398, time: 0:01:51
 Epoch: 236, lr: 2.0e-03, train_loss: 0.2955, train_acc: 0.8988 test_loss: 2.1222, test_acc: 0.7198, best: 0.7398, time: 0:01:50
 Epoch: 237, lr: 2.0e-03, train_loss: 0.3070, train_acc: 0.8960 test_loss: 1.5587, test_acc: 0.7329, best: 0.7398, time: 0:01:51
 Epoch: 238, lr: 2.0e-03, train_loss: 0.2962, train_acc: 0.8992 test_loss: 1.6591, test_acc: 0.7330, best: 0.7398, time: 0:01:51
 Epoch: 239, lr: 2.0e-03, train_loss: 0.2894, train_acc: 0.9070 test_loss: 2.0005, test_acc: 0.7298, best: 0.7398, time: 0:01:49
 Epoch: 240, lr: 4.0e-04, train_loss: 0.2809, train_acc: 0.9014 test_loss: 1.3741, test_acc: 0.7360, best: 0.7398, time: 0:01:49
 Epoch: 241, lr: 4.0e-04, train_loss: 0.2955, train_acc: 0.8956 test_loss: 1.3259, test_acc: 0.7459, best: 0.7459, time: 0:01:51
 Epoch: 242, lr: 4.0e-04, train_loss: 0.2940, train_acc: 0.9006 test_loss: 1.3990, test_acc: 0.7402, best: 0.7459, time: 0:01:51
 Epoch: 243, lr: 4.0e-04, train_loss: 0.2781, train_acc: 0.9096 test_loss: 2.4567, test_acc: 0.7269, best: 0.7459, time: 0:01:49
 Epoch: 244, lr: 4.0e-04, train_loss: 0.2805, train_acc: 0.9018 test_loss: 1.2734, test_acc: 0.7382, best: 0.7459, time: 0:01:46
 Epoch: 245, lr: 4.0e-04, train_loss: 0.2681, train_acc: 0.9098 test_loss: 1.4292, test_acc: 0.7379, best: 0.7459, time: 0:01:46
 Epoch: 246, lr: 4.0e-04, train_loss: 0.2826, train_acc: 0.9032 test_loss: 1.8239, test_acc: 0.7354, best: 0.7459, time: 0:01:48
 Epoch: 247, lr: 4.0e-04, train_loss: 0.2828, train_acc: 0.9038 test_loss: 1.6412, test_acc: 0.7324, best: 0.7459, time: 0:01:50
 Epoch: 248, lr: 4.0e-04, train_loss: 0.2695, train_acc: 0.9034 test_loss: 1.6833, test_acc: 0.7472, best: 0.7472, time: 0:01:52
 Epoch: 249, lr: 4.0e-04, train_loss: 0.2668, train_acc: 0.9046 test_loss: 1.8195, test_acc: 0.7350, best: 0.7472, time: 0:01:50
 Epoch: 250, lr: 4.0e-04, train_loss: 0.2568, train_acc: 0.9138 test_loss: 1.4508, test_acc: 0.7435, best: 0.7472, time: 0:01:50
 Epoch: 251, lr: 4.0e-04, train_loss: 0.2574, train_acc: 0.9120 test_loss: 1.3649, test_acc: 0.7415, best: 0.7472, time: 0:01:50
 Epoch: 252, lr: 4.0e-04, train_loss: 0.2711, train_acc: 0.9096 test_loss: 1.4836, test_acc: 0.7474, best: 0.7474, time: 0:01:51
 Epoch: 253, lr: 4.0e-04, train_loss: 0.2649, train_acc: 0.9098 test_loss: 2.3539, test_acc: 0.7205, best: 0.7474, time: 0:01:51
 Epoch: 254, lr: 4.0e-04, train_loss: 0.2754, train_acc: 0.9072 test_loss: 2.3302, test_acc: 0.7328, best: 0.7474, time: 0:01:51
 Epoch: 255, lr: 4.0e-04, train_loss: 0.2552, train_acc: 0.9138 test_loss: 1.9360, test_acc: 0.7335, best: 0.7474, time: 0:01:49
 Epoch: 256, lr: 4.0e-04, train_loss: 0.2793, train_acc: 0.9068 test_loss: 1.4124, test_acc: 0.7392, best: 0.7474, time: 0:01:48
 Epoch: 257, lr: 4.0e-04, train_loss: 0.2696, train_acc: 0.9020 test_loss: 1.5204, test_acc: 0.7401, best: 0.7474, time: 0:01:50
 Epoch: 258, lr: 4.0e-04, train_loss: 0.2612, train_acc: 0.9124 test_loss: 1.7393, test_acc: 0.7371, best: 0.7474, time: 0:01:49
 Epoch: 259, lr: 4.0e-04, train_loss: 0.2684, train_acc: 0.9060 test_loss: 1.5309, test_acc: 0.7401, best: 0.7474, time: 0:01:50
 Epoch: 260, lr: 4.0e-04, train_loss: 0.2785, train_acc: 0.9104 test_loss: 1.9342, test_acc: 0.7066, best: 0.7474, time: 0:01:50
 Epoch: 261, lr: 4.0e-04, train_loss: 0.2378, train_acc: 0.9226 test_loss: 1.6368, test_acc: 0.7445, best: 0.7474, time: 0:01:51
 Epoch: 262, lr: 4.0e-04, train_loss: 0.2527, train_acc: 0.9138 test_loss: 1.3392, test_acc: 0.7395, best: 0.7474, time: 0:01:50
 Epoch: 263, lr: 4.0e-04, train_loss: 0.2551, train_acc: 0.9094 test_loss: 2.9178, test_acc: 0.7236, best: 0.7474, time: 0:01:51
 Epoch: 264, lr: 4.0e-04, train_loss: 0.2654, train_acc: 0.9086 test_loss: 1.4892, test_acc: 0.7399, best: 0.7474, time: 0:01:51
 Epoch: 265, lr: 4.0e-04, train_loss: 0.2770, train_acc: 0.9056 test_loss: 2.2087, test_acc: 0.7342, best: 0.7474, time: 0:01:50
 Epoch: 266, lr: 4.0e-04, train_loss: 0.2607, train_acc: 0.9092 test_loss: 2.0926, test_acc: 0.7332, best: 0.7474, time: 0:01:43
 Epoch: 267, lr: 4.0e-04, train_loss: 0.2639, train_acc: 0.9100 test_loss: 1.6028, test_acc: 0.7388, best: 0.7474, time: 0:01:50
 Epoch: 268, lr: 4.0e-04, train_loss: 0.2663, train_acc: 0.9128 test_loss: 2.0904, test_acc: 0.7351, best: 0.7474, time: 0:01:51
 Epoch: 269, lr: 4.0e-04, train_loss: 0.2656, train_acc: 0.9052 test_loss: 2.0467, test_acc: 0.7286, best: 0.7474, time: 0:01:44
 Epoch: 270, lr: 8.0e-05, train_loss: 0.2788, train_acc: 0.9046 test_loss: 1.3557, test_acc: 0.7438, best: 0.7474, time: 0:01:49
 Epoch: 271, lr: 8.0e-05, train_loss: 0.2486, train_acc: 0.9120 test_loss: 3.2828, test_acc: 0.7110, best: 0.7474, time: 0:01:50
 Epoch: 272, lr: 8.0e-05, train_loss: 0.2666, train_acc: 0.9134 test_loss: 1.4838, test_acc: 0.7392, best: 0.7474, time: 0:01:51
 Epoch: 273, lr: 8.0e-05, train_loss: 0.2657, train_acc: 0.9106 test_loss: 1.4441, test_acc: 0.7321, best: 0.7474, time: 0:01:51
 Epoch: 274, lr: 8.0e-05, train_loss: 0.2528, train_acc: 0.9140 test_loss: 1.6190, test_acc: 0.7378, best: 0.7474, time: 0:01:50
 Epoch: 275, lr: 8.0e-05, train_loss: 0.2651, train_acc: 0.9110 test_loss: 1.5065, test_acc: 0.7372, best: 0.7474, time: 0:01:51
 Epoch: 276, lr: 8.0e-05, train_loss: 0.2561, train_acc: 0.9138 test_loss: 1.9831, test_acc: 0.7338, best: 0.7474, time: 0:01:46
 Epoch: 277, lr: 8.0e-05, train_loss: 0.2427, train_acc: 0.9158 test_loss: 1.3960, test_acc: 0.7432, best: 0.7474, time: 0:01:48
 Epoch: 278, lr: 8.0e-05, train_loss: 0.2649, train_acc: 0.9108 test_loss: 1.9027, test_acc: 0.7416, best: 0.7474, time: 0:01:48
 Epoch: 279, lr: 8.0e-05, train_loss: 0.2414, train_acc: 0.9228 test_loss: 1.4535, test_acc: 0.7412, best: 0.7474, time: 0:01:47
 Epoch: 280, lr: 8.0e-05, train_loss: 0.2542, train_acc: 0.9144 test_loss: 1.6371, test_acc: 0.7430, best: 0.7474, time: 0:01:51
 Epoch: 281, lr: 8.0e-05, train_loss: 0.2573, train_acc: 0.9138 test_loss: 1.4883, test_acc: 0.7404, best: 0.7474, time: 0:01:49
 Epoch: 282, lr: 8.0e-05, train_loss: 0.2567, train_acc: 0.9122 test_loss: 1.4793, test_acc: 0.7431, best: 0.7474, time: 0:01:50
 Epoch: 283, lr: 8.0e-05, train_loss: 0.2475, train_acc: 0.9148 test_loss: 1.5782, test_acc: 0.7340, best: 0.7474, time: 0:01:50
 Epoch: 284, lr: 8.0e-05, train_loss: 0.2693, train_acc: 0.9080 test_loss: 1.2309, test_acc: 0.7491, best: 0.7491, time: 0:01:51
 Epoch: 285, lr: 8.0e-05, train_loss: 0.2421, train_acc: 0.9196 test_loss: 1.8912, test_acc: 0.7462, best: 0.7491, time: 0:01:48
 Epoch: 286, lr: 8.0e-05, train_loss: 0.2566, train_acc: 0.9124 test_loss: 1.3882, test_acc: 0.7458, best: 0.7491, time: 0:01:50
 Epoch: 287, lr: 8.0e-05, train_loss: 0.2776, train_acc: 0.9066 test_loss: 2.4582, test_acc: 0.7292, best: 0.7491, time: 0:01:51
 Epoch: 288, lr: 8.0e-05, train_loss: 0.2472, train_acc: 0.9156 test_loss: 2.2109, test_acc: 0.7345, best: 0.7491, time: 0:01:50
 Epoch: 289, lr: 8.0e-05, train_loss: 0.2773, train_acc: 0.9066 test_loss: 1.7518, test_acc: 0.7345, best: 0.7491, time: 0:01:50
 Epoch: 290, lr: 8.0e-05, train_loss: 0.2418, train_acc: 0.9174 test_loss: 1.3892, test_acc: 0.7372, best: 0.7491, time: 0:01:46
 Epoch: 291, lr: 8.0e-05, train_loss: 0.2732, train_acc: 0.9072 test_loss: 1.5734, test_acc: 0.7469, best: 0.7491, time: 0:01:49
 Epoch: 292, lr: 8.0e-05, train_loss: 0.2578, train_acc: 0.9076 test_loss: 2.8270, test_acc: 0.7255, best: 0.7491, time: 0:01:49
 Epoch: 293, lr: 8.0e-05, train_loss: 0.2717, train_acc: 0.9040 test_loss: 1.4751, test_acc: 0.7472, best: 0.7491, time: 0:01:47
 Epoch: 294, lr: 8.0e-05, train_loss: 0.2497, train_acc: 0.9142 test_loss: 1.8211, test_acc: 0.7389, best: 0.7491, time: 0:01:47
 Epoch: 295, lr: 8.0e-05, train_loss: 0.2539, train_acc: 0.9102 test_loss: 1.9783, test_acc: 0.7370, best: 0.7491, time: 0:01:49
 Epoch: 296, lr: 8.0e-05, train_loss: 0.2485, train_acc: 0.9134 test_loss: 2.7192, test_acc: 0.7124, best: 0.7491, time: 0:01:49
 Epoch: 297, lr: 8.0e-05, train_loss: 0.2505, train_acc: 0.9180 test_loss: 1.6323, test_acc: 0.7289, best: 0.7491, time: 0:01:49
 Epoch: 298, lr: 8.0e-05, train_loss: 0.2643, train_acc: 0.9088 test_loss: 1.8138, test_acc: 0.7262, best: 0.7491, time: 0:01:49
 Epoch: 299, lr: 8.0e-05, train_loss: 0.2564, train_acc: 0.9158 test_loss: 1.4031, test_acc: 0.7341, best: 0.7491, time: 0:01:48
 Highest accuracy: 0.7491