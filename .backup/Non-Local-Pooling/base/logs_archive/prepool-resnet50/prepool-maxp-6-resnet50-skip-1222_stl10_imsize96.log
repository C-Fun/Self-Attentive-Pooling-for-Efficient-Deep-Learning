
 Run on time: 2022-07-01 13:44:57.317648

 Architecture: prepool-maxp-6-resnet50-skip-1222

 Pool Config: {
    "arch": "resnet50",
    "conv1": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "skip",
            "_stride": 1,
            "_psize": null,
            "_dim_reduced_ratio": null,
            "_num_heads": null,
            "_conv2d": null,
            "_win_norm": null
        }
    },
    "pool": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "maxp",
            "_stride": 6,
            "_psize": null,
            "_dim_reduced_ratio": null,
            "_num_heads": null,
            "_conv2d": null,
            "_win_norm": null
        }
    },
    "layer1": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "skip",
            "_stride": 1,
            "_psize": null,
            "_dim_reduced_ratio": null,
            "_num_heads": null,
            "_conv2d": null,
            "_win_norm": null
        }
    },
    "layer2": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "skip",
            "_stride": 2,
            "_psize": null,
            "_dim_reduced_ratio": null,
            "_num_heads": null,
            "_conv2d": null,
            "_win_norm": null
        }
    },
    "layer3": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "skip",
            "_stride": 2,
            "_psize": null,
            "_dim_reduced_ratio": null,
            "_num_heads": null,
            "_conv2d": null,
            "_win_norm": null
        }
    },
    "layer4": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "skip",
            "_stride": 2,
            "_psize": null,
            "_dim_reduced_ratio": null,
            "_num_heads": null,
            "_conv2d": null,
            "_win_norm": null
        }
    }
}

 Arguments:
	 gpu                  : True
	 log                  : True
	 seed                 : 0
	 momentum             : 0.9
	 amsgrad              : True
	 dataset              : STL10
	 batch_size           : 8
	 architecture         : PREPOOL-MAXP-6-RESNET50-SKIP-1222
	 im_size              : None
	 relu_threshold       : 4.0
	 learning_rate        : 0.01
	 pretrained_backbone  : 
	 pretrained_ann       : 
	 weight_decay         : 0.0
	 test_only            : False
	 epochs               : 300
	 lr_interval          : [180, 240, 270]
	 lr_reduce            : 5
	 optimizer            : SGD
	 dropout              : 0.2
	 kernel_size          : 3
	 dont_save            : False
	 visualize            : False
	 devices              : 0
 DataParallel(
  (module): Network(
    (net): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (pool): MaxPool2d(kernel_size=6, stride=6, padding=0, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer4): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
      (fc): Linear(in_features=2048, out_features=10, bias=True)
    )
  )
)
 SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0
)
 Epoch: 1, lr: 1.0e-02, train_loss: 4.5840, train_acc: 0.1112 test_loss: 3.8990, test_acc: 0.1092, best: 0.1092, time: 0:01:00
 Epoch: 2, lr: 1.0e-02, train_loss: 2.4529, train_acc: 0.1450 test_loss: 2.6121, test_acc: 0.2140, best: 0.2140, time: 0:00:59
 Epoch: 3, lr: 1.0e-02, train_loss: 2.1817, train_acc: 0.1844 test_loss: 2.0954, test_acc: 0.2521, best: 0.2521, time: 0:01:01
 Epoch: 4, lr: 1.0e-02, train_loss: 2.0806, train_acc: 0.2032 test_loss: 2.4331, test_acc: 0.2929, best: 0.2929, time: 0:01:01
 Epoch: 5, lr: 1.0e-02, train_loss: 2.0216, train_acc: 0.2310 test_loss: 1.8821, test_acc: 0.2557, best: 0.2929, time: 0:01:01
 Epoch: 6, lr: 1.0e-02, train_loss: 1.9918, train_acc: 0.2462 test_loss: 2.7829, test_acc: 0.3246, best: 0.3246, time: 0:01:01
 Epoch: 7, lr: 1.0e-02, train_loss: 1.9584, train_acc: 0.2554 test_loss: 2.2077, test_acc: 0.2976, best: 0.3246, time: 0:01:00
 Epoch: 8, lr: 1.0e-02, train_loss: 1.9089, train_acc: 0.2704 test_loss: 1.9507, test_acc: 0.3449, best: 0.3449, time: 0:01:00
 Epoch: 9, lr: 1.0e-02, train_loss: 1.8932, train_acc: 0.2782 test_loss: 2.4591, test_acc: 0.3443, best: 0.3449, time: 0:01:00
 Epoch: 10, lr: 1.0e-02, train_loss: 1.8387, train_acc: 0.3054 test_loss: 1.9357, test_acc: 0.3656, best: 0.3656, time: 0:01:01
 Epoch: 11, lr: 1.0e-02, train_loss: 1.8264, train_acc: 0.3046 test_loss: 2.1711, test_acc: 0.3649, best: 0.3656, time: 0:01:00
 Epoch: 12, lr: 1.0e-02, train_loss: 1.8196, train_acc: 0.3106 test_loss: 1.5804, test_acc: 0.3834, best: 0.3834, time: 0:01:00
 Epoch: 13, lr: 1.0e-02, train_loss: 1.7777, train_acc: 0.3200 test_loss: 1.8285, test_acc: 0.3733, best: 0.3834, time: 0:01:00
 Epoch: 14, lr: 1.0e-02, train_loss: 1.7834, train_acc: 0.3302 test_loss: 1.8082, test_acc: 0.4090, best: 0.4090, time: 0:01:01
 Epoch: 15, lr: 1.0e-02, train_loss: 1.7776, train_acc: 0.3324 test_loss: 1.8425, test_acc: 0.4194, best: 0.4194, time: 0:01:00
 Epoch: 16, lr: 1.0e-02, train_loss: 1.7579, train_acc: 0.3288 test_loss: 1.5289, test_acc: 0.4130, best: 0.4194, time: 0:01:00
 Epoch: 17, lr: 1.0e-02, train_loss: 1.7422, train_acc: 0.3412 test_loss: 1.6330, test_acc: 0.4170, best: 0.4194, time: 0:01:00
 Epoch: 18, lr: 1.0e-02, train_loss: 1.7125, train_acc: 0.3544 test_loss: 1.6324, test_acc: 0.4186, best: 0.4194, time: 0:01:00
 Epoch: 19, lr: 1.0e-02, train_loss: 1.7216, train_acc: 0.3472 test_loss: 1.5166, test_acc: 0.4482, best: 0.4482, time: 0:01:00
 Epoch: 20, lr: 1.0e-02, train_loss: 1.7091, train_acc: 0.3552 test_loss: 1.5681, test_acc: 0.4278, best: 0.4482, time: 0:01:00
 Epoch: 21, lr: 1.0e-02, train_loss: 1.7253, train_acc: 0.3548 test_loss: 1.5426, test_acc: 0.4271, best: 0.4482, time: 0:01:00
 Epoch: 22, lr: 1.0e-02, train_loss: 1.6938, train_acc: 0.3652 test_loss: 1.5582, test_acc: 0.4537, best: 0.4537, time: 0:01:01
 Epoch: 23, lr: 1.0e-02, train_loss: 1.6665, train_acc: 0.3808 test_loss: 1.4672, test_acc: 0.4551, best: 0.4551, time: 0:01:00
 Epoch: 24, lr: 1.0e-02, train_loss: 1.6454, train_acc: 0.3844 test_loss: 1.7061, test_acc: 0.4371, best: 0.4551, time: 0:01:00
 Epoch: 25, lr: 1.0e-02, train_loss: 1.6099, train_acc: 0.4030 test_loss: 1.9884, test_acc: 0.4584, best: 0.4584, time: 0:01:00
 Epoch: 26, lr: 1.0e-02, train_loss: 1.6130, train_acc: 0.4064 test_loss: 1.4732, test_acc: 0.4679, best: 0.4679, time: 0:01:00
 Epoch: 27, lr: 1.0e-02, train_loss: 1.5999, train_acc: 0.4060 test_loss: 2.0378, test_acc: 0.4739, best: 0.4739, time: 0:01:00
 Epoch: 28, lr: 1.0e-02, train_loss: 1.5824, train_acc: 0.4114 test_loss: 1.8417, test_acc: 0.4881, best: 0.4881, time: 0:01:00
 Epoch: 29, lr: 1.0e-02, train_loss: 1.5732, train_acc: 0.4170 test_loss: 1.8293, test_acc: 0.4944, best: 0.4944, time: 0:01:00
 Epoch: 30, lr: 1.0e-02, train_loss: 1.6108, train_acc: 0.3992 test_loss: 3.5095, test_acc: 0.4059, best: 0.4944, time: 0:01:00
 Epoch: 31, lr: 1.0e-02, train_loss: 1.6503, train_acc: 0.3806 test_loss: 2.2340, test_acc: 0.4622, best: 0.4944, time: 0:01:00
 Epoch: 32, lr: 1.0e-02, train_loss: 1.5986, train_acc: 0.4068 test_loss: 3.7776, test_acc: 0.4534, best: 0.4944, time: 0:01:00
 Epoch: 33, lr: 1.0e-02, train_loss: 1.5625, train_acc: 0.4236 test_loss: 3.8398, test_acc: 0.4874, best: 0.4944, time: 0:01:00
 Epoch: 34, lr: 1.0e-02, train_loss: 1.5646, train_acc: 0.4188 test_loss: 1.7072, test_acc: 0.4968, best: 0.4968, time: 0:01:00
 Epoch: 35, lr: 1.0e-02, train_loss: 1.5365, train_acc: 0.4366 test_loss: 2.3036, test_acc: 0.4948, best: 0.4968, time: 0:01:00
 Epoch: 36, lr: 1.0e-02, train_loss: 1.5061, train_acc: 0.4412 test_loss: 3.7246, test_acc: 0.5125, best: 0.5125, time: 0:01:00
 Epoch: 37, lr: 1.0e-02, train_loss: 1.4960, train_acc: 0.4366 test_loss: 1.6372, test_acc: 0.5015, best: 0.5125, time: 0:01:00
 Epoch: 38, lr: 1.0e-02, train_loss: 1.4932, train_acc: 0.4508 test_loss: 2.2229, test_acc: 0.5162, best: 0.5162, time: 0:01:00
 Epoch: 39, lr: 1.0e-02, train_loss: 1.4710, train_acc: 0.4630 test_loss: 1.6240, test_acc: 0.5155, best: 0.5162, time: 0:01:00
 Epoch: 40, lr: 1.0e-02, train_loss: 1.4635, train_acc: 0.4624 test_loss: 4.3004, test_acc: 0.5209, best: 0.5209, time: 0:01:00
 Epoch: 41, lr: 1.0e-02, train_loss: 1.4739, train_acc: 0.4544 test_loss: 8.1740, test_acc: 0.4703, best: 0.5209, time: 0:01:00
 Epoch: 42, lr: 1.0e-02, train_loss: 1.4875, train_acc: 0.4486 test_loss: 4.0645, test_acc: 0.5090, best: 0.5209, time: 0:01:00
 Epoch: 43, lr: 1.0e-02, train_loss: 1.4598, train_acc: 0.4632 test_loss: 3.0817, test_acc: 0.5092, best: 0.5209, time: 0:01:00
 Epoch: 44, lr: 1.0e-02, train_loss: 1.4555, train_acc: 0.4576 test_loss: 4.1732, test_acc: 0.5124, best: 0.5209, time: 0:01:00
 Epoch: 45, lr: 1.0e-02, train_loss: 1.4414, train_acc: 0.4662 test_loss: 6.0005, test_acc: 0.5305, best: 0.5305, time: 0:01:00
 Epoch: 46, lr: 1.0e-02, train_loss: 1.4161, train_acc: 0.4784 test_loss: 5.1430, test_acc: 0.5308, best: 0.5308, time: 0:01:00
 Epoch: 47, lr: 1.0e-02, train_loss: 1.4284, train_acc: 0.4646 test_loss: 4.2518, test_acc: 0.5323, best: 0.5323, time: 0:01:00
 Epoch: 48, lr: 1.0e-02, train_loss: 1.4014, train_acc: 0.4888 test_loss: 7.2023, test_acc: 0.5411, best: 0.5411, time: 0:01:00
 Epoch: 49, lr: 1.0e-02, train_loss: 1.3983, train_acc: 0.4912 test_loss: 10.5800, test_acc: 0.5379, best: 0.5411, time: 0:01:00
 Epoch: 50, lr: 1.0e-02, train_loss: 1.3731, train_acc: 0.4966 test_loss: 6.9906, test_acc: 0.5539, best: 0.5539, time: 0:01:00
 Epoch: 51, lr: 1.0e-02, train_loss: 1.3718, train_acc: 0.5076 test_loss: 7.9529, test_acc: 0.5466, best: 0.5539, time: 0:01:00
 Epoch: 52, lr: 1.0e-02, train_loss: 1.3524, train_acc: 0.5032 test_loss: 5.7916, test_acc: 0.5499, best: 0.5539, time: 0:01:00
 Epoch: 53, lr: 1.0e-02, train_loss: 1.3335, train_acc: 0.5144 test_loss: 9.8190, test_acc: 0.5690, best: 0.5690, time: 0:01:00
 Epoch: 54, lr: 1.0e-02, train_loss: 1.3293, train_acc: 0.5106 test_loss: 6.5360, test_acc: 0.5733, best: 0.5733, time: 0:01:00
 Epoch: 55, lr: 1.0e-02, train_loss: 1.3342, train_acc: 0.5140 test_loss: 2.5962, test_acc: 0.5396, best: 0.5733, time: 0:01:00
 Epoch: 56, lr: 1.0e-02, train_loss: 1.3067, train_acc: 0.5290 test_loss: 6.2139, test_acc: 0.5405, best: 0.5733, time: 0:01:00
 Epoch: 57, lr: 1.0e-02, train_loss: 1.2952, train_acc: 0.5270 test_loss: 5.8163, test_acc: 0.5686, best: 0.5733, time: 0:01:00
 Epoch: 58, lr: 1.0e-02, train_loss: 1.3078, train_acc: 0.5292 test_loss: 2.9506, test_acc: 0.5780, best: 0.5780, time: 0:01:00
 Epoch: 59, lr: 1.0e-02, train_loss: 1.2535, train_acc: 0.5398 test_loss: 7.3909, test_acc: 0.5625, best: 0.5780, time: 0:01:00
 Epoch: 60, lr: 1.0e-02, train_loss: 1.2592, train_acc: 0.5366 test_loss: 9.6074, test_acc: 0.5551, best: 0.5780, time: 0:01:00
 Epoch: 61, lr: 1.0e-02, train_loss: 1.2534, train_acc: 0.5462 test_loss: 15.1117, test_acc: 0.5704, best: 0.5780, time: 0:01:00
 Epoch: 62, lr: 1.0e-02, train_loss: 1.2426, train_acc: 0.5520 test_loss: 10.1292, test_acc: 0.5610, best: 0.5780, time: 0:01:00
 Epoch: 63, lr: 1.0e-02, train_loss: 1.2443, train_acc: 0.5452 test_loss: 3.7765, test_acc: 0.5734, best: 0.5780, time: 0:01:00
 Epoch: 64, lr: 1.0e-02, train_loss: 1.2277, train_acc: 0.5558 test_loss: 4.7679, test_acc: 0.6168, best: 0.6168, time: 0:01:00
 Epoch: 65, lr: 1.0e-02, train_loss: 1.2099, train_acc: 0.5616 test_loss: 6.0212, test_acc: 0.5900, best: 0.6168, time: 0:01:00
 Epoch: 66, lr: 1.0e-02, train_loss: 1.2151, train_acc: 0.5540 test_loss: 4.6586, test_acc: 0.5703, best: 0.6168, time: 0:01:00
 Epoch: 67, lr: 1.0e-02, train_loss: 1.2133, train_acc: 0.5536 test_loss: 2.4143, test_acc: 0.6059, best: 0.6168, time: 0:01:00
 Epoch: 68, lr: 1.0e-02, train_loss: 1.1966, train_acc: 0.5688 test_loss: 3.7227, test_acc: 0.5807, best: 0.6168, time: 0:01:00
 Epoch: 69, lr: 1.0e-02, train_loss: 1.1988, train_acc: 0.5730 test_loss: 5.1608, test_acc: 0.5992, best: 0.6168, time: 0:01:00
 Epoch: 70, lr: 1.0e-02, train_loss: 1.2599, train_acc: 0.5352 test_loss: 2.7721, test_acc: 0.5821, best: 0.6168, time: 0:01:00
 Epoch: 71, lr: 1.0e-02, train_loss: 1.2010, train_acc: 0.5606 test_loss: 3.3914, test_acc: 0.5959, best: 0.6168, time: 0:01:00
 Epoch: 72, lr: 1.0e-02, train_loss: 1.1805, train_acc: 0.5718 test_loss: 1.7476, test_acc: 0.5801, best: 0.6168, time: 0:01:00
 Epoch: 73, lr: 1.0e-02, train_loss: 1.2086, train_acc: 0.5716 test_loss: 2.1074, test_acc: 0.5995, best: 0.6168, time: 0:01:00
 Epoch: 74, lr: 1.0e-02, train_loss: 1.1576, train_acc: 0.5758 test_loss: 4.3120, test_acc: 0.5853, best: 0.6168, time: 0:01:00
 Epoch: 75, lr: 1.0e-02, train_loss: 1.1659, train_acc: 0.5776 test_loss: 1.4197, test_acc: 0.6085, best: 0.6168, time: 0:01:00
 Epoch: 76, lr: 1.0e-02, train_loss: 1.1552, train_acc: 0.5756 test_loss: 2.3406, test_acc: 0.5706, best: 0.6168, time: 0:01:00
 Epoch: 77, lr: 1.0e-02, train_loss: 1.1578, train_acc: 0.5812 test_loss: 1.6546, test_acc: 0.5992, best: 0.6168, time: 0:00:59
 Epoch: 78, lr: 1.0e-02, train_loss: 1.2711, train_acc: 0.5428 test_loss: 4.4056, test_acc: 0.5795, best: 0.6168, time: 0:01:00
 Epoch: 79, lr: 1.0e-02, train_loss: 1.2052, train_acc: 0.5540 test_loss: 2.9121, test_acc: 0.5894, best: 0.6168, time: 0:01:00
 Epoch: 80, lr: 1.0e-02, train_loss: 1.1655, train_acc: 0.5754 test_loss: 3.9048, test_acc: 0.6182, best: 0.6182, time: 0:01:00
 Epoch: 81, lr: 1.0e-02, train_loss: 1.2038, train_acc: 0.5630 test_loss: 3.0755, test_acc: 0.5879, best: 0.6182, time: 0:01:00
 Epoch: 82, lr: 1.0e-02, train_loss: 1.1985, train_acc: 0.5622 test_loss: 1.2296, test_acc: 0.5814, best: 0.6182, time: 0:01:00
 Epoch: 83, lr: 1.0e-02, train_loss: 1.2364, train_acc: 0.5404 test_loss: 2.0181, test_acc: 0.5811, best: 0.6182, time: 0:01:00
 Epoch: 84, lr: 1.0e-02, train_loss: 1.1569, train_acc: 0.5792 test_loss: 1.7818, test_acc: 0.6031, best: 0.6182, time: 0:01:00
 Epoch: 85, lr: 1.0e-02, train_loss: 1.1336, train_acc: 0.5870 test_loss: 2.3432, test_acc: 0.6136, best: 0.6182, time: 0:01:00
 Epoch: 86, lr: 1.0e-02, train_loss: 1.1158, train_acc: 0.5908 test_loss: 1.3538, test_acc: 0.6168, best: 0.6182, time: 0:00:59
 Epoch: 87, lr: 1.0e-02, train_loss: 1.0865, train_acc: 0.6094 test_loss: 2.5235, test_acc: 0.6106, best: 0.6182, time: 0:01:00
 Epoch: 88, lr: 1.0e-02, train_loss: 1.0908, train_acc: 0.6112 test_loss: 2.5260, test_acc: 0.6231, best: 0.6231, time: 0:01:00
 Epoch: 89, lr: 1.0e-02, train_loss: 1.0948, train_acc: 0.6062 test_loss: 1.5106, test_acc: 0.6238, best: 0.6238, time: 0:01:00
 Epoch: 90, lr: 1.0e-02, train_loss: 1.0696, train_acc: 0.6094 test_loss: 3.3074, test_acc: 0.5992, best: 0.6238, time: 0:01:00
 Epoch: 91, lr: 1.0e-02, train_loss: 1.0545, train_acc: 0.6174 test_loss: 1.9253, test_acc: 0.6424, best: 0.6424, time: 0:01:00
 Epoch: 92, lr: 1.0e-02, train_loss: 1.1060, train_acc: 0.6066 test_loss: 1.7876, test_acc: 0.5756, best: 0.6424, time: 0:01:00
 Epoch: 93, lr: 1.0e-02, train_loss: 1.1111, train_acc: 0.6022 test_loss: 1.7104, test_acc: 0.6258, best: 0.6424, time: 0:01:00
 Epoch: 94, lr: 1.0e-02, train_loss: 1.0584, train_acc: 0.6186 test_loss: 2.1128, test_acc: 0.6286, best: 0.6424, time: 0:01:00
 Epoch: 95, lr: 1.0e-02, train_loss: 1.0430, train_acc: 0.6236 test_loss: 1.5301, test_acc: 0.6305, best: 0.6424, time: 0:01:00
 Epoch: 96, lr: 1.0e-02, train_loss: 1.0379, train_acc: 0.6296 test_loss: 1.1904, test_acc: 0.6308, best: 0.6424, time: 0:01:00
 Epoch: 97, lr: 1.0e-02, train_loss: 1.0216, train_acc: 0.6326 test_loss: 2.9588, test_acc: 0.5823, best: 0.6424, time: 0:01:00
 Epoch: 98, lr: 1.0e-02, train_loss: 1.0397, train_acc: 0.6288 test_loss: 1.1365, test_acc: 0.6465, best: 0.6465, time: 0:01:00
 Epoch: 99, lr: 1.0e-02, train_loss: 1.0346, train_acc: 0.6312 test_loss: 1.1234, test_acc: 0.6506, best: 0.6506, time: 0:01:00
 Epoch: 100, lr: 1.0e-02, train_loss: 1.0370, train_acc: 0.6244 test_loss: 1.3949, test_acc: 0.5956, best: 0.6506, time: 0:01:00
 Epoch: 101, lr: 1.0e-02, train_loss: 1.0931, train_acc: 0.6068 test_loss: 1.1290, test_acc: 0.6236, best: 0.6506, time: 0:01:00
 Epoch: 102, lr: 1.0e-02, train_loss: 1.0159, train_acc: 0.6332 test_loss: 1.4001, test_acc: 0.6446, best: 0.6506, time: 0:01:00
 Epoch: 103, lr: 1.0e-02, train_loss: 0.9914, train_acc: 0.6398 test_loss: 1.9763, test_acc: 0.6084, best: 0.6506, time: 0:01:00
 Epoch: 104, lr: 1.0e-02, train_loss: 1.0632, train_acc: 0.6088 test_loss: 1.1294, test_acc: 0.6248, best: 0.6506, time: 0:01:00
 Epoch: 105, lr: 1.0e-02, train_loss: 1.0503, train_acc: 0.6106 test_loss: 1.0899, test_acc: 0.6510, best: 0.6510, time: 0:01:00
 Epoch: 106, lr: 1.0e-02, train_loss: 1.0289, train_acc: 0.6274 test_loss: 1.3470, test_acc: 0.6264, best: 0.6510, time: 0:01:00
 Epoch: 107, lr: 1.0e-02, train_loss: 1.0129, train_acc: 0.6300 test_loss: 1.2007, test_acc: 0.6296, best: 0.6510, time: 0:00:59
 Epoch: 108, lr: 1.0e-02, train_loss: 1.0019, train_acc: 0.6350 test_loss: 1.0895, test_acc: 0.6489, best: 0.6510, time: 0:01:00
 Epoch: 109, lr: 1.0e-02, train_loss: 1.0597, train_acc: 0.6178 test_loss: 1.2922, test_acc: 0.6319, best: 0.6510, time: 0:01:00
 Epoch: 110, lr: 1.0e-02, train_loss: 1.0224, train_acc: 0.6262 test_loss: 1.9549, test_acc: 0.5940, best: 0.6510, time: 0:01:00
 Epoch: 111, lr: 1.0e-02, train_loss: 1.0419, train_acc: 0.6248 test_loss: 4.2628, test_acc: 0.5623, best: 0.6510, time: 0:01:00
 Epoch: 112, lr: 1.0e-02, train_loss: 1.0100, train_acc: 0.6410 test_loss: 1.8378, test_acc: 0.5915, best: 0.6510, time: 0:01:00
 Epoch: 113, lr: 1.0e-02, train_loss: 0.9901, train_acc: 0.6432 test_loss: 1.8661, test_acc: 0.5889, best: 0.6510, time: 0:01:00
 Epoch: 114, lr: 1.0e-02, train_loss: 1.0035, train_acc: 0.6362 test_loss: 1.1759, test_acc: 0.6372, best: 0.6510, time: 0:01:00
 Epoch: 115, lr: 1.0e-02, train_loss: 1.0521, train_acc: 0.6204 test_loss: 1.1741, test_acc: 0.6136, best: 0.6510, time: 0:01:00
 Epoch: 116, lr: 1.0e-02, train_loss: 1.0186, train_acc: 0.6358 test_loss: 1.1955, test_acc: 0.6268, best: 0.6510, time: 0:01:00
 Epoch: 117, lr: 1.0e-02, train_loss: 0.9843, train_acc: 0.6420 test_loss: 1.9814, test_acc: 0.5674, best: 0.6510, time: 0:01:00
 Epoch: 118, lr: 1.0e-02, train_loss: 1.0419, train_acc: 0.6268 test_loss: 1.2085, test_acc: 0.6168, best: 0.6510, time: 0:01:00
 Epoch: 119, lr: 1.0e-02, train_loss: 0.9684, train_acc: 0.6570 test_loss: 1.8169, test_acc: 0.5825, best: 0.6510, time: 0:01:00
 Epoch: 120, lr: 1.0e-02, train_loss: 0.9825, train_acc: 0.6500 test_loss: 2.1086, test_acc: 0.5826, best: 0.6510, time: 0:01:00
 Epoch: 121, lr: 1.0e-02, train_loss: 0.9406, train_acc: 0.6650 test_loss: 1.2821, test_acc: 0.6285, best: 0.6510, time: 0:01:00
 Epoch: 122, lr: 1.0e-02, train_loss: 0.9783, train_acc: 0.6496 test_loss: 1.4227, test_acc: 0.6188, best: 0.6510, time: 0:01:00
 Epoch: 123, lr: 1.0e-02, train_loss: 1.0092, train_acc: 0.6378 test_loss: 1.1060, test_acc: 0.6331, best: 0.6510, time: 0:01:00
 Epoch: 124, lr: 1.0e-02, train_loss: 0.9784, train_acc: 0.6412 test_loss: 1.4014, test_acc: 0.6085, best: 0.6510, time: 0:01:00
 Epoch: 125, lr: 1.0e-02, train_loss: 0.9419, train_acc: 0.6606 test_loss: 1.5712, test_acc: 0.6054, best: 0.6510, time: 0:01:00
 Epoch: 126, lr: 1.0e-02, train_loss: 0.9548, train_acc: 0.6642 test_loss: 1.0538, test_acc: 0.6567, best: 0.6567, time: 0:01:00
 Epoch: 127, lr: 1.0e-02, train_loss: 0.9478, train_acc: 0.6584 test_loss: 1.1555, test_acc: 0.6406, best: 0.6567, time: 0:01:00
 Epoch: 128, lr: 1.0e-02, train_loss: 0.9397, train_acc: 0.6592 test_loss: 1.2813, test_acc: 0.6394, best: 0.6567, time: 0:01:00
 Epoch: 129, lr: 1.0e-02, train_loss: 0.9507, train_acc: 0.6546 test_loss: 1.2098, test_acc: 0.6421, best: 0.6567, time: 0:01:00
 Epoch: 130, lr: 1.0e-02, train_loss: 0.9281, train_acc: 0.6710 test_loss: 1.1838, test_acc: 0.6358, best: 0.6567, time: 0:01:00
 Epoch: 131, lr: 1.0e-02, train_loss: 0.9083, train_acc: 0.6838 test_loss: 2.0174, test_acc: 0.6152, best: 0.6567, time: 0:01:00
 Epoch: 132, lr: 1.0e-02, train_loss: 0.9212, train_acc: 0.6592 test_loss: 1.2148, test_acc: 0.6439, best: 0.6567, time: 0:01:00
 Epoch: 133, lr: 1.0e-02, train_loss: 0.9106, train_acc: 0.6740 test_loss: 1.1296, test_acc: 0.6531, best: 0.6567, time: 0:01:00
 Epoch: 134, lr: 1.0e-02, train_loss: 0.8858, train_acc: 0.6826 test_loss: 1.2008, test_acc: 0.6310, best: 0.6567, time: 0:01:00
 Epoch: 135, lr: 1.0e-02, train_loss: 0.9238, train_acc: 0.6734 test_loss: 1.1196, test_acc: 0.6409, best: 0.6567, time: 0:01:00
 Epoch: 136, lr: 1.0e-02, train_loss: 0.8772, train_acc: 0.6906 test_loss: 1.3930, test_acc: 0.6269, best: 0.6567, time: 0:01:00
 Epoch: 137, lr: 1.0e-02, train_loss: 0.8666, train_acc: 0.6916 test_loss: 1.7743, test_acc: 0.5876, best: 0.6567, time: 0:01:00
 Epoch: 138, lr: 1.0e-02, train_loss: 0.8964, train_acc: 0.6774 test_loss: 1.8091, test_acc: 0.6265, best: 0.6567, time: 0:01:00
 Epoch: 139, lr: 1.0e-02, train_loss: 0.8785, train_acc: 0.6862 test_loss: 4.2139, test_acc: 0.5877, best: 0.6567, time: 0:01:00
 Epoch: 140, lr: 1.0e-02, train_loss: 0.8758, train_acc: 0.6904 test_loss: 1.7803, test_acc: 0.6096, best: 0.6567, time: 0:01:00
 Epoch: 141, lr: 1.0e-02, train_loss: 0.8561, train_acc: 0.6994 test_loss: 1.5093, test_acc: 0.6406, best: 0.6567, time: 0:01:00
 Epoch: 142, lr: 1.0e-02, train_loss: 0.8803, train_acc: 0.6872 test_loss: 1.9707, test_acc: 0.6248, best: 0.6567, time: 0:01:00
 Epoch: 143, lr: 1.0e-02, train_loss: 0.8388, train_acc: 0.6996 test_loss: 1.1596, test_acc: 0.6471, best: 0.6567, time: 0:01:00
 Epoch: 144, lr: 1.0e-02, train_loss: 0.8651, train_acc: 0.6926 test_loss: 2.3894, test_acc: 0.6400, best: 0.6567, time: 0:01:00
 Epoch: 145, lr: 1.0e-02, train_loss: 0.8231, train_acc: 0.7166 test_loss: 1.4261, test_acc: 0.6331, best: 0.6567, time: 0:01:00
 Epoch: 146, lr: 1.0e-02, train_loss: 0.8205, train_acc: 0.7148 test_loss: 3.0215, test_acc: 0.6070, best: 0.6567, time: 0:01:00
 Epoch: 147, lr: 1.0e-02, train_loss: 0.8322, train_acc: 0.7030 test_loss: 3.4256, test_acc: 0.6160, best: 0.6567, time: 0:01:00
 Epoch: 148, lr: 1.0e-02, train_loss: 0.8610, train_acc: 0.6912 test_loss: 1.3266, test_acc: 0.6617, best: 0.6617, time: 0:01:00
 Epoch: 149, lr: 1.0e-02, train_loss: 0.8877, train_acc: 0.6878 test_loss: 1.9717, test_acc: 0.6285, best: 0.6617, time: 0:01:00
 Epoch: 150, lr: 1.0e-02, train_loss: 0.8490, train_acc: 0.7032 test_loss: 1.1908, test_acc: 0.6360, best: 0.6617, time: 0:01:00
 Epoch: 151, lr: 1.0e-02, train_loss: 0.8112, train_acc: 0.7192 test_loss: 1.1753, test_acc: 0.6581, best: 0.6617, time: 0:01:00
 Epoch: 152, lr: 1.0e-02, train_loss: 0.7907, train_acc: 0.7216 test_loss: 1.3192, test_acc: 0.6630, best: 0.6630, time: 0:01:00
 Epoch: 153, lr: 1.0e-02, train_loss: 0.8073, train_acc: 0.7094 test_loss: 1.1087, test_acc: 0.6562, best: 0.6630, time: 0:01:00
 Epoch: 154, lr: 1.0e-02, train_loss: 0.8445, train_acc: 0.7066 test_loss: 1.3488, test_acc: 0.6118, best: 0.6630, time: 0:01:00
 Epoch: 155, lr: 1.0e-02, train_loss: 0.8433, train_acc: 0.6992 test_loss: 1.2166, test_acc: 0.6270, best: 0.6630, time: 0:01:00
 Epoch: 156, lr: 1.0e-02, train_loss: 0.7936, train_acc: 0.7182 test_loss: 1.1370, test_acc: 0.6556, best: 0.6630, time: 0:01:00
 Epoch: 157, lr: 1.0e-02, train_loss: 0.8020, train_acc: 0.7180 test_loss: 1.7412, test_acc: 0.6160, best: 0.6630, time: 0:01:00
 Epoch: 158, lr: 1.0e-02, train_loss: 0.8283, train_acc: 0.7134 test_loss: 1.1584, test_acc: 0.6444, best: 0.6630, time: 0:01:00
 Epoch: 159, lr: 1.0e-02, train_loss: 0.7856, train_acc: 0.7238 test_loss: 1.1163, test_acc: 0.6690, best: 0.6690, time: 0:01:00
 Epoch: 160, lr: 1.0e-02, train_loss: 0.8029, train_acc: 0.7164 test_loss: 1.1909, test_acc: 0.6531, best: 0.6690, time: 0:01:00
 Epoch: 161, lr: 1.0e-02, train_loss: 0.8013, train_acc: 0.7146 test_loss: 1.1134, test_acc: 0.6621, best: 0.6690, time: 0:01:00
 Epoch: 162, lr: 1.0e-02, train_loss: 0.7922, train_acc: 0.7182 test_loss: 2.0275, test_acc: 0.6145, best: 0.6690, time: 0:01:00
 Epoch: 163, lr: 1.0e-02, train_loss: 0.7441, train_acc: 0.7384 test_loss: 2.1713, test_acc: 0.5935, best: 0.6690, time: 0:01:00
 Epoch: 164, lr: 1.0e-02, train_loss: 0.7741, train_acc: 0.7222 test_loss: 1.4644, test_acc: 0.6275, best: 0.6690, time: 0:01:00
 Epoch: 165, lr: 1.0e-02, train_loss: 0.7893, train_acc: 0.7172 test_loss: 1.5846, test_acc: 0.6302, best: 0.6690, time: 0:01:00
 Epoch: 166, lr: 1.0e-02, train_loss: 0.7634, train_acc: 0.7348 test_loss: 1.6890, test_acc: 0.6245, best: 0.6690, time: 0:01:00
 Epoch: 167, lr: 1.0e-02, train_loss: 0.7829, train_acc: 0.7218 test_loss: 1.6349, test_acc: 0.6085, best: 0.6690, time: 0:01:00
 Epoch: 168, lr: 1.0e-02, train_loss: 0.7628, train_acc: 0.7346 test_loss: 1.7419, test_acc: 0.6300, best: 0.6690, time: 0:01:00
 Epoch: 169, lr: 1.0e-02, train_loss: 0.7410, train_acc: 0.7350 test_loss: 1.2860, test_acc: 0.6444, best: 0.6690, time: 0:01:00
 Epoch: 170, lr: 1.0e-02, train_loss: 0.7672, train_acc: 0.7300 test_loss: 1.4058, test_acc: 0.6222, best: 0.6690, time: 0:01:00
 Epoch: 171, lr: 1.0e-02, train_loss: 0.7360, train_acc: 0.7436 test_loss: 1.9799, test_acc: 0.6154, best: 0.6690, time: 0:01:00
 Epoch: 172, lr: 1.0e-02, train_loss: 0.7425, train_acc: 0.7362 test_loss: 1.2635, test_acc: 0.6587, best: 0.6690, time: 0:01:00
 Epoch: 173, lr: 1.0e-02, train_loss: 0.7175, train_acc: 0.7422 test_loss: 1.4828, test_acc: 0.6501, best: 0.6690, time: 0:01:00
 Epoch: 174, lr: 1.0e-02, train_loss: 0.7058, train_acc: 0.7560 test_loss: 1.9912, test_acc: 0.6290, best: 0.6690, time: 0:01:00
 Epoch: 175, lr: 1.0e-02, train_loss: 0.7236, train_acc: 0.7388 test_loss: 1.2597, test_acc: 0.6605, best: 0.6690, time: 0:01:00
 Epoch: 176, lr: 1.0e-02, train_loss: 0.7383, train_acc: 0.7362 test_loss: 1.3766, test_acc: 0.6454, best: 0.6690, time: 0:01:00
 Epoch: 177, lr: 1.0e-02, train_loss: 0.7119, train_acc: 0.7470 test_loss: 1.6625, test_acc: 0.6422, best: 0.6690, time: 0:01:00
 Epoch: 178, lr: 1.0e-02, train_loss: 0.7259, train_acc: 0.7460 test_loss: 2.1405, test_acc: 0.6356, best: 0.6690, time: 0:01:00
 Epoch: 179, lr: 1.0e-02, train_loss: 0.7073, train_acc: 0.7586 test_loss: 1.4537, test_acc: 0.6336, best: 0.6690, time: 0:01:00
 Epoch: 180, lr: 2.0e-03, train_loss: 0.6500, train_acc: 0.7692 test_loss: 1.2720, test_acc: 0.6757, best: 0.6757, time: 0:01:00
 Epoch: 181, lr: 2.0e-03, train_loss: 0.5992, train_acc: 0.7896 test_loss: 2.1149, test_acc: 0.6445, best: 0.6757, time: 0:01:00
 Epoch: 182, lr: 2.0e-03, train_loss: 0.5894, train_acc: 0.7890 test_loss: 1.4291, test_acc: 0.6705, best: 0.6757, time: 0:01:00
 Epoch: 183, lr: 2.0e-03, train_loss: 0.5943, train_acc: 0.7938 test_loss: 2.2673, test_acc: 0.6631, best: 0.6757, time: 0:01:00
 Epoch: 184, lr: 2.0e-03, train_loss: 0.5948, train_acc: 0.7890 test_loss: 4.3215, test_acc: 0.6430, best: 0.6757, time: 0:01:00
 Epoch: 185, lr: 2.0e-03, train_loss: 0.5571, train_acc: 0.8088 test_loss: 2.9172, test_acc: 0.6439, best: 0.6757, time: 0:01:00
 Epoch: 186, lr: 2.0e-03, train_loss: 0.5785, train_acc: 0.7998 test_loss: 1.3685, test_acc: 0.6765, best: 0.6765, time: 0:01:00
 Epoch: 187, lr: 2.0e-03, train_loss: 0.5477, train_acc: 0.8102 test_loss: 1.6811, test_acc: 0.6623, best: 0.6765, time: 0:01:00
 Epoch: 188, lr: 2.0e-03, train_loss: 0.5433, train_acc: 0.8068 test_loss: 1.3124, test_acc: 0.6829, best: 0.6829, time: 0:01:00
 Epoch: 189, lr: 2.0e-03, train_loss: 0.5448, train_acc: 0.8128 test_loss: 2.2045, test_acc: 0.6697, best: 0.6829, time: 0:01:00
 Epoch: 190, lr: 2.0e-03, train_loss: 0.5685, train_acc: 0.8066 test_loss: 2.3441, test_acc: 0.6616, best: 0.6829, time: 0:01:00
 Epoch: 191, lr: 2.0e-03, train_loss: 0.5694, train_acc: 0.8070 test_loss: 2.0143, test_acc: 0.6609, best: 0.6829, time: 0:01:00
 Epoch: 192, lr: 2.0e-03, train_loss: 0.5364, train_acc: 0.8176 test_loss: 1.9808, test_acc: 0.6727, best: 0.6829, time: 0:01:00
 Epoch: 193, lr: 2.0e-03, train_loss: 0.5737, train_acc: 0.8034 test_loss: 2.3428, test_acc: 0.6617, best: 0.6829, time: 0:01:00
 Epoch: 194, lr: 2.0e-03, train_loss: 0.5524, train_acc: 0.8098 test_loss: 7.1396, test_acc: 0.6621, best: 0.6829, time: 0:01:00
 Epoch: 195, lr: 2.0e-03, train_loss: 0.5532, train_acc: 0.8112 test_loss: 2.7839, test_acc: 0.6787, best: 0.6829, time: 0:01:00
 Epoch: 196, lr: 2.0e-03, train_loss: 0.5474, train_acc: 0.8092 test_loss: 2.8477, test_acc: 0.6823, best: 0.6829, time: 0:01:00
 Epoch: 197, lr: 2.0e-03, train_loss: 0.5361, train_acc: 0.8182 test_loss: 5.9768, test_acc: 0.6370, best: 0.6829, time: 0:01:00
 Epoch: 198, lr: 2.0e-03, train_loss: 0.5264, train_acc: 0.8170 test_loss: 4.6422, test_acc: 0.6606, best: 0.6829, time: 0:01:00
 Epoch: 199, lr: 2.0e-03, train_loss: 0.5635, train_acc: 0.8020 test_loss: 4.8262, test_acc: 0.6639, best: 0.6829, time: 0:01:00
 Epoch: 200, lr: 2.0e-03, train_loss: 0.5164, train_acc: 0.8186 test_loss: 2.5777, test_acc: 0.6775, best: 0.6829, time: 0:01:00
 Epoch: 201, lr: 2.0e-03, train_loss: 0.5299, train_acc: 0.8156 test_loss: 2.5466, test_acc: 0.6795, best: 0.6829, time: 0:01:00
 Epoch: 202, lr: 2.0e-03, train_loss: 0.5051, train_acc: 0.8208 test_loss: 1.4113, test_acc: 0.6841, best: 0.6841, time: 0:01:00
 Epoch: 203, lr: 2.0e-03, train_loss: 0.5235, train_acc: 0.8186 test_loss: 1.8335, test_acc: 0.6746, best: 0.6841, time: 0:01:00
 Epoch: 204, lr: 2.0e-03, train_loss: 0.5424, train_acc: 0.8146 test_loss: 1.9206, test_acc: 0.6696, best: 0.6841, time: 0:01:00
 Epoch: 205, lr: 2.0e-03, train_loss: 0.4868, train_acc: 0.8334 test_loss: 2.4412, test_acc: 0.6458, best: 0.6841, time: 0:01:00
 Epoch: 206, lr: 2.0e-03, train_loss: 0.5202, train_acc: 0.8172 test_loss: 1.9173, test_acc: 0.6737, best: 0.6841, time: 0:01:00
 Epoch: 207, lr: 2.0e-03, train_loss: 0.5245, train_acc: 0.8208 test_loss: 2.8083, test_acc: 0.6442, best: 0.6841, time: 0:01:00
 Epoch: 208, lr: 2.0e-03, train_loss: 0.5206, train_acc: 0.8204 test_loss: 2.5090, test_acc: 0.6604, best: 0.6841, time: 0:01:00
 Epoch: 209, lr: 2.0e-03, train_loss: 0.5161, train_acc: 0.8188 test_loss: 1.8586, test_acc: 0.6685, best: 0.6841, time: 0:01:00
 Epoch: 210, lr: 2.0e-03, train_loss: 0.4891, train_acc: 0.8330 test_loss: 1.8237, test_acc: 0.6763, best: 0.6841, time: 0:01:00
 Epoch: 211, lr: 2.0e-03, train_loss: 0.5033, train_acc: 0.8188 test_loss: 2.1045, test_acc: 0.6599, best: 0.6841, time: 0:01:00
 Epoch: 212, lr: 2.0e-03, train_loss: 0.4807, train_acc: 0.8316 test_loss: 1.9641, test_acc: 0.6644, best: 0.6841, time: 0:01:00
 Epoch: 213, lr: 2.0e-03, train_loss: 0.5027, train_acc: 0.8286 test_loss: 2.1663, test_acc: 0.6600, best: 0.6841, time: 0:01:00
 Epoch: 214, lr: 2.0e-03, train_loss: 0.5006, train_acc: 0.8316 test_loss: 2.2000, test_acc: 0.6573, best: 0.6841, time: 0:01:00
 Epoch: 215, lr: 2.0e-03, train_loss: 0.4878, train_acc: 0.8276 test_loss: 2.1820, test_acc: 0.6538, best: 0.6841, time: 0:01:00
 Epoch: 216, lr: 2.0e-03, train_loss: 0.4908, train_acc: 0.8374 test_loss: 1.9148, test_acc: 0.6627, best: 0.6841, time: 0:01:00
 Epoch: 217, lr: 2.0e-03, train_loss: 0.5019, train_acc: 0.8264 test_loss: 1.3763, test_acc: 0.6764, best: 0.6841, time: 0:01:00
 Epoch: 218, lr: 2.0e-03, train_loss: 0.5098, train_acc: 0.8216 test_loss: 2.0402, test_acc: 0.6657, best: 0.6841, time: 0:01:00
 Epoch: 219, lr: 2.0e-03, train_loss: 0.5037, train_acc: 0.8260 test_loss: 1.4931, test_acc: 0.6724, best: 0.6841, time: 0:01:00
 Epoch: 220, lr: 2.0e-03, train_loss: 0.5024, train_acc: 0.8260 test_loss: 1.8291, test_acc: 0.6586, best: 0.6841, time: 0:01:00
 Epoch: 221, lr: 2.0e-03, train_loss: 0.4894, train_acc: 0.8338 test_loss: 1.4494, test_acc: 0.6820, best: 0.6841, time: 0:01:00
 Epoch: 222, lr: 2.0e-03, train_loss: 0.4893, train_acc: 0.8320 test_loss: 3.0575, test_acc: 0.6435, best: 0.6841, time: 0:01:00
 Epoch: 223, lr: 2.0e-03, train_loss: 0.4753, train_acc: 0.8386 test_loss: 1.3840, test_acc: 0.6827, best: 0.6841, time: 0:01:00
 Epoch: 224, lr: 2.0e-03, train_loss: 0.4665, train_acc: 0.8400 test_loss: 1.6020, test_acc: 0.6730, best: 0.6841, time: 0:01:00
 Epoch: 225, lr: 2.0e-03, train_loss: 0.5013, train_acc: 0.8272 test_loss: 1.4118, test_acc: 0.6836, best: 0.6841, time: 0:01:00
 Epoch: 226, lr: 2.0e-03, train_loss: 0.5017, train_acc: 0.8278 test_loss: 1.5345, test_acc: 0.6770, best: 0.6841, time: 0:01:00
 Epoch: 227, lr: 2.0e-03, train_loss: 0.4938, train_acc: 0.8304 test_loss: 1.2590, test_acc: 0.6870, best: 0.6870, time: 0:01:00
 Epoch: 228, lr: 2.0e-03, train_loss: 0.4825, train_acc: 0.8322 test_loss: 1.2289, test_acc: 0.6864, best: 0.6870, time: 0:01:00
 Epoch: 229, lr: 2.0e-03, train_loss: 0.4772, train_acc: 0.8384 test_loss: 1.4289, test_acc: 0.6729, best: 0.6870, time: 0:01:00
 Epoch: 230, lr: 2.0e-03, train_loss: 0.4822, train_acc: 0.8386 test_loss: 1.6602, test_acc: 0.6641, best: 0.6870, time: 0:01:00
 Epoch: 231, lr: 2.0e-03, train_loss: 0.4663, train_acc: 0.8406 test_loss: 1.3671, test_acc: 0.6836, best: 0.6870, time: 0:01:00
 Epoch: 232, lr: 2.0e-03, train_loss: 0.4773, train_acc: 0.8372 test_loss: 1.1539, test_acc: 0.6907, best: 0.6907, time: 0:01:00
 Epoch: 233, lr: 2.0e-03, train_loss: 0.4939, train_acc: 0.8284 test_loss: 1.2873, test_acc: 0.6846, best: 0.6907, time: 0:01:00
 Epoch: 234, lr: 2.0e-03, train_loss: 0.4753, train_acc: 0.8352 test_loss: 1.9370, test_acc: 0.6355, best: 0.6907, time: 0:01:00
 Epoch: 235, lr: 2.0e-03, train_loss: 0.4674, train_acc: 0.8378 test_loss: 1.7864, test_acc: 0.6607, best: 0.6907, time: 0:01:00
 Epoch: 236, lr: 2.0e-03, train_loss: 0.4839, train_acc: 0.8350 test_loss: 2.4921, test_acc: 0.6198, best: 0.6907, time: 0:01:00
 Epoch: 237, lr: 2.0e-03, train_loss: 0.4777, train_acc: 0.8322 test_loss: 1.6719, test_acc: 0.6660, best: 0.6907, time: 0:01:00
 Epoch: 238, lr: 2.0e-03, train_loss: 0.4811, train_acc: 0.8388 test_loss: 2.5207, test_acc: 0.6386, best: 0.6907, time: 0:01:00
 Epoch: 239, lr: 2.0e-03, train_loss: 0.4590, train_acc: 0.8426 test_loss: 3.1215, test_acc: 0.6215, best: 0.6907, time: 0:01:00
 Epoch: 240, lr: 4.0e-04, train_loss: 0.4646, train_acc: 0.8416 test_loss: 1.9374, test_acc: 0.6600, best: 0.6907, time: 0:01:00
 Epoch: 241, lr: 4.0e-04, train_loss: 0.4677, train_acc: 0.8456 test_loss: 1.5304, test_acc: 0.6817, best: 0.6907, time: 0:01:00
 Epoch: 242, lr: 4.0e-04, train_loss: 0.4582, train_acc: 0.8432 test_loss: 1.2319, test_acc: 0.6949, best: 0.6949, time: 0:01:00
 Epoch: 243, lr: 4.0e-04, train_loss: 0.4583, train_acc: 0.8432 test_loss: 1.4171, test_acc: 0.6863, best: 0.6949, time: 0:01:00
 Epoch: 244, lr: 4.0e-04, train_loss: 0.4488, train_acc: 0.8438 test_loss: 2.1659, test_acc: 0.6508, best: 0.6949, time: 0:01:00
 Epoch: 245, lr: 4.0e-04, train_loss: 0.4361, train_acc: 0.8510 test_loss: 2.0831, test_acc: 0.6574, best: 0.6949, time: 0:01:00
 Epoch: 246, lr: 4.0e-04, train_loss: 0.4463, train_acc: 0.8524 test_loss: 1.9348, test_acc: 0.6604, best: 0.6949, time: 0:01:00
 Epoch: 247, lr: 4.0e-04, train_loss: 0.4450, train_acc: 0.8454 test_loss: 2.1224, test_acc: 0.6522, best: 0.6949, time: 0:01:00
 Epoch: 248, lr: 4.0e-04, train_loss: 0.4345, train_acc: 0.8486 test_loss: 1.7079, test_acc: 0.6731, best: 0.6949, time: 0:01:00
 Epoch: 249, lr: 4.0e-04, train_loss: 0.4427, train_acc: 0.8426 test_loss: 2.4648, test_acc: 0.6261, best: 0.6949, time: 0:01:00
 Epoch: 250, lr: 4.0e-04, train_loss: 0.4278, train_acc: 0.8516 test_loss: 1.7718, test_acc: 0.6631, best: 0.6949, time: 0:01:00
 Epoch: 251, lr: 4.0e-04, train_loss: 0.4465, train_acc: 0.8444 test_loss: 1.4506, test_acc: 0.6813, best: 0.6949, time: 0:01:00
 Epoch: 252, lr: 4.0e-04, train_loss: 0.4441, train_acc: 0.8446 test_loss: 1.5748, test_acc: 0.6675, best: 0.6949, time: 0:01:00
 Epoch: 253, lr: 4.0e-04, train_loss: 0.4300, train_acc: 0.8548 test_loss: 1.6441, test_acc: 0.6613, best: 0.6949, time: 0:01:00
 Epoch: 254, lr: 4.0e-04, train_loss: 0.4477, train_acc: 0.8496 test_loss: 1.7654, test_acc: 0.6643, best: 0.6949, time: 0:01:00
 Epoch: 255, lr: 4.0e-04, train_loss: 0.4151, train_acc: 0.8576 test_loss: 1.6302, test_acc: 0.6720, best: 0.6949, time: 0:01:00
 Epoch: 256, lr: 4.0e-04, train_loss: 0.4456, train_acc: 0.8476 test_loss: 2.1720, test_acc: 0.6399, best: 0.6949, time: 0:01:00
 Epoch: 257, lr: 4.0e-04, train_loss: 0.4340, train_acc: 0.8474 test_loss: 1.7752, test_acc: 0.6591, best: 0.6949, time: 0:01:09
 Epoch: 258, lr: 4.0e-04, train_loss: 0.4342, train_acc: 0.8472 test_loss: 1.3841, test_acc: 0.6799, best: 0.6949, time: 0:01:00
 Epoch: 259, lr: 4.0e-04, train_loss: 0.4482, train_acc: 0.8404 test_loss: 1.6125, test_acc: 0.6726, best: 0.6949, time: 0:00:58
 Epoch: 260, lr: 4.0e-04, train_loss: 0.4372, train_acc: 0.8484 test_loss: 2.2897, test_acc: 0.6386, best: 0.6949, time: 0:00:58
 Epoch: 261, lr: 4.0e-04, train_loss: 0.4170, train_acc: 0.8566 test_loss: 1.3029, test_acc: 0.6875, best: 0.6949, time: 0:00:58
 Epoch: 262, lr: 4.0e-04, train_loss: 0.4262, train_acc: 0.8528 test_loss: 2.0307, test_acc: 0.6435, best: 0.6949, time: 0:00:58
 Epoch: 263, lr: 4.0e-04, train_loss: 0.4129, train_acc: 0.8542 test_loss: 3.8537, test_acc: 0.5815, best: 0.6949, time: 0:00:58
 Epoch: 264, lr: 4.0e-04, train_loss: 0.4338, train_acc: 0.8536 test_loss: 1.4045, test_acc: 0.6810, best: 0.6949, time: 0:00:58
 Epoch: 265, lr: 4.0e-04, train_loss: 0.4317, train_acc: 0.8546 test_loss: 1.4163, test_acc: 0.6796, best: 0.6949, time: 0:00:58
 Epoch: 266, lr: 4.0e-04, train_loss: 0.4237, train_acc: 0.8532 test_loss: 1.5694, test_acc: 0.6707, best: 0.6949, time: 0:00:58
 Epoch: 267, lr: 4.0e-04, train_loss: 0.4307, train_acc: 0.8496 test_loss: 1.9889, test_acc: 0.6502, best: 0.6949, time: 0:00:58
 Epoch: 268, lr: 4.0e-04, train_loss: 0.4431, train_acc: 0.8530 test_loss: 1.5873, test_acc: 0.6754, best: 0.6949, time: 0:00:58
 Epoch: 269, lr: 4.0e-04, train_loss: 0.4365, train_acc: 0.8442 test_loss: 2.4082, test_acc: 0.6315, best: 0.6949, time: 0:00:58
 Epoch: 270, lr: 8.0e-05, train_loss: 0.4462, train_acc: 0.8568 test_loss: 1.9682, test_acc: 0.6501, best: 0.6949, time: 0:00:58
 Epoch: 271, lr: 8.0e-05, train_loss: 0.4065, train_acc: 0.8556 test_loss: 2.5578, test_acc: 0.6290, best: 0.6949, time: 0:00:58
 Epoch: 272, lr: 8.0e-05, train_loss: 0.4255, train_acc: 0.8612 test_loss: 1.8887, test_acc: 0.6581, best: 0.6949, time: 0:00:58
 Epoch: 273, lr: 8.0e-05, train_loss: 0.4283, train_acc: 0.8552 test_loss: 2.1927, test_acc: 0.6410, best: 0.6949, time: 0:00:58
 Epoch: 274, lr: 8.0e-05, train_loss: 0.4056, train_acc: 0.8602 test_loss: 1.7683, test_acc: 0.6616, best: 0.6949, time: 0:00:58
 Epoch: 275, lr: 8.0e-05, train_loss: 0.4150, train_acc: 0.8534 test_loss: 1.9154, test_acc: 0.6600, best: 0.6949, time: 0:00:58
 Epoch: 276, lr: 8.0e-05, train_loss: 0.4199, train_acc: 0.8528 test_loss: 1.4304, test_acc: 0.6841, best: 0.6949, time: 0:00:57
 Epoch: 277, lr: 8.0e-05, train_loss: 0.4143, train_acc: 0.8530 test_loss: 1.6080, test_acc: 0.6704, best: 0.6949, time: 0:00:57
 Epoch: 278, lr: 8.0e-05, train_loss: 0.4275, train_acc: 0.8572 test_loss: 1.4367, test_acc: 0.6843, best: 0.6949, time: 0:00:57
 Epoch: 279, lr: 8.0e-05, train_loss: 0.4140, train_acc: 0.8576 test_loss: 1.4600, test_acc: 0.6803, best: 0.6949, time: 0:00:57
 Epoch: 280, lr: 8.0e-05, train_loss: 0.4264, train_acc: 0.8550 test_loss: 1.5349, test_acc: 0.6766, best: 0.6949, time: 0:00:58
 Epoch: 281, lr: 8.0e-05, train_loss: 0.4237, train_acc: 0.8562 test_loss: 1.6625, test_acc: 0.6707, best: 0.6949, time: 0:00:58
 Epoch: 282, lr: 8.0e-05, train_loss: 0.4292, train_acc: 0.8522 test_loss: 2.0055, test_acc: 0.6581, best: 0.6949, time: 0:00:58
 Epoch: 283, lr: 8.0e-05, train_loss: 0.3928, train_acc: 0.8666 test_loss: 1.3808, test_acc: 0.6866, best: 0.6949, time: 0:00:58
 Epoch: 284, lr: 8.0e-05, train_loss: 0.4255, train_acc: 0.8580 test_loss: 1.5495, test_acc: 0.6773, best: 0.6949, time: 0:00:58
 Epoch: 285, lr: 8.0e-05, train_loss: 0.3967, train_acc: 0.8668 test_loss: 1.3985, test_acc: 0.6863, best: 0.6949, time: 0:00:58
 Epoch: 286, lr: 8.0e-05, train_loss: 0.4339, train_acc: 0.8526 test_loss: 1.2380, test_acc: 0.6990, best: 0.6990, time: 0:00:58
 Epoch: 287, lr: 8.0e-05, train_loss: 0.4331, train_acc: 0.8548 test_loss: 1.4134, test_acc: 0.6790, best: 0.6990, time: 0:00:58
 Epoch: 288, lr: 8.0e-05, train_loss: 0.4106, train_acc: 0.8564 test_loss: 2.0615, test_acc: 0.6468, best: 0.6990, time: 0:00:58
 Epoch: 289, lr: 8.0e-05, train_loss: 0.4242, train_acc: 0.8608 test_loss: 2.1628, test_acc: 0.6480, best: 0.6990, time: 0:00:58
 Epoch: 290, lr: 8.0e-05, train_loss: 0.4085, train_acc: 0.8580 test_loss: 1.4314, test_acc: 0.6784, best: 0.6990, time: 0:00:58
 Epoch: 291, lr: 8.0e-05, train_loss: 0.4372, train_acc: 0.8488 test_loss: 1.6791, test_acc: 0.6660, best: 0.6990, time: 0:00:58
 Epoch: 292, lr: 8.0e-05, train_loss: 0.4103, train_acc: 0.8594 test_loss: 2.4939, test_acc: 0.6319, best: 0.6990, time: 0:00:58
 Epoch: 293, lr: 8.0e-05, train_loss: 0.4254, train_acc: 0.8512 test_loss: 1.7172, test_acc: 0.6680, best: 0.6990, time: 0:00:58
 Epoch: 294, lr: 8.0e-05, train_loss: 0.4092, train_acc: 0.8572 test_loss: 1.5295, test_acc: 0.6766, best: 0.6990, time: 0:00:58
 Epoch: 295, lr: 8.0e-05, train_loss: 0.4145, train_acc: 0.8576 test_loss: 1.3419, test_acc: 0.6894, best: 0.6990, time: 0:00:58
 Epoch: 296, lr: 8.0e-05, train_loss: 0.4225, train_acc: 0.8530 test_loss: 2.0672, test_acc: 0.6439, best: 0.6990, time: 0:00:58
 Epoch: 297, lr: 8.0e-05, train_loss: 0.4178, train_acc: 0.8544 test_loss: 1.4760, test_acc: 0.6725, best: 0.6990, time: 0:00:58
 Epoch: 298, lr: 8.0e-05, train_loss: 0.4297, train_acc: 0.8522 test_loss: 2.7453, test_acc: 0.6274, best: 0.6990, time: 0:00:58
 Epoch: 299, lr: 8.0e-05, train_loss: 0.4103, train_acc: 0.8628 test_loss: 1.6593, test_acc: 0.6691, best: 0.6990, time: 0:00:58
 Highest accuracy: 0.6990