
 Run on time: 2022-06-29 16:01:01.601783

 Arguments:
	 gpu                  : True
	 log                  : True
	 seed                 : 0
	 momentum             : 0.9
	 amsgrad              : True
	 dataset              : STL10
	 batch_size           : 8
	 architecture         : RESNET18_GAUSSIAN_POOL_4222
	 im_size              : 128
	 relu_threshold       : 4.0
	 learning_rate        : 0.01
	 pretrained_backbone  : 
	 pretrained_ann       : 
	 weight_decay         : 0.0
	 test_only            : False
	 epochs               : 300
	 lr_interval          : [180, 240, 270]
	 lr_reduce            : 5
	 optimizer            : SGD
	 dropout              : 0.2
	 kernel_size          : 3
	 dont_save            : False
	 visualize            : False
	 devices              : 0
 DataParallel(
  (module): NetworkByName(
    (net): ResNet_v2(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (downsample): Sequential(
            (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (pooling): GaussianPooling2d(
            kernel_size=4, stride=4, padding=0
            (ToHidden): Sequential(
              (0): AdaptiveAvgPool2d(output_size=(1, 1))
              (1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
              (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (3): ReLU()
            )
            (ToMean): Sequential(
              (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (ToSigma): Sequential(
              (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Sigmoid()
            )
            (activation): Softplus(beta=1, threshold=20)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (layer2): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (downsample): Sequential(
            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (pooling): GaussianPooling2d(
            kernel_size=2, stride=2, padding=0
            (ToHidden): Sequential(
              (0): AdaptiveAvgPool2d(output_size=(1, 1))
              (1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
              (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (3): ReLU()
            )
            (ToMean): Sequential(
              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (ToSigma): Sequential(
              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Sigmoid()
            )
            (activation): Softplus(beta=1, threshold=20)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (layer3): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (downsample): Sequential(
            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (pooling): GaussianPooling2d(
            kernel_size=2, stride=2, padding=0
            (ToHidden): Sequential(
              (0): AdaptiveAvgPool2d(output_size=(1, 1))
              (1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
              (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (3): ReLU()
            )
            (ToMean): Sequential(
              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (ToSigma): Sequential(
              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Sigmoid()
            )
            (activation): Softplus(beta=1, threshold=20)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (layer4): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (pooling): GaussianPooling2d(
            kernel_size=2, stride=2, padding=0
            (ToHidden): Sequential(
              (0): AdaptiveAvgPool2d(output_size=(1, 1))
              (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
              (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (3): ReLU()
            )
            (ToMean): Sequential(
              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (ToSigma): Sequential(
              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Sigmoid()
            )
            (activation): Softplus(beta=1, threshold=20)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
      (fc): Linear(in_features=512, out_features=10, bias=True)
    )
  )
)
 SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0
)
 Epoch: 1, lr: 1.0e-02, train_loss: 3.1095, train_acc: 0.1580 test_loss: 2.4875, test_acc: 0.2034, best: 0.2034, time: 0:00:49
 Epoch: 2, lr: 1.0e-02, train_loss: 2.3688, train_acc: 0.1846 test_loss: 16.0008, test_acc: 0.2559, best: 0.2559, time: 0:00:49
 Epoch: 3, lr: 1.0e-02, train_loss: 2.1025, train_acc: 0.2276 test_loss: 73.6056, test_acc: 0.2499, best: 0.2559, time: 0:00:49
 Epoch: 4, lr: 1.0e-02, train_loss: 2.0123, train_acc: 0.2466 test_loss: 41.2942, test_acc: 0.3167, best: 0.3167, time: 0:00:49
 Epoch: 5, lr: 1.0e-02, train_loss: 1.9464, train_acc: 0.2654 test_loss: 57.1486, test_acc: 0.3573, best: 0.3573, time: 0:00:49
 Epoch: 6, lr: 1.0e-02, train_loss: 1.8840, train_acc: 0.2840 test_loss: 7.4508, test_acc: 0.3663, best: 0.3663, time: 0:00:49
 Epoch: 7, lr: 1.0e-02, train_loss: 1.8617, train_acc: 0.3014 test_loss: 67.5207, test_acc: 0.3561, best: 0.3663, time: 0:00:49
 Epoch: 8, lr: 1.0e-02, train_loss: 1.8059, train_acc: 0.3184 test_loss: 76929.9431, test_acc: 0.4096, best: 0.4096, time: 0:00:49
 Epoch: 9, lr: 1.0e-02, train_loss: 1.7854, train_acc: 0.3260 test_loss: 11539.5534, test_acc: 0.3664, best: 0.4096, time: 0:00:49
 Epoch: 10, lr: 1.0e-02, train_loss: 1.7419, train_acc: 0.3488 test_loss: 14067.9279, test_acc: 0.4062, best: 0.4096, time: 0:00:49
 Epoch: 11, lr: 1.0e-02, train_loss: 1.7132, train_acc: 0.3574 test_loss: 17105.6860, test_acc: 0.4034, best: 0.4096, time: 0:00:49
 Epoch: 12, lr: 1.0e-02, train_loss: 1.6747, train_acc: 0.3770 test_loss: 4754.3762, test_acc: 0.4480, best: 0.4480, time: 0:00:49
 Epoch: 13, lr: 1.0e-02, train_loss: 1.6547, train_acc: 0.3824 test_loss: 43.0152, test_acc: 0.4740, best: 0.4740, time: 0:00:49
 Epoch: 14, lr: 1.0e-02, train_loss: 1.6181, train_acc: 0.4038 test_loss: 1200.3784, test_acc: 0.4689, best: 0.4740, time: 0:00:49
 Epoch: 15, lr: 1.0e-02, train_loss: 1.6066, train_acc: 0.4108 test_loss: 64.9180, test_acc: 0.4721, best: 0.4740, time: 0:00:49
 Epoch: 16, lr: 1.0e-02, train_loss: 1.6369, train_acc: 0.3962 test_loss: 3.4101, test_acc: 0.4370, best: 0.4740, time: 0:00:49
 Epoch: 17, lr: 1.0e-02, train_loss: 1.5683, train_acc: 0.4216 test_loss: 3198.2300, test_acc: 0.4569, best: 0.4740, time: 0:00:49
 Epoch: 18, lr: 1.0e-02, train_loss: 1.5874, train_acc: 0.4208 test_loss: 1109.3780, test_acc: 0.4735, best: 0.4740, time: 0:00:49
 Epoch: 19, lr: 1.0e-02, train_loss: 1.5541, train_acc: 0.4308 test_loss: 1864.7073, test_acc: 0.5071, best: 0.5071, time: 0:00:49
 Epoch: 20, lr: 1.0e-02, train_loss: 1.5032, train_acc: 0.4432 test_loss: 2.0355, test_acc: 0.5155, best: 0.5155, time: 0:00:49
 Epoch: 21, lr: 1.0e-02, train_loss: 1.5029, train_acc: 0.4476 test_loss: 2.6155, test_acc: 0.5078, best: 0.5155, time: 0:00:49
 Epoch: 22, lr: 1.0e-02, train_loss: 1.5182, train_acc: 0.4456 test_loss: 336.7194, test_acc: 0.5012, best: 0.5155, time: 0:00:49
 Epoch: 23, lr: 1.0e-02, train_loss: 1.4999, train_acc: 0.4498 test_loss: 39.9087, test_acc: 0.5091, best: 0.5155, time: 0:00:49
 Epoch: 24, lr: 1.0e-02, train_loss: 1.5126, train_acc: 0.4518 test_loss: 13.7977, test_acc: 0.5011, best: 0.5155, time: 0:00:49
 Epoch: 25, lr: 1.0e-02, train_loss: 1.4601, train_acc: 0.4682 test_loss: 97.5842, test_acc: 0.5029, best: 0.5155, time: 0:00:49
 Epoch: 26, lr: 1.0e-02, train_loss: 1.4283, train_acc: 0.4760 test_loss: 33.0959, test_acc: 0.5240, best: 0.5240, time: 0:00:49
 Epoch: 27, lr: 1.0e-02, train_loss: 1.4084, train_acc: 0.4858 test_loss: 160.9957, test_acc: 0.5159, best: 0.5240, time: 0:00:49
 Epoch: 28, lr: 1.0e-02, train_loss: 1.3843, train_acc: 0.5038 test_loss: 1557.8155, test_acc: 0.5429, best: 0.5429, time: 0:00:49
 Epoch: 29, lr: 1.0e-02, train_loss: 1.3688, train_acc: 0.4940 test_loss: 214.9583, test_acc: 0.5245, best: 0.5429, time: 0:00:49
 Epoch: 30, lr: 1.0e-02, train_loss: 1.3318, train_acc: 0.5162 test_loss: 92.3132, test_acc: 0.5232, best: 0.5429, time: 0:00:49
 Epoch: 31, lr: 1.0e-02, train_loss: 1.3401, train_acc: 0.5096 test_loss: 5291.7597, test_acc: 0.5420, best: 0.5429, time: 0:00:49
 Epoch: 32, lr: 1.0e-02, train_loss: 1.2995, train_acc: 0.5284 test_loss: 647.4450, test_acc: 0.4995, best: 0.5429, time: 0:00:49
 Epoch: 33, lr: 1.0e-02, train_loss: 1.3193, train_acc: 0.5172 test_loss: 35.8655, test_acc: 0.5410, best: 0.5429, time: 0:00:49
 Epoch: 34, lr: 1.0e-02, train_loss: 1.3269, train_acc: 0.5126 test_loss: 152.1746, test_acc: 0.5284, best: 0.5429, time: 0:00:49
 Epoch: 35, lr: 1.0e-02, train_loss: 1.3307, train_acc: 0.5092 test_loss: 61.3533, test_acc: 0.5286, best: 0.5429, time: 0:00:49
 Epoch: 36, lr: 1.0e-02, train_loss: 1.2714, train_acc: 0.5324 test_loss: 38.5700, test_acc: 0.5331, best: 0.5429, time: 0:00:49
 Epoch: 37, lr: 1.0e-02, train_loss: 1.2612, train_acc: 0.5492 test_loss: 147.9446, test_acc: 0.5347, best: 0.5429, time: 0:00:49
 Epoch: 38, lr: 1.0e-02, train_loss: 1.2683, train_acc: 0.5406 test_loss: 10.8587, test_acc: 0.5744, best: 0.5744, time: 0:00:49
 Epoch: 39, lr: 1.0e-02, train_loss: 1.2275, train_acc: 0.5606 test_loss: 23.6703, test_acc: 0.5657, best: 0.5744, time: 0:00:49
 Epoch: 40, lr: 1.0e-02, train_loss: 1.2196, train_acc: 0.5650 test_loss: 5.5426, test_acc: 0.5853, best: 0.5853, time: 0:00:49
 Epoch: 41, lr: 1.0e-02, train_loss: 1.2430, train_acc: 0.5470 test_loss: 814.0007, test_acc: 0.5523, best: 0.5853, time: 0:00:49
 Epoch: 42, lr: 1.0e-02, train_loss: 1.1943, train_acc: 0.5686 test_loss: 9.8303, test_acc: 0.5730, best: 0.5853, time: 0:00:49
 Epoch: 43, lr: 1.0e-02, train_loss: 1.1838, train_acc: 0.5692 test_loss: 66.3151, test_acc: 0.5809, best: 0.5853, time: 0:00:49
 Epoch: 44, lr: 1.0e-02, train_loss: 1.1885, train_acc: 0.5686 test_loss: 1419.3870, test_acc: 0.5276, best: 0.5853, time: 0:00:49
 Epoch: 45, lr: 1.0e-02, train_loss: 1.1783, train_acc: 0.5750 test_loss: 72.6653, test_acc: 0.5774, best: 0.5853, time: 0:00:49
 Epoch: 46, lr: 1.0e-02, train_loss: 1.1720, train_acc: 0.5776 test_loss: 236.3032, test_acc: 0.5416, best: 0.5853, time: 0:00:49
 Epoch: 47, lr: 1.0e-02, train_loss: 1.1779, train_acc: 0.5730 test_loss: 455.7505, test_acc: 0.5660, best: 0.5853, time: 0:00:49
 Epoch: 48, lr: 1.0e-02, train_loss: 1.1524, train_acc: 0.5836 test_loss: 293.5776, test_acc: 0.5671, best: 0.5853, time: 0:00:49
 Epoch: 49, lr: 1.0e-02, train_loss: 1.1199, train_acc: 0.6064 test_loss: 947.7369, test_acc: 0.5324, best: 0.5853, time: 0:00:49
 Epoch: 50, lr: 1.0e-02, train_loss: 1.1088, train_acc: 0.6006 test_loss: 100.8731, test_acc: 0.5877, best: 0.5877, time: 0:00:49
 Epoch: 51, lr: 1.0e-02, train_loss: 1.1191, train_acc: 0.5996 test_loss: 47.6757, test_acc: 0.5633, best: 0.5877, time: 0:00:49
 Epoch: 52, lr: 1.0e-02, train_loss: 1.1271, train_acc: 0.5960 test_loss: 778.3159, test_acc: 0.5296, best: 0.5877, time: 0:00:49
 Epoch: 53, lr: 1.0e-02, train_loss: 1.0948, train_acc: 0.6126 test_loss: 70.5006, test_acc: 0.6068, best: 0.6068, time: 0:00:49
 Epoch: 54, lr: 1.0e-02, train_loss: 1.0959, train_acc: 0.6042 test_loss: 220.6152, test_acc: 0.5816, best: 0.6068, time: 0:00:49
 Epoch: 55, lr: 1.0e-02, train_loss: 1.0846, train_acc: 0.6134 test_loss: 22.8637, test_acc: 0.5974, best: 0.6068, time: 0:00:49
 Epoch: 56, lr: 1.0e-02, train_loss: 1.0538, train_acc: 0.6198 test_loss: 46.5219, test_acc: 0.6092, best: 0.6092, time: 0:00:49
 Epoch: 57, lr: 1.0e-02, train_loss: 1.0785, train_acc: 0.6218 test_loss: 26.5395, test_acc: 0.6255, best: 0.6255, time: 0:00:49
 Epoch: 58, lr: 1.0e-02, train_loss: 1.0435, train_acc: 0.6290 test_loss: 185.0866, test_acc: 0.5945, best: 0.6255, time: 0:00:49
 Epoch: 59, lr: 1.0e-02, train_loss: 1.0580, train_acc: 0.6206 test_loss: 171.2654, test_acc: 0.5571, best: 0.6255, time: 0:00:49
 Epoch: 60, lr: 1.0e-02, train_loss: 1.0263, train_acc: 0.6294 test_loss: 700.1941, test_acc: 0.5278, best: 0.6255, time: 0:00:49
 Epoch: 61, lr: 1.0e-02, train_loss: 1.0034, train_acc: 0.6434 test_loss: 1313.4640, test_acc: 0.5604, best: 0.6255, time: 0:00:49
 Epoch: 62, lr: 1.0e-02, train_loss: 1.0283, train_acc: 0.6270 test_loss: 6329.7481, test_acc: 0.5491, best: 0.6255, time: 0:00:49
 Epoch: 63, lr: 1.0e-02, train_loss: 0.9915, train_acc: 0.6484 test_loss: 143.4081, test_acc: 0.5971, best: 0.6255, time: 0:00:49
 Epoch: 64, lr: 1.0e-02, train_loss: 0.9929, train_acc: 0.6468 test_loss: 1296.6899, test_acc: 0.5410, best: 0.6255, time: 0:00:49
 Epoch: 65, lr: 1.0e-02, train_loss: 1.0013, train_acc: 0.6530 test_loss: 680.5215, test_acc: 0.5521, best: 0.6255, time: 0:00:48
 Epoch: 66, lr: 1.0e-02, train_loss: 0.9776, train_acc: 0.6438 test_loss: 334.4602, test_acc: 0.5971, best: 0.6255, time: 0:00:48
 Epoch: 67, lr: 1.0e-02, train_loss: 0.9763, train_acc: 0.6464 test_loss: 41.0702, test_acc: 0.6492, best: 0.6492, time: 0:00:49
 Epoch: 68, lr: 1.0e-02, train_loss: 0.9899, train_acc: 0.6476 test_loss: 7.8297, test_acc: 0.5991, best: 0.6492, time: 0:00:48
 Epoch: 69, lr: 1.0e-02, train_loss: 0.9674, train_acc: 0.6452 test_loss: 13.3616, test_acc: 0.6009, best: 0.6492, time: 0:00:48
 Epoch: 70, lr: 1.0e-02, train_loss: 0.9587, train_acc: 0.6550 test_loss: 96.3883, test_acc: 0.5669, best: 0.6492, time: 0:00:48
 Epoch: 71, lr: 1.0e-02, train_loss: 0.9538, train_acc: 0.6604 test_loss: 27.6478, test_acc: 0.6195, best: 0.6492, time: 0:00:48
 Epoch: 72, lr: 1.0e-02, train_loss: 0.9131, train_acc: 0.6746 test_loss: 104.9106, test_acc: 0.5691, best: 0.6492, time: 0:00:48
 Epoch: 73, lr: 1.0e-02, train_loss: 0.9423, train_acc: 0.6626 test_loss: 81.5815, test_acc: 0.6012, best: 0.6492, time: 0:00:48
 Epoch: 74, lr: 1.0e-02, train_loss: 0.9314, train_acc: 0.6678 test_loss: 18.2595, test_acc: 0.6416, best: 0.6492, time: 0:00:48
 Epoch: 75, lr: 1.0e-02, train_loss: 0.9399, train_acc: 0.6658 test_loss: 50.7602, test_acc: 0.6189, best: 0.6492, time: 0:00:48
 Epoch: 76, lr: 1.0e-02, train_loss: 0.9315, train_acc: 0.6730 test_loss: 55.0034, test_acc: 0.6169, best: 0.6492, time: 0:00:48
 Epoch: 77, lr: 1.0e-02, train_loss: 0.9081, train_acc: 0.6660 test_loss: 70.3169, test_acc: 0.6226, best: 0.6492, time: 0:00:48
 Epoch: 78, lr: 1.0e-02, train_loss: 0.8847, train_acc: 0.6872 test_loss: 39.8266, test_acc: 0.6382, best: 0.6492, time: 0:00:50
 Epoch: 79, lr: 1.0e-02, train_loss: 0.8773, train_acc: 0.6906 test_loss: 66.5330, test_acc: 0.6221, best: 0.6492, time: 0:00:48
 Epoch: 80, lr: 1.0e-02, train_loss: 0.8784, train_acc: 0.6908 test_loss: 39.2182, test_acc: 0.6335, best: 0.6492, time: 0:00:48
 Epoch: 81, lr: 1.0e-02, train_loss: 0.8655, train_acc: 0.6880 test_loss: 46.0601, test_acc: 0.6182, best: 0.6492, time: 0:00:48
 Epoch: 82, lr: 1.0e-02, train_loss: 0.8659, train_acc: 0.6852 test_loss: 2.8043, test_acc: 0.6685, best: 0.6685, time: 0:00:49
 Epoch: 83, lr: 1.0e-02, train_loss: 0.8839, train_acc: 0.6916 test_loss: 239.6452, test_acc: 0.6025, best: 0.6685, time: 0:00:48
 Epoch: 84, lr: 1.0e-02, train_loss: 0.8750, train_acc: 0.6884 test_loss: 119.0641, test_acc: 0.5981, best: 0.6685, time: 0:00:48
 Epoch: 85, lr: 1.0e-02, train_loss: 0.8569, train_acc: 0.6916 test_loss: 143.6948, test_acc: 0.6061, best: 0.6685, time: 0:00:48
 Epoch: 86, lr: 1.0e-02, train_loss: 0.8412, train_acc: 0.7072 test_loss: 148.3989, test_acc: 0.6420, best: 0.6685, time: 0:00:48
 Epoch: 87, lr: 1.0e-02, train_loss: 0.8817, train_acc: 0.6942 test_loss: 32.1488, test_acc: 0.6559, best: 0.6685, time: 0:00:48
 Epoch: 88, lr: 1.0e-02, train_loss: 0.8355, train_acc: 0.7018 test_loss: 7.6483, test_acc: 0.6653, best: 0.6685, time: 0:00:48
 Epoch: 89, lr: 1.0e-02, train_loss: 0.8304, train_acc: 0.6998 test_loss: 11.6661, test_acc: 0.6490, best: 0.6685, time: 0:00:48
 Epoch: 90, lr: 1.0e-02, train_loss: 0.8158, train_acc: 0.7164 test_loss: 30.6541, test_acc: 0.6480, best: 0.6685, time: 0:00:48
 Epoch: 91, lr: 1.0e-02, train_loss: 0.8349, train_acc: 0.7070 test_loss: 204.8874, test_acc: 0.5956, best: 0.6685, time: 0:00:48
 Epoch: 92, lr: 1.0e-02, train_loss: 0.8430, train_acc: 0.6982 test_loss: 32.1166, test_acc: 0.6354, best: 0.6685, time: 0:00:48
 Epoch: 93, lr: 1.0e-02, train_loss: 0.8088, train_acc: 0.7216 test_loss: 80.5197, test_acc: 0.6356, best: 0.6685, time: 0:00:48
 Epoch: 94, lr: 1.0e-02, train_loss: 0.8033, train_acc: 0.7132 test_loss: 176.8028, test_acc: 0.6321, best: 0.6685, time: 0:00:48
 Epoch: 95, lr: 1.0e-02, train_loss: 0.8302, train_acc: 0.7082 test_loss: 5.0586, test_acc: 0.6683, best: 0.6685, time: 0:00:48
 Epoch: 96, lr: 1.0e-02, train_loss: 0.7935, train_acc: 0.7286 test_loss: 131.2360, test_acc: 0.6400, best: 0.6685, time: 0:00:48
 Epoch: 97, lr: 1.0e-02, train_loss: 0.7640, train_acc: 0.7274 test_loss: 262.2558, test_acc: 0.6299, best: 0.6685, time: 0:00:48
 Epoch: 98, lr: 1.0e-02, train_loss: 0.7811, train_acc: 0.7198 test_loss: 118.2825, test_acc: 0.6525, best: 0.6685, time: 0:00:48
 Epoch: 99, lr: 1.0e-02, train_loss: 0.7732, train_acc: 0.7270 test_loss: 9.7011, test_acc: 0.6647, best: 0.6685, time: 0:00:48
 Epoch: 100, lr: 1.0e-02, train_loss: 0.7838, train_acc: 0.7268 test_loss: 16.9235, test_acc: 0.6506, best: 0.6685, time: 0:00:48
 Epoch: 101, lr: 1.0e-02, train_loss: 0.7509, train_acc: 0.7336 test_loss: 7.6893, test_acc: 0.6710, best: 0.6710, time: 0:00:48
 Epoch: 102, lr: 1.0e-02, train_loss: 0.7442, train_acc: 0.7462 test_loss: 91.1384, test_acc: 0.6429, best: 0.6710, time: 0:00:48
 Epoch: 103, lr: 1.0e-02, train_loss: 0.7621, train_acc: 0.7260 test_loss: 91.3591, test_acc: 0.6481, best: 0.6710, time: 0:00:48
 Epoch: 104, lr: 1.0e-02, train_loss: 0.7579, train_acc: 0.7340 test_loss: 6.1763, test_acc: 0.6670, best: 0.6710, time: 0:00:48
 Epoch: 105, lr: 1.0e-02, train_loss: 0.7622, train_acc: 0.7352 test_loss: 58.1940, test_acc: 0.6599, best: 0.6710, time: 0:00:48
 Epoch: 106, lr: 1.0e-02, train_loss: 0.7337, train_acc: 0.7368 test_loss: 23.3225, test_acc: 0.6210, best: 0.6710, time: 0:00:48
 Epoch: 107, lr: 1.0e-02, train_loss: 0.7562, train_acc: 0.7282 test_loss: 79.9557, test_acc: 0.6436, best: 0.6710, time: 0:00:48
 Epoch: 108, lr: 1.0e-02, train_loss: 0.7344, train_acc: 0.7400 test_loss: 139.7984, test_acc: 0.5954, best: 0.6710, time: 0:00:48
 Epoch: 109, lr: 1.0e-02, train_loss: 0.7784, train_acc: 0.7232 test_loss: 139.4977, test_acc: 0.6529, best: 0.6710, time: 0:00:48
 Epoch: 110, lr: 1.0e-02, train_loss: 0.7673, train_acc: 0.7294 test_loss: 4.7968, test_acc: 0.6806, best: 0.6806, time: 0:00:48
 Epoch: 111, lr: 1.0e-02, train_loss: 0.7926, train_acc: 0.7218 test_loss: 12.7619, test_acc: 0.6361, best: 0.6806, time: 0:00:48
 Epoch: 112, lr: 1.0e-02, train_loss: 0.7980, train_acc: 0.7108 test_loss: 65.7548, test_acc: 0.6390, best: 0.6806, time: 0:00:48
 Epoch: 113, lr: 1.0e-02, train_loss: 0.7544, train_acc: 0.7372 test_loss: 11.2623, test_acc: 0.6671, best: 0.6806, time: 0:00:48
 Epoch: 114, lr: 1.0e-02, train_loss: 0.7129, train_acc: 0.7538 test_loss: 72.9985, test_acc: 0.6505, best: 0.6806, time: 0:00:48
 Epoch: 115, lr: 1.0e-02, train_loss: 0.7135, train_acc: 0.7508 test_loss: 49.0174, test_acc: 0.6544, best: 0.6806, time: 0:00:48
 Epoch: 116, lr: 1.0e-02, train_loss: 0.7322, train_acc: 0.7450 test_loss: 23.5569, test_acc: 0.6803, best: 0.6806, time: 0:00:48
 Epoch: 117, lr: 1.0e-02, train_loss: 0.7160, train_acc: 0.7494 test_loss: 113.0024, test_acc: 0.6675, best: 0.6806, time: 0:00:48
 Epoch: 118, lr: 1.0e-02, train_loss: 0.7153, train_acc: 0.7466 test_loss: 1.6020, test_acc: 0.6909, best: 0.6909, time: 0:00:48
 Epoch: 119, lr: 1.0e-02, train_loss: 0.7043, train_acc: 0.7538 test_loss: 21.1580, test_acc: 0.6747, best: 0.6909, time: 0:00:48
 Epoch: 120, lr: 1.0e-02, train_loss: 0.6989, train_acc: 0.7522 test_loss: 8.9491, test_acc: 0.6687, best: 0.6909, time: 0:00:48
 Epoch: 121, lr: 1.0e-02, train_loss: 0.6977, train_acc: 0.7478 test_loss: 36.1791, test_acc: 0.6645, best: 0.6909, time: 0:00:48
 Epoch: 122, lr: 1.0e-02, train_loss: 0.7038, train_acc: 0.7480 test_loss: 175.6708, test_acc: 0.6030, best: 0.6909, time: 0:00:48
 Epoch: 123, lr: 1.0e-02, train_loss: 0.6541, train_acc: 0.7658 test_loss: 10.1253, test_acc: 0.6646, best: 0.6909, time: 0:00:48
 Epoch: 124, lr: 1.0e-02, train_loss: 0.6935, train_acc: 0.7544 test_loss: 4.9625, test_acc: 0.6727, best: 0.6909, time: 0:00:48
 Epoch: 125, lr: 1.0e-02, train_loss: 0.6934, train_acc: 0.7586 test_loss: 65.7884, test_acc: 0.6045, best: 0.6909, time: 0:00:48
 Epoch: 126, lr: 1.0e-02, train_loss: 0.6662, train_acc: 0.7664 test_loss: 7.4237, test_acc: 0.6465, best: 0.6909, time: 0:00:48
 Epoch: 127, lr: 1.0e-02, train_loss: 0.6592, train_acc: 0.7640 test_loss: 61.1123, test_acc: 0.6336, best: 0.6909, time: 0:00:48
 Epoch: 128, lr: 1.0e-02, train_loss: 0.6511, train_acc: 0.7700 test_loss: 5.5720, test_acc: 0.6667, best: 0.6909, time: 0:00:48
 Epoch: 129, lr: 1.0e-02, train_loss: 0.6612, train_acc: 0.7702 test_loss: 13.7843, test_acc: 0.6675, best: 0.6909, time: 0:00:48
 Epoch: 130, lr: 1.0e-02, train_loss: 0.6696, train_acc: 0.7678 test_loss: 7.6206, test_acc: 0.6450, best: 0.6909, time: 0:00:48
 Epoch: 131, lr: 1.0e-02, train_loss: 0.6640, train_acc: 0.7654 test_loss: 5.6614, test_acc: 0.6803, best: 0.6909, time: 0:00:48
 Epoch: 132, lr: 1.0e-02, train_loss: 0.6481, train_acc: 0.7738 test_loss: 7.2146, test_acc: 0.6813, best: 0.6909, time: 0:00:48
 Epoch: 133, lr: 1.0e-02, train_loss: 0.6573, train_acc: 0.7704 test_loss: 2.7084, test_acc: 0.6975, best: 0.6975, time: 0:00:48
 Epoch: 134, lr: 1.0e-02, train_loss: 0.6366, train_acc: 0.7758 test_loss: 2.0397, test_acc: 0.6956, best: 0.6975, time: 0:00:48
 Epoch: 135, lr: 1.0e-02, train_loss: 0.6075, train_acc: 0.7846 test_loss: 1.8802, test_acc: 0.6944, best: 0.6975, time: 0:00:48
 Epoch: 136, lr: 1.0e-02, train_loss: 0.6317, train_acc: 0.7826 test_loss: 22.9146, test_acc: 0.6579, best: 0.6975, time: 0:00:48
 Epoch: 137, lr: 1.0e-02, train_loss: 0.6292, train_acc: 0.7820 test_loss: 12.4284, test_acc: 0.6717, best: 0.6975, time: 0:00:48
 Epoch: 138, lr: 1.0e-02, train_loss: 0.6230, train_acc: 0.7792 test_loss: 63.5123, test_acc: 0.6271, best: 0.6975, time: 0:00:48
 Epoch: 139, lr: 1.0e-02, train_loss: 0.6372, train_acc: 0.7744 test_loss: 61.1288, test_acc: 0.6182, best: 0.6975, time: 0:00:48
 Epoch: 140, lr: 1.0e-02, train_loss: 0.6257, train_acc: 0.7804 test_loss: 42.3239, test_acc: 0.6475, best: 0.6975, time: 0:00:48
 Epoch: 141, lr: 1.0e-02, train_loss: 0.6113, train_acc: 0.7898 test_loss: 50.4574, test_acc: 0.6314, best: 0.6975, time: 0:00:48
 Epoch: 142, lr: 1.0e-02, train_loss: 0.6181, train_acc: 0.7808 test_loss: 5.7529, test_acc: 0.6927, best: 0.6975, time: 0:00:48
 Epoch: 143, lr: 1.0e-02, train_loss: 0.6165, train_acc: 0.7868 test_loss: 68.3906, test_acc: 0.6296, best: 0.6975, time: 0:00:48
 Epoch: 144, lr: 1.0e-02, train_loss: 0.6046, train_acc: 0.7928 test_loss: 57.5996, test_acc: 0.6305, best: 0.6975, time: 0:00:48
 Epoch: 145, lr: 1.0e-02, train_loss: 0.6047, train_acc: 0.7880 test_loss: 39.0727, test_acc: 0.6360, best: 0.6975, time: 0:00:48
 Epoch: 146, lr: 1.0e-02, train_loss: 0.5916, train_acc: 0.7938 test_loss: 59.4444, test_acc: 0.6454, best: 0.6975, time: 0:00:48
 Epoch: 147, lr: 1.0e-02, train_loss: 0.5978, train_acc: 0.7902 test_loss: 56.3764, test_acc: 0.6446, best: 0.6975, time: 0:00:48
 Epoch: 148, lr: 1.0e-02, train_loss: 0.6078, train_acc: 0.7870 test_loss: 7.6551, test_acc: 0.6824, best: 0.6975, time: 0:00:48
 Epoch: 149, lr: 1.0e-02, train_loss: 0.6015, train_acc: 0.7896 test_loss: 20.5622, test_acc: 0.6491, best: 0.6975, time: 0:00:48
 Epoch: 150, lr: 1.0e-02, train_loss: 0.5958, train_acc: 0.7890 test_loss: 20.0277, test_acc: 0.6462, best: 0.6975, time: 0:00:48
 Epoch: 151, lr: 1.0e-02, train_loss: 0.5962, train_acc: 0.7908 test_loss: 3.5738, test_acc: 0.6953, best: 0.6975, time: 0:00:48
 Epoch: 152, lr: 1.0e-02, train_loss: 0.5661, train_acc: 0.8012 test_loss: 36.3081, test_acc: 0.6358, best: 0.6975, time: 0:00:48
 Epoch: 153, lr: 1.0e-02, train_loss: 0.5586, train_acc: 0.8028 test_loss: 163.9304, test_acc: 0.6090, best: 0.6975, time: 0:00:48
 Epoch: 154, lr: 1.0e-02, train_loss: 0.5698, train_acc: 0.8040 test_loss: 29.9322, test_acc: 0.6418, best: 0.6975, time: 0:00:48
 Epoch: 155, lr: 1.0e-02, train_loss: 0.5720, train_acc: 0.7922 test_loss: 30.1488, test_acc: 0.6479, best: 0.6975, time: 0:00:48
 Epoch: 156, lr: 1.0e-02, train_loss: 0.5749, train_acc: 0.7926 test_loss: 5.1927, test_acc: 0.6844, best: 0.6975, time: 0:00:48
 Epoch: 157, lr: 1.0e-02, train_loss: 0.6014, train_acc: 0.7918 test_loss: 9.0951, test_acc: 0.6789, best: 0.6975, time: 0:00:48
 Epoch: 158, lr: 1.0e-02, train_loss: 0.5769, train_acc: 0.7976 test_loss: 4.2953, test_acc: 0.7005, best: 0.7005, time: 0:00:48
 Epoch: 159, lr: 1.0e-02, train_loss: 0.5680, train_acc: 0.8028 test_loss: 2.6113, test_acc: 0.6861, best: 0.7005, time: 0:00:48
 Epoch: 160, lr: 1.0e-02, train_loss: 0.5670, train_acc: 0.8024 test_loss: 12.2067, test_acc: 0.6775, best: 0.7005, time: 0:00:48
 Epoch: 161, lr: 1.0e-02, train_loss: 0.5678, train_acc: 0.8010 test_loss: 3.3847, test_acc: 0.6880, best: 0.7005, time: 0:00:48
 Epoch: 162, lr: 1.0e-02, train_loss: 0.5535, train_acc: 0.8068 test_loss: 2.1088, test_acc: 0.7147, best: 0.7147, time: 0:00:49
 Epoch: 163, lr: 1.0e-02, train_loss: 0.5272, train_acc: 0.8156 test_loss: 4.1465, test_acc: 0.6885, best: 0.7147, time: 0:00:48
 Epoch: 164, lr: 1.0e-02, train_loss: 0.5501, train_acc: 0.8128 test_loss: 2.7921, test_acc: 0.6937, best: 0.7147, time: 0:00:48
 Epoch: 165, lr: 1.0e-02, train_loss: 0.5462, train_acc: 0.8058 test_loss: 22.3099, test_acc: 0.6468, best: 0.7147, time: 0:00:48
 Epoch: 166, lr: 1.0e-02, train_loss: 0.5502, train_acc: 0.8078 test_loss: 1.7841, test_acc: 0.6974, best: 0.7147, time: 0:00:48
 Epoch: 167, lr: 1.0e-02, train_loss: 0.5402, train_acc: 0.8174 test_loss: 4.3553, test_acc: 0.6777, best: 0.7147, time: 0:00:48
 Epoch: 168, lr: 1.0e-02, train_loss: 0.5511, train_acc: 0.8050 test_loss: 4.3607, test_acc: 0.6614, best: 0.7147, time: 0:00:48
 Epoch: 169, lr: 1.0e-02, train_loss: 0.5429, train_acc: 0.8132 test_loss: 6.6350, test_acc: 0.6851, best: 0.7147, time: 0:00:48
 Epoch: 170, lr: 1.0e-02, train_loss: 0.5355, train_acc: 0.8176 test_loss: 1.6325, test_acc: 0.7070, best: 0.7147, time: 0:00:48
 Epoch: 171, lr: 1.0e-02, train_loss: 0.5999, train_acc: 0.7912 test_loss: 4.3034, test_acc: 0.7040, best: 0.7147, time: 0:00:48
 Epoch: 172, lr: 1.0e-02, train_loss: 0.5562, train_acc: 0.8056 test_loss: 3.0457, test_acc: 0.7003, best: 0.7147, time: 0:00:48
 Epoch: 173, lr: 1.0e-02, train_loss: 0.5076, train_acc: 0.8240 test_loss: 6.2247, test_acc: 0.6775, best: 0.7147, time: 0:00:48
 Epoch: 174, lr: 1.0e-02, train_loss: 0.5315, train_acc: 0.8180 test_loss: 10.2455, test_acc: 0.6916, best: 0.7147, time: 0:00:48
 Epoch: 175, lr: 1.0e-02, train_loss: 0.5303, train_acc: 0.8108 test_loss: 12.1287, test_acc: 0.6871, best: 0.7147, time: 0:00:48
 Epoch: 176, lr: 1.0e-02, train_loss: 0.5467, train_acc: 0.8082 test_loss: 142.7618, test_acc: 0.6402, best: 0.7147, time: 0:00:48
 Epoch: 177, lr: 1.0e-02, train_loss: 0.5081, train_acc: 0.8226 test_loss: 147.2848, test_acc: 0.6691, best: 0.7147, time: 0:00:48
 Epoch: 178, lr: 1.0e-02, train_loss: 0.5107, train_acc: 0.8234 test_loss: 44.9210, test_acc: 0.6526, best: 0.7147, time: 0:00:48
 Epoch: 179, lr: 1.0e-02, train_loss: 0.4984, train_acc: 0.8316 test_loss: 143.2444, test_acc: 0.6346, best: 0.7147, time: 0:00:48
 Epoch: 180, lr: 2.0e-03, train_loss: 0.4637, train_acc: 0.8430 test_loss: 80.4022, test_acc: 0.6751, best: 0.7147, time: 0:00:48
 Epoch: 181, lr: 2.0e-03, train_loss: 0.4390, train_acc: 0.8466 test_loss: 431.6294, test_acc: 0.6315, best: 0.7147, time: 0:00:48
 Epoch: 182, lr: 2.0e-03, train_loss: 0.4139, train_acc: 0.8566 test_loss: 20.4407, test_acc: 0.6946, best: 0.7147, time: 0:00:48
 Epoch: 183, lr: 2.0e-03, train_loss: 0.4231, train_acc: 0.8534 test_loss: 97.6702, test_acc: 0.6695, best: 0.7147, time: 0:00:48
 Epoch: 184, lr: 2.0e-03, train_loss: 0.4137, train_acc: 0.8560 test_loss: 65.6279, test_acc: 0.6756, best: 0.7147, time: 0:00:48
 Epoch: 185, lr: 2.0e-03, train_loss: 0.3935, train_acc: 0.8632 test_loss: 135.9428, test_acc: 0.6534, best: 0.7147, time: 0:00:48
 Epoch: 186, lr: 2.0e-03, train_loss: 0.4020, train_acc: 0.8564 test_loss: 42.9371, test_acc: 0.6874, best: 0.7147, time: 0:00:48
 Epoch: 187, lr: 2.0e-03, train_loss: 0.3923, train_acc: 0.8628 test_loss: 10.0430, test_acc: 0.7053, best: 0.7147, time: 0:00:48
 Epoch: 188, lr: 2.0e-03, train_loss: 0.3860, train_acc: 0.8636 test_loss: 5.8768, test_acc: 0.7180, best: 0.7180, time: 0:00:48
 Epoch: 189, lr: 2.0e-03, train_loss: 0.3806, train_acc: 0.8632 test_loss: 14.8534, test_acc: 0.7064, best: 0.7180, time: 0:00:48
 Epoch: 190, lr: 2.0e-03, train_loss: 0.3930, train_acc: 0.8648 test_loss: 154.5379, test_acc: 0.6364, best: 0.7180, time: 0:00:48
 Epoch: 191, lr: 2.0e-03, train_loss: 0.3673, train_acc: 0.8732 test_loss: 25.2344, test_acc: 0.6890, best: 0.7180, time: 0:00:48
 Epoch: 192, lr: 2.0e-03, train_loss: 0.3681, train_acc: 0.8704 test_loss: 36.8480, test_acc: 0.6875, best: 0.7180, time: 0:00:48
 Epoch: 193, lr: 2.0e-03, train_loss: 0.3665, train_acc: 0.8734 test_loss: 29.6647, test_acc: 0.6879, best: 0.7180, time: 0:00:48
 Epoch: 194, lr: 2.0e-03, train_loss: 0.3806, train_acc: 0.8682 test_loss: 24.8505, test_acc: 0.6931, best: 0.7180, time: 0:00:48
 Epoch: 195, lr: 2.0e-03, train_loss: 0.3791, train_acc: 0.8680 test_loss: 34.3846, test_acc: 0.6759, best: 0.7180, time: 0:00:48
 Epoch: 196, lr: 2.0e-03, train_loss: 0.3758, train_acc: 0.8714 test_loss: 44.2832, test_acc: 0.6871, best: 0.7180, time: 0:00:48
 Epoch: 197, lr: 2.0e-03, train_loss: 0.3845, train_acc: 0.8638 test_loss: 11.0970, test_acc: 0.7137, best: 0.7180, time: 0:00:48
 Epoch: 198, lr: 2.0e-03, train_loss: 0.3612, train_acc: 0.8778 test_loss: 22.7291, test_acc: 0.7010, best: 0.7180, time: 0:00:48
 Epoch: 199, lr: 2.0e-03, train_loss: 0.3825, train_acc: 0.8672 test_loss: 79.0966, test_acc: 0.6595, best: 0.7180, time: 0:00:48
 Epoch: 200, lr: 2.0e-03, train_loss: 0.3797, train_acc: 0.8718 test_loss: 35.8653, test_acc: 0.6839, best: 0.7180, time: 0:00:48
 Epoch: 201, lr: 2.0e-03, train_loss: 0.3496, train_acc: 0.8806 test_loss: 12.6347, test_acc: 0.7031, best: 0.7180, time: 0:00:48
 Epoch: 202, lr: 2.0e-03, train_loss: 0.3568, train_acc: 0.8782 test_loss: 8.1785, test_acc: 0.7166, best: 0.7180, time: 0:00:48
 Epoch: 203, lr: 2.0e-03, train_loss: 0.3579, train_acc: 0.8754 test_loss: 155.1515, test_acc: 0.6562, best: 0.7180, time: 0:00:48
 Epoch: 204, lr: 2.0e-03, train_loss: 0.3533, train_acc: 0.8812 test_loss: 21.4849, test_acc: 0.6963, best: 0.7180, time: 0:00:48
 Epoch: 205, lr: 2.0e-03, train_loss: 0.3663, train_acc: 0.8720 test_loss: 45.3206, test_acc: 0.6846, best: 0.7180, time: 0:00:48
 Epoch: 206, lr: 2.0e-03, train_loss: 0.3434, train_acc: 0.8804 test_loss: 26.0967, test_acc: 0.6860, best: 0.7180, time: 0:00:48
 Epoch: 207, lr: 2.0e-03, train_loss: 0.3839, train_acc: 0.8700 test_loss: 579.8901, test_acc: 0.6138, best: 0.7180, time: 0:00:48
 Epoch: 208, lr: 2.0e-03, train_loss: 0.3586, train_acc: 0.8744 test_loss: 23.4642, test_acc: 0.6949, best: 0.7180, time: 0:00:48
 Epoch: 209, lr: 2.0e-03, train_loss: 0.3563, train_acc: 0.8760 test_loss: 37.2751, test_acc: 0.6755, best: 0.7180, time: 0:00:48
 Epoch: 210, lr: 2.0e-03, train_loss: 0.3718, train_acc: 0.8730 test_loss: 13.8717, test_acc: 0.7001, best: 0.7180, time: 0:00:48
 Epoch: 211, lr: 2.0e-03, train_loss: 0.3435, train_acc: 0.8798 test_loss: 5.4938, test_acc: 0.7151, best: 0.7180, time: 0:00:48
 Epoch: 212, lr: 2.0e-03, train_loss: 0.3699, train_acc: 0.8730 test_loss: 10.0536, test_acc: 0.7159, best: 0.7180, time: 0:00:48
 Epoch: 213, lr: 2.0e-03, train_loss: 0.3377, train_acc: 0.8820 test_loss: 55.0223, test_acc: 0.6701, best: 0.7180, time: 0:00:48
 Epoch: 214, lr: 2.0e-03, train_loss: 0.3287, train_acc: 0.8852 test_loss: 2.8965, test_acc: 0.7359, best: 0.7359, time: 0:00:48
 Epoch: 215, lr: 2.0e-03, train_loss: 0.3307, train_acc: 0.8880 test_loss: 23.8515, test_acc: 0.6951, best: 0.7359, time: 0:00:48
 Epoch: 216, lr: 2.0e-03, train_loss: 0.3481, train_acc: 0.8876 test_loss: 61.6763, test_acc: 0.6796, best: 0.7359, time: 0:00:48
 Epoch: 217, lr: 2.0e-03, train_loss: 0.3365, train_acc: 0.8840 test_loss: 144.0034, test_acc: 0.6629, best: 0.7359, time: 0:00:48
 Epoch: 218, lr: 2.0e-03, train_loss: 0.3434, train_acc: 0.8828 test_loss: 11.9389, test_acc: 0.7184, best: 0.7359, time: 0:00:48
 Epoch: 219, lr: 2.0e-03, train_loss: 0.3398, train_acc: 0.8836 test_loss: 17.0100, test_acc: 0.7155, best: 0.7359, time: 0:00:48
 Epoch: 220, lr: 2.0e-03, train_loss: 0.3303, train_acc: 0.8870 test_loss: 32.9479, test_acc: 0.7071, best: 0.7359, time: 0:00:48
 Epoch: 221, lr: 2.0e-03, train_loss: 0.3433, train_acc: 0.8794 test_loss: 4.5901, test_acc: 0.7324, best: 0.7359, time: 0:00:48
 Epoch: 222, lr: 2.0e-03, train_loss: 0.3514, train_acc: 0.8826 test_loss: 21.5640, test_acc: 0.6890, best: 0.7359, time: 0:00:48
 Epoch: 223, lr: 2.0e-03, train_loss: 0.3334, train_acc: 0.8838 test_loss: 14.0802, test_acc: 0.7196, best: 0.7359, time: 0:00:48
 Epoch: 224, lr: 2.0e-03, train_loss: 0.3435, train_acc: 0.8774 test_loss: 13.8170, test_acc: 0.7103, best: 0.7359, time: 0:00:48
 Epoch: 225, lr: 2.0e-03, train_loss: 0.3544, train_acc: 0.8790 test_loss: 72.6875, test_acc: 0.6615, best: 0.7359, time: 0:00:48
 Epoch: 226, lr: 2.0e-03, train_loss: 0.3246, train_acc: 0.8918 test_loss: 53.6728, test_acc: 0.6761, best: 0.7359, time: 0:00:48
 Epoch: 227, lr: 2.0e-03, train_loss: 0.3367, train_acc: 0.8828 test_loss: 12.7397, test_acc: 0.7120, best: 0.7359, time: 0:00:48
 Epoch: 228, lr: 2.0e-03, train_loss: 0.3384, train_acc: 0.8820 test_loss: 10.9644, test_acc: 0.7153, best: 0.7359, time: 0:00:48
 Epoch: 229, lr: 2.0e-03, train_loss: 0.3459, train_acc: 0.8822 test_loss: 24.6333, test_acc: 0.6805, best: 0.7359, time: 0:00:48
 Epoch: 230, lr: 2.0e-03, train_loss: 0.3271, train_acc: 0.8876 test_loss: 20.7885, test_acc: 0.6974, best: 0.7359, time: 0:00:48
 Epoch: 231, lr: 2.0e-03, train_loss: 0.3352, train_acc: 0.8812 test_loss: 9.7962, test_acc: 0.7189, best: 0.7359, time: 0:00:48
 Epoch: 232, lr: 2.0e-03, train_loss: 0.3374, train_acc: 0.8806 test_loss: 33.5786, test_acc: 0.6931, best: 0.7359, time: 0:00:48
 Epoch: 233, lr: 2.0e-03, train_loss: 0.3258, train_acc: 0.8892 test_loss: 9.1526, test_acc: 0.7166, best: 0.7359, time: 0:00:48
 Epoch: 234, lr: 2.0e-03, train_loss: 0.3341, train_acc: 0.8824 test_loss: 20.7009, test_acc: 0.6996, best: 0.7359, time: 0:00:48
 Epoch: 235, lr: 2.0e-03, train_loss: 0.3448, train_acc: 0.8816 test_loss: 32.1458, test_acc: 0.6785, best: 0.7359, time: 0:00:48
 Epoch: 236, lr: 2.0e-03, train_loss: 0.3290, train_acc: 0.8838 test_loss: 9.3956, test_acc: 0.7127, best: 0.7359, time: 0:00:48
 Epoch: 237, lr: 2.0e-03, train_loss: 0.3250, train_acc: 0.8930 test_loss: 38.0701, test_acc: 0.6756, best: 0.7359, time: 0:00:48
 Epoch: 238, lr: 2.0e-03, train_loss: 0.3338, train_acc: 0.8856 test_loss: 36.7784, test_acc: 0.6885, best: 0.7359, time: 0:00:48
 Epoch: 239, lr: 2.0e-03, train_loss: 0.3242, train_acc: 0.8880 test_loss: 28.8371, test_acc: 0.6843, best: 0.7359, time: 0:00:48
 Epoch: 240, lr: 4.0e-04, train_loss: 0.3317, train_acc: 0.8872 test_loss: 3.1073, test_acc: 0.7341, best: 0.7359, time: 0:00:48
 Epoch: 241, lr: 4.0e-04, train_loss: 0.3185, train_acc: 0.8898 test_loss: 74.1850, test_acc: 0.6552, best: 0.7359, time: 0:00:48
 Epoch: 242, lr: 4.0e-04, train_loss: 0.3334, train_acc: 0.8834 test_loss: 22.2133, test_acc: 0.6976, best: 0.7359, time: 0:00:48
 Epoch: 243, lr: 4.0e-04, train_loss: 0.3257, train_acc: 0.8908 test_loss: 6.8918, test_acc: 0.7236, best: 0.7359, time: 0:00:48
 Epoch: 244, lr: 4.0e-04, train_loss: 0.3094, train_acc: 0.8926 test_loss: 3.5008, test_acc: 0.7290, best: 0.7359, time: 0:00:48
 Epoch: 245, lr: 4.0e-04, train_loss: 0.3184, train_acc: 0.8876 test_loss: 6.5320, test_acc: 0.7212, best: 0.7359, time: 0:00:48
 Epoch: 246, lr: 4.0e-04, train_loss: 0.3219, train_acc: 0.8850 test_loss: 83.6663, test_acc: 0.6609, best: 0.7359, time: 0:00:48
 Epoch: 247, lr: 4.0e-04, train_loss: 0.3034, train_acc: 0.8954 test_loss: 2.6705, test_acc: 0.7332, best: 0.7359, time: 0:00:48
 Epoch: 248, lr: 4.0e-04, train_loss: 0.3217, train_acc: 0.8900 test_loss: 27.2874, test_acc: 0.6900, best: 0.7359, time: 0:00:48
 Epoch: 249, lr: 4.0e-04, train_loss: 0.2881, train_acc: 0.8972 test_loss: 18.2140, test_acc: 0.6996, best: 0.7359, time: 0:00:48
 Epoch: 250, lr: 4.0e-04, train_loss: 0.3114, train_acc: 0.8932 test_loss: 8.8351, test_acc: 0.7114, best: 0.7359, time: 0:00:48
 Epoch: 251, lr: 4.0e-04, train_loss: 0.3005, train_acc: 0.8944 test_loss: 11.1931, test_acc: 0.7110, best: 0.7359, time: 0:00:48
 Epoch: 252, lr: 4.0e-04, train_loss: 0.3097, train_acc: 0.8880 test_loss: 14.1696, test_acc: 0.7096, best: 0.7359, time: 0:00:48
 Epoch: 253, lr: 4.0e-04, train_loss: 0.2837, train_acc: 0.9054 test_loss: 12.9207, test_acc: 0.7123, best: 0.7359, time: 0:00:48
 Epoch: 254, lr: 4.0e-04, train_loss: 0.2974, train_acc: 0.8994 test_loss: 46.3832, test_acc: 0.6869, best: 0.7359, time: 0:00:48
 Epoch: 255, lr: 4.0e-04, train_loss: 0.3086, train_acc: 0.8956 test_loss: 16.5375, test_acc: 0.7079, best: 0.7359, time: 0:00:48
 Epoch: 256, lr: 4.0e-04, train_loss: 0.3140, train_acc: 0.8864 test_loss: 15.9824, test_acc: 0.7071, best: 0.7359, time: 0:00:48
 Epoch: 257, lr: 4.0e-04, train_loss: 0.3101, train_acc: 0.8914 test_loss: 16.3876, test_acc: 0.7106, best: 0.7359, time: 0:00:48
 Epoch: 258, lr: 4.0e-04, train_loss: 0.2934, train_acc: 0.9028 test_loss: 10.0911, test_acc: 0.7226, best: 0.7359, time: 0:00:48
 Epoch: 259, lr: 4.0e-04, train_loss: 0.3156, train_acc: 0.8862 test_loss: 59.9656, test_acc: 0.6783, best: 0.7359, time: 0:00:48
 Epoch: 260, lr: 4.0e-04, train_loss: 0.3119, train_acc: 0.8876 test_loss: 10.9938, test_acc: 0.7171, best: 0.7359, time: 0:00:48
 Epoch: 261, lr: 4.0e-04, train_loss: 0.3203, train_acc: 0.8882 test_loss: 56.5722, test_acc: 0.6781, best: 0.7359, time: 0:00:48
 Epoch: 262, lr: 4.0e-04, train_loss: 0.3168, train_acc: 0.8910 test_loss: 16.7002, test_acc: 0.7053, best: 0.7359, time: 0:00:48
 Epoch: 263, lr: 4.0e-04, train_loss: 0.2944, train_acc: 0.8974 test_loss: 12.2337, test_acc: 0.7200, best: 0.7359, time: 0:00:48
 Epoch: 264, lr: 4.0e-04, train_loss: 0.2988, train_acc: 0.8946 test_loss: 33.3282, test_acc: 0.6893, best: 0.7359, time: 0:00:48
 Epoch: 265, lr: 4.0e-04, train_loss: 0.3055, train_acc: 0.8956 test_loss: 4.8013, test_acc: 0.7335, best: 0.7359, time: 0:00:48
 Epoch: 266, lr: 4.0e-04, train_loss: 0.2865, train_acc: 0.9014 test_loss: 11.6741, test_acc: 0.7194, best: 0.7359, time: 0:00:48
 Epoch: 267, lr: 4.0e-04, train_loss: 0.3063, train_acc: 0.8952 test_loss: 1.9886, test_acc: 0.7461, best: 0.7461, time: 0:00:49
 Epoch: 268, lr: 4.0e-04, train_loss: 0.3173, train_acc: 0.8884 test_loss: 22.7401, test_acc: 0.7007, best: 0.7461, time: 0:00:48
 Epoch: 269, lr: 4.0e-04, train_loss: 0.2947, train_acc: 0.8974 test_loss: 30.8309, test_acc: 0.6946, best: 0.7461, time: 0:00:48
 Epoch: 270, lr: 8.0e-05, train_loss: 0.2894, train_acc: 0.8964 test_loss: 31.3760, test_acc: 0.6953, best: 0.7461, time: 0:00:48
 Epoch: 271, lr: 8.0e-05, train_loss: 0.3009, train_acc: 0.8976 test_loss: 3.5314, test_acc: 0.7394, best: 0.7461, time: 0:00:48
 Epoch: 272, lr: 8.0e-05, train_loss: 0.3042, train_acc: 0.8942 test_loss: 31.8656, test_acc: 0.7005, best: 0.7461, time: 0:00:48
 Epoch: 273, lr: 8.0e-05, train_loss: 0.2925, train_acc: 0.8954 test_loss: 13.7603, test_acc: 0.7065, best: 0.7461, time: 0:00:48
 Epoch: 274, lr: 8.0e-05, train_loss: 0.2953, train_acc: 0.8990 test_loss: 6.5203, test_acc: 0.7229, best: 0.7461, time: 0:00:48
 Epoch: 275, lr: 8.0e-05, train_loss: 0.2940, train_acc: 0.8992 test_loss: 47.1189, test_acc: 0.6720, best: 0.7461, time: 0:00:48
 Epoch: 276, lr: 8.0e-05, train_loss: 0.2833, train_acc: 0.9046 test_loss: 16.4004, test_acc: 0.7054, best: 0.7461, time: 0:00:48
 Epoch: 277, lr: 8.0e-05, train_loss: 0.3032, train_acc: 0.8950 test_loss: 108.8008, test_acc: 0.6559, best: 0.7461, time: 0:00:48
 Epoch: 278, lr: 8.0e-05, train_loss: 0.2935, train_acc: 0.9004 test_loss: 11.6649, test_acc: 0.7174, best: 0.7461, time: 0:00:48
 Epoch: 279, lr: 8.0e-05, train_loss: 0.3039, train_acc: 0.8944 test_loss: 94.4567, test_acc: 0.6549, best: 0.7461, time: 0:00:48
 Epoch: 280, lr: 8.0e-05, train_loss: 0.2996, train_acc: 0.8976 test_loss: 8.1189, test_acc: 0.7176, best: 0.7461, time: 0:00:48
 Epoch: 281, lr: 8.0e-05, train_loss: 0.3087, train_acc: 0.8954 test_loss: 13.2925, test_acc: 0.7150, best: 0.7461, time: 0:00:48
 Epoch: 282, lr: 8.0e-05, train_loss: 0.3095, train_acc: 0.8916 test_loss: 30.6975, test_acc: 0.7013, best: 0.7461, time: 0:00:48
 Epoch: 283, lr: 8.0e-05, train_loss: 0.3047, train_acc: 0.8954 test_loss: 3.0262, test_acc: 0.7410, best: 0.7461, time: 0:00:48
 Epoch: 284, lr: 8.0e-05, train_loss: 0.3099, train_acc: 0.8936 test_loss: 18.4830, test_acc: 0.7069, best: 0.7461, time: 0:00:48
 Epoch: 285, lr: 8.0e-05, train_loss: 0.2901, train_acc: 0.9006 test_loss: 5.3303, test_acc: 0.7271, best: 0.7461, time: 0:00:48
 Epoch: 286, lr: 8.0e-05, train_loss: 0.3112, train_acc: 0.8910 test_loss: 16.1168, test_acc: 0.7065, best: 0.7461, time: 0:00:48
 Epoch: 287, lr: 8.0e-05, train_loss: 0.3060, train_acc: 0.8954 test_loss: 44.9123, test_acc: 0.6821, best: 0.7461, time: 0:00:48
 Epoch: 288, lr: 8.0e-05, train_loss: 0.3090, train_acc: 0.8922 test_loss: 6.3616, test_acc: 0.7285, best: 0.7461, time: 0:00:48
 Epoch: 289, lr: 8.0e-05, train_loss: 0.3066, train_acc: 0.8906 test_loss: 2.7708, test_acc: 0.7359, best: 0.7461, time: 0:00:48
 Epoch: 290, lr: 8.0e-05, train_loss: 0.2967, train_acc: 0.8976 test_loss: 9.7197, test_acc: 0.7215, best: 0.7461, time: 0:00:48
 Epoch: 291, lr: 8.0e-05, train_loss: 0.2899, train_acc: 0.8982 test_loss: 10.7509, test_acc: 0.7156, best: 0.7461, time: 0:00:48
 Epoch: 292, lr: 8.0e-05, train_loss: 0.2782, train_acc: 0.9034 test_loss: 2.6508, test_acc: 0.7360, best: 0.7461, time: 0:00:48
 Epoch: 293, lr: 8.0e-05, train_loss: 0.2941, train_acc: 0.8994 test_loss: 22.1092, test_acc: 0.6931, best: 0.7461, time: 0:00:48
 Epoch: 294, lr: 8.0e-05, train_loss: 0.2854, train_acc: 0.8972 test_loss: 34.5445, test_acc: 0.6884, best: 0.7461, time: 0:00:48
 Epoch: 295, lr: 8.0e-05, train_loss: 0.2875, train_acc: 0.9020 test_loss: 4.2430, test_acc: 0.7285, best: 0.7461, time: 0:00:48
 Epoch: 296, lr: 8.0e-05, train_loss: 0.3173, train_acc: 0.8902 test_loss: 10.6070, test_acc: 0.7185, best: 0.7461, time: 0:00:48
 Epoch: 297, lr: 8.0e-05, train_loss: 0.3083, train_acc: 0.8958 test_loss: 13.3484, test_acc: 0.7141, best: 0.7461, time: 0:00:48
 Epoch: 298, lr: 8.0e-05, train_loss: 0.3099, train_acc: 0.8956 test_loss: 9.9238, test_acc: 0.7167, best: 0.7461, time: 0:00:48
 Epoch: 299, lr: 8.0e-05, train_loss: 0.2858, train_acc: 0.9060 test_loss: 3.8132, test_acc: 0.7382, best: 0.7461, time: 0:00:48
 Highest accuracy: 0.7461