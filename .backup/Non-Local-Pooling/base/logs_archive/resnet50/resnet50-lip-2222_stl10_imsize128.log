
 Run on time: 2022-06-29 22:09:51.803608

 Arguments:
	 gpu                  : True
	 log                  : True
	 seed                 : 0
	 momentum             : 0.9
	 amsgrad              : True
	 dataset              : STL10
	 batch_size           : 8
	 architecture         : RESNET50_LIP_2222
	 im_size              : 128
	 relu_threshold       : 4.0
	 learning_rate        : 0.01
	 pretrained_backbone  : 
	 pretrained_ann       : 
	 weight_decay         : 0.0
	 test_only            : False
	 epochs               : 300
	 lr_interval          : [180, 240, 270]
	 lr_reduce            : 5
	 optimizer            : SGD
	 dropout              : 0.2
	 kernel_size          : 3
	 dont_save            : False
	 visualize            : False
	 devices              : 0
 DataParallel(
  (module): NetworkByName(
    (net): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (pool): Pool2d(
          (logit): Sequential(
            (pool_weight): LIP_BASE(
              (logit): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Sigmoid()
              )
            )
          )
          (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (pool): Pool2d(
          (logit): Sequential(
            (pool_weight): LIP_BASE(
              (logit): Sequential(
                (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
                (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Sigmoid()
              )
            )
          )
          (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (pool): Pool2d(
          (logit): Sequential(
            (pool_weight): LIP_BASE(
              (logit): Sequential(
                (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
                (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Sigmoid()
              )
            )
          )
          (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        )
      )
      (layer4): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (pool): Pool2d(
          (logit): Sequential(
            (pool_weight): LIP_BASE(
              (logit): Sequential(
                (0): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1))
                (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Sigmoid()
              )
            )
          )
          (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        )
      )
      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
      (fc): Linear(in_features=2048, out_features=10, bias=True)
    )
  )
)
 SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0
)
 Epoch: 1, lr: 1.0e-02, train_loss: 3.2312, train_acc: 0.1466 test_loss: 2.2353, test_acc: 0.2592, best: 0.2592, time: 0:01:22
 Epoch: 2, lr: 1.0e-02, train_loss: 2.1113, train_acc: 0.2160 test_loss: 2.6076, test_acc: 0.2639, best: 0.2639, time: 0:01:22
 Epoch: 3, lr: 1.0e-02, train_loss: 1.9996, train_acc: 0.2290 test_loss: 2.0768, test_acc: 0.2963, best: 0.2963, time: 0:01:22
 Epoch: 4, lr: 1.0e-02, train_loss: 1.9574, train_acc: 0.2536 test_loss: 1.9125, test_acc: 0.3340, best: 0.3340, time: 0:01:22
 Epoch: 5, lr: 1.0e-02, train_loss: 1.9067, train_acc: 0.2738 test_loss: 1.9226, test_acc: 0.3261, best: 0.3340, time: 0:01:21
 Epoch: 6, lr: 1.0e-02, train_loss: 1.8713, train_acc: 0.2878 test_loss: 1.8482, test_acc: 0.3379, best: 0.3379, time: 0:01:22
 Epoch: 7, lr: 1.0e-02, train_loss: 1.8211, train_acc: 0.3140 test_loss: 1.6185, test_acc: 0.3795, best: 0.3795, time: 0:01:22
 Epoch: 8, lr: 1.0e-02, train_loss: 1.7755, train_acc: 0.3304 test_loss: 1.9811, test_acc: 0.3538, best: 0.3795, time: 0:01:21
 Epoch: 9, lr: 1.0e-02, train_loss: 1.7567, train_acc: 0.3394 test_loss: 1.7473, test_acc: 0.4062, best: 0.4062, time: 0:01:22
 Epoch: 10, lr: 1.0e-02, train_loss: 1.7341, train_acc: 0.3600 test_loss: 1.8540, test_acc: 0.3905, best: 0.4062, time: 0:01:21
 Epoch: 11, lr: 1.0e-02, train_loss: 1.6929, train_acc: 0.3606 test_loss: 1.7217, test_acc: 0.4183, best: 0.4183, time: 0:01:22
 Epoch: 12, lr: 1.0e-02, train_loss: 1.6463, train_acc: 0.3810 test_loss: 1.9037, test_acc: 0.3972, best: 0.4183, time: 0:01:21
 Epoch: 13, lr: 1.0e-02, train_loss: 1.6155, train_acc: 0.4052 test_loss: 2.0112, test_acc: 0.4151, best: 0.4183, time: 0:01:21
 Epoch: 14, lr: 1.0e-02, train_loss: 1.5853, train_acc: 0.4132 test_loss: 3.1915, test_acc: 0.4007, best: 0.4183, time: 0:01:21
 Epoch: 15, lr: 1.0e-02, train_loss: 1.5388, train_acc: 0.4290 test_loss: 2.0738, test_acc: 0.4721, best: 0.4721, time: 0:01:21
 Epoch: 16, lr: 1.0e-02, train_loss: 1.5110, train_acc: 0.4452 test_loss: 2.5211, test_acc: 0.4769, best: 0.4769, time: 0:01:21
 Epoch: 17, lr: 1.0e-02, train_loss: 1.4704, train_acc: 0.4534 test_loss: 7.3576, test_acc: 0.3769, best: 0.4769, time: 0:01:20
 Epoch: 18, lr: 1.0e-02, train_loss: 1.4689, train_acc: 0.4606 test_loss: 2.0828, test_acc: 0.4774, best: 0.4774, time: 0:01:21
 Epoch: 19, lr: 1.0e-02, train_loss: 1.4473, train_acc: 0.4648 test_loss: 2.7062, test_acc: 0.4596, best: 0.4774, time: 0:01:20
 Epoch: 20, lr: 1.0e-02, train_loss: 1.4160, train_acc: 0.4798 test_loss: 1.8776, test_acc: 0.4928, best: 0.4928, time: 0:01:21
 Epoch: 21, lr: 1.0e-02, train_loss: 1.3828, train_acc: 0.4896 test_loss: 1.3752, test_acc: 0.5272, best: 0.5272, time: 0:01:21
 Epoch: 22, lr: 1.0e-02, train_loss: 1.3493, train_acc: 0.5046 test_loss: 2.3144, test_acc: 0.5262, best: 0.5272, time: 0:01:20
 Epoch: 23, lr: 1.0e-02, train_loss: 1.3378, train_acc: 0.5108 test_loss: 2.0921, test_acc: 0.5159, best: 0.5272, time: 0:01:20
 Epoch: 24, lr: 1.0e-02, train_loss: 1.3164, train_acc: 0.5128 test_loss: 1.7282, test_acc: 0.5477, best: 0.5477, time: 0:01:21
 Epoch: 25, lr: 1.0e-02, train_loss: 1.3138, train_acc: 0.5168 test_loss: 1.9377, test_acc: 0.5416, best: 0.5477, time: 0:01:20
 Epoch: 26, lr: 1.0e-02, train_loss: 1.2839, train_acc: 0.5328 test_loss: 1.5366, test_acc: 0.5563, best: 0.5563, time: 0:01:21
 Epoch: 27, lr: 1.0e-02, train_loss: 1.2626, train_acc: 0.5370 test_loss: 5.4136, test_acc: 0.5346, best: 0.5563, time: 0:01:20
 Epoch: 28, lr: 1.0e-02, train_loss: 1.3004, train_acc: 0.5362 test_loss: 1.3238, test_acc: 0.5406, best: 0.5563, time: 0:01:20
 Epoch: 29, lr: 1.0e-02, train_loss: 1.2466, train_acc: 0.5440 test_loss: 2.5022, test_acc: 0.5405, best: 0.5563, time: 0:01:20
 Epoch: 30, lr: 1.0e-02, train_loss: 1.2120, train_acc: 0.5590 test_loss: 3.1804, test_acc: 0.5550, best: 0.5563, time: 0:01:20
 Epoch: 31, lr: 1.0e-02, train_loss: 1.1940, train_acc: 0.5760 test_loss: 2.3498, test_acc: 0.5660, best: 0.5660, time: 0:01:21
 Epoch: 32, lr: 1.0e-02, train_loss: 1.1894, train_acc: 0.5666 test_loss: 1.4894, test_acc: 0.5560, best: 0.5660, time: 0:01:20
 Epoch: 33, lr: 1.0e-02, train_loss: 1.1924, train_acc: 0.5756 test_loss: 1.3491, test_acc: 0.5857, best: 0.5857, time: 0:01:20
 Epoch: 34, lr: 1.0e-02, train_loss: 1.1647, train_acc: 0.5748 test_loss: 1.8044, test_acc: 0.5723, best: 0.5857, time: 0:01:20
 Epoch: 35, lr: 1.0e-02, train_loss: 1.1574, train_acc: 0.5826 test_loss: 1.7225, test_acc: 0.5905, best: 0.5905, time: 0:01:21
 Epoch: 36, lr: 1.0e-02, train_loss: 1.1191, train_acc: 0.5992 test_loss: 1.1915, test_acc: 0.6026, best: 0.6026, time: 0:01:21
 Epoch: 37, lr: 1.0e-02, train_loss: 1.1124, train_acc: 0.5946 test_loss: 1.3854, test_acc: 0.5797, best: 0.6026, time: 0:01:20
 Epoch: 38, lr: 1.0e-02, train_loss: 1.1027, train_acc: 0.5978 test_loss: 1.5342, test_acc: 0.5767, best: 0.6026, time: 0:01:20
 Epoch: 39, lr: 1.0e-02, train_loss: 1.0874, train_acc: 0.6054 test_loss: 1.0889, test_acc: 0.6276, best: 0.6276, time: 0:01:20
 Epoch: 40, lr: 1.0e-02, train_loss: 1.0586, train_acc: 0.6138 test_loss: 1.3317, test_acc: 0.5936, best: 0.6276, time: 0:01:20
 Epoch: 41, lr: 1.0e-02, train_loss: 1.0657, train_acc: 0.6134 test_loss: 1.1620, test_acc: 0.6352, best: 0.6352, time: 0:01:21
 Epoch: 42, lr: 1.0e-02, train_loss: 1.0788, train_acc: 0.6230 test_loss: 1.3083, test_acc: 0.6148, best: 0.6352, time: 0:01:20
 Epoch: 43, lr: 1.0e-02, train_loss: 1.0244, train_acc: 0.6278 test_loss: 1.6399, test_acc: 0.6178, best: 0.6352, time: 0:01:20
 Epoch: 44, lr: 1.0e-02, train_loss: 1.0240, train_acc: 0.6324 test_loss: 2.6673, test_acc: 0.5683, best: 0.6352, time: 0:01:20
 Epoch: 45, lr: 1.0e-02, train_loss: 1.0019, train_acc: 0.6392 test_loss: 1.2835, test_acc: 0.5907, best: 0.6352, time: 0:01:20
 Epoch: 46, lr: 1.0e-02, train_loss: 0.9999, train_acc: 0.6414 test_loss: 1.1117, test_acc: 0.6285, best: 0.6352, time: 0:01:20
 Epoch: 47, lr: 1.0e-02, train_loss: 0.9621, train_acc: 0.6452 test_loss: 1.0786, test_acc: 0.6410, best: 0.6410, time: 0:01:22
 Epoch: 48, lr: 1.0e-02, train_loss: 1.0014, train_acc: 0.6478 test_loss: 1.0536, test_acc: 0.6465, best: 0.6465, time: 0:01:21
 Epoch: 49, lr: 1.0e-02, train_loss: 0.9574, train_acc: 0.6566 test_loss: 1.1208, test_acc: 0.6414, best: 0.6465, time: 0:01:20
 Epoch: 50, lr: 1.0e-02, train_loss: 1.0452, train_acc: 0.6178 test_loss: 1.0786, test_acc: 0.6301, best: 0.6465, time: 0:01:20
 Epoch: 51, lr: 1.0e-02, train_loss: 0.9871, train_acc: 0.6508 test_loss: 1.0427, test_acc: 0.6555, best: 0.6555, time: 0:01:21
 Epoch: 52, lr: 1.0e-02, train_loss: 0.9489, train_acc: 0.6664 test_loss: 1.0545, test_acc: 0.6492, best: 0.6555, time: 0:01:20
 Epoch: 53, lr: 1.0e-02, train_loss: 0.9403, train_acc: 0.6588 test_loss: 1.1729, test_acc: 0.6376, best: 0.6555, time: 0:01:20
 Epoch: 54, lr: 1.0e-02, train_loss: 0.9233, train_acc: 0.6648 test_loss: 1.1473, test_acc: 0.6459, best: 0.6555, time: 0:01:20
 Epoch: 55, lr: 1.0e-02, train_loss: 0.8947, train_acc: 0.6814 test_loss: 1.0455, test_acc: 0.6491, best: 0.6555, time: 0:01:20
 Epoch: 56, lr: 1.0e-02, train_loss: 0.9069, train_acc: 0.6756 test_loss: 1.0934, test_acc: 0.6544, best: 0.6555, time: 0:01:20
 Epoch: 57, lr: 1.0e-02, train_loss: 0.8860, train_acc: 0.6782 test_loss: 0.9771, test_acc: 0.6654, best: 0.6654, time: 0:01:20
 Epoch: 58, lr: 1.0e-02, train_loss: 0.8540, train_acc: 0.6980 test_loss: 1.3015, test_acc: 0.6215, best: 0.6654, time: 0:01:20
 Epoch: 59, lr: 1.0e-02, train_loss: 0.8480, train_acc: 0.6968 test_loss: 0.9959, test_acc: 0.6661, best: 0.6661, time: 0:01:21
 Epoch: 60, lr: 1.0e-02, train_loss: 0.8388, train_acc: 0.7040 test_loss: 1.0407, test_acc: 0.6664, best: 0.6664, time: 0:01:20
 Epoch: 61, lr: 1.0e-02, train_loss: 0.8498, train_acc: 0.6926 test_loss: 1.8526, test_acc: 0.6124, best: 0.6664, time: 0:01:20
 Epoch: 62, lr: 1.0e-02, train_loss: 0.8381, train_acc: 0.7018 test_loss: 1.4575, test_acc: 0.6525, best: 0.6664, time: 0:01:20
 Epoch: 63, lr: 1.0e-02, train_loss: 0.8297, train_acc: 0.7026 test_loss: 2.6100, test_acc: 0.6151, best: 0.6664, time: 0:01:20
 Epoch: 64, lr: 1.0e-02, train_loss: 0.8143, train_acc: 0.7050 test_loss: 1.2568, test_acc: 0.6691, best: 0.6691, time: 0:01:21
 Epoch: 65, lr: 1.0e-02, train_loss: 0.7927, train_acc: 0.7224 test_loss: 1.0361, test_acc: 0.6824, best: 0.6824, time: 0:01:20
 Epoch: 66, lr: 1.0e-02, train_loss: 0.7709, train_acc: 0.7232 test_loss: 1.3328, test_acc: 0.6807, best: 0.6824, time: 0:01:20
 Epoch: 67, lr: 1.0e-02, train_loss: 0.8017, train_acc: 0.7064 test_loss: 2.1568, test_acc: 0.6495, best: 0.6824, time: 0:01:20
 Epoch: 68, lr: 1.0e-02, train_loss: 0.7782, train_acc: 0.7234 test_loss: 1.5985, test_acc: 0.6554, best: 0.6824, time: 0:01:20
 Epoch: 69, lr: 1.0e-02, train_loss: 0.7621, train_acc: 0.7358 test_loss: 0.9829, test_acc: 0.6960, best: 0.6960, time: 0:01:20
 Epoch: 70, lr: 1.0e-02, train_loss: 0.7504, train_acc: 0.7282 test_loss: 1.0768, test_acc: 0.6776, best: 0.6960, time: 0:01:20
 Epoch: 71, lr: 1.0e-02, train_loss: 0.7432, train_acc: 0.7350 test_loss: 1.0350, test_acc: 0.6720, best: 0.6960, time: 0:01:20
 Epoch: 72, lr: 1.0e-02, train_loss: 0.7395, train_acc: 0.7398 test_loss: 1.3418, test_acc: 0.6609, best: 0.6960, time: 0:01:20
 Epoch: 73, lr: 1.0e-02, train_loss: 0.7278, train_acc: 0.7392 test_loss: 2.1219, test_acc: 0.6621, best: 0.6960, time: 0:01:20
 Epoch: 74, lr: 1.0e-02, train_loss: 0.7117, train_acc: 0.7424 test_loss: 1.5370, test_acc: 0.6739, best: 0.6960, time: 0:01:20
 Epoch: 75, lr: 1.0e-02, train_loss: 0.7210, train_acc: 0.7456 test_loss: 1.2695, test_acc: 0.6694, best: 0.6960, time: 0:01:20
 Epoch: 76, lr: 1.0e-02, train_loss: 0.7551, train_acc: 0.7284 test_loss: 0.9913, test_acc: 0.6777, best: 0.6960, time: 0:01:20
 Epoch: 77, lr: 1.0e-02, train_loss: 0.7804, train_acc: 0.7224 test_loss: 0.8919, test_acc: 0.7027, best: 0.7027, time: 0:01:20
 Epoch: 78, lr: 1.0e-02, train_loss: 0.7625, train_acc: 0.7216 test_loss: 0.9201, test_acc: 0.6994, best: 0.7027, time: 0:01:20
 Epoch: 79, lr: 1.0e-02, train_loss: 0.7880, train_acc: 0.7260 test_loss: 0.9759, test_acc: 0.6803, best: 0.7027, time: 0:01:20
 Epoch: 80, lr: 1.0e-02, train_loss: 0.8228, train_acc: 0.7050 test_loss: 1.0076, test_acc: 0.6839, best: 0.7027, time: 0:01:20
 Epoch: 81, lr: 1.0e-02, train_loss: 0.7614, train_acc: 0.7334 test_loss: 0.9949, test_acc: 0.6883, best: 0.7027, time: 0:01:17
 Epoch: 82, lr: 1.0e-02, train_loss: 0.7095, train_acc: 0.7456 test_loss: 0.9699, test_acc: 0.6959, best: 0.7027, time: 0:01:16
 Epoch: 83, lr: 1.0e-02, train_loss: 0.7123, train_acc: 0.7474 test_loss: 1.0353, test_acc: 0.6836, best: 0.7027, time: 0:01:16
 Epoch: 84, lr: 1.0e-02, train_loss: 0.7061, train_acc: 0.7524 test_loss: 0.9318, test_acc: 0.7087, best: 0.7087, time: 0:01:16
 Epoch: 85, lr: 1.0e-02, train_loss: 0.7098, train_acc: 0.7506 test_loss: 0.9650, test_acc: 0.7026, best: 0.7087, time: 0:01:16
 Epoch: 86, lr: 1.0e-02, train_loss: 0.6783, train_acc: 0.7632 test_loss: 1.0141, test_acc: 0.6850, best: 0.7087, time: 0:01:16
 Epoch: 87, lr: 1.0e-02, train_loss: 0.6759, train_acc: 0.7646 test_loss: 0.9282, test_acc: 0.7076, best: 0.7087, time: 0:01:16
 Epoch: 88, lr: 1.0e-02, train_loss: 0.6170, train_acc: 0.7830 test_loss: 1.0185, test_acc: 0.7139, best: 0.7139, time: 0:01:17
 Epoch: 89, lr: 1.0e-02, train_loss: 0.6473, train_acc: 0.7676 test_loss: 1.0412, test_acc: 0.7029, best: 0.7139, time: 0:01:16
 Epoch: 90, lr: 1.0e-02, train_loss: 0.6574, train_acc: 0.7680 test_loss: 0.8599, test_acc: 0.7166, best: 0.7166, time: 0:01:16
 Epoch: 91, lr: 1.0e-02, train_loss: 0.6285, train_acc: 0.7818 test_loss: 0.9222, test_acc: 0.7177, best: 0.7177, time: 0:01:17
 Epoch: 92, lr: 1.0e-02, train_loss: 0.6155, train_acc: 0.7864 test_loss: 1.0371, test_acc: 0.6984, best: 0.7177, time: 0:01:16
 Epoch: 93, lr: 1.0e-02, train_loss: 0.6056, train_acc: 0.7820 test_loss: 0.9785, test_acc: 0.7111, best: 0.7177, time: 0:01:16
 Epoch: 94, lr: 1.0e-02, train_loss: 0.6040, train_acc: 0.7916 test_loss: 1.2331, test_acc: 0.6960, best: 0.7177, time: 0:01:16
 Epoch: 95, lr: 1.0e-02, train_loss: 0.6128, train_acc: 0.7812 test_loss: 1.0118, test_acc: 0.7060, best: 0.7177, time: 0:01:16
 Epoch: 96, lr: 1.0e-02, train_loss: 0.5743, train_acc: 0.7984 test_loss: 0.9790, test_acc: 0.7181, best: 0.7181, time: 0:01:16
 Epoch: 97, lr: 1.0e-02, train_loss: 0.5695, train_acc: 0.7926 test_loss: 1.0239, test_acc: 0.7185, best: 0.7185, time: 0:01:16
 Epoch: 98, lr: 1.0e-02, train_loss: 0.5749, train_acc: 0.7982 test_loss: 1.1397, test_acc: 0.7017, best: 0.7185, time: 0:01:16
 Epoch: 99, lr: 1.0e-02, train_loss: 0.5604, train_acc: 0.8064 test_loss: 1.0264, test_acc: 0.7116, best: 0.7185, time: 0:01:16
 Epoch: 100, lr: 1.0e-02, train_loss: 0.5500, train_acc: 0.8092 test_loss: 1.1284, test_acc: 0.6971, best: 0.7185, time: 0:01:16
 Epoch: 101, lr: 1.0e-02, train_loss: 0.5345, train_acc: 0.8166 test_loss: 1.2698, test_acc: 0.6955, best: 0.7185, time: 0:01:16
 Epoch: 102, lr: 1.0e-02, train_loss: 0.5275, train_acc: 0.8186 test_loss: 0.9155, test_acc: 0.7268, best: 0.7268, time: 0:01:16
 Epoch: 103, lr: 1.0e-02, train_loss: 0.5389, train_acc: 0.8136 test_loss: 1.1011, test_acc: 0.7086, best: 0.7268, time: 0:01:16
 Epoch: 104, lr: 1.0e-02, train_loss: 0.5276, train_acc: 0.8146 test_loss: 1.0484, test_acc: 0.7130, best: 0.7268, time: 0:01:16
 Epoch: 105, lr: 1.0e-02, train_loss: 0.5214, train_acc: 0.8222 test_loss: 0.9579, test_acc: 0.7350, best: 0.7350, time: 0:01:17
 Epoch: 106, lr: 1.0e-02, train_loss: 0.5170, train_acc: 0.8234 test_loss: 1.1141, test_acc: 0.6943, best: 0.7350, time: 0:01:16
 Epoch: 107, lr: 1.0e-02, train_loss: 0.5170, train_acc: 0.8162 test_loss: 0.9690, test_acc: 0.7269, best: 0.7350, time: 0:01:16
 Epoch: 108, lr: 1.0e-02, train_loss: 0.4894, train_acc: 0.8228 test_loss: 1.0958, test_acc: 0.7212, best: 0.7350, time: 0:01:16
 Epoch: 109, lr: 1.0e-02, train_loss: 0.5099, train_acc: 0.8268 test_loss: 0.9608, test_acc: 0.7320, best: 0.7350, time: 0:01:16
 Epoch: 110, lr: 1.0e-02, train_loss: 0.5041, train_acc: 0.8252 test_loss: 0.9796, test_acc: 0.7218, best: 0.7350, time: 0:01:16
 Epoch: 111, lr: 1.0e-02, train_loss: 0.4875, train_acc: 0.8324 test_loss: 1.0027, test_acc: 0.7238, best: 0.7350, time: 0:01:16
 Epoch: 112, lr: 1.0e-02, train_loss: 0.4959, train_acc: 0.8268 test_loss: 0.9388, test_acc: 0.7205, best: 0.7350, time: 0:01:16
 Epoch: 113, lr: 1.0e-02, train_loss: 0.5159, train_acc: 0.8186 test_loss: 1.3310, test_acc: 0.7120, best: 0.7350, time: 0:01:16
 Epoch: 114, lr: 1.0e-02, train_loss: 0.4789, train_acc: 0.8268 test_loss: 1.1654, test_acc: 0.7156, best: 0.7350, time: 0:01:16
 Epoch: 115, lr: 1.0e-02, train_loss: 0.4821, train_acc: 0.8288 test_loss: 0.9347, test_acc: 0.7280, best: 0.7350, time: 0:01:16
 Epoch: 116, lr: 1.0e-02, train_loss: 0.4766, train_acc: 0.8326 test_loss: 1.1807, test_acc: 0.7099, best: 0.7350, time: 0:01:16
 Epoch: 117, lr: 1.0e-02, train_loss: 0.4734, train_acc: 0.8302 test_loss: 1.6082, test_acc: 0.6916, best: 0.7350, time: 0:01:16
 Epoch: 118, lr: 1.0e-02, train_loss: 0.4745, train_acc: 0.8350 test_loss: 1.3732, test_acc: 0.6869, best: 0.7350, time: 0:01:16
 Epoch: 119, lr: 1.0e-02, train_loss: 0.4600, train_acc: 0.8400 test_loss: 1.1404, test_acc: 0.7143, best: 0.7350, time: 0:01:16
 Epoch: 120, lr: 1.0e-02, train_loss: 0.4662, train_acc: 0.8388 test_loss: 1.0623, test_acc: 0.7014, best: 0.7350, time: 0:01:16
 Epoch: 121, lr: 1.0e-02, train_loss: 0.4444, train_acc: 0.8438 test_loss: 0.9846, test_acc: 0.7352, best: 0.7352, time: 0:01:17
 Epoch: 122, lr: 1.0e-02, train_loss: 0.4479, train_acc: 0.8412 test_loss: 1.1037, test_acc: 0.7164, best: 0.7352, time: 0:01:16
 Epoch: 123, lr: 1.0e-02, train_loss: 0.4373, train_acc: 0.8490 test_loss: 1.1301, test_acc: 0.7230, best: 0.7352, time: 0:01:16
 Epoch: 124, lr: 1.0e-02, train_loss: 0.4459, train_acc: 0.8486 test_loss: 1.1156, test_acc: 0.7184, best: 0.7352, time: 0:01:16
 Epoch: 125, lr: 1.0e-02, train_loss: 0.4515, train_acc: 0.8444 test_loss: 1.4557, test_acc: 0.6944, best: 0.7352, time: 0:01:16
 Epoch: 126, lr: 1.0e-02, train_loss: 0.4618, train_acc: 0.8384 test_loss: 1.1981, test_acc: 0.7141, best: 0.7352, time: 0:01:16
 Epoch: 127, lr: 1.0e-02, train_loss: 0.4536, train_acc: 0.8416 test_loss: 1.2666, test_acc: 0.7174, best: 0.7352, time: 0:01:16
 Epoch: 128, lr: 1.0e-02, train_loss: 0.4443, train_acc: 0.8422 test_loss: 1.3577, test_acc: 0.6795, best: 0.7352, time: 0:01:16
 Epoch: 129, lr: 1.0e-02, train_loss: 0.4299, train_acc: 0.8542 test_loss: 1.5486, test_acc: 0.6941, best: 0.7352, time: 0:01:16
 Epoch: 130, lr: 1.0e-02, train_loss: 0.4288, train_acc: 0.8516 test_loss: 1.0085, test_acc: 0.7146, best: 0.7352, time: 0:01:16
 Epoch: 131, lr: 1.0e-02, train_loss: 0.4320, train_acc: 0.8460 test_loss: 1.2248, test_acc: 0.7106, best: 0.7352, time: 0:01:16
 Epoch: 132, lr: 1.0e-02, train_loss: 0.4169, train_acc: 0.8610 test_loss: 0.9979, test_acc: 0.7338, best: 0.7352, time: 0:01:16
 Epoch: 133, lr: 1.0e-02, train_loss: 0.4163, train_acc: 0.8576 test_loss: 1.0453, test_acc: 0.7176, best: 0.7352, time: 0:01:16
 Epoch: 134, lr: 1.0e-02, train_loss: 0.4090, train_acc: 0.8584 test_loss: 1.0411, test_acc: 0.7170, best: 0.7352, time: 0:01:16
 Epoch: 135, lr: 1.0e-02, train_loss: 0.4171, train_acc: 0.8520 test_loss: 0.9677, test_acc: 0.7465, best: 0.7465, time: 0:01:16
 Epoch: 136, lr: 1.0e-02, train_loss: 0.3927, train_acc: 0.8618 test_loss: 1.1575, test_acc: 0.7134, best: 0.7465, time: 0:01:16
 Epoch: 137, lr: 1.0e-02, train_loss: 0.3939, train_acc: 0.8598 test_loss: 1.3227, test_acc: 0.7222, best: 0.7465, time: 0:01:16
 Epoch: 138, lr: 1.0e-02, train_loss: 0.3894, train_acc: 0.8602 test_loss: 1.1364, test_acc: 0.7302, best: 0.7465, time: 0:01:16
 Epoch: 139, lr: 1.0e-02, train_loss: 0.3802, train_acc: 0.8688 test_loss: 0.9991, test_acc: 0.7282, best: 0.7465, time: 0:01:16
 Epoch: 140, lr: 1.0e-02, train_loss: 0.3830, train_acc: 0.8650 test_loss: 1.1958, test_acc: 0.7320, best: 0.7465, time: 0:01:16
 Epoch: 141, lr: 1.0e-02, train_loss: 0.3849, train_acc: 0.8610 test_loss: 1.2307, test_acc: 0.7177, best: 0.7465, time: 0:01:16
 Epoch: 142, lr: 1.0e-02, train_loss: 0.3678, train_acc: 0.8762 test_loss: 1.1512, test_acc: 0.7221, best: 0.7465, time: 0:01:16
 Epoch: 143, lr: 1.0e-02, train_loss: 0.3552, train_acc: 0.8762 test_loss: 1.1534, test_acc: 0.7198, best: 0.7465, time: 0:01:16
 Epoch: 144, lr: 1.0e-02, train_loss: 0.3847, train_acc: 0.8704 test_loss: 1.2660, test_acc: 0.7229, best: 0.7465, time: 0:01:16
 Epoch: 145, lr: 1.0e-02, train_loss: 0.3841, train_acc: 0.8680 test_loss: 1.2006, test_acc: 0.7173, best: 0.7465, time: 0:01:16
 Epoch: 146, lr: 1.0e-02, train_loss: 0.3709, train_acc: 0.8716 test_loss: 1.3494, test_acc: 0.6997, best: 0.7465, time: 0:01:16
 Epoch: 147, lr: 1.0e-02, train_loss: 0.3685, train_acc: 0.8724 test_loss: 1.0988, test_acc: 0.7326, best: 0.7465, time: 0:01:16
 Epoch: 148, lr: 1.0e-02, train_loss: 0.3628, train_acc: 0.8684 test_loss: 0.9631, test_acc: 0.7500, best: 0.7500, time: 0:01:16
 Epoch: 149, lr: 1.0e-02, train_loss: 0.3771, train_acc: 0.8660 test_loss: 1.0418, test_acc: 0.7428, best: 0.7500, time: 0:01:16
 Epoch: 150, lr: 1.0e-02, train_loss: 0.3616, train_acc: 0.8788 test_loss: 2.2945, test_acc: 0.6624, best: 0.7500, time: 0:01:16
 Epoch: 151, lr: 1.0e-02, train_loss: 0.3646, train_acc: 0.8762 test_loss: 0.9929, test_acc: 0.7405, best: 0.7500, time: 0:01:16
 Epoch: 152, lr: 1.0e-02, train_loss: 0.3644, train_acc: 0.8732 test_loss: 1.0190, test_acc: 0.7381, best: 0.7500, time: 0:01:16
 Epoch: 153, lr: 1.0e-02, train_loss: 0.3547, train_acc: 0.8786 test_loss: 1.0905, test_acc: 0.7298, best: 0.7500, time: 0:01:16
 Epoch: 154, lr: 1.0e-02, train_loss: 0.3661, train_acc: 0.8664 test_loss: 1.1196, test_acc: 0.7260, best: 0.7500, time: 0:01:16
 Epoch: 155, lr: 1.0e-02, train_loss: 0.3576, train_acc: 0.8756 test_loss: 1.1526, test_acc: 0.7311, best: 0.7500, time: 0:01:16
 Epoch: 156, lr: 1.0e-02, train_loss: 0.3855, train_acc: 0.8666 test_loss: 1.0392, test_acc: 0.7268, best: 0.7500, time: 0:01:16
 Epoch: 157, lr: 1.0e-02, train_loss: 0.3694, train_acc: 0.8728 test_loss: 1.1444, test_acc: 0.7300, best: 0.7500, time: 0:01:16
 Epoch: 158, lr: 1.0e-02, train_loss: 0.3555, train_acc: 0.8754 test_loss: 1.6330, test_acc: 0.6924, best: 0.7500, time: 0:01:16
 Epoch: 159, lr: 1.0e-02, train_loss: 0.3682, train_acc: 0.8692 test_loss: 1.5973, test_acc: 0.7155, best: 0.7500, time: 0:01:16
 Epoch: 160, lr: 1.0e-02, train_loss: 0.3597, train_acc: 0.8748 test_loss: 1.1103, test_acc: 0.7324, best: 0.7500, time: 0:01:16
 Epoch: 161, lr: 1.0e-02, train_loss: 0.3501, train_acc: 0.8806 test_loss: 1.1690, test_acc: 0.7356, best: 0.7500, time: 0:01:16
 Epoch: 162, lr: 1.0e-02, train_loss: 0.3543, train_acc: 0.8774 test_loss: 1.1835, test_acc: 0.7258, best: 0.7500, time: 0:01:16
 Epoch: 163, lr: 1.0e-02, train_loss: 0.3449, train_acc: 0.8842 test_loss: 1.6685, test_acc: 0.6976, best: 0.7500, time: 0:01:16
 Epoch: 164, lr: 1.0e-02, train_loss: 0.3596, train_acc: 0.8740 test_loss: 1.1023, test_acc: 0.7298, best: 0.7500, time: 0:01:16
 Epoch: 165, lr: 1.0e-02, train_loss: 0.3434, train_acc: 0.8844 test_loss: 1.0831, test_acc: 0.7359, best: 0.7500, time: 0:01:16
 Epoch: 166, lr: 1.0e-02, train_loss: 0.3380, train_acc: 0.8800 test_loss: 0.9850, test_acc: 0.7528, best: 0.7528, time: 0:01:17
 Epoch: 167, lr: 1.0e-02, train_loss: 0.3231, train_acc: 0.8888 test_loss: 1.0607, test_acc: 0.7439, best: 0.7528, time: 0:01:16
 Epoch: 168, lr: 1.0e-02, train_loss: 0.3284, train_acc: 0.8880 test_loss: 1.1236, test_acc: 0.7446, best: 0.7528, time: 0:01:16
 Epoch: 169, lr: 1.0e-02, train_loss: 0.3203, train_acc: 0.8894 test_loss: 1.1926, test_acc: 0.7279, best: 0.7528, time: 0:01:16
 Epoch: 170, lr: 1.0e-02, train_loss: 0.3172, train_acc: 0.8890 test_loss: 1.1312, test_acc: 0.7432, best: 0.7528, time: 0:01:16
 Epoch: 171, lr: 1.0e-02, train_loss: 0.3415, train_acc: 0.8800 test_loss: 0.9979, test_acc: 0.7436, best: 0.7528, time: 0:01:16
 Epoch: 172, lr: 1.0e-02, train_loss: 0.3014, train_acc: 0.8972 test_loss: 1.0161, test_acc: 0.7486, best: 0.7528, time: 0:01:16
 Epoch: 173, lr: 1.0e-02, train_loss: 0.3015, train_acc: 0.8990 test_loss: 1.0212, test_acc: 0.7575, best: 0.7575, time: 0:01:17
 Epoch: 174, lr: 1.0e-02, train_loss: 0.2938, train_acc: 0.8954 test_loss: 1.2158, test_acc: 0.7389, best: 0.7575, time: 0:01:16
 Epoch: 175, lr: 1.0e-02, train_loss: 0.3230, train_acc: 0.8856 test_loss: 0.9769, test_acc: 0.7592, best: 0.7592, time: 0:01:16
 Epoch: 176, lr: 1.0e-02, train_loss: 0.3769, train_acc: 0.8736 test_loss: 1.0975, test_acc: 0.7259, best: 0.7592, time: 0:01:16
 Epoch: 177, lr: 1.0e-02, train_loss: 0.3700, train_acc: 0.8704 test_loss: 1.0126, test_acc: 0.7368, best: 0.7592, time: 0:01:16
 Epoch: 178, lr: 1.0e-02, train_loss: 0.3439, train_acc: 0.8808 test_loss: 1.0235, test_acc: 0.7404, best: 0.7592, time: 0:01:16
 Epoch: 179, lr: 1.0e-02, train_loss: 0.3330, train_acc: 0.8834 test_loss: 1.0545, test_acc: 0.7414, best: 0.7592, time: 0:01:16
 Epoch: 180, lr: 2.0e-03, train_loss: 0.2832, train_acc: 0.9060 test_loss: 1.1027, test_acc: 0.7462, best: 0.7592, time: 0:01:16
 Epoch: 181, lr: 2.0e-03, train_loss: 0.2576, train_acc: 0.9144 test_loss: 0.9858, test_acc: 0.7662, best: 0.7662, time: 0:01:16
 Epoch: 182, lr: 2.0e-03, train_loss: 0.2329, train_acc: 0.9192 test_loss: 0.9946, test_acc: 0.7654, best: 0.7662, time: 0:01:16
 Epoch: 183, lr: 2.0e-03, train_loss: 0.2263, train_acc: 0.9246 test_loss: 1.0164, test_acc: 0.7606, best: 0.7662, time: 0:01:16
 Epoch: 184, lr: 2.0e-03, train_loss: 0.2278, train_acc: 0.9220 test_loss: 1.0131, test_acc: 0.7676, best: 0.7676, time: 0:01:16
 Epoch: 185, lr: 2.0e-03, train_loss: 0.2418, train_acc: 0.9120 test_loss: 1.0281, test_acc: 0.7626, best: 0.7676, time: 0:01:16
 Epoch: 186, lr: 2.0e-03, train_loss: 0.2197, train_acc: 0.9264 test_loss: 1.0160, test_acc: 0.7656, best: 0.7676, time: 0:01:16
 Epoch: 187, lr: 2.0e-03, train_loss: 0.2090, train_acc: 0.9280 test_loss: 0.9796, test_acc: 0.7694, best: 0.7694, time: 0:01:16
 Epoch: 188, lr: 2.0e-03, train_loss: 0.2118, train_acc: 0.9252 test_loss: 1.0641, test_acc: 0.7569, best: 0.7694, time: 0:01:16
 Epoch: 189, lr: 2.0e-03, train_loss: 0.2043, train_acc: 0.9298 test_loss: 0.9908, test_acc: 0.7714, best: 0.7714, time: 0:01:16
 Epoch: 190, lr: 2.0e-03, train_loss: 0.2107, train_acc: 0.9236 test_loss: 1.0614, test_acc: 0.7622, best: 0.7714, time: 0:01:16
 Epoch: 191, lr: 2.0e-03, train_loss: 0.2027, train_acc: 0.9296 test_loss: 1.0431, test_acc: 0.7644, best: 0.7714, time: 0:01:16
 Epoch: 192, lr: 2.0e-03, train_loss: 0.1883, train_acc: 0.9340 test_loss: 1.0381, test_acc: 0.7669, best: 0.7714, time: 0:01:16
 Epoch: 193, lr: 2.0e-03, train_loss: 0.1938, train_acc: 0.9326 test_loss: 1.0506, test_acc: 0.7651, best: 0.7714, time: 0:01:16
 Epoch: 194, lr: 2.0e-03, train_loss: 0.2164, train_acc: 0.9284 test_loss: 1.0733, test_acc: 0.7579, best: 0.7714, time: 0:01:16
 Epoch: 195, lr: 2.0e-03, train_loss: 0.1993, train_acc: 0.9298 test_loss: 1.0964, test_acc: 0.7625, best: 0.7714, time: 0:01:16
 Epoch: 196, lr: 2.0e-03, train_loss: 0.1990, train_acc: 0.9328 test_loss: 1.0732, test_acc: 0.7675, best: 0.7714, time: 0:01:16
 Epoch: 197, lr: 2.0e-03, train_loss: 0.2129, train_acc: 0.9232 test_loss: 1.1458, test_acc: 0.7562, best: 0.7714, time: 0:01:16
 Epoch: 198, lr: 2.0e-03, train_loss: 0.1909, train_acc: 0.9298 test_loss: 1.0496, test_acc: 0.7669, best: 0.7714, time: 0:01:16
 Epoch: 199, lr: 2.0e-03, train_loss: 0.2099, train_acc: 0.9300 test_loss: 1.0441, test_acc: 0.7722, best: 0.7722, time: 0:01:16
 Epoch: 200, lr: 2.0e-03, train_loss: 0.2054, train_acc: 0.9276 test_loss: 1.0903, test_acc: 0.7611, best: 0.7722, time: 0:01:16
 Epoch: 201, lr: 2.0e-03, train_loss: 0.1905, train_acc: 0.9346 test_loss: 1.0210, test_acc: 0.7670, best: 0.7722, time: 0:01:16
 Epoch: 202, lr: 2.0e-03, train_loss: 0.1881, train_acc: 0.9374 test_loss: 1.0425, test_acc: 0.7660, best: 0.7722, time: 0:01:16
 Epoch: 203, lr: 2.0e-03, train_loss: 0.1928, train_acc: 0.9324 test_loss: 1.0230, test_acc: 0.7710, best: 0.7722, time: 0:01:16
 Epoch: 204, lr: 2.0e-03, train_loss: 0.1982, train_acc: 0.9278 test_loss: 1.0419, test_acc: 0.7682, best: 0.7722, time: 0:01:16
 Epoch: 205, lr: 2.0e-03, train_loss: 0.2051, train_acc: 0.9288 test_loss: 1.0073, test_acc: 0.7685, best: 0.7722, time: 0:01:16
 Epoch: 206, lr: 2.0e-03, train_loss: 0.1920, train_acc: 0.9346 test_loss: 1.0470, test_acc: 0.7644, best: 0.7722, time: 0:01:16
 Epoch: 207, lr: 2.0e-03, train_loss: 0.1929, train_acc: 0.9342 test_loss: 1.0759, test_acc: 0.7706, best: 0.7722, time: 0:01:16
 Epoch: 208, lr: 2.0e-03, train_loss: 0.2117, train_acc: 0.9276 test_loss: 1.0427, test_acc: 0.7711, best: 0.7722, time: 0:01:16
 Epoch: 209, lr: 2.0e-03, train_loss: 0.1897, train_acc: 0.9352 test_loss: 1.0513, test_acc: 0.7691, best: 0.7722, time: 0:01:16
 Epoch: 210, lr: 2.0e-03, train_loss: 0.1825, train_acc: 0.9372 test_loss: 1.0625, test_acc: 0.7720, best: 0.7722, time: 0:01:16
 Epoch: 211, lr: 2.0e-03, train_loss: 0.1830, train_acc: 0.9412 test_loss: 1.1064, test_acc: 0.7679, best: 0.7722, time: 0:01:16
 Epoch: 212, lr: 2.0e-03, train_loss: 0.1839, train_acc: 0.9372 test_loss: 1.1650, test_acc: 0.7582, best: 0.7722, time: 0:01:16
 Epoch: 213, lr: 2.0e-03, train_loss: 0.1763, train_acc: 0.9422 test_loss: 1.1492, test_acc: 0.7629, best: 0.7722, time: 0:01:16
 Epoch: 214, lr: 2.0e-03, train_loss: 0.1926, train_acc: 0.9328 test_loss: 1.1040, test_acc: 0.7661, best: 0.7722, time: 0:01:16
 Epoch: 215, lr: 2.0e-03, train_loss: 0.1824, train_acc: 0.9360 test_loss: 1.0870, test_acc: 0.7671, best: 0.7722, time: 0:01:16
 Epoch: 216, lr: 2.0e-03, train_loss: 0.2014, train_acc: 0.9276 test_loss: 1.0990, test_acc: 0.7631, best: 0.7722, time: 0:01:16
 Epoch: 217, lr: 2.0e-03, train_loss: 0.1860, train_acc: 0.9334 test_loss: 1.0534, test_acc: 0.7652, best: 0.7722, time: 0:01:16
 Epoch: 218, lr: 2.0e-03, train_loss: 0.1911, train_acc: 0.9360 test_loss: 1.0625, test_acc: 0.7682, best: 0.7722, time: 0:01:16
 Epoch: 219, lr: 2.0e-03, train_loss: 0.1800, train_acc: 0.9382 test_loss: 1.0187, test_acc: 0.7714, best: 0.7722, time: 0:01:16
 Epoch: 220, lr: 2.0e-03, train_loss: 0.1754, train_acc: 0.9410 test_loss: 1.0496, test_acc: 0.7722, best: 0.7722, time: 0:01:16
 Epoch: 221, lr: 2.0e-03, train_loss: 0.1716, train_acc: 0.9394 test_loss: 1.2425, test_acc: 0.7389, best: 0.7722, time: 0:01:16
 Epoch: 222, lr: 2.0e-03, train_loss: 0.1841, train_acc: 0.9342 test_loss: 1.0644, test_acc: 0.7686, best: 0.7722, time: 0:01:16
 Epoch: 223, lr: 2.0e-03, train_loss: 0.1921, train_acc: 0.9362 test_loss: 1.0339, test_acc: 0.7728, best: 0.7728, time: 0:01:16
 Epoch: 224, lr: 2.0e-03, train_loss: 0.1900, train_acc: 0.9340 test_loss: 1.0089, test_acc: 0.7705, best: 0.7728, time: 0:01:16
 Epoch: 225, lr: 2.0e-03, train_loss: 0.1871, train_acc: 0.9352 test_loss: 1.0509, test_acc: 0.7688, best: 0.7728, time: 0:01:16
 Epoch: 226, lr: 2.0e-03, train_loss: 0.1702, train_acc: 0.9422 test_loss: 1.0979, test_acc: 0.7676, best: 0.7728, time: 0:01:16
 Epoch: 227, lr: 2.0e-03, train_loss: 0.1823, train_acc: 0.9398 test_loss: 1.0361, test_acc: 0.7655, best: 0.7728, time: 0:01:16
 Epoch: 228, lr: 2.0e-03, train_loss: 0.1727, train_acc: 0.9442 test_loss: 1.0682, test_acc: 0.7702, best: 0.7728, time: 0:01:16
 Epoch: 229, lr: 2.0e-03, train_loss: 0.1781, train_acc: 0.9360 test_loss: 1.1211, test_acc: 0.7608, best: 0.7728, time: 0:01:16
 Epoch: 230, lr: 2.0e-03, train_loss: 0.1867, train_acc: 0.9406 test_loss: 1.0447, test_acc: 0.7671, best: 0.7728, time: 0:01:16
 Epoch: 231, lr: 2.0e-03, train_loss: 0.1669, train_acc: 0.9440 test_loss: 1.0391, test_acc: 0.7676, best: 0.7728, time: 0:01:16
 Epoch: 232, lr: 2.0e-03, train_loss: 0.1851, train_acc: 0.9380 test_loss: 1.0089, test_acc: 0.7742, best: 0.7742, time: 0:01:16
 Epoch: 233, lr: 2.0e-03, train_loss: 0.1756, train_acc: 0.9378 test_loss: 1.0444, test_acc: 0.7715, best: 0.7742, time: 0:01:16
 Epoch: 234, lr: 2.0e-03, train_loss: 0.1701, train_acc: 0.9442 test_loss: 1.0591, test_acc: 0.7654, best: 0.7742, time: 0:01:16
 Epoch: 235, lr: 2.0e-03, train_loss: 0.1571, train_acc: 0.9484 test_loss: 1.0639, test_acc: 0.7672, best: 0.7742, time: 0:01:16
 Epoch: 236, lr: 2.0e-03, train_loss: 0.1797, train_acc: 0.9356 test_loss: 1.1071, test_acc: 0.7664, best: 0.7742, time: 0:01:16
 Epoch: 237, lr: 2.0e-03, train_loss: 0.1771, train_acc: 0.9422 test_loss: 1.0956, test_acc: 0.7616, best: 0.7742, time: 0:01:16
 Epoch: 238, lr: 2.0e-03, train_loss: 0.1852, train_acc: 0.9364 test_loss: 1.0798, test_acc: 0.7700, best: 0.7742, time: 0:01:16
 Epoch: 239, lr: 2.0e-03, train_loss: 0.1691, train_acc: 0.9430 test_loss: 1.1393, test_acc: 0.7658, best: 0.7742, time: 0:01:16
 Epoch: 240, lr: 4.0e-04, train_loss: 0.1692, train_acc: 0.9416 test_loss: 1.1022, test_acc: 0.7695, best: 0.7742, time: 0:01:16
 Epoch: 241, lr: 4.0e-04, train_loss: 0.1724, train_acc: 0.9404 test_loss: 1.0834, test_acc: 0.7766, best: 0.7766, time: 0:01:16
 Epoch: 242, lr: 4.0e-04, train_loss: 0.1671, train_acc: 0.9438 test_loss: 1.0541, test_acc: 0.7732, best: 0.7766, time: 0:01:16
 Epoch: 243, lr: 4.0e-04, train_loss: 0.1434, train_acc: 0.9476 test_loss: 1.1024, test_acc: 0.7756, best: 0.7766, time: 0:01:16
 Epoch: 244, lr: 4.0e-04, train_loss: 0.1614, train_acc: 0.9398 test_loss: 1.0834, test_acc: 0.7691, best: 0.7766, time: 0:01:16
 Epoch: 245, lr: 4.0e-04, train_loss: 0.1584, train_acc: 0.9444 test_loss: 1.0739, test_acc: 0.7694, best: 0.7766, time: 0:01:16
 Epoch: 246, lr: 4.0e-04, train_loss: 0.1881, train_acc: 0.9354 test_loss: 1.0507, test_acc: 0.7711, best: 0.7766, time: 0:01:16
 Epoch: 247, lr: 4.0e-04, train_loss: 0.1646, train_acc: 0.9434 test_loss: 1.0220, test_acc: 0.7708, best: 0.7766, time: 0:01:16
 Epoch: 248, lr: 4.0e-04, train_loss: 0.1673, train_acc: 0.9434 test_loss: 1.0506, test_acc: 0.7709, best: 0.7766, time: 0:01:16
 Epoch: 249, lr: 4.0e-04, train_loss: 0.1591, train_acc: 0.9456 test_loss: 1.0418, test_acc: 0.7685, best: 0.7766, time: 0:01:16
 Epoch: 250, lr: 4.0e-04, train_loss: 0.1556, train_acc: 0.9482 test_loss: 1.0333, test_acc: 0.7741, best: 0.7766, time: 0:01:16
 Epoch: 251, lr: 4.0e-04, train_loss: 0.1575, train_acc: 0.9460 test_loss: 1.0449, test_acc: 0.7714, best: 0.7766, time: 0:01:15
 Epoch: 252, lr: 4.0e-04, train_loss: 0.1626, train_acc: 0.9448 test_loss: 1.0756, test_acc: 0.7734, best: 0.7766, time: 0:01:16
 Epoch: 253, lr: 4.0e-04, train_loss: 0.1682, train_acc: 0.9394 test_loss: 1.0914, test_acc: 0.7716, best: 0.7766, time: 0:01:16
 Epoch: 254, lr: 4.0e-04, train_loss: 0.1676, train_acc: 0.9414 test_loss: 1.0781, test_acc: 0.7701, best: 0.7766, time: 0:01:16
 Epoch: 255, lr: 4.0e-04, train_loss: 0.1476, train_acc: 0.9512 test_loss: 1.0471, test_acc: 0.7734, best: 0.7766, time: 0:01:16
 Epoch: 256, lr: 4.0e-04, train_loss: 0.1489, train_acc: 0.9488 test_loss: 1.0352, test_acc: 0.7730, best: 0.7766, time: 0:01:15
 Epoch: 257, lr: 4.0e-04, train_loss: 0.1420, train_acc: 0.9530 test_loss: 1.0715, test_acc: 0.7684, best: 0.7766, time: 0:01:15
 Epoch: 258, lr: 4.0e-04, train_loss: 0.1619, train_acc: 0.9454 test_loss: 1.0159, test_acc: 0.7724, best: 0.7766, time: 0:01:16
 Epoch: 259, lr: 4.0e-04, train_loss: 0.1521, train_acc: 0.9476 test_loss: 1.0751, test_acc: 0.7716, best: 0.7766, time: 0:01:16
 Epoch: 260, lr: 4.0e-04, train_loss: 0.1522, train_acc: 0.9484 test_loss: 1.0307, test_acc: 0.7696, best: 0.7766, time: 0:01:16
 Epoch: 261, lr: 4.0e-04, train_loss: 0.1643, train_acc: 0.9408 test_loss: 1.0611, test_acc: 0.7736, best: 0.7766, time: 0:01:16
 Epoch: 262, lr: 4.0e-04, train_loss: 0.1506, train_acc: 0.9492 test_loss: 1.0583, test_acc: 0.7698, best: 0.7766, time: 0:01:16
 Epoch: 263, lr: 4.0e-04, train_loss: 0.1587, train_acc: 0.9472 test_loss: 1.0812, test_acc: 0.7675, best: 0.7766, time: 0:01:16
 Epoch: 264, lr: 4.0e-04, train_loss: 0.1406, train_acc: 0.9526 test_loss: 1.0380, test_acc: 0.7719, best: 0.7766, time: 0:01:16
 Epoch: 265, lr: 4.0e-04, train_loss: 0.1476, train_acc: 0.9520 test_loss: 1.0525, test_acc: 0.7695, best: 0.7766, time: 0:01:16
 Epoch: 266, lr: 4.0e-04, train_loss: 0.1491, train_acc: 0.9478 test_loss: 1.1023, test_acc: 0.7701, best: 0.7766, time: 0:01:16
 Epoch: 267, lr: 4.0e-04, train_loss: 0.1535, train_acc: 0.9518 test_loss: 1.0493, test_acc: 0.7735, best: 0.7766, time: 0:01:16
 Epoch: 268, lr: 4.0e-04, train_loss: 0.1592, train_acc: 0.9432 test_loss: 1.0249, test_acc: 0.7758, best: 0.7766, time: 0:01:16
 Epoch: 269, lr: 4.0e-04, train_loss: 0.1553, train_acc: 0.9474 test_loss: 1.0304, test_acc: 0.7728, best: 0.7766, time: 0:01:16
 Epoch: 270, lr: 8.0e-05, train_loss: 0.1558, train_acc: 0.9446 test_loss: 1.0430, test_acc: 0.7685, best: 0.7766, time: 0:01:16
 Epoch: 271, lr: 8.0e-05, train_loss: 0.1527, train_acc: 0.9482 test_loss: 1.0511, test_acc: 0.7725, best: 0.7766, time: 0:01:16
 Epoch: 272, lr: 8.0e-05, train_loss: 0.1495, train_acc: 0.9464 test_loss: 1.0400, test_acc: 0.7722, best: 0.7766, time: 0:01:16
 Epoch: 273, lr: 8.0e-05, train_loss: 0.1327, train_acc: 0.9550 test_loss: 1.0184, test_acc: 0.7712, best: 0.7766, time: 0:01:16
 Epoch: 274, lr: 8.0e-05, train_loss: 0.1388, train_acc: 0.9550 test_loss: 1.0847, test_acc: 0.7676, best: 0.7766, time: 0:01:16
 Epoch: 275, lr: 8.0e-05, train_loss: 0.1696, train_acc: 0.9408 test_loss: 1.0610, test_acc: 0.7694, best: 0.7766, time: 0:01:16
 Epoch: 276, lr: 8.0e-05, train_loss: 0.1482, train_acc: 0.9494 test_loss: 1.0389, test_acc: 0.7688, best: 0.7766, time: 0:01:16
 Epoch: 277, lr: 8.0e-05, train_loss: 0.1517, train_acc: 0.9476 test_loss: 1.0617, test_acc: 0.7685, best: 0.7766, time: 0:01:16
 Epoch: 278, lr: 8.0e-05, train_loss: 0.1560, train_acc: 0.9464 test_loss: 1.0478, test_acc: 0.7760, best: 0.7766, time: 0:01:16
 Epoch: 279, lr: 8.0e-05, train_loss: 0.1518, train_acc: 0.9472 test_loss: 1.0824, test_acc: 0.7696, best: 0.7766, time: 0:01:16
 Epoch: 280, lr: 8.0e-05, train_loss: 0.1479, train_acc: 0.9486 test_loss: 1.0732, test_acc: 0.7716, best: 0.7766, time: 0:01:16
 Epoch: 281, lr: 8.0e-05, train_loss: 0.1537, train_acc: 0.9512 test_loss: 1.0810, test_acc: 0.7724, best: 0.7766, time: 0:01:16
 Epoch: 282, lr: 8.0e-05, train_loss: 0.1544, train_acc: 0.9460 test_loss: 1.0481, test_acc: 0.7725, best: 0.7766, time: 0:01:16
 Epoch: 283, lr: 8.0e-05, train_loss: 0.1432, train_acc: 0.9494 test_loss: 1.0650, test_acc: 0.7706, best: 0.7766, time: 0:01:16
 Epoch: 284, lr: 8.0e-05, train_loss: 0.1480, train_acc: 0.9510 test_loss: 1.0585, test_acc: 0.7679, best: 0.7766, time: 0:01:16
 Epoch: 285, lr: 8.0e-05, train_loss: 0.1575, train_acc: 0.9460 test_loss: 1.0525, test_acc: 0.7745, best: 0.7766, time: 0:01:16
 Epoch: 286, lr: 8.0e-05, train_loss: 0.1507, train_acc: 0.9488 test_loss: 1.0791, test_acc: 0.7710, best: 0.7766, time: 0:01:16
 Epoch: 287, lr: 8.0e-05, train_loss: 0.1555, train_acc: 0.9460 test_loss: 1.0632, test_acc: 0.7749, best: 0.7766, time: 0:01:16
 Epoch: 288, lr: 8.0e-05, train_loss: 0.1417, train_acc: 0.9516 test_loss: 1.0522, test_acc: 0.7708, best: 0.7766, time: 0:01:16
 Epoch: 289, lr: 8.0e-05, train_loss: 0.1516, train_acc: 0.9482 test_loss: 1.0608, test_acc: 0.7684, best: 0.7766, time: 0:01:16
 Epoch: 290, lr: 8.0e-05, train_loss: 0.1621, train_acc: 0.9432 test_loss: 1.0344, test_acc: 0.7712, best: 0.7766, time: 0:01:16
 Epoch: 291, lr: 8.0e-05, train_loss: 0.1561, train_acc: 0.9466 test_loss: 1.0714, test_acc: 0.7695, best: 0.7766, time: 0:01:16
 Epoch: 292, lr: 8.0e-05, train_loss: 0.1458, train_acc: 0.9494 test_loss: 1.0768, test_acc: 0.7689, best: 0.7766, time: 0:01:16
 Epoch: 293, lr: 8.0e-05, train_loss: 0.1479, train_acc: 0.9512 test_loss: 1.0231, test_acc: 0.7742, best: 0.7766, time: 0:01:16
 Epoch: 294, lr: 8.0e-05, train_loss: 0.1459, train_acc: 0.9496 test_loss: 1.0397, test_acc: 0.7686, best: 0.7766, time: 0:01:16
 Epoch: 295, lr: 8.0e-05, train_loss: 0.1486, train_acc: 0.9498 test_loss: 1.0577, test_acc: 0.7701, best: 0.7766, time: 0:01:16
 Epoch: 296, lr: 8.0e-05, train_loss: 0.1555, train_acc: 0.9470 test_loss: 1.0745, test_acc: 0.7696, best: 0.7766, time: 0:01:16
 Epoch: 297, lr: 8.0e-05, train_loss: 0.1579, train_acc: 0.9450 test_loss: 1.0753, test_acc: 0.7688, best: 0.7766, time: 0:01:16
 Epoch: 298, lr: 8.0e-05, train_loss: 0.1507, train_acc: 0.9500 test_loss: 1.0579, test_acc: 0.7695, best: 0.7766, time: 0:01:16
 Epoch: 299, lr: 8.0e-05, train_loss: 0.1502, train_acc: 0.9480 test_loss: 1.0694, test_acc: 0.7700, best: 0.7766, time: 0:01:16
 Highest accuracy: 0.7766