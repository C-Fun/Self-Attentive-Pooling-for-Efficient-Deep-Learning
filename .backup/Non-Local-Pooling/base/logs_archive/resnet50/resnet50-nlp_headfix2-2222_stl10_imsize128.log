
 Run on time: 2022-07-01 02:33:23.334307

 Architecture: resnet50-nlp_headfix2-2222

 Pool Config: {
    "arch": "resnet50",
    "conv1": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "skip",
            "_stride": 2,
            "_psize": null,
            "_dim_reduced_ratio": null,
            "_num_heads": null,
            "_conv2d": null,
            "_win_norm": null
        }
    },
    "pool": {
        "_conv2d": null,
        "pool_cfg": {
            "_ptype": "maxp",
            "_stride": 2,
            "_psize": null,
            "_dim_reduced_ratio": null,
            "_num_heads": null,
            "_conv2d": null,
            "_win_norm": null
        }
    },
    "layer1": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "nlp",
            "_stride": 2,
            "_psize": 1,
            "_dim_reduced_ratio": 1,
            "_num_heads": 2,
            "_conv2d": "norm",
            "_win_norm": true
        }
    },
    "layer2": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "nlp",
            "_stride": 2,
            "_psize": 1,
            "_dim_reduced_ratio": 1,
            "_num_heads": 2,
            "_conv2d": "norm",
            "_win_norm": true
        }
    },
    "layer3": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "nlp",
            "_stride": 2,
            "_psize": 1,
            "_dim_reduced_ratio": 1,
            "_num_heads": 2,
            "_conv2d": "norm",
            "_win_norm": true
        }
    },
    "layer4": {
        "_conv2d": "norm",
        "pool_cfg": {
            "_ptype": "nlp",
            "_stride": 2,
            "_psize": 1,
            "_dim_reduced_ratio": 1,
            "_num_heads": 2,
            "_conv2d": "norm",
            "_win_norm": true
        }
    }
}

 Arguments:
	 gpu                  : True
	 log                  : True
	 seed                 : 0
	 momentum             : 0.9
	 amsgrad              : True
	 dataset              : STL10
	 batch_size           : 8
	 architecture         : RESNET50-NLP_HEADFIX2-2222
	 im_size              : 128
	 relu_threshold       : 4.0
	 learning_rate        : 0.01
	 pretrained_backbone  : 
	 pretrained_ann       : 
	 weight_decay         : 0.0
	 test_only            : False
	 epochs               : 300
	 lr_interval          : [180, 240, 270]
	 lr_reduce            : 5
	 optimizer            : SGD
	 dropout              : 0.2
	 kernel_size          : 3
	 dont_save            : False
	 visualize            : False
	 devices              : 0
 DataParallel(
  (module): Network(
    (net): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (pool): Pool2d(
          (logit): Sequential(
            (pool_weight): NLP_BASE(
              (downsample): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (multihead_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (restore): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Sigmoid()
              )
              (pos_embed): PositionEmbeddingLearned(
                (row_embed): Embedding(256, 128)
                (col_embed): Embedding(256, 128)
              )
            )
          )
          (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (pool): Pool2d(
          (logit): Sequential(
            (pool_weight): NLP_BASE(
              (downsample): Sequential(
                (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
                (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (multihead_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (restore): Sequential(
                (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
                (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Sigmoid()
              )
              (pos_embed): PositionEmbeddingLearned(
                (row_embed): Embedding(256, 256)
                (col_embed): Embedding(256, 256)
              )
            )
          )
          (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (pool): Pool2d(
          (logit): Sequential(
            (pool_weight): NLP_BASE(
              (downsample): Sequential(
                (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
                (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (multihead_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (restore): Sequential(
                (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
                (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Sigmoid()
              )
              (pos_embed): PositionEmbeddingLearned(
                (row_embed): Embedding(256, 512)
                (col_embed): Embedding(256, 512)
              )
            )
          )
          (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        )
      )
      (layer4): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (pool): Pool2d(
          (logit): Sequential(
            (pool_weight): NLP_BASE(
              (downsample): Sequential(
                (0): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1))
                (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU(inplace=True)
              )
              (multihead_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=2048, out_features=2048, bias=True)
              )
              (restore): Sequential(
                (0): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1))
                (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Sigmoid()
              )
              (pos_embed): PositionEmbeddingLearned(
                (row_embed): Embedding(256, 1024)
                (col_embed): Embedding(256, 1024)
              )
            )
          )
          (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        )
      )
      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
      (fc): Linear(in_features=2048, out_features=10, bias=True)
    )
  )
)
 SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0
)
 Epoch: 1, lr: 1.0e-02, train_loss: 3.1618, train_acc: 0.1516 test_loss: 3.7308, test_acc: 0.2099, best: 0.2099, time: 0:02:45
 Epoch: 2, lr: 1.0e-02, train_loss: 2.1107, train_acc: 0.2098 test_loss: 1.9859, test_acc: 0.2634, best: 0.2634, time: 0:02:46
 Epoch: 3, lr: 1.0e-02, train_loss: 1.9728, train_acc: 0.2460 test_loss: 1.8186, test_acc: 0.3021, best: 0.3021, time: 0:02:48
 Epoch: 4, lr: 1.0e-02, train_loss: 1.9051, train_acc: 0.2714 test_loss: 1.9422, test_acc: 0.3405, best: 0.3405, time: 0:02:48
 Epoch: 5, lr: 1.0e-02, train_loss: 1.8753, train_acc: 0.2856 test_loss: 1.7569, test_acc: 0.3399, best: 0.3405, time: 0:02:45
 Epoch: 6, lr: 1.0e-02, train_loss: 1.8248, train_acc: 0.3048 test_loss: 1.9457, test_acc: 0.3811, best: 0.3811, time: 0:02:47
 Epoch: 7, lr: 1.0e-02, train_loss: 1.8048, train_acc: 0.3260 test_loss: 1.7928, test_acc: 0.3390, best: 0.3811, time: 0:02:45
 Epoch: 8, lr: 1.0e-02, train_loss: 1.8106, train_acc: 0.3226 test_loss: 1.9485, test_acc: 0.3911, best: 0.3911, time: 0:02:50
 Epoch: 9, lr: 1.0e-02, train_loss: 1.7528, train_acc: 0.3402 test_loss: 1.7860, test_acc: 0.3877, best: 0.3911, time: 0:02:45
 Epoch: 10, lr: 1.0e-02, train_loss: 1.7116, train_acc: 0.3532 test_loss: 4.6278, test_acc: 0.3966, best: 0.3966, time: 0:02:48
 Epoch: 11, lr: 1.0e-02, train_loss: 1.6840, train_acc: 0.3668 test_loss: 2.3091, test_acc: 0.4060, best: 0.4060, time: 0:02:50
 Epoch: 12, lr: 1.0e-02, train_loss: 1.6491, train_acc: 0.3816 test_loss: 2.1584, test_acc: 0.4404, best: 0.4404, time: 0:02:47
 Epoch: 13, lr: 1.0e-02, train_loss: 1.6054, train_acc: 0.3946 test_loss: 3.2643, test_acc: 0.4416, best: 0.4416, time: 0:02:47
 Epoch: 14, lr: 1.0e-02, train_loss: 1.5604, train_acc: 0.4186 test_loss: 2.0299, test_acc: 0.4514, best: 0.4514, time: 0:03:03
 Epoch: 15, lr: 1.0e-02, train_loss: 1.5422, train_acc: 0.4234 test_loss: 1.9893, test_acc: 0.4358, best: 0.4514, time: 0:02:47
 Epoch: 16, lr: 1.0e-02, train_loss: 1.5304, train_acc: 0.4302 test_loss: 1.8469, test_acc: 0.4710, best: 0.4710, time: 0:02:56
 Epoch: 17, lr: 1.0e-02, train_loss: 1.5029, train_acc: 0.4296 test_loss: 2.0664, test_acc: 0.4833, best: 0.4833, time: 0:02:46
 Epoch: 18, lr: 1.0e-02, train_loss: 1.4980, train_acc: 0.4406 test_loss: 2.2211, test_acc: 0.5015, best: 0.5015, time: 0:02:53
 Epoch: 19, lr: 1.0e-02, train_loss: 1.4733, train_acc: 0.4522 test_loss: 1.5144, test_acc: 0.4853, best: 0.5015, time: 0:02:46
 Epoch: 20, lr: 1.0e-02, train_loss: 1.4457, train_acc: 0.4570 test_loss: 3.1520, test_acc: 0.4542, best: 0.5015, time: 0:02:50
 Epoch: 21, lr: 1.0e-02, train_loss: 1.4479, train_acc: 0.4656 test_loss: 1.5678, test_acc: 0.5010, best: 0.5015, time: 0:02:40
 Epoch: 22, lr: 1.0e-02, train_loss: 1.4018, train_acc: 0.4840 test_loss: 1.8679, test_acc: 0.5142, best: 0.5142, time: 0:02:47
 Epoch: 23, lr: 1.0e-02, train_loss: 1.4190, train_acc: 0.4786 test_loss: 2.3492, test_acc: 0.5068, best: 0.5142, time: 0:02:45
 Epoch: 24, lr: 1.0e-02, train_loss: 1.3647, train_acc: 0.5002 test_loss: 1.7951, test_acc: 0.4995, best: 0.5142, time: 0:02:46
 Epoch: 25, lr: 1.0e-02, train_loss: 1.3646, train_acc: 0.4984 test_loss: 1.6107, test_acc: 0.5231, best: 0.5231, time: 0:02:44
 Epoch: 26, lr: 1.0e-02, train_loss: 1.3800, train_acc: 0.4948 test_loss: 1.5116, test_acc: 0.5309, best: 0.5309, time: 0:02:52
 Epoch: 27, lr: 1.0e-02, train_loss: 1.3771, train_acc: 0.4906 test_loss: 2.1014, test_acc: 0.5420, best: 0.5420, time: 0:02:47
 Epoch: 28, lr: 1.0e-02, train_loss: 1.3247, train_acc: 0.5140 test_loss: 2.0406, test_acc: 0.5483, best: 0.5483, time: 0:02:50
 Epoch: 29, lr: 1.0e-02, train_loss: 1.3110, train_acc: 0.5184 test_loss: 2.3604, test_acc: 0.5390, best: 0.5483, time: 0:02:49
 Epoch: 30, lr: 1.0e-02, train_loss: 1.2750, train_acc: 0.5318 test_loss: 2.0305, test_acc: 0.5459, best: 0.5483, time: 0:02:45
 Epoch: 31, lr: 1.0e-02, train_loss: 1.2673, train_acc: 0.5292 test_loss: 1.5684, test_acc: 0.5519, best: 0.5519, time: 0:02:46
 Epoch: 32, lr: 1.0e-02, train_loss: 1.2656, train_acc: 0.5318 test_loss: 1.8490, test_acc: 0.5477, best: 0.5519, time: 0:02:46
 Epoch: 33, lr: 1.0e-02, train_loss: 1.2245, train_acc: 0.5586 test_loss: 1.5755, test_acc: 0.5689, best: 0.5689, time: 0:02:46
 Epoch: 34, lr: 1.0e-02, train_loss: 1.2053, train_acc: 0.5628 test_loss: 1.6212, test_acc: 0.5833, best: 0.5833, time: 0:02:45
 Epoch: 35, lr: 1.0e-02, train_loss: 1.1957, train_acc: 0.5624 test_loss: 1.4189, test_acc: 0.5924, best: 0.5924, time: 0:02:47
 Epoch: 36, lr: 1.0e-02, train_loss: 1.1681, train_acc: 0.5740 test_loss: 2.1787, test_acc: 0.5745, best: 0.5924, time: 0:02:44
 Epoch: 37, lr: 1.0e-02, train_loss: 1.1648, train_acc: 0.5778 test_loss: 2.2486, test_acc: 0.5711, best: 0.5924, time: 0:02:44
 Epoch: 38, lr: 1.0e-02, train_loss: 1.1477, train_acc: 0.5878 test_loss: 3.7690, test_acc: 0.5567, best: 0.5924, time: 0:02:45
 Epoch: 39, lr: 1.0e-02, train_loss: 1.1516, train_acc: 0.5844 test_loss: 1.7382, test_acc: 0.5505, best: 0.5924, time: 0:02:46
 Epoch: 40, lr: 1.0e-02, train_loss: 1.1133, train_acc: 0.5952 test_loss: 4.0321, test_acc: 0.5893, best: 0.5924, time: 0:02:52
 Epoch: 41, lr: 1.0e-02, train_loss: 1.1055, train_acc: 0.6032 test_loss: 4.6515, test_acc: 0.5703, best: 0.5924, time: 0:02:44
 Epoch: 42, lr: 1.0e-02, train_loss: 1.1095, train_acc: 0.5994 test_loss: 2.0006, test_acc: 0.5893, best: 0.5924, time: 0:02:48
 Epoch: 43, lr: 1.0e-02, train_loss: 1.0990, train_acc: 0.5978 test_loss: 2.4748, test_acc: 0.5785, best: 0.5924, time: 0:02:45
 Epoch: 44, lr: 1.0e-02, train_loss: 1.0899, train_acc: 0.6078 test_loss: 2.1043, test_acc: 0.6105, best: 0.6105, time: 0:02:47
 Epoch: 45, lr: 1.0e-02, train_loss: 1.0568, train_acc: 0.6190 test_loss: 1.6440, test_acc: 0.6144, best: 0.6144, time: 0:02:45
 Epoch: 46, lr: 1.0e-02, train_loss: 1.0484, train_acc: 0.6254 test_loss: 2.5902, test_acc: 0.6161, best: 0.6161, time: 0:02:45
 Epoch: 47, lr: 1.0e-02, train_loss: 1.0421, train_acc: 0.6246 test_loss: 3.7240, test_acc: 0.5978, best: 0.6161, time: 0:02:47
 Epoch: 48, lr: 1.0e-02, train_loss: 1.0332, train_acc: 0.6284 test_loss: 2.8769, test_acc: 0.5904, best: 0.6161, time: 0:02:43
 Epoch: 49, lr: 1.0e-02, train_loss: 1.0115, train_acc: 0.6300 test_loss: 1.7279, test_acc: 0.6158, best: 0.6161, time: 0:02:49
 Epoch: 50, lr: 1.0e-02, train_loss: 1.0114, train_acc: 0.6354 test_loss: 2.1693, test_acc: 0.6082, best: 0.6161, time: 0:02:45
 Epoch: 51, lr: 1.0e-02, train_loss: 1.0032, train_acc: 0.6388 test_loss: 1.7837, test_acc: 0.6165, best: 0.6165, time: 0:02:48
 Epoch: 52, lr: 1.0e-02, train_loss: 0.9799, train_acc: 0.6430 test_loss: 2.1978, test_acc: 0.6020, best: 0.6165, time: 0:02:41
 Epoch: 53, lr: 1.0e-02, train_loss: 0.9784, train_acc: 0.6478 test_loss: 9.5152, test_acc: 0.5891, best: 0.6165, time: 0:02:45
 Epoch: 54, lr: 1.0e-02, train_loss: 0.9680, train_acc: 0.6530 test_loss: 3.1455, test_acc: 0.5914, best: 0.6165, time: 0:02:42
 Epoch: 55, lr: 1.0e-02, train_loss: 0.9312, train_acc: 0.6628 test_loss: 1.5892, test_acc: 0.6256, best: 0.6256, time: 0:02:50
 Epoch: 56, lr: 1.0e-02, train_loss: 0.9261, train_acc: 0.6642 test_loss: 4.1280, test_acc: 0.5827, best: 0.6256, time: 0:02:44
 Epoch: 57, lr: 1.0e-02, train_loss: 0.9368, train_acc: 0.6648 test_loss: 1.9404, test_acc: 0.6151, best: 0.6256, time: 0:02:43
 Epoch: 58, lr: 1.0e-02, train_loss: 0.9067, train_acc: 0.6748 test_loss: 1.3239, test_acc: 0.6190, best: 0.6256, time: 0:02:46
 Epoch: 59, lr: 1.0e-02, train_loss: 0.9438, train_acc: 0.6620 test_loss: 2.5983, test_acc: 0.6095, best: 0.6256, time: 0:02:44
 Epoch: 60, lr: 1.0e-02, train_loss: 0.9494, train_acc: 0.6588 test_loss: 2.3998, test_acc: 0.6086, best: 0.6256, time: 0:02:51
 Epoch: 61, lr: 1.0e-02, train_loss: 0.8958, train_acc: 0.6862 test_loss: 1.2214, test_acc: 0.6571, best: 0.6571, time: 0:02:46
 Epoch: 62, lr: 1.0e-02, train_loss: 0.8946, train_acc: 0.6782 test_loss: 1.2955, test_acc: 0.6505, best: 0.6571, time: 0:02:44
 Epoch: 63, lr: 1.0e-02, train_loss: 0.8785, train_acc: 0.6792 test_loss: 1.7091, test_acc: 0.6419, best: 0.6571, time: 0:02:44
 Epoch: 64, lr: 1.0e-02, train_loss: 0.9011, train_acc: 0.6790 test_loss: 1.5550, test_acc: 0.6392, best: 0.6571, time: 0:02:46
 Epoch: 65, lr: 1.0e-02, train_loss: 0.8633, train_acc: 0.6920 test_loss: 1.5427, test_acc: 0.6401, best: 0.6571, time: 0:02:45
 Epoch: 66, lr: 1.0e-02, train_loss: 0.8649, train_acc: 0.6858 test_loss: 1.7547, test_acc: 0.6571, best: 0.6571, time: 0:02:44
 Epoch: 67, lr: 1.0e-02, train_loss: 0.8656, train_acc: 0.6918 test_loss: 3.2420, test_acc: 0.6492, best: 0.6571, time: 0:02:48
 Epoch: 68, lr: 1.0e-02, train_loss: 0.8297, train_acc: 0.7000 test_loss: 4.7270, test_acc: 0.6261, best: 0.6571, time: 0:02:45
 Epoch: 69, lr: 1.0e-02, train_loss: 0.8286, train_acc: 0.7050 test_loss: 1.9804, test_acc: 0.6520, best: 0.6571, time: 0:02:45
 Epoch: 70, lr: 1.0e-02, train_loss: 0.8097, train_acc: 0.7100 test_loss: 4.3680, test_acc: 0.6364, best: 0.6571, time: 0:02:45
 Epoch: 71, lr: 1.0e-02, train_loss: 0.8263, train_acc: 0.7058 test_loss: 2.4424, test_acc: 0.6366, best: 0.6571, time: 0:02:44
 Epoch: 72, lr: 1.0e-02, train_loss: 0.7930, train_acc: 0.7162 test_loss: 1.8037, test_acc: 0.6425, best: 0.6571, time: 0:02:45
 Epoch: 73, lr: 1.0e-02, train_loss: 0.7850, train_acc: 0.7216 test_loss: 1.3633, test_acc: 0.6216, best: 0.6571, time: 0:02:47
 Epoch: 74, lr: 1.0e-02, train_loss: 0.8334, train_acc: 0.7004 test_loss: 1.8162, test_acc: 0.6254, best: 0.6571, time: 0:02:45
 Epoch: 75, lr: 1.0e-02, train_loss: 0.8551, train_acc: 0.7028 test_loss: 0.9740, test_acc: 0.6540, best: 0.6571, time: 0:02:50
 Epoch: 76, lr: 1.0e-02, train_loss: 0.8366, train_acc: 0.6974 test_loss: 0.9710, test_acc: 0.6771, best: 0.6771, time: 0:02:47
 Epoch: 77, lr: 1.0e-02, train_loss: 0.8163, train_acc: 0.7096 test_loss: 1.0344, test_acc: 0.6657, best: 0.6771, time: 0:02:44
 Epoch: 78, lr: 1.0e-02, train_loss: 0.8114, train_acc: 0.7082 test_loss: 1.2587, test_acc: 0.6585, best: 0.6771, time: 0:02:45
 Epoch: 79, lr: 1.0e-02, train_loss: 0.8006, train_acc: 0.7150 test_loss: 1.6044, test_acc: 0.6431, best: 0.6771, time: 0:02:48
 Epoch: 80, lr: 1.0e-02, train_loss: 0.7675, train_acc: 0.7352 test_loss: 1.3415, test_acc: 0.6466, best: 0.6771, time: 0:02:45
 Epoch: 81, lr: 1.0e-02, train_loss: 0.7677, train_acc: 0.7266 test_loss: 1.2277, test_acc: 0.6625, best: 0.6771, time: 0:02:46
 Epoch: 82, lr: 1.0e-02, train_loss: 0.7318, train_acc: 0.7396 test_loss: 1.3088, test_acc: 0.6659, best: 0.6771, time: 0:02:47
 Epoch: 83, lr: 1.0e-02, train_loss: 0.7631, train_acc: 0.7254 test_loss: 1.1212, test_acc: 0.6734, best: 0.6771, time: 0:02:44
 Epoch: 84, lr: 1.0e-02, train_loss: 0.7467, train_acc: 0.7404 test_loss: 1.2294, test_acc: 0.6607, best: 0.6771, time: 0:02:47
 Epoch: 85, lr: 1.0e-02, train_loss: 0.7589, train_acc: 0.7260 test_loss: 1.0559, test_acc: 0.6657, best: 0.6771, time: 0:02:43
 Epoch: 86, lr: 1.0e-02, train_loss: 0.7110, train_acc: 0.7446 test_loss: 0.9776, test_acc: 0.6774, best: 0.6774, time: 0:02:47
 Epoch: 87, lr: 1.0e-02, train_loss: 0.6888, train_acc: 0.7562 test_loss: 1.1004, test_acc: 0.6640, best: 0.6774, time: 0:02:44
 Epoch: 88, lr: 1.0e-02, train_loss: 0.7157, train_acc: 0.7526 test_loss: 0.9804, test_acc: 0.6866, best: 0.6866, time: 0:02:47
 Epoch: 89, lr: 1.0e-02, train_loss: 0.6987, train_acc: 0.7552 test_loss: 1.0070, test_acc: 0.6765, best: 0.6866, time: 0:02:43
 Epoch: 90, lr: 1.0e-02, train_loss: 0.7132, train_acc: 0.7526 test_loss: 1.1502, test_acc: 0.6586, best: 0.6866, time: 0:02:48
 Epoch: 91, lr: 1.0e-02, train_loss: 0.6956, train_acc: 0.7550 test_loss: 1.0452, test_acc: 0.6744, best: 0.6866, time: 0:02:47
 Epoch: 92, lr: 1.0e-02, train_loss: 0.6812, train_acc: 0.7610 test_loss: 1.0515, test_acc: 0.6734, best: 0.6866, time: 0:02:45
 Epoch: 93, lr: 1.0e-02, train_loss: 0.6865, train_acc: 0.7534 test_loss: 1.1116, test_acc: 0.6773, best: 0.6866, time: 0:02:44
 Epoch: 94, lr: 1.0e-02, train_loss: 0.6496, train_acc: 0.7732 test_loss: 1.0295, test_acc: 0.6779, best: 0.6866, time: 0:02:42
 Epoch: 95, lr: 1.0e-02, train_loss: 0.6596, train_acc: 0.7742 test_loss: 1.0534, test_acc: 0.6894, best: 0.6894, time: 0:02:47
 Epoch: 96, lr: 1.0e-02, train_loss: 0.6323, train_acc: 0.7770 test_loss: 1.1744, test_acc: 0.6739, best: 0.6894, time: 0:02:44
 Epoch: 97, lr: 1.0e-02, train_loss: 0.6259, train_acc: 0.7778 test_loss: 1.0968, test_acc: 0.6846, best: 0.6894, time: 0:02:41
 Epoch: 98, lr: 1.0e-02, train_loss: 0.6198, train_acc: 0.7872 test_loss: 1.1207, test_acc: 0.6830, best: 0.6894, time: 0:02:44
 Epoch: 99, lr: 1.0e-02, train_loss: 0.6279, train_acc: 0.7810 test_loss: 0.9971, test_acc: 0.6841, best: 0.6894, time: 0:02:44
 Epoch: 100, lr: 1.0e-02, train_loss: 0.6384, train_acc: 0.7710 test_loss: 0.9659, test_acc: 0.6890, best: 0.6894, time: 0:02:43
 Epoch: 101, lr: 1.0e-02, train_loss: 0.6161, train_acc: 0.7852 test_loss: 0.9895, test_acc: 0.6837, best: 0.6894, time: 0:02:43
 Epoch: 102, lr: 1.0e-02, train_loss: 0.6087, train_acc: 0.7874 test_loss: 1.0078, test_acc: 0.6911, best: 0.6911, time: 0:02:48
 Epoch: 103, lr: 1.0e-02, train_loss: 0.5892, train_acc: 0.7920 test_loss: 1.0060, test_acc: 0.6949, best: 0.6949, time: 0:02:45
 Epoch: 104, lr: 1.0e-02, train_loss: 0.5916, train_acc: 0.7920 test_loss: 1.0743, test_acc: 0.6856, best: 0.6949, time: 0:02:46
 Epoch: 105, lr: 1.0e-02, train_loss: 0.5985, train_acc: 0.7918 test_loss: 1.2046, test_acc: 0.6724, best: 0.6949, time: 0:02:44
 Epoch: 106, lr: 1.0e-02, train_loss: 0.5831, train_acc: 0.7976 test_loss: 1.0781, test_acc: 0.6866, best: 0.6949, time: 0:02:47
 Epoch: 107, lr: 1.0e-02, train_loss: 0.5558, train_acc: 0.8082 test_loss: 0.9980, test_acc: 0.7087, best: 0.7087, time: 0:02:46
 Epoch: 108, lr: 1.0e-02, train_loss: 0.5562, train_acc: 0.8042 test_loss: 1.1820, test_acc: 0.6755, best: 0.7087, time: 0:02:46
 Epoch: 109, lr: 1.0e-02, train_loss: 0.5488, train_acc: 0.8068 test_loss: 1.2331, test_acc: 0.6713, best: 0.7087, time: 0:02:47
 Epoch: 110, lr: 1.0e-02, train_loss: 0.5459, train_acc: 0.8148 test_loss: 1.0869, test_acc: 0.6945, best: 0.7087, time: 0:02:47
 Epoch: 111, lr: 1.0e-02, train_loss: 0.5559, train_acc: 0.8052 test_loss: 1.1732, test_acc: 0.6847, best: 0.7087, time: 0:02:45
 Epoch: 112, lr: 1.0e-02, train_loss: 0.5460, train_acc: 0.8092 test_loss: 1.0067, test_acc: 0.7016, best: 0.7087, time: 0:02:42
 Epoch: 113, lr: 1.0e-02, train_loss: 0.5352, train_acc: 0.8144 test_loss: 1.0192, test_acc: 0.6920, best: 0.7087, time: 0:02:41
 Epoch: 114, lr: 1.0e-02, train_loss: 0.5338, train_acc: 0.8166 test_loss: 1.0398, test_acc: 0.6963, best: 0.7087, time: 0:02:31
 Epoch: 115, lr: 1.0e-02, train_loss: 0.5332, train_acc: 0.8114 test_loss: 1.0608, test_acc: 0.6940, best: 0.7087, time: 0:02:41
 Epoch: 116, lr: 1.0e-02, train_loss: 0.5458, train_acc: 0.8056 test_loss: 1.0478, test_acc: 0.7063, best: 0.7087, time: 0:02:45
 Epoch: 117, lr: 1.0e-02, train_loss: 0.5008, train_acc: 0.8230 test_loss: 1.2621, test_acc: 0.6866, best: 0.7087, time: 0:02:52
 Epoch: 118, lr: 1.0e-02, train_loss: 0.5125, train_acc: 0.8198 test_loss: 1.1804, test_acc: 0.7006, best: 0.7087, time: 0:02:46
 Epoch: 119, lr: 1.0e-02, train_loss: 0.5236, train_acc: 0.8142 test_loss: 1.0961, test_acc: 0.6884, best: 0.7087, time: 0:02:46
 Epoch: 120, lr: 1.0e-02, train_loss: 0.5010, train_acc: 0.8248 test_loss: 1.0271, test_acc: 0.7006, best: 0.7087, time: 0:02:42
 Epoch: 121, lr: 1.0e-02, train_loss: 0.5213, train_acc: 0.8178 test_loss: 1.7769, test_acc: 0.6771, best: 0.7087, time: 0:02:49
 Epoch: 122, lr: 1.0e-02, train_loss: 0.5257, train_acc: 0.8124 test_loss: 1.0757, test_acc: 0.7071, best: 0.7087, time: 0:02:44
 Epoch: 123, lr: 1.0e-02, train_loss: 0.5001, train_acc: 0.8298 test_loss: 1.1223, test_acc: 0.7000, best: 0.7087, time: 0:02:40
 Epoch: 124, lr: 1.0e-02, train_loss: 0.4937, train_acc: 0.8290 test_loss: 1.1686, test_acc: 0.7004, best: 0.7087, time: 0:02:40
 Epoch: 125, lr: 1.0e-02, train_loss: 0.4817, train_acc: 0.8378 test_loss: 2.1563, test_acc: 0.6594, best: 0.7087, time: 0:02:47
 Epoch: 126, lr: 1.0e-02, train_loss: 0.4881, train_acc: 0.8286 test_loss: 2.1036, test_acc: 0.6801, best: 0.7087, time: 0:02:48
 Epoch: 127, lr: 1.0e-02, train_loss: 0.4895, train_acc: 0.8258 test_loss: 1.8902, test_acc: 0.6800, best: 0.7087, time: 0:02:44
 Epoch: 128, lr: 1.0e-02, train_loss: 0.4753, train_acc: 0.8348 test_loss: 1.0629, test_acc: 0.7031, best: 0.7087, time: 0:02:46
 Epoch: 129, lr: 1.0e-02, train_loss: 0.4719, train_acc: 0.8332 test_loss: 2.4710, test_acc: 0.6901, best: 0.7087, time: 0:02:43
 Epoch: 130, lr: 1.0e-02, train_loss: 0.4742, train_acc: 0.8404 test_loss: 1.4361, test_acc: 0.6825, best: 0.7087, time: 0:02:44
 Epoch: 131, lr: 1.0e-02, train_loss: 0.4675, train_acc: 0.8414 test_loss: 1.2655, test_acc: 0.6981, best: 0.7087, time: 0:02:44
 Epoch: 132, lr: 1.0e-02, train_loss: 0.4629, train_acc: 0.8378 test_loss: 1.2166, test_acc: 0.6960, best: 0.7087, time: 0:02:45
 Epoch: 133, lr: 1.0e-02, train_loss: 0.4751, train_acc: 0.8426 test_loss: 1.1425, test_acc: 0.7019, best: 0.7087, time: 0:02:46
 Epoch: 134, lr: 1.0e-02, train_loss: 0.4554, train_acc: 0.8474 test_loss: 1.1467, test_acc: 0.7071, best: 0.7087, time: 0:02:43
 Epoch: 135, lr: 1.0e-02, train_loss: 0.4588, train_acc: 0.8434 test_loss: 1.2347, test_acc: 0.6957, best: 0.7087, time: 0:02:45
 Epoch: 136, lr: 1.0e-02, train_loss: 0.4553, train_acc: 0.8478 test_loss: 1.0815, test_acc: 0.7174, best: 0.7174, time: 0:02:46
 Epoch: 137, lr: 1.0e-02, train_loss: 0.4424, train_acc: 0.8458 test_loss: 1.0741, test_acc: 0.7228, best: 0.7228, time: 0:02:45
 Epoch: 138, lr: 1.0e-02, train_loss: 0.4348, train_acc: 0.8476 test_loss: 1.1546, test_acc: 0.7089, best: 0.7228, time: 0:02:44
 Epoch: 139, lr: 1.0e-02, train_loss: 0.4405, train_acc: 0.8504 test_loss: 1.2365, test_acc: 0.6931, best: 0.7228, time: 0:02:44
 Epoch: 140, lr: 1.0e-02, train_loss: 0.4298, train_acc: 0.8524 test_loss: 1.2630, test_acc: 0.6995, best: 0.7228, time: 0:02:44
 Epoch: 141, lr: 1.0e-02, train_loss: 0.4384, train_acc: 0.8482 test_loss: 1.1962, test_acc: 0.6951, best: 0.7228, time: 0:02:47
 Epoch: 142, lr: 1.0e-02, train_loss: 0.4167, train_acc: 0.8554 test_loss: 1.1617, test_acc: 0.7093, best: 0.7228, time: 0:02:44
 Epoch: 143, lr: 1.0e-02, train_loss: 0.4212, train_acc: 0.8512 test_loss: 1.1946, test_acc: 0.7060, best: 0.7228, time: 0:02:44
 Epoch: 144, lr: 1.0e-02, train_loss: 0.4116, train_acc: 0.8562 test_loss: 1.0726, test_acc: 0.7117, best: 0.7228, time: 0:02:44
 Epoch: 145, lr: 1.0e-02, train_loss: 0.4060, train_acc: 0.8622 test_loss: 1.0991, test_acc: 0.7063, best: 0.7228, time: 0:02:46
 Epoch: 146, lr: 1.0e-02, train_loss: 0.4131, train_acc: 0.8558 test_loss: 1.1041, test_acc: 0.7129, best: 0.7228, time: 0:02:44
 Epoch: 147, lr: 1.0e-02, train_loss: 0.4098, train_acc: 0.8586 test_loss: 1.2020, test_acc: 0.6964, best: 0.7228, time: 0:02:43
 Epoch: 148, lr: 1.0e-02, train_loss: 0.4136, train_acc: 0.8542 test_loss: 1.0738, test_acc: 0.7164, best: 0.7228, time: 0:02:47
 Epoch: 149, lr: 1.0e-02, train_loss: 0.3992, train_acc: 0.8618 test_loss: 1.1787, test_acc: 0.6983, best: 0.7228, time: 0:02:44
 Epoch: 150, lr: 1.0e-02, train_loss: 0.3833, train_acc: 0.8690 test_loss: 1.2007, test_acc: 0.7036, best: 0.7228, time: 0:02:45
 Epoch: 151, lr: 1.0e-02, train_loss: 0.4126, train_acc: 0.8584 test_loss: 1.1647, test_acc: 0.7165, best: 0.7228, time: 0:02:46
 Epoch: 152, lr: 1.0e-02, train_loss: 0.4229, train_acc: 0.8516 test_loss: 1.1991, test_acc: 0.6969, best: 0.7228, time: 0:02:47
 Epoch: 153, lr: 1.0e-02, train_loss: 0.3986, train_acc: 0.8608 test_loss: 1.2060, test_acc: 0.7120, best: 0.7228, time: 0:02:42
 Epoch: 154, lr: 1.0e-02, train_loss: 0.3956, train_acc: 0.8552 test_loss: 1.1388, test_acc: 0.7164, best: 0.7228, time: 0:02:45
 Epoch: 155, lr: 1.0e-02, train_loss: 0.3894, train_acc: 0.8646 test_loss: 1.2831, test_acc: 0.7106, best: 0.7228, time: 0:02:44
 Epoch: 156, lr: 1.0e-02, train_loss: 0.3537, train_acc: 0.8766 test_loss: 1.1492, test_acc: 0.7121, best: 0.7228, time: 0:02:46
 Epoch: 157, lr: 1.0e-02, train_loss: 0.3955, train_acc: 0.8656 test_loss: 1.4524, test_acc: 0.7119, best: 0.7228, time: 0:02:46
 Epoch: 158, lr: 1.0e-02, train_loss: 0.3786, train_acc: 0.8716 test_loss: 1.2120, test_acc: 0.7149, best: 0.7228, time: 0:02:44
 Epoch: 159, lr: 1.0e-02, train_loss: 0.3928, train_acc: 0.8694 test_loss: 1.1728, test_acc: 0.7083, best: 0.7228, time: 0:02:43
 Epoch: 160, lr: 1.0e-02, train_loss: 0.3791, train_acc: 0.8648 test_loss: 1.2158, test_acc: 0.7046, best: 0.7228, time: 0:02:44
 Epoch: 161, lr: 1.0e-02, train_loss: 0.3807, train_acc: 0.8652 test_loss: 1.1665, test_acc: 0.7106, best: 0.7228, time: 0:02:46
 Epoch: 162, lr: 1.0e-02, train_loss: 0.3750, train_acc: 0.8724 test_loss: 1.1799, test_acc: 0.7104, best: 0.7228, time: 0:02:46
 Epoch: 163, lr: 1.0e-02, train_loss: 0.3530, train_acc: 0.8838 test_loss: 1.2823, test_acc: 0.7035, best: 0.7228, time: 0:02:46
 Epoch: 164, lr: 1.0e-02, train_loss: 0.3477, train_acc: 0.8766 test_loss: 1.2140, test_acc: 0.7031, best: 0.7228, time: 0:02:45
 Epoch: 165, lr: 1.0e-02, train_loss: 0.3746, train_acc: 0.8744 test_loss: 1.1291, test_acc: 0.7260, best: 0.7260, time: 0:02:45
 Epoch: 166, lr: 1.0e-02, train_loss: 0.3563, train_acc: 0.8832 test_loss: 1.3390, test_acc: 0.7115, best: 0.7260, time: 0:02:49
 Epoch: 167, lr: 1.0e-02, train_loss: 0.3675, train_acc: 0.8730 test_loss: 1.4352, test_acc: 0.6870, best: 0.7260, time: 0:02:44
 Epoch: 168, lr: 1.0e-02, train_loss: 0.3686, train_acc: 0.8732 test_loss: 1.2688, test_acc: 0.7225, best: 0.7260, time: 0:02:46
 Epoch: 169, lr: 1.0e-02, train_loss: 0.3661, train_acc: 0.8684 test_loss: 1.1846, test_acc: 0.7184, best: 0.7260, time: 0:02:45
 Epoch: 170, lr: 1.0e-02, train_loss: 0.3539, train_acc: 0.8774 test_loss: 1.0651, test_acc: 0.7240, best: 0.7260, time: 0:02:50
 Epoch: 171, lr: 1.0e-02, train_loss: 0.3486, train_acc: 0.8824 test_loss: 1.0928, test_acc: 0.7194, best: 0.7260, time: 0:02:45
 Epoch: 172, lr: 1.0e-02, train_loss: 0.3542, train_acc: 0.8842 test_loss: 1.0807, test_acc: 0.7298, best: 0.7298, time: 0:02:46
 Epoch: 173, lr: 1.0e-02, train_loss: 0.3518, train_acc: 0.8834 test_loss: 1.3570, test_acc: 0.6916, best: 0.7298, time: 0:02:43
 Epoch: 174, lr: 1.0e-02, train_loss: 0.3519, train_acc: 0.8804 test_loss: 1.1053, test_acc: 0.7216, best: 0.7298, time: 0:02:42
 Epoch: 175, lr: 1.0e-02, train_loss: 0.3360, train_acc: 0.8838 test_loss: 1.1709, test_acc: 0.7262, best: 0.7298, time: 0:02:45
 Epoch: 176, lr: 1.0e-02, train_loss: 0.3486, train_acc: 0.8780 test_loss: 1.2141, test_acc: 0.7111, best: 0.7298, time: 0:02:47
 Epoch: 177, lr: 1.0e-02, train_loss: 0.3301, train_acc: 0.8836 test_loss: 1.1273, test_acc: 0.7289, best: 0.7298, time: 0:02:45
 Epoch: 178, lr: 1.0e-02, train_loss: 0.3330, train_acc: 0.8828 test_loss: 1.1856, test_acc: 0.7150, best: 0.7298, time: 0:02:44
 Epoch: 179, lr: 1.0e-02, train_loss: 0.3469, train_acc: 0.8834 test_loss: 1.2143, test_acc: 0.7189, best: 0.7298, time: 0:02:44
 Epoch: 180, lr: 2.0e-03, train_loss: 0.2659, train_acc: 0.9076 test_loss: 1.1276, test_acc: 0.7325, best: 0.7325, time: 0:02:47
 Epoch: 181, lr: 2.0e-03, train_loss: 0.2689, train_acc: 0.9080 test_loss: 1.1273, test_acc: 0.7344, best: 0.7344, time: 0:02:41
 Epoch: 182, lr: 2.0e-03, train_loss: 0.2496, train_acc: 0.9150 test_loss: 1.1769, test_acc: 0.7361, best: 0.7361, time: 0:02:47
 Epoch: 183, lr: 2.0e-03, train_loss: 0.2319, train_acc: 0.9204 test_loss: 1.1516, test_acc: 0.7335, best: 0.7361, time: 0:02:46
 Epoch: 184, lr: 2.0e-03, train_loss: 0.2419, train_acc: 0.9160 test_loss: 1.1447, test_acc: 0.7389, best: 0.7389, time: 0:02:51
 Epoch: 185, lr: 2.0e-03, train_loss: 0.2419, train_acc: 0.9208 test_loss: 1.1116, test_acc: 0.7404, best: 0.7404, time: 0:02:47
 Epoch: 186, lr: 2.0e-03, train_loss: 0.2303, train_acc: 0.9238 test_loss: 1.2247, test_acc: 0.7339, best: 0.7404, time: 0:02:44
 Epoch: 187, lr: 2.0e-03, train_loss: 0.2305, train_acc: 0.9230 test_loss: 1.1250, test_acc: 0.7446, best: 0.7446, time: 0:02:46
 Epoch: 188, lr: 2.0e-03, train_loss: 0.2272, train_acc: 0.9218 test_loss: 1.1891, test_acc: 0.7456, best: 0.7456, time: 0:02:47
 Epoch: 189, lr: 2.0e-03, train_loss: 0.2360, train_acc: 0.9186 test_loss: 1.1326, test_acc: 0.7445, best: 0.7456, time: 0:02:42
 Epoch: 190, lr: 2.0e-03, train_loss: 0.2136, train_acc: 0.9288 test_loss: 1.1129, test_acc: 0.7490, best: 0.7490, time: 0:02:46
 Epoch: 191, lr: 2.0e-03, train_loss: 0.2004, train_acc: 0.9324 test_loss: 1.1744, test_acc: 0.7428, best: 0.7490, time: 0:02:43
 Epoch: 192, lr: 2.0e-03, train_loss: 0.2157, train_acc: 0.9276 test_loss: 1.2015, test_acc: 0.7400, best: 0.7490, time: 0:02:42
 Epoch: 193, lr: 2.0e-03, train_loss: 0.2180, train_acc: 0.9286 test_loss: 1.1804, test_acc: 0.7392, best: 0.7490, time: 0:02:42
 Epoch: 194, lr: 2.0e-03, train_loss: 0.2281, train_acc: 0.9260 test_loss: 1.1205, test_acc: 0.7464, best: 0.7490, time: 0:02:43
 Epoch: 195, lr: 2.0e-03, train_loss: 0.2283, train_acc: 0.9212 test_loss: 1.1805, test_acc: 0.7364, best: 0.7490, time: 0:02:52
 Epoch: 196, lr: 2.0e-03, train_loss: 0.2148, train_acc: 0.9268 test_loss: 1.1656, test_acc: 0.7361, best: 0.7490, time: 0:02:42
 Epoch: 197, lr: 2.0e-03, train_loss: 0.2020, train_acc: 0.9356 test_loss: 1.2619, test_acc: 0.7354, best: 0.7490, time: 0:02:46
 Epoch: 198, lr: 2.0e-03, train_loss: 0.2193, train_acc: 0.9224 test_loss: 1.2220, test_acc: 0.7312, best: 0.7490, time: 0:02:43
 Epoch: 199, lr: 2.0e-03, train_loss: 0.2097, train_acc: 0.9298 test_loss: 1.1748, test_acc: 0.7424, best: 0.7490, time: 0:02:44
 Epoch: 200, lr: 2.0e-03, train_loss: 0.2172, train_acc: 0.9260 test_loss: 1.0844, test_acc: 0.7549, best: 0.7549, time: 0:02:44
 Epoch: 201, lr: 2.0e-03, train_loss: 0.2207, train_acc: 0.9252 test_loss: 1.1572, test_acc: 0.7474, best: 0.7549, time: 0:02:42
 Epoch: 202, lr: 2.0e-03, train_loss: 0.2284, train_acc: 0.9172 test_loss: 1.1081, test_acc: 0.7462, best: 0.7549, time: 0:02:45
 Epoch: 203, lr: 2.0e-03, train_loss: 0.2143, train_acc: 0.9300 test_loss: 1.1747, test_acc: 0.7455, best: 0.7549, time: 0:02:43
 Epoch: 204, lr: 2.0e-03, train_loss: 0.2072, train_acc: 0.9292 test_loss: 1.1763, test_acc: 0.7485, best: 0.7549, time: 0:02:42
 Epoch: 205, lr: 2.0e-03, train_loss: 0.2058, train_acc: 0.9306 test_loss: 1.1782, test_acc: 0.7411, best: 0.7549, time: 0:02:42
 Epoch: 206, lr: 2.0e-03, train_loss: 0.2047, train_acc: 0.9300 test_loss: 1.1680, test_acc: 0.7425, best: 0.7549, time: 0:02:45
 Epoch: 207, lr: 2.0e-03, train_loss: 0.2103, train_acc: 0.9282 test_loss: 1.1767, test_acc: 0.7352, best: 0.7549, time: 0:02:42
 Epoch: 208, lr: 2.0e-03, train_loss: 0.2132, train_acc: 0.9274 test_loss: 1.2600, test_acc: 0.7355, best: 0.7549, time: 0:02:42
 Epoch: 209, lr: 2.0e-03, train_loss: 0.1989, train_acc: 0.9332 test_loss: 1.2331, test_acc: 0.7375, best: 0.7549, time: 0:02:42
 Epoch: 210, lr: 2.0e-03, train_loss: 0.2081, train_acc: 0.9276 test_loss: 1.1892, test_acc: 0.7428, best: 0.7549, time: 0:02:43
 Epoch: 211, lr: 2.0e-03, train_loss: 0.2036, train_acc: 0.9312 test_loss: 1.2190, test_acc: 0.7425, best: 0.7549, time: 0:02:41
 Epoch: 212, lr: 2.0e-03, train_loss: 0.2168, train_acc: 0.9220 test_loss: 1.1353, test_acc: 0.7438, best: 0.7549, time: 0:02:41
 Epoch: 213, lr: 2.0e-03, train_loss: 0.1868, train_acc: 0.9376 test_loss: 1.2270, test_acc: 0.7386, best: 0.7549, time: 0:02:41
 Epoch: 214, lr: 2.0e-03, train_loss: 0.1713, train_acc: 0.9414 test_loss: 1.2251, test_acc: 0.7436, best: 0.7549, time: 0:02:43
 Epoch: 215, lr: 2.0e-03, train_loss: 0.1988, train_acc: 0.9324 test_loss: 1.2194, test_acc: 0.7378, best: 0.7549, time: 0:02:42
 Epoch: 216, lr: 2.0e-03, train_loss: 0.2013, train_acc: 0.9324 test_loss: 1.1960, test_acc: 0.7466, best: 0.7549, time: 0:02:41
 Epoch: 217, lr: 2.0e-03, train_loss: 0.1993, train_acc: 0.9322 test_loss: 1.1736, test_acc: 0.7428, best: 0.7549, time: 0:02:41
 Epoch: 218, lr: 2.0e-03, train_loss: 0.1945, train_acc: 0.9332 test_loss: 1.2230, test_acc: 0.7408, best: 0.7549, time: 0:02:42
 Epoch: 219, lr: 2.0e-03, train_loss: 0.1965, train_acc: 0.9334 test_loss: 1.2390, test_acc: 0.7356, best: 0.7549, time: 0:02:41
 Epoch: 220, lr: 2.0e-03, train_loss: 0.2037, train_acc: 0.9318 test_loss: 1.2080, test_acc: 0.7365, best: 0.7549, time: 0:02:41
 Epoch: 221, lr: 2.0e-03, train_loss: 0.1901, train_acc: 0.9372 test_loss: 1.2890, test_acc: 0.7348, best: 0.7549, time: 0:02:41
 Epoch: 222, lr: 2.0e-03, train_loss: 0.1928, train_acc: 0.9304 test_loss: 1.2203, test_acc: 0.7420, best: 0.7549, time: 0:02:42
 Epoch: 223, lr: 2.0e-03, train_loss: 0.2025, train_acc: 0.9312 test_loss: 1.4659, test_acc: 0.7235, best: 0.7549, time: 0:02:41
 Epoch: 224, lr: 2.0e-03, train_loss: 0.1779, train_acc: 0.9416 test_loss: 1.2501, test_acc: 0.7389, best: 0.7549, time: 0:02:42
 Epoch: 225, lr: 2.0e-03, train_loss: 0.1989, train_acc: 0.9328 test_loss: 1.3001, test_acc: 0.7329, best: 0.7549, time: 0:02:41
 Epoch: 226, lr: 2.0e-03, train_loss: 0.1939, train_acc: 0.9372 test_loss: 1.2273, test_acc: 0.7500, best: 0.7549, time: 0:02:43
 Epoch: 227, lr: 2.0e-03, train_loss: 0.1907, train_acc: 0.9330 test_loss: 1.3705, test_acc: 0.7371, best: 0.7549, time: 0:02:42
 Epoch: 228, lr: 2.0e-03, train_loss: 0.1803, train_acc: 0.9380 test_loss: 1.2143, test_acc: 0.7504, best: 0.7549, time: 0:02:44
 Epoch: 229, lr: 2.0e-03, train_loss: 0.1832, train_acc: 0.9368 test_loss: 1.2995, test_acc: 0.7421, best: 0.7549, time: 0:02:43
 Epoch: 230, lr: 2.0e-03, train_loss: 0.1907, train_acc: 0.9362 test_loss: 1.1825, test_acc: 0.7438, best: 0.7549, time: 0:02:42
 Epoch: 231, lr: 2.0e-03, train_loss: 0.1854, train_acc: 0.9378 test_loss: 1.3291, test_acc: 0.7276, best: 0.7549, time: 0:02:41
 Epoch: 232, lr: 2.0e-03, train_loss: 0.1909, train_acc: 0.9256 test_loss: 1.1745, test_acc: 0.7450, best: 0.7549, time: 0:02:42
 Epoch: 233, lr: 2.0e-03, train_loss: 0.1918, train_acc: 0.9320 test_loss: 1.2330, test_acc: 0.7399, best: 0.7549, time: 0:02:42
 Epoch: 234, lr: 2.0e-03, train_loss: 0.1849, train_acc: 0.9374 test_loss: 1.2191, test_acc: 0.7430, best: 0.7549, time: 0:02:43
 Epoch: 235, lr: 2.0e-03, train_loss: 0.2045, train_acc: 0.9288 test_loss: 1.1889, test_acc: 0.7395, best: 0.7549, time: 0:02:42
 Epoch: 236, lr: 2.0e-03, train_loss: 0.2033, train_acc: 0.9314 test_loss: 1.2213, test_acc: 0.7384, best: 0.7549, time: 0:02:42
 Epoch: 237, lr: 2.0e-03, train_loss: 0.1875, train_acc: 0.9408 test_loss: 1.2130, test_acc: 0.7398, best: 0.7549, time: 0:02:45
 Epoch: 238, lr: 2.0e-03, train_loss: 0.1991, train_acc: 0.9340 test_loss: 1.2086, test_acc: 0.7342, best: 0.7549, time: 0:02:43
 Epoch: 239, lr: 2.0e-03, train_loss: 0.1990, train_acc: 0.9348 test_loss: 1.2344, test_acc: 0.7422, best: 0.7549, time: 0:02:43
 Epoch: 240, lr: 4.0e-04, train_loss: 0.1893, train_acc: 0.9338 test_loss: 1.1518, test_acc: 0.7496, best: 0.7549, time: 0:02:43
 Epoch: 241, lr: 4.0e-04, train_loss: 0.1798, train_acc: 0.9398 test_loss: 1.1629, test_acc: 0.7484, best: 0.7549, time: 0:02:42
 Epoch: 242, lr: 4.0e-04, train_loss: 0.1721, train_acc: 0.9408 test_loss: 1.2228, test_acc: 0.7469, best: 0.7549, time: 0:02:46
 Epoch: 243, lr: 4.0e-04, train_loss: 0.1777, train_acc: 0.9402 test_loss: 1.1591, test_acc: 0.7511, best: 0.7549, time: 0:02:43
 Epoch: 244, lr: 4.0e-04, train_loss: 0.1775, train_acc: 0.9408 test_loss: 1.2203, test_acc: 0.7400, best: 0.7549, time: 0:03:02
 Epoch: 245, lr: 4.0e-04, train_loss: 0.1644, train_acc: 0.9450 test_loss: 1.2635, test_acc: 0.7424, best: 0.7549, time: 0:03:00
 Epoch: 246, lr: 4.0e-04, train_loss: 0.1717, train_acc: 0.9456 test_loss: 1.2014, test_acc: 0.7430, best: 0.7549, time: 0:02:45
 Epoch: 247, lr: 4.0e-04, train_loss: 0.1670, train_acc: 0.9416 test_loss: 1.2166, test_acc: 0.7412, best: 0.7549, time: 0:02:44
 Epoch: 248, lr: 4.0e-04, train_loss: 0.1680, train_acc: 0.9420 test_loss: 1.1422, test_acc: 0.7488, best: 0.7549, time: 0:02:45
 Epoch: 249, lr: 4.0e-04, train_loss: 0.1729, train_acc: 0.9418 test_loss: 1.2121, test_acc: 0.7465, best: 0.7549, time: 0:02:44
 Epoch: 250, lr: 4.0e-04, train_loss: 0.1629, train_acc: 0.9462 test_loss: 1.1793, test_acc: 0.7478, best: 0.7549, time: 0:02:43
 Epoch: 251, lr: 4.0e-04, train_loss: 0.1711, train_acc: 0.9414 test_loss: 1.1758, test_acc: 0.7464, best: 0.7549, time: 0:02:51
 Epoch: 252, lr: 4.0e-04, train_loss: 0.1656, train_acc: 0.9430 test_loss: 1.2251, test_acc: 0.7466, best: 0.7549, time: 0:02:45
 Epoch: 253, lr: 4.0e-04, train_loss: 0.1587, train_acc: 0.9476 test_loss: 1.1680, test_acc: 0.7475, best: 0.7549, time: 0:02:44
 Epoch: 254, lr: 4.0e-04, train_loss: 0.1723, train_acc: 0.9410 test_loss: 1.1600, test_acc: 0.7474, best: 0.7549, time: 0:02:45
 Epoch: 255, lr: 4.0e-04, train_loss: 0.1742, train_acc: 0.9390 test_loss: 1.1665, test_acc: 0.7470, best: 0.7549, time: 0:02:44
 Epoch: 256, lr: 4.0e-04, train_loss: 0.1638, train_acc: 0.9478 test_loss: 1.1771, test_acc: 0.7482, best: 0.7549, time: 0:02:46
 Epoch: 257, lr: 4.0e-04, train_loss: 0.1620, train_acc: 0.9466 test_loss: 1.1374, test_acc: 0.7486, best: 0.7549, time: 0:02:50
 Epoch: 258, lr: 4.0e-04, train_loss: 0.1808, train_acc: 0.9392 test_loss: 1.1416, test_acc: 0.7505, best: 0.7549, time: 0:02:45
 Epoch: 259, lr: 4.0e-04, train_loss: 0.1773, train_acc: 0.9394 test_loss: 1.1362, test_acc: 0.7519, best: 0.7549, time: 0:02:44
 Epoch: 260, lr: 4.0e-04, train_loss: 0.1612, train_acc: 0.9448 test_loss: 1.1371, test_acc: 0.7502, best: 0.7549, time: 0:03:02
 Epoch: 261, lr: 4.0e-04, train_loss: 0.1753, train_acc: 0.9418 test_loss: 1.1342, test_acc: 0.7505, best: 0.7549, time: 0:03:19
 Epoch: 262, lr: 4.0e-04, train_loss: 0.1562, train_acc: 0.9460 test_loss: 1.1831, test_acc: 0.7482, best: 0.7549, time: 0:02:44
 Epoch: 263, lr: 4.0e-04, train_loss: 0.1516, train_acc: 0.9458 test_loss: 1.2112, test_acc: 0.7465, best: 0.7549, time: 0:02:42
 Epoch: 264, lr: 4.0e-04, train_loss: 0.1620, train_acc: 0.9428 test_loss: 1.1735, test_acc: 0.7511, best: 0.7549, time: 0:02:47
 Epoch: 265, lr: 4.0e-04, train_loss: 0.1586, train_acc: 0.9468 test_loss: 1.1798, test_acc: 0.7489, best: 0.7549, time: 0:02:42
 Epoch: 266, lr: 4.0e-04, train_loss: 0.1711, train_acc: 0.9428 test_loss: 1.2748, test_acc: 0.7412, best: 0.7549, time: 0:02:43
 Epoch: 267, lr: 4.0e-04, train_loss: 0.1601, train_acc: 0.9468 test_loss: 1.1750, test_acc: 0.7505, best: 0.7549, time: 0:02:46
 Epoch: 268, lr: 4.0e-04, train_loss: 0.1681, train_acc: 0.9416 test_loss: 1.1698, test_acc: 0.7542, best: 0.7549, time: 0:02:43
 Epoch: 269, lr: 4.0e-04, train_loss: 0.1825, train_acc: 0.9388 test_loss: 1.1418, test_acc: 0.7491, best: 0.7549, time: 0:02:44
 Epoch: 270, lr: 8.0e-05, train_loss: 0.1679, train_acc: 0.9426 test_loss: 1.2073, test_acc: 0.7446, best: 0.7549, time: 0:02:44
 Epoch: 271, lr: 8.0e-05, train_loss: 0.1584, train_acc: 0.9410 test_loss: 1.1657, test_acc: 0.7470, best: 0.7549, time: 0:02:43
 Epoch: 272, lr: 8.0e-05, train_loss: 0.1558, train_acc: 0.9478 test_loss: 1.1686, test_acc: 0.7469, best: 0.7549, time: 0:02:48
 Epoch: 273, lr: 8.0e-05, train_loss: 0.1650, train_acc: 0.9430 test_loss: 1.1640, test_acc: 0.7498, best: 0.7549, time: 0:02:44
 Epoch: 274, lr: 8.0e-05, train_loss: 0.1719, train_acc: 0.9410 test_loss: 1.1925, test_acc: 0.7456, best: 0.7549, time: 0:02:43
 Epoch: 275, lr: 8.0e-05, train_loss: 0.1638, train_acc: 0.9404 test_loss: 1.2216, test_acc: 0.7439, best: 0.7549, time: 0:02:45
 Epoch: 276, lr: 8.0e-05, train_loss: 0.1557, train_acc: 0.9480 test_loss: 1.1620, test_acc: 0.7455, best: 0.7549, time: 0:02:44
 Epoch: 277, lr: 8.0e-05, train_loss: 0.1741, train_acc: 0.9424 test_loss: 1.2251, test_acc: 0.7471, best: 0.7549, time: 0:02:45
 Epoch: 278, lr: 8.0e-05, train_loss: 0.1589, train_acc: 0.9456 test_loss: 1.2041, test_acc: 0.7486, best: 0.7549, time: 0:02:45
 Epoch: 279, lr: 8.0e-05, train_loss: 0.1672, train_acc: 0.9474 test_loss: 1.2067, test_acc: 0.7468, best: 0.7549, time: 0:02:44
 Epoch: 280, lr: 8.0e-05, train_loss: 0.1671, train_acc: 0.9426 test_loss: 1.3213, test_acc: 0.7356, best: 0.7549, time: 0:02:49
 Epoch: 281, lr: 8.0e-05, train_loss: 0.1640, train_acc: 0.9412 test_loss: 1.1895, test_acc: 0.7439, best: 0.7549, time: 0:02:48
 Epoch: 282, lr: 8.0e-05, train_loss: 0.1579, train_acc: 0.9484 test_loss: 1.1584, test_acc: 0.7470, best: 0.7549, time: 0:02:46
 Epoch: 283, lr: 8.0e-05, train_loss: 0.1611, train_acc: 0.9440 test_loss: 1.2170, test_acc: 0.7411, best: 0.7549, time: 0:02:45
 Epoch: 284, lr: 8.0e-05, train_loss: 0.1535, train_acc: 0.9468 test_loss: 1.1796, test_acc: 0.7488, best: 0.7549, time: 0:02:46
 Epoch: 285, lr: 8.0e-05, train_loss: 0.1592, train_acc: 0.9452 test_loss: 1.2330, test_acc: 0.7425, best: 0.7549, time: 0:02:43
 Epoch: 286, lr: 8.0e-05, train_loss: 0.1663, train_acc: 0.9442 test_loss: 1.2371, test_acc: 0.7444, best: 0.7549, time: 0:02:45
 Epoch: 287, lr: 8.0e-05, train_loss: 0.1578, train_acc: 0.9496 test_loss: 1.2339, test_acc: 0.7469, best: 0.7549, time: 0:03:11
 Epoch: 288, lr: 8.0e-05, train_loss: 0.1603, train_acc: 0.9450 test_loss: 1.1727, test_acc: 0.7524, best: 0.7549, time: 0:02:45
 Epoch: 289, lr: 8.0e-05, train_loss: 0.1530, train_acc: 0.9478 test_loss: 1.2038, test_acc: 0.7485, best: 0.7549, time: 0:02:44
 Epoch: 290, lr: 8.0e-05, train_loss: 0.1719, train_acc: 0.9390 test_loss: 1.1725, test_acc: 0.7474, best: 0.7549, time: 0:02:47
 Epoch: 291, lr: 8.0e-05, train_loss: 0.1674, train_acc: 0.9434 test_loss: 1.3020, test_acc: 0.7430, best: 0.7549, time: 0:02:44
 Epoch: 292, lr: 8.0e-05, train_loss: 0.1618, train_acc: 0.9464 test_loss: 1.1646, test_acc: 0.7471, best: 0.7549, time: 0:02:46
 Epoch: 293, lr: 8.0e-05, train_loss: 0.1667, train_acc: 0.9410 test_loss: 1.2062, test_acc: 0.7469, best: 0.7549, time: 0:02:54
 Epoch: 294, lr: 8.0e-05, train_loss: 0.1603, train_acc: 0.9472 test_loss: 1.1979, test_acc: 0.7470, best: 0.7549, time: 0:02:58
 Epoch: 295, lr: 8.0e-05, train_loss: 0.1559, train_acc: 0.9452 test_loss: 1.1912, test_acc: 0.7422, best: 0.7549, time: 0:02:42
 Epoch: 296, lr: 8.0e-05, train_loss: 0.1569, train_acc: 0.9480 test_loss: 1.1893, test_acc: 0.7420, best: 0.7549, time: 0:02:45
 Epoch: 297, lr: 8.0e-05, train_loss: 0.1649, train_acc: 0.9430 test_loss: 1.2019, test_acc: 0.7502, best: 0.7549, time: 0:02:45
 Epoch: 298, lr: 8.0e-05, train_loss: 0.1601, train_acc: 0.9456 test_loss: 1.1989, test_acc: 0.7504, best: 0.7549, time: 0:02:45
 Epoch: 299, lr: 8.0e-05, train_loss: 0.1561, train_acc: 0.9456 test_loss: 1.3512, test_acc: 0.7342, best: 0.7549, time: 0:02:43
 Highest accuracy: 0.7549