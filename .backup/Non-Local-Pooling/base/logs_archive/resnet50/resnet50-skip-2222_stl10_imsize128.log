
 Run on time: 2022-06-29 22:08:43.416076

 Arguments:
	 gpu                  : True
	 log                  : True
	 seed                 : 0
	 momentum             : 0.9
	 amsgrad              : True
	 dataset              : STL10
	 batch_size           : 8
	 architecture         : RESNET50_2222
	 im_size              : 128
	 relu_threshold       : 4.0
	 learning_rate        : 0.01
	 pretrained_backbone  : 
	 pretrained_ann       : 
	 weight_decay         : 0.0
	 test_only            : False
	 epochs               : 300
	 lr_interval          : [180, 240, 270]
	 lr_reduce            : 5
	 optimizer            : SGD
	 dropout              : 0.2
	 kernel_size          : 3
	 dont_save            : False
	 visualize            : False
	 devices              : 0
 DataParallel(
  (module): NetworkByName(
    (net): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer4): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
      (fc): Linear(in_features=2048, out_features=10, bias=True)
    )
  )
)
 SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0
)
 Epoch: 1, lr: 1.0e-02, train_loss: 4.1968, train_acc: 0.1158 test_loss: 6.2464, test_acc: 0.1454, best: 0.1454, time: 0:01:43
 Epoch: 2, lr: 1.0e-02, train_loss: 2.4652, train_acc: 0.1352 test_loss: 3.0237, test_acc: 0.1676, best: 0.1676, time: 0:01:44
 Epoch: 3, lr: 1.0e-02, train_loss: 2.2818, train_acc: 0.1666 test_loss: 2.2172, test_acc: 0.1866, best: 0.1866, time: 0:01:44
 Epoch: 4, lr: 1.0e-02, train_loss: 2.1728, train_acc: 0.1908 test_loss: 2.2826, test_acc: 0.1654, best: 0.1866, time: 0:01:42
 Epoch: 5, lr: 1.0e-02, train_loss: 2.0832, train_acc: 0.2136 test_loss: 2.0404, test_acc: 0.2676, best: 0.2676, time: 0:01:45
 Epoch: 6, lr: 1.0e-02, train_loss: 2.0420, train_acc: 0.2320 test_loss: 1.9095, test_acc: 0.2843, best: 0.2843, time: 0:01:43
 Epoch: 7, lr: 1.0e-02, train_loss: 1.9797, train_acc: 0.2586 test_loss: 1.9465, test_acc: 0.3078, best: 0.3078, time: 0:01:42
 Epoch: 8, lr: 1.0e-02, train_loss: 1.9455, train_acc: 0.2678 test_loss: 2.0762, test_acc: 0.3029, best: 0.3078, time: 0:01:42
 Epoch: 9, lr: 1.0e-02, train_loss: 1.9348, train_acc: 0.2596 test_loss: 1.9790, test_acc: 0.3202, best: 0.3202, time: 0:01:43
 Epoch: 10, lr: 1.0e-02, train_loss: 1.8816, train_acc: 0.2836 test_loss: 1.9024, test_acc: 0.3424, best: 0.3424, time: 0:01:42
 Epoch: 11, lr: 1.0e-02, train_loss: 1.8451, train_acc: 0.3054 test_loss: 1.6401, test_acc: 0.3906, best: 0.3906, time: 0:01:43
 Epoch: 12, lr: 1.0e-02, train_loss: 1.8354, train_acc: 0.3078 test_loss: 2.1255, test_acc: 0.3445, best: 0.3906, time: 0:01:42
 Epoch: 13, lr: 1.0e-02, train_loss: 1.8343, train_acc: 0.3092 test_loss: 1.7924, test_acc: 0.3085, best: 0.3906, time: 0:01:44
 Epoch: 14, lr: 1.0e-02, train_loss: 1.8618, train_acc: 0.2990 test_loss: 4.6188, test_acc: 0.3611, best: 0.3906, time: 0:01:44
 Epoch: 15, lr: 1.0e-02, train_loss: 1.8242, train_acc: 0.3054 test_loss: 1.7680, test_acc: 0.3706, best: 0.3906, time: 0:01:44
 Epoch: 16, lr: 1.0e-02, train_loss: 1.8059, train_acc: 0.3080 test_loss: 1.6283, test_acc: 0.4110, best: 0.4110, time: 0:01:45
 Epoch: 17, lr: 1.0e-02, train_loss: 1.7875, train_acc: 0.3180 test_loss: 2.0469, test_acc: 0.3733, best: 0.4110, time: 0:01:44
 Epoch: 18, lr: 1.0e-02, train_loss: 1.7955, train_acc: 0.3248 test_loss: 1.7976, test_acc: 0.3879, best: 0.4110, time: 0:01:45
 Epoch: 19, lr: 1.0e-02, train_loss: 1.7455, train_acc: 0.3366 test_loss: 1.8815, test_acc: 0.3885, best: 0.4110, time: 0:01:44
 Epoch: 20, lr: 1.0e-02, train_loss: 1.7763, train_acc: 0.3294 test_loss: 1.7705, test_acc: 0.3573, best: 0.4110, time: 0:01:45
 Epoch: 21, lr: 1.0e-02, train_loss: 1.8086, train_acc: 0.3076 test_loss: 1.8121, test_acc: 0.3785, best: 0.4110, time: 0:01:45
 Epoch: 22, lr: 1.0e-02, train_loss: 1.8268, train_acc: 0.3080 test_loss: 1.7296, test_acc: 0.3573, best: 0.4110, time: 0:01:44
 Epoch: 23, lr: 1.0e-02, train_loss: 1.7669, train_acc: 0.3366 test_loss: 1.6023, test_acc: 0.4079, best: 0.4110, time: 0:01:44
 Epoch: 24, lr: 1.0e-02, train_loss: 1.7506, train_acc: 0.3334 test_loss: 1.7855, test_acc: 0.4021, best: 0.4110, time: 0:01:46
 Epoch: 25, lr: 1.0e-02, train_loss: 1.7456, train_acc: 0.3520 test_loss: 2.6526, test_acc: 0.3964, best: 0.4110, time: 0:01:45
 Epoch: 26, lr: 1.0e-02, train_loss: 1.7134, train_acc: 0.3502 test_loss: 2.1454, test_acc: 0.4052, best: 0.4110, time: 0:01:44
 Epoch: 27, lr: 1.0e-02, train_loss: 1.7062, train_acc: 0.3490 test_loss: 1.6657, test_acc: 0.3919, best: 0.4110, time: 0:01:45
 Epoch: 28, lr: 1.0e-02, train_loss: 1.7056, train_acc: 0.3654 test_loss: 3.5117, test_acc: 0.3962, best: 0.4110, time: 0:01:45
 Epoch: 29, lr: 1.0e-02, train_loss: 1.6823, train_acc: 0.3638 test_loss: 1.6548, test_acc: 0.4304, best: 0.4304, time: 0:01:45
 Epoch: 30, lr: 1.0e-02, train_loss: 1.6740, train_acc: 0.3762 test_loss: 4.7866, test_acc: 0.3645, best: 0.4304, time: 0:01:45
 Epoch: 31, lr: 1.0e-02, train_loss: 1.6670, train_acc: 0.3772 test_loss: 2.9500, test_acc: 0.4052, best: 0.4304, time: 0:01:48
 Epoch: 32, lr: 1.0e-02, train_loss: 1.6422, train_acc: 0.3784 test_loss: 2.6047, test_acc: 0.4238, best: 0.4304, time: 0:01:45
 Epoch: 33, lr: 1.0e-02, train_loss: 1.6385, train_acc: 0.3774 test_loss: 4.5384, test_acc: 0.4229, best: 0.4304, time: 0:01:45
 Epoch: 34, lr: 1.0e-02, train_loss: 1.6321, train_acc: 0.3854 test_loss: 3.0710, test_acc: 0.4176, best: 0.4304, time: 0:01:45
 Epoch: 35, lr: 1.0e-02, train_loss: 1.6437, train_acc: 0.3806 test_loss: 3.8060, test_acc: 0.4095, best: 0.4304, time: 0:01:43
 Epoch: 36, lr: 1.0e-02, train_loss: 1.6349, train_acc: 0.3896 test_loss: 4.5813, test_acc: 0.3907, best: 0.4304, time: 0:01:45
 Epoch: 37, lr: 1.0e-02, train_loss: 1.5954, train_acc: 0.3982 test_loss: 1.8124, test_acc: 0.4456, best: 0.4456, time: 0:01:46
 Epoch: 38, lr: 1.0e-02, train_loss: 1.6020, train_acc: 0.3934 test_loss: 2.6990, test_acc: 0.4148, best: 0.4456, time: 0:01:44
 Epoch: 39, lr: 1.0e-02, train_loss: 1.6083, train_acc: 0.3982 test_loss: 2.0183, test_acc: 0.4559, best: 0.4559, time: 0:01:45
 Epoch: 40, lr: 1.0e-02, train_loss: 1.5794, train_acc: 0.4108 test_loss: 2.1509, test_acc: 0.4526, best: 0.4559, time: 0:01:42
 Epoch: 41, lr: 1.0e-02, train_loss: 1.5574, train_acc: 0.4186 test_loss: 3.1688, test_acc: 0.4220, best: 0.4559, time: 0:01:42
 Epoch: 42, lr: 1.0e-02, train_loss: 1.5445, train_acc: 0.4204 test_loss: 2.0500, test_acc: 0.4679, best: 0.4679, time: 0:01:44
 Epoch: 43, lr: 1.0e-02, train_loss: 1.5744, train_acc: 0.4162 test_loss: 3.2558, test_acc: 0.4676, best: 0.4679, time: 0:01:43
 Epoch: 44, lr: 1.0e-02, train_loss: 1.5824, train_acc: 0.4106 test_loss: 1.9953, test_acc: 0.4309, best: 0.4679, time: 0:01:44
 Epoch: 45, lr: 1.0e-02, train_loss: 1.5703, train_acc: 0.4118 test_loss: 2.3051, test_acc: 0.4550, best: 0.4679, time: 0:01:43
 Epoch: 46, lr: 1.0e-02, train_loss: 1.5743, train_acc: 0.4184 test_loss: 1.9137, test_acc: 0.4414, best: 0.4679, time: 0:01:40
 Epoch: 47, lr: 1.0e-02, train_loss: 1.5816, train_acc: 0.4142 test_loss: 1.9440, test_acc: 0.4580, best: 0.4679, time: 0:01:41
 Epoch: 48, lr: 1.0e-02, train_loss: 1.5574, train_acc: 0.4212 test_loss: 1.7288, test_acc: 0.4526, best: 0.4679, time: 0:01:40
 Epoch: 49, lr: 1.0e-02, train_loss: 1.5621, train_acc: 0.4036 test_loss: 1.9023, test_acc: 0.4209, best: 0.4679, time: 0:01:41
 Epoch: 50, lr: 1.0e-02, train_loss: 1.5671, train_acc: 0.4130 test_loss: 2.4066, test_acc: 0.4645, best: 0.4679, time: 0:01:42
 Epoch: 51, lr: 1.0e-02, train_loss: 1.5335, train_acc: 0.4290 test_loss: 1.3983, test_acc: 0.4909, best: 0.4909, time: 0:01:45
 Epoch: 52, lr: 1.0e-02, train_loss: 1.5324, train_acc: 0.4300 test_loss: 1.6978, test_acc: 0.4839, best: 0.4909, time: 0:01:45
 Epoch: 53, lr: 1.0e-02, train_loss: 1.5031, train_acc: 0.4444 test_loss: 1.5460, test_acc: 0.4963, best: 0.4963, time: 0:01:42
 Epoch: 54, lr: 1.0e-02, train_loss: 1.4823, train_acc: 0.4562 test_loss: 1.5157, test_acc: 0.4931, best: 0.4963, time: 0:01:41
 Epoch: 55, lr: 1.0e-02, train_loss: 1.4613, train_acc: 0.4550 test_loss: 1.6125, test_acc: 0.4930, best: 0.4963, time: 0:01:41
 Epoch: 56, lr: 1.0e-02, train_loss: 1.4649, train_acc: 0.4538 test_loss: 3.4682, test_acc: 0.4771, best: 0.4963, time: 0:01:42
 Epoch: 57, lr: 1.0e-02, train_loss: 1.4496, train_acc: 0.4606 test_loss: 2.5216, test_acc: 0.5018, best: 0.5018, time: 0:01:42
 Epoch: 58, lr: 1.0e-02, train_loss: 1.4207, train_acc: 0.4742 test_loss: 1.9932, test_acc: 0.5014, best: 0.5018, time: 0:01:42
 Epoch: 59, lr: 1.0e-02, train_loss: 1.4360, train_acc: 0.4700 test_loss: 3.5908, test_acc: 0.4723, best: 0.5018, time: 0:01:42
 Epoch: 60, lr: 1.0e-02, train_loss: 1.4236, train_acc: 0.4764 test_loss: 1.5222, test_acc: 0.5229, best: 0.5229, time: 0:01:41
 Epoch: 61, lr: 1.0e-02, train_loss: 1.4173, train_acc: 0.4730 test_loss: 1.8878, test_acc: 0.5145, best: 0.5229, time: 0:01:41
 Epoch: 62, lr: 1.0e-02, train_loss: 1.3987, train_acc: 0.4892 test_loss: 3.1172, test_acc: 0.4935, best: 0.5229, time: 0:01:42
 Epoch: 63, lr: 1.0e-02, train_loss: 1.4026, train_acc: 0.4900 test_loss: 1.6478, test_acc: 0.5246, best: 0.5246, time: 0:01:43
 Epoch: 64, lr: 1.0e-02, train_loss: 1.3859, train_acc: 0.4886 test_loss: 6.6379, test_acc: 0.4708, best: 0.5246, time: 0:01:40
 Epoch: 65, lr: 1.0e-02, train_loss: 1.3650, train_acc: 0.5010 test_loss: 1.8071, test_acc: 0.5399, best: 0.5399, time: 0:01:42
 Epoch: 66, lr: 1.0e-02, train_loss: 1.3765, train_acc: 0.4882 test_loss: 3.5117, test_acc: 0.5324, best: 0.5399, time: 0:01:44
 Epoch: 67, lr: 1.0e-02, train_loss: 1.3322, train_acc: 0.5114 test_loss: 4.6349, test_acc: 0.5252, best: 0.5399, time: 0:01:42
 Epoch: 68, lr: 1.0e-02, train_loss: 1.3395, train_acc: 0.5098 test_loss: 1.6873, test_acc: 0.5480, best: 0.5480, time: 0:01:42
 Epoch: 69, lr: 1.0e-02, train_loss: 1.3572, train_acc: 0.5062 test_loss: 5.6923, test_acc: 0.5279, best: 0.5480, time: 0:01:43
 Epoch: 70, lr: 1.0e-02, train_loss: 1.3371, train_acc: 0.5108 test_loss: 2.2881, test_acc: 0.5396, best: 0.5480, time: 0:01:44
 Epoch: 71, lr: 1.0e-02, train_loss: 1.3126, train_acc: 0.5144 test_loss: 2.8374, test_acc: 0.5367, best: 0.5480, time: 0:01:45
 Epoch: 72, lr: 1.0e-02, train_loss: 1.3015, train_acc: 0.5228 test_loss: 5.5479, test_acc: 0.5251, best: 0.5480, time: 0:01:42
 Epoch: 73, lr: 1.0e-02, train_loss: 1.2809, train_acc: 0.5286 test_loss: 5.0759, test_acc: 0.5389, best: 0.5480, time: 0:01:43
 Epoch: 74, lr: 1.0e-02, train_loss: 1.2777, train_acc: 0.5316 test_loss: 7.4067, test_acc: 0.5394, best: 0.5480, time: 0:01:45
 Epoch: 75, lr: 1.0e-02, train_loss: 1.2667, train_acc: 0.5328 test_loss: 3.5466, test_acc: 0.5533, best: 0.5533, time: 0:01:45
 Epoch: 76, lr: 1.0e-02, train_loss: 1.2954, train_acc: 0.5276 test_loss: 3.9004, test_acc: 0.5473, best: 0.5533, time: 0:01:42
 Epoch: 77, lr: 1.0e-02, train_loss: 1.2879, train_acc: 0.5366 test_loss: 10.7790, test_acc: 0.5174, best: 0.5533, time: 0:01:42
 Epoch: 78, lr: 1.0e-02, train_loss: 1.2654, train_acc: 0.5420 test_loss: 7.7785, test_acc: 0.5406, best: 0.5533, time: 0:01:43
 Epoch: 79, lr: 1.0e-02, train_loss: 1.2500, train_acc: 0.5390 test_loss: 10.2595, test_acc: 0.5108, best: 0.5533, time: 0:01:44
 Epoch: 80, lr: 1.0e-02, train_loss: 1.2396, train_acc: 0.5464 test_loss: 6.3630, test_acc: 0.5394, best: 0.5533, time: 0:01:44
 Epoch: 81, lr: 1.0e-02, train_loss: 1.2246, train_acc: 0.5560 test_loss: 8.6787, test_acc: 0.5401, best: 0.5533, time: 0:01:43
 Epoch: 82, lr: 1.0e-02, train_loss: 1.2338, train_acc: 0.5522 test_loss: 10.0235, test_acc: 0.5506, best: 0.5533, time: 0:01:42
 Epoch: 83, lr: 1.0e-02, train_loss: 1.1792, train_acc: 0.5786 test_loss: 3.5767, test_acc: 0.5557, best: 0.5557, time: 0:01:43
 Epoch: 84, lr: 1.0e-02, train_loss: 1.2525, train_acc: 0.5444 test_loss: 1.6409, test_acc: 0.5202, best: 0.5557, time: 0:01:42
 Epoch: 85, lr: 1.0e-02, train_loss: 1.3075, train_acc: 0.5190 test_loss: 1.7412, test_acc: 0.5763, best: 0.5763, time: 0:01:43
 Epoch: 86, lr: 1.0e-02, train_loss: 1.2452, train_acc: 0.5534 test_loss: 2.0738, test_acc: 0.5493, best: 0.5763, time: 0:01:42
 Epoch: 87, lr: 1.0e-02, train_loss: 1.2488, train_acc: 0.5476 test_loss: 1.5170, test_acc: 0.5650, best: 0.5763, time: 0:01:43
 Epoch: 88, lr: 1.0e-02, train_loss: 1.2311, train_acc: 0.5578 test_loss: 1.6304, test_acc: 0.5122, best: 0.5763, time: 0:01:41
 Epoch: 89, lr: 1.0e-02, train_loss: 1.2339, train_acc: 0.5514 test_loss: 2.4277, test_acc: 0.5534, best: 0.5763, time: 0:01:42
 Epoch: 90, lr: 1.0e-02, train_loss: 1.2227, train_acc: 0.5504 test_loss: 1.2535, test_acc: 0.5929, best: 0.5929, time: 0:01:43
 Epoch: 91, lr: 1.0e-02, train_loss: 1.1996, train_acc: 0.5622 test_loss: 3.9074, test_acc: 0.5525, best: 0.5929, time: 0:01:42
 Epoch: 92, lr: 1.0e-02, train_loss: 1.1908, train_acc: 0.5620 test_loss: 2.4428, test_acc: 0.5619, best: 0.5929, time: 0:01:44
 Epoch: 93, lr: 1.0e-02, train_loss: 1.2010, train_acc: 0.5610 test_loss: 1.8650, test_acc: 0.5873, best: 0.5929, time: 0:01:45
 Epoch: 94, lr: 1.0e-02, train_loss: 1.2023, train_acc: 0.5618 test_loss: 1.5489, test_acc: 0.5769, best: 0.5929, time: 0:01:47
 Epoch: 95, lr: 1.0e-02, train_loss: 1.2023, train_acc: 0.5644 test_loss: 1.4058, test_acc: 0.5723, best: 0.5929, time: 0:01:47
 Epoch: 96, lr: 1.0e-02, train_loss: 1.1521, train_acc: 0.5772 test_loss: 2.5718, test_acc: 0.5665, best: 0.5929, time: 0:01:46
 Epoch: 97, lr: 1.0e-02, train_loss: 1.1447, train_acc: 0.5892 test_loss: 1.7787, test_acc: 0.5673, best: 0.5929, time: 0:01:48
 Epoch: 98, lr: 1.0e-02, train_loss: 1.2331, train_acc: 0.5476 test_loss: 5.0719, test_acc: 0.5351, best: 0.5929, time: 0:01:48
 Epoch: 99, lr: 1.0e-02, train_loss: 1.1484, train_acc: 0.5784 test_loss: 1.8566, test_acc: 0.5829, best: 0.5929, time: 0:01:45
 Epoch: 100, lr: 1.0e-02, train_loss: 1.1322, train_acc: 0.5910 test_loss: 2.9685, test_acc: 0.6091, best: 0.6091, time: 0:01:46
 Epoch: 101, lr: 1.0e-02, train_loss: 1.1203, train_acc: 0.5914 test_loss: 2.9183, test_acc: 0.5946, best: 0.6091, time: 0:01:43
 Epoch: 102, lr: 1.0e-02, train_loss: 1.1118, train_acc: 0.5986 test_loss: 3.6452, test_acc: 0.5972, best: 0.6091, time: 0:01:43
 Epoch: 103, lr: 1.0e-02, train_loss: 1.1099, train_acc: 0.6056 test_loss: 4.3733, test_acc: 0.5895, best: 0.6091, time: 0:01:42
 Epoch: 104, lr: 1.0e-02, train_loss: 1.1088, train_acc: 0.5954 test_loss: 4.8871, test_acc: 0.5887, best: 0.6091, time: 0:01:43
 Epoch: 105, lr: 1.0e-02, train_loss: 1.0922, train_acc: 0.6024 test_loss: 3.3405, test_acc: 0.5807, best: 0.6091, time: 0:01:43
 Epoch: 106, lr: 1.0e-02, train_loss: 1.0859, train_acc: 0.6082 test_loss: 3.6343, test_acc: 0.5836, best: 0.6091, time: 0:01:43
 Epoch: 107, lr: 1.0e-02, train_loss: 1.0673, train_acc: 0.6162 test_loss: 2.3410, test_acc: 0.5944, best: 0.6091, time: 0:01:43
 Epoch: 108, lr: 1.0e-02, train_loss: 1.0751, train_acc: 0.6074 test_loss: 2.0087, test_acc: 0.5881, best: 0.6091, time: 0:01:43
 Epoch: 109, lr: 1.0e-02, train_loss: 1.0528, train_acc: 0.6202 test_loss: 5.9292, test_acc: 0.5874, best: 0.6091, time: 0:01:43
 Epoch: 110, lr: 1.0e-02, train_loss: 1.4124, train_acc: 0.4872 test_loss: 1.7936, test_acc: 0.5329, best: 0.6091, time: 0:01:44
 Epoch: 111, lr: 1.0e-02, train_loss: 1.3159, train_acc: 0.5220 test_loss: 4.8354, test_acc: 0.5365, best: 0.6091, time: 0:01:44
 Epoch: 112, lr: 1.0e-02, train_loss: 1.2853, train_acc: 0.5308 test_loss: 2.3572, test_acc: 0.5584, best: 0.6091, time: 0:01:44
 Epoch: 113, lr: 1.0e-02, train_loss: 1.2053, train_acc: 0.5646 test_loss: 2.0572, test_acc: 0.5673, best: 0.6091, time: 0:01:45
 Epoch: 114, lr: 1.0e-02, train_loss: 1.2006, train_acc: 0.5704 test_loss: 3.8310, test_acc: 0.5721, best: 0.6091, time: 0:01:42
 Epoch: 115, lr: 1.0e-02, train_loss: 1.1605, train_acc: 0.5790 test_loss: 2.0429, test_acc: 0.5900, best: 0.6091, time: 0:01:42
 Epoch: 116, lr: 1.0e-02, train_loss: 1.1529, train_acc: 0.5848 test_loss: 5.5406, test_acc: 0.5410, best: 0.6091, time: 0:01:41
 Epoch: 117, lr: 1.0e-02, train_loss: 1.1398, train_acc: 0.5816 test_loss: 1.6091, test_acc: 0.5854, best: 0.6091, time: 0:01:41
 Epoch: 118, lr: 1.0e-02, train_loss: 1.1195, train_acc: 0.5934 test_loss: 2.3886, test_acc: 0.5935, best: 0.6091, time: 0:01:40
 Epoch: 119, lr: 1.0e-02, train_loss: 1.0895, train_acc: 0.6052 test_loss: 2.4198, test_acc: 0.5811, best: 0.6091, time: 0:01:44
 Epoch: 120, lr: 1.0e-02, train_loss: 1.1007, train_acc: 0.6066 test_loss: 4.5024, test_acc: 0.5565, best: 0.6091, time: 0:01:44
 Epoch: 121, lr: 1.0e-02, train_loss: 1.0907, train_acc: 0.6060 test_loss: 6.8097, test_acc: 0.5550, best: 0.6091, time: 0:01:44
 Epoch: 122, lr: 1.0e-02, train_loss: 1.0813, train_acc: 0.6098 test_loss: 1.5849, test_acc: 0.5965, best: 0.6091, time: 0:01:44
 Epoch: 123, lr: 1.0e-02, train_loss: 1.0542, train_acc: 0.6156 test_loss: 5.8952, test_acc: 0.5809, best: 0.6091, time: 0:01:44
 Epoch: 124, lr: 1.0e-02, train_loss: 1.0561, train_acc: 0.6190 test_loss: 2.0356, test_acc: 0.5900, best: 0.6091, time: 0:01:43
 Epoch: 125, lr: 1.0e-02, train_loss: 1.0473, train_acc: 0.6228 test_loss: 2.0288, test_acc: 0.5765, best: 0.6091, time: 0:01:43
 Epoch: 126, lr: 1.0e-02, train_loss: 1.0468, train_acc: 0.6150 test_loss: 2.0121, test_acc: 0.5875, best: 0.6091, time: 0:01:43
 Epoch: 127, lr: 1.0e-02, train_loss: 1.0334, train_acc: 0.6344 test_loss: 2.0285, test_acc: 0.5954, best: 0.6091, time: 0:01:43
 Epoch: 128, lr: 1.0e-02, train_loss: 1.0210, train_acc: 0.6356 test_loss: 3.8582, test_acc: 0.5936, best: 0.6091, time: 0:01:44
 Epoch: 129, lr: 1.0e-02, train_loss: 1.0163, train_acc: 0.6362 test_loss: 2.9529, test_acc: 0.5946, best: 0.6091, time: 0:01:44
 Epoch: 130, lr: 1.0e-02, train_loss: 1.0016, train_acc: 0.6420 test_loss: 2.3315, test_acc: 0.6135, best: 0.6135, time: 0:01:43
 Epoch: 131, lr: 1.0e-02, train_loss: 0.9735, train_acc: 0.6460 test_loss: 2.8614, test_acc: 0.5979, best: 0.6135, time: 0:01:45
 Epoch: 132, lr: 1.0e-02, train_loss: 1.0144, train_acc: 0.6428 test_loss: 4.2293, test_acc: 0.5846, best: 0.6135, time: 0:01:44
 Epoch: 133, lr: 1.0e-02, train_loss: 0.9955, train_acc: 0.6410 test_loss: 1.3564, test_acc: 0.6261, best: 0.6261, time: 0:01:48
 Epoch: 134, lr: 1.0e-02, train_loss: 0.9708, train_acc: 0.6514 test_loss: 1.6808, test_acc: 0.6221, best: 0.6261, time: 0:01:46
 Epoch: 135, lr: 1.0e-02, train_loss: 0.9956, train_acc: 0.6448 test_loss: 2.1777, test_acc: 0.6072, best: 0.6261, time: 0:01:45
 Epoch: 136, lr: 1.0e-02, train_loss: 0.9537, train_acc: 0.6528 test_loss: 3.1783, test_acc: 0.6130, best: 0.6261, time: 0:01:44
 Epoch: 137, lr: 1.0e-02, train_loss: 0.9830, train_acc: 0.6472 test_loss: 1.1508, test_acc: 0.6210, best: 0.6261, time: 0:01:44
 Epoch: 138, lr: 1.0e-02, train_loss: 0.9642, train_acc: 0.6588 test_loss: 1.7843, test_acc: 0.6271, best: 0.6271, time: 0:01:45
 Epoch: 139, lr: 1.0e-02, train_loss: 0.9364, train_acc: 0.6646 test_loss: 1.3958, test_acc: 0.6022, best: 0.6271, time: 0:01:45
 Epoch: 140, lr: 1.0e-02, train_loss: 0.9446, train_acc: 0.6608 test_loss: 1.7752, test_acc: 0.6286, best: 0.6286, time: 0:01:47
 Epoch: 141, lr: 1.0e-02, train_loss: 0.9486, train_acc: 0.6604 test_loss: 1.2261, test_acc: 0.6394, best: 0.6394, time: 0:01:45
 Epoch: 142, lr: 1.0e-02, train_loss: 0.9402, train_acc: 0.6666 test_loss: 2.0654, test_acc: 0.6186, best: 0.6394, time: 0:01:43
 Epoch: 143, lr: 1.0e-02, train_loss: 0.9519, train_acc: 0.6644 test_loss: 1.7573, test_acc: 0.6192, best: 0.6394, time: 0:01:44
 Epoch: 144, lr: 1.0e-02, train_loss: 0.9421, train_acc: 0.6618 test_loss: 1.2363, test_acc: 0.6404, best: 0.6404, time: 0:01:45
 Epoch: 145, lr: 1.0e-02, train_loss: 0.9192, train_acc: 0.6662 test_loss: 1.4815, test_acc: 0.6146, best: 0.6404, time: 0:01:44
 Epoch: 146, lr: 1.0e-02, train_loss: 0.8908, train_acc: 0.6844 test_loss: 3.2537, test_acc: 0.6160, best: 0.6404, time: 0:01:44
 Epoch: 147, lr: 1.0e-02, train_loss: 0.9119, train_acc: 0.6754 test_loss: 3.0243, test_acc: 0.6224, best: 0.6404, time: 0:01:43
 Epoch: 148, lr: 1.0e-02, train_loss: 0.9016, train_acc: 0.6768 test_loss: 2.0275, test_acc: 0.6372, best: 0.6404, time: 0:01:43
 Epoch: 149, lr: 1.0e-02, train_loss: 0.8764, train_acc: 0.6836 test_loss: 1.3399, test_acc: 0.6420, best: 0.6420, time: 0:01:46
 Epoch: 150, lr: 1.0e-02, train_loss: 0.8724, train_acc: 0.6846 test_loss: 2.5172, test_acc: 0.6048, best: 0.6420, time: 0:01:42
 Epoch: 151, lr: 1.0e-02, train_loss: 0.8940, train_acc: 0.6810 test_loss: 4.6804, test_acc: 0.6091, best: 0.6420, time: 0:01:43
 Epoch: 152, lr: 1.0e-02, train_loss: 0.8664, train_acc: 0.6952 test_loss: 2.7157, test_acc: 0.6120, best: 0.6420, time: 0:01:45
 Epoch: 153, lr: 1.0e-02, train_loss: 0.8768, train_acc: 0.6826 test_loss: 4.3918, test_acc: 0.6008, best: 0.6420, time: 0:01:43
 Epoch: 154, lr: 1.0e-02, train_loss: 0.8827, train_acc: 0.6772 test_loss: 2.2573, test_acc: 0.6366, best: 0.6420, time: 0:01:44
 Epoch: 155, lr: 1.0e-02, train_loss: 0.8878, train_acc: 0.6876 test_loss: 7.1853, test_acc: 0.5910, best: 0.6420, time: 0:01:42
 Epoch: 156, lr: 1.0e-02, train_loss: 0.8541, train_acc: 0.6926 test_loss: 2.3917, test_acc: 0.6359, best: 0.6420, time: 0:01:43
 Epoch: 157, lr: 1.0e-02, train_loss: 0.8602, train_acc: 0.7022 test_loss: 11.4262, test_acc: 0.5859, best: 0.6420, time: 0:01:42
 Epoch: 158, lr: 1.0e-02, train_loss: 0.8549, train_acc: 0.6956 test_loss: 11.7693, test_acc: 0.6096, best: 0.6420, time: 0:01:42
 Epoch: 159, lr: 1.0e-02, train_loss: 0.8399, train_acc: 0.7072 test_loss: 20.1471, test_acc: 0.5978, best: 0.6420, time: 0:01:42
 Epoch: 160, lr: 1.0e-02, train_loss: 0.8323, train_acc: 0.7074 test_loss: 5.0808, test_acc: 0.6085, best: 0.6420, time: 0:01:42
 Epoch: 161, lr: 1.0e-02, train_loss: 0.8403, train_acc: 0.6982 test_loss: 5.8534, test_acc: 0.6150, best: 0.6420, time: 0:01:41
 Epoch: 162, lr: 1.0e-02, train_loss: 0.8413, train_acc: 0.7064 test_loss: 2.5196, test_acc: 0.6189, best: 0.6420, time: 0:01:42
 Epoch: 163, lr: 1.0e-02, train_loss: 0.8732, train_acc: 0.6912 test_loss: 2.1797, test_acc: 0.6391, best: 0.6420, time: 0:01:43
 Epoch: 164, lr: 1.0e-02, train_loss: 0.8441, train_acc: 0.7062 test_loss: 4.1534, test_acc: 0.6080, best: 0.6420, time: 0:01:44
 Epoch: 165, lr: 1.0e-02, train_loss: 0.8515, train_acc: 0.6954 test_loss: 7.7363, test_acc: 0.5996, best: 0.6420, time: 0:01:44
 Epoch: 166, lr: 1.0e-02, train_loss: 0.8249, train_acc: 0.7104 test_loss: 10.2377, test_acc: 0.5850, best: 0.6420, time: 0:01:43
 Epoch: 167, lr: 1.0e-02, train_loss: 0.8232, train_acc: 0.6996 test_loss: 3.9895, test_acc: 0.6375, best: 0.6420, time: 0:01:43
 Epoch: 168, lr: 1.0e-02, train_loss: 0.8266, train_acc: 0.7058 test_loss: 6.9158, test_acc: 0.5994, best: 0.6420, time: 0:01:44
 Epoch: 169, lr: 1.0e-02, train_loss: 0.8403, train_acc: 0.7132 test_loss: 2.1683, test_acc: 0.5936, best: 0.6420, time: 0:01:43
 Epoch: 170, lr: 1.0e-02, train_loss: 0.9494, train_acc: 0.6602 test_loss: 1.1141, test_acc: 0.6491, best: 0.6491, time: 0:01:45
 Epoch: 171, lr: 1.0e-02, train_loss: 0.8718, train_acc: 0.6864 test_loss: 1.4055, test_acc: 0.6229, best: 0.6491, time: 0:01:44
 Epoch: 172, lr: 1.0e-02, train_loss: 0.9198, train_acc: 0.6738 test_loss: 1.3361, test_acc: 0.6185, best: 0.6491, time: 0:01:43
 Epoch: 173, lr: 1.0e-02, train_loss: 0.8721, train_acc: 0.6900 test_loss: 1.7992, test_acc: 0.6232, best: 0.6491, time: 0:01:43
 Epoch: 174, lr: 1.0e-02, train_loss: 0.8744, train_acc: 0.6870 test_loss: 1.3355, test_acc: 0.6384, best: 0.6491, time: 0:01:43
 Epoch: 175, lr: 1.0e-02, train_loss: 0.8055, train_acc: 0.7206 test_loss: 3.6060, test_acc: 0.6281, best: 0.6491, time: 0:01:44
 Epoch: 176, lr: 1.0e-02, train_loss: 0.7992, train_acc: 0.7234 test_loss: 3.4747, test_acc: 0.6084, best: 0.6491, time: 0:01:45
 Epoch: 177, lr: 1.0e-02, train_loss: 0.7871, train_acc: 0.7226 test_loss: 7.9042, test_acc: 0.5701, best: 0.6491, time: 0:01:44
 Epoch: 178, lr: 1.0e-02, train_loss: 0.7686, train_acc: 0.7260 test_loss: 2.3045, test_acc: 0.6385, best: 0.6491, time: 0:01:45
 Epoch: 179, lr: 1.0e-02, train_loss: 0.8026, train_acc: 0.7106 test_loss: 3.0602, test_acc: 0.6224, best: 0.6491, time: 0:01:44
 Epoch: 180, lr: 2.0e-03, train_loss: 0.7041, train_acc: 0.7532 test_loss: 4.1916, test_acc: 0.6288, best: 0.6491, time: 0:01:44
 Epoch: 181, lr: 2.0e-03, train_loss: 0.6643, train_acc: 0.7718 test_loss: 2.3386, test_acc: 0.6556, best: 0.6556, time: 0:01:46
 Epoch: 182, lr: 2.0e-03, train_loss: 0.6554, train_acc: 0.7680 test_loss: 6.0391, test_acc: 0.6265, best: 0.6556, time: 0:01:44
 Epoch: 183, lr: 2.0e-03, train_loss: 0.6238, train_acc: 0.7864 test_loss: 5.3866, test_acc: 0.6404, best: 0.6556, time: 0:01:44
 Epoch: 184, lr: 2.0e-03, train_loss: 0.6539, train_acc: 0.7694 test_loss: 4.3830, test_acc: 0.6502, best: 0.6556, time: 0:01:43
 Epoch: 185, lr: 2.0e-03, train_loss: 0.6093, train_acc: 0.7940 test_loss: 3.4260, test_acc: 0.6561, best: 0.6561, time: 0:01:44
 Epoch: 186, lr: 2.0e-03, train_loss: 0.6384, train_acc: 0.7812 test_loss: 2.9449, test_acc: 0.6544, best: 0.6561, time: 0:01:44
 Epoch: 187, lr: 2.0e-03, train_loss: 0.6067, train_acc: 0.7862 test_loss: 8.9840, test_acc: 0.6158, best: 0.6561, time: 0:01:45
 Epoch: 188, lr: 2.0e-03, train_loss: 0.6094, train_acc: 0.7898 test_loss: 2.7709, test_acc: 0.6594, best: 0.6594, time: 0:01:45
 Epoch: 189, lr: 2.0e-03, train_loss: 0.6110, train_acc: 0.7910 test_loss: 2.8281, test_acc: 0.6584, best: 0.6594, time: 0:01:46
 Epoch: 190, lr: 2.0e-03, train_loss: 0.6314, train_acc: 0.7780 test_loss: 2.5947, test_acc: 0.6624, best: 0.6624, time: 0:01:46
 Epoch: 191, lr: 2.0e-03, train_loss: 0.6046, train_acc: 0.7958 test_loss: 3.4917, test_acc: 0.6584, best: 0.6624, time: 0:01:45
 Epoch: 192, lr: 2.0e-03, train_loss: 0.6072, train_acc: 0.7864 test_loss: 5.3657, test_acc: 0.6535, best: 0.6624, time: 0:01:46
 Epoch: 193, lr: 2.0e-03, train_loss: 0.5906, train_acc: 0.7922 test_loss: 2.9510, test_acc: 0.6676, best: 0.6676, time: 0:01:46
 Epoch: 194, lr: 2.0e-03, train_loss: 0.5957, train_acc: 0.7942 test_loss: 8.1604, test_acc: 0.6326, best: 0.6676, time: 0:01:44
 Epoch: 195, lr: 2.0e-03, train_loss: 0.5840, train_acc: 0.7980 test_loss: 2.9276, test_acc: 0.6640, best: 0.6676, time: 0:01:44
 Epoch: 196, lr: 2.0e-03, train_loss: 0.5905, train_acc: 0.7878 test_loss: 5.8720, test_acc: 0.6575, best: 0.6676, time: 0:01:44
 Epoch: 197, lr: 2.0e-03, train_loss: 0.5898, train_acc: 0.7940 test_loss: 4.3286, test_acc: 0.6531, best: 0.6676, time: 0:01:44
 Epoch: 198, lr: 2.0e-03, train_loss: 0.5745, train_acc: 0.8030 test_loss: 4.4582, test_acc: 0.6541, best: 0.6676, time: 0:01:42
 Epoch: 199, lr: 2.0e-03, train_loss: 0.5854, train_acc: 0.7988 test_loss: 13.8391, test_acc: 0.6301, best: 0.6676, time: 0:01:42
 Epoch: 200, lr: 2.0e-03, train_loss: 0.5872, train_acc: 0.7964 test_loss: 13.8455, test_acc: 0.6336, best: 0.6676, time: 0:01:46
 Epoch: 201, lr: 2.0e-03, train_loss: 0.5671, train_acc: 0.7996 test_loss: 8.3095, test_acc: 0.6451, best: 0.6676, time: 0:01:45
 Epoch: 202, lr: 2.0e-03, train_loss: 0.5865, train_acc: 0.7932 test_loss: 6.5449, test_acc: 0.6408, best: 0.6676, time: 0:01:45
 Epoch: 203, lr: 2.0e-03, train_loss: 0.5914, train_acc: 0.7918 test_loss: 5.7603, test_acc: 0.6431, best: 0.6676, time: 0:01:43
 Epoch: 204, lr: 2.0e-03, train_loss: 0.5714, train_acc: 0.8020 test_loss: 13.7948, test_acc: 0.6110, best: 0.6676, time: 0:01:43
 Epoch: 205, lr: 2.0e-03, train_loss: 0.5690, train_acc: 0.8084 test_loss: 19.1997, test_acc: 0.6058, best: 0.6676, time: 0:01:42
 Epoch: 206, lr: 2.0e-03, train_loss: 0.5787, train_acc: 0.7978 test_loss: 7.0430, test_acc: 0.6319, best: 0.6676, time: 0:01:43
 Epoch: 207, lr: 2.0e-03, train_loss: 0.5605, train_acc: 0.8060 test_loss: 9.4205, test_acc: 0.6254, best: 0.6676, time: 0:01:43
 Epoch: 208, lr: 2.0e-03, train_loss: 0.5512, train_acc: 0.7992 test_loss: 3.6710, test_acc: 0.6519, best: 0.6676, time: 0:01:43
 Epoch: 209, lr: 2.0e-03, train_loss: 0.5540, train_acc: 0.8072 test_loss: 4.9732, test_acc: 0.6366, best: 0.6676, time: 0:01:45
 Epoch: 210, lr: 2.0e-03, train_loss: 0.5398, train_acc: 0.8078 test_loss: 3.2141, test_acc: 0.6621, best: 0.6676, time: 0:01:42
 Epoch: 211, lr: 2.0e-03, train_loss: 0.5703, train_acc: 0.8080 test_loss: 5.7876, test_acc: 0.6470, best: 0.6676, time: 0:01:42
 Epoch: 212, lr: 2.0e-03, train_loss: 0.5548, train_acc: 0.8118 test_loss: 1.7649, test_acc: 0.6775, best: 0.6775, time: 0:01:44
 Epoch: 213, lr: 2.0e-03, train_loss: 0.5528, train_acc: 0.8100 test_loss: 5.0084, test_acc: 0.6401, best: 0.6775, time: 0:01:41
 Epoch: 214, lr: 2.0e-03, train_loss: 0.5468, train_acc: 0.8072 test_loss: 2.6932, test_acc: 0.6657, best: 0.6775, time: 0:01:42
 Epoch: 215, lr: 2.0e-03, train_loss: 0.5518, train_acc: 0.8032 test_loss: 3.0884, test_acc: 0.6625, best: 0.6775, time: 0:01:43
 Epoch: 216, lr: 2.0e-03, train_loss: 0.5711, train_acc: 0.8026 test_loss: 2.6471, test_acc: 0.6635, best: 0.6775, time: 0:01:42
 Epoch: 217, lr: 2.0e-03, train_loss: 0.5439, train_acc: 0.8168 test_loss: 3.7062, test_acc: 0.6532, best: 0.6775, time: 0:01:43
 Epoch: 218, lr: 2.0e-03, train_loss: 0.5534, train_acc: 0.8056 test_loss: 3.4046, test_acc: 0.6611, best: 0.6775, time: 0:01:42
 Epoch: 219, lr: 2.0e-03, train_loss: 0.5483, train_acc: 0.8076 test_loss: 6.2598, test_acc: 0.6409, best: 0.6775, time: 0:01:43
 Epoch: 220, lr: 2.0e-03, train_loss: 0.5351, train_acc: 0.8164 test_loss: 6.3781, test_acc: 0.6365, best: 0.6775, time: 0:01:43
 Epoch: 221, lr: 2.0e-03, train_loss: 0.5518, train_acc: 0.8134 test_loss: 8.6050, test_acc: 0.6289, best: 0.6775, time: 0:01:44
 Epoch: 222, lr: 2.0e-03, train_loss: 0.5377, train_acc: 0.8148 test_loss: 3.1982, test_acc: 0.6589, best: 0.6775, time: 0:01:45
 Epoch: 223, lr: 2.0e-03, train_loss: 0.5354, train_acc: 0.8198 test_loss: 4.3689, test_acc: 0.6368, best: 0.6775, time: 0:01:45
 Epoch: 224, lr: 2.0e-03, train_loss: 0.5333, train_acc: 0.8162 test_loss: 4.3217, test_acc: 0.6309, best: 0.6775, time: 0:01:44
 Epoch: 225, lr: 2.0e-03, train_loss: 0.5226, train_acc: 0.8184 test_loss: 2.7627, test_acc: 0.6567, best: 0.6775, time: 0:01:42
 Epoch: 226, lr: 2.0e-03, train_loss: 0.5394, train_acc: 0.8200 test_loss: 5.5903, test_acc: 0.6399, best: 0.6775, time: 0:01:42
 Epoch: 227, lr: 2.0e-03, train_loss: 0.5285, train_acc: 0.8134 test_loss: 4.2987, test_acc: 0.6566, best: 0.6775, time: 0:01:43
 Epoch: 228, lr: 2.0e-03, train_loss: 0.5383, train_acc: 0.8122 test_loss: 4.3410, test_acc: 0.6474, best: 0.6775, time: 0:01:43
 Epoch: 229, lr: 2.0e-03, train_loss: 0.5387, train_acc: 0.8144 test_loss: 3.6850, test_acc: 0.6519, best: 0.6775, time: 0:01:45
 Epoch: 230, lr: 2.0e-03, train_loss: 0.5098, train_acc: 0.8230 test_loss: 1.9251, test_acc: 0.6649, best: 0.6775, time: 0:01:44
 Epoch: 231, lr: 2.0e-03, train_loss: 0.5108, train_acc: 0.8276 test_loss: 3.4972, test_acc: 0.6454, best: 0.6775, time: 0:01:44
 Epoch: 232, lr: 2.0e-03, train_loss: 0.5352, train_acc: 0.8154 test_loss: 3.6646, test_acc: 0.6546, best: 0.6775, time: 0:01:45
 Epoch: 233, lr: 2.0e-03, train_loss: 0.5087, train_acc: 0.8258 test_loss: 5.0966, test_acc: 0.6412, best: 0.6775, time: 0:01:44
 Epoch: 234, lr: 2.0e-03, train_loss: 0.5201, train_acc: 0.8210 test_loss: 4.9601, test_acc: 0.6485, best: 0.6775, time: 0:01:44
 Epoch: 235, lr: 2.0e-03, train_loss: 0.5146, train_acc: 0.8226 test_loss: 7.3287, test_acc: 0.6224, best: 0.6775, time: 0:01:44
 Epoch: 236, lr: 2.0e-03, train_loss: 0.5187, train_acc: 0.8234 test_loss: 5.6623, test_acc: 0.6422, best: 0.6775, time: 0:01:45
 Epoch: 237, lr: 2.0e-03, train_loss: 0.5336, train_acc: 0.8248 test_loss: 4.1395, test_acc: 0.6454, best: 0.6775, time: 0:01:45
 Epoch: 238, lr: 2.0e-03, train_loss: 0.5094, train_acc: 0.8234 test_loss: 7.9297, test_acc: 0.6345, best: 0.6775, time: 0:01:46
 Epoch: 239, lr: 2.0e-03, train_loss: 0.5324, train_acc: 0.8148 test_loss: 5.9026, test_acc: 0.6455, best: 0.6775, time: 0:01:46
 Epoch: 240, lr: 4.0e-04, train_loss: 0.5035, train_acc: 0.8276 test_loss: 13.3224, test_acc: 0.6030, best: 0.6775, time: 0:01:44
 Epoch: 241, lr: 4.0e-04, train_loss: 0.5074, train_acc: 0.8282 test_loss: 4.0418, test_acc: 0.6518, best: 0.6775, time: 0:01:47
 Epoch: 242, lr: 4.0e-04, train_loss: 0.4866, train_acc: 0.8332 test_loss: 2.8274, test_acc: 0.6636, best: 0.6775, time: 0:01:45
 Epoch: 243, lr: 4.0e-04, train_loss: 0.4892, train_acc: 0.8310 test_loss: 6.2228, test_acc: 0.6375, best: 0.6775, time: 0:01:48
 Epoch: 244, lr: 4.0e-04, train_loss: 0.4848, train_acc: 0.8346 test_loss: 8.6318, test_acc: 0.6374, best: 0.6775, time: 0:01:46
 Epoch: 245, lr: 4.0e-04, train_loss: 0.4793, train_acc: 0.8342 test_loss: 12.5569, test_acc: 0.6199, best: 0.6775, time: 0:01:44
 Epoch: 246, lr: 4.0e-04, train_loss: 0.4787, train_acc: 0.8324 test_loss: 4.0631, test_acc: 0.6574, best: 0.6775, time: 0:01:46
 Epoch: 247, lr: 4.0e-04, train_loss: 0.4721, train_acc: 0.8390 test_loss: 5.0030, test_acc: 0.6554, best: 0.6775, time: 0:01:46
 Epoch: 248, lr: 4.0e-04, train_loss: 0.4841, train_acc: 0.8278 test_loss: 8.4084, test_acc: 0.6388, best: 0.6775, time: 0:01:46
 Epoch: 249, lr: 4.0e-04, train_loss: 0.4769, train_acc: 0.8370 test_loss: 7.1572, test_acc: 0.6354, best: 0.6775, time: 0:01:46
 Epoch: 250, lr: 4.0e-04, train_loss: 0.4869, train_acc: 0.8300 test_loss: 5.6881, test_acc: 0.6559, best: 0.6775, time: 0:01:46
 Epoch: 251, lr: 4.0e-04, train_loss: 0.4796, train_acc: 0.8346 test_loss: 3.4676, test_acc: 0.6595, best: 0.6775, time: 0:01:43
 Epoch: 252, lr: 4.0e-04, train_loss: 0.4731, train_acc: 0.8398 test_loss: 3.1211, test_acc: 0.6465, best: 0.6775, time: 0:01:44
 Epoch: 253, lr: 4.0e-04, train_loss: 0.5001, train_acc: 0.8312 test_loss: 4.5369, test_acc: 0.6520, best: 0.6775, time: 0:01:43
 Epoch: 254, lr: 4.0e-04, train_loss: 0.4709, train_acc: 0.8388 test_loss: 6.4007, test_acc: 0.6378, best: 0.6775, time: 0:01:45
 Epoch: 255, lr: 4.0e-04, train_loss: 0.4880, train_acc: 0.8324 test_loss: 3.6828, test_acc: 0.6545, best: 0.6775, time: 0:01:46
 Epoch: 256, lr: 4.0e-04, train_loss: 0.4814, train_acc: 0.8328 test_loss: 1.7205, test_acc: 0.6784, best: 0.6784, time: 0:01:44
 Epoch: 257, lr: 4.0e-04, train_loss: 0.4628, train_acc: 0.8436 test_loss: 5.3007, test_acc: 0.6465, best: 0.6784, time: 0:01:45
 Epoch: 258, lr: 4.0e-04, train_loss: 0.4701, train_acc: 0.8350 test_loss: 6.9688, test_acc: 0.6396, best: 0.6784, time: 0:01:44
 Epoch: 259, lr: 4.0e-04, train_loss: 0.4932, train_acc: 0.8346 test_loss: 11.6871, test_acc: 0.6306, best: 0.6784, time: 0:01:44
 Epoch: 260, lr: 4.0e-04, train_loss: 0.4746, train_acc: 0.8364 test_loss: 4.4807, test_acc: 0.6518, best: 0.6784, time: 0:01:43
 Epoch: 261, lr: 4.0e-04, train_loss: 0.4655, train_acc: 0.8380 test_loss: 2.7629, test_acc: 0.6653, best: 0.6784, time: 0:01:44
 Epoch: 262, lr: 4.0e-04, train_loss: 0.4448, train_acc: 0.8504 test_loss: 5.7987, test_acc: 0.6424, best: 0.6784, time: 0:01:43
 Epoch: 263, lr: 4.0e-04, train_loss: 0.4581, train_acc: 0.8444 test_loss: 7.2637, test_acc: 0.6381, best: 0.6784, time: 0:01:43
 Epoch: 264, lr: 4.0e-04, train_loss: 0.4919, train_acc: 0.8286 test_loss: 12.5645, test_acc: 0.6151, best: 0.6784, time: 0:01:42
 Epoch: 265, lr: 4.0e-04, train_loss: 0.4614, train_acc: 0.8378 test_loss: 4.9695, test_acc: 0.6462, best: 0.6784, time: 0:01:43
 Epoch: 266, lr: 4.0e-04, train_loss: 0.4785, train_acc: 0.8402 test_loss: 3.6798, test_acc: 0.6530, best: 0.6784, time: 0:01:41
 Epoch: 267, lr: 4.0e-04, train_loss: 0.4649, train_acc: 0.8412 test_loss: 5.1840, test_acc: 0.6478, best: 0.6784, time: 0:01:43
 Epoch: 268, lr: 4.0e-04, train_loss: 0.4671, train_acc: 0.8406 test_loss: 7.7400, test_acc: 0.6348, best: 0.6784, time: 0:01:42
 Epoch: 269, lr: 4.0e-04, train_loss: 0.4599, train_acc: 0.8440 test_loss: 8.1476, test_acc: 0.6352, best: 0.6784, time: 0:01:42
 Epoch: 270, lr: 8.0e-05, train_loss: 0.4632, train_acc: 0.8406 test_loss: 7.4028, test_acc: 0.6300, best: 0.6784, time: 0:01:41
 Epoch: 271, lr: 8.0e-05, train_loss: 0.4630, train_acc: 0.8410 test_loss: 5.3237, test_acc: 0.6422, best: 0.6784, time: 0:01:41
 Epoch: 272, lr: 8.0e-05, train_loss: 0.4651, train_acc: 0.8358 test_loss: 19.4151, test_acc: 0.5962, best: 0.6784, time: 0:01:41
 Epoch: 273, lr: 8.0e-05, train_loss: 0.4734, train_acc: 0.8404 test_loss: 6.4320, test_acc: 0.6361, best: 0.6784, time: 0:01:42
 Epoch: 274, lr: 8.0e-05, train_loss: 0.4617, train_acc: 0.8414 test_loss: 5.4573, test_acc: 0.6451, best: 0.6784, time: 0:01:43
 Epoch: 275, lr: 8.0e-05, train_loss: 0.4618, train_acc: 0.8406 test_loss: 3.7281, test_acc: 0.6549, best: 0.6784, time: 0:01:45
 Epoch: 276, lr: 8.0e-05, train_loss: 0.4641, train_acc: 0.8388 test_loss: 11.4676, test_acc: 0.6228, best: 0.6784, time: 0:01:43
 Epoch: 277, lr: 8.0e-05, train_loss: 0.4519, train_acc: 0.8434 test_loss: 6.1173, test_acc: 0.6421, best: 0.6784, time: 0:01:46
 Epoch: 278, lr: 8.0e-05, train_loss: 0.4456, train_acc: 0.8420 test_loss: 3.2466, test_acc: 0.6560, best: 0.6784, time: 0:01:57
 Epoch: 279, lr: 8.0e-05, train_loss: 0.4924, train_acc: 0.8286 test_loss: 14.2376, test_acc: 0.6174, best: 0.6784, time: 0:01:45
 Epoch: 280, lr: 8.0e-05, train_loss: 0.4665, train_acc: 0.8456 test_loss: 7.2491, test_acc: 0.6325, best: 0.6784, time: 0:01:47
 Epoch: 281, lr: 8.0e-05, train_loss: 0.4465, train_acc: 0.8410 test_loss: 4.9054, test_acc: 0.6422, best: 0.6784, time: 0:01:47
 Epoch: 282, lr: 8.0e-05, train_loss: 0.4511, train_acc: 0.8462 test_loss: 5.9877, test_acc: 0.6362, best: 0.6784, time: 0:01:47
 Epoch: 283, lr: 8.0e-05, train_loss: 0.4411, train_acc: 0.8478 test_loss: 4.1640, test_acc: 0.6461, best: 0.6784, time: 0:01:46
 Epoch: 284, lr: 8.0e-05, train_loss: 0.4663, train_acc: 0.8384 test_loss: 7.7389, test_acc: 0.6281, best: 0.6784, time: 0:01:47
 Epoch: 285, lr: 8.0e-05, train_loss: 0.4534, train_acc: 0.8458 test_loss: 5.6076, test_acc: 0.6426, best: 0.6784, time: 0:01:47
 Epoch: 286, lr: 8.0e-05, train_loss: 0.4667, train_acc: 0.8450 test_loss: 5.5298, test_acc: 0.6464, best: 0.6784, time: 0:01:47
 Epoch: 287, lr: 8.0e-05, train_loss: 0.4580, train_acc: 0.8422 test_loss: 4.1853, test_acc: 0.6488, best: 0.6784, time: 0:01:45
 Epoch: 288, lr: 8.0e-05, train_loss: 0.4465, train_acc: 0.8532 test_loss: 4.5536, test_acc: 0.6486, best: 0.6784, time: 0:01:46
 Epoch: 289, lr: 8.0e-05, train_loss: 0.4834, train_acc: 0.8334 test_loss: 6.2879, test_acc: 0.6466, best: 0.6784, time: 0:01:45
 Epoch: 290, lr: 8.0e-05, train_loss: 0.4298, train_acc: 0.8478 test_loss: 8.6528, test_acc: 0.6216, best: 0.6784, time: 0:01:42
 Epoch: 291, lr: 8.0e-05, train_loss: 0.4497, train_acc: 0.8504 test_loss: 11.6521, test_acc: 0.6201, best: 0.6784, time: 0:01:44
 Epoch: 292, lr: 8.0e-05, train_loss: 0.4558, train_acc: 0.8438 test_loss: 4.7710, test_acc: 0.6409, best: 0.6784, time: 0:01:44
 Epoch: 293, lr: 8.0e-05, train_loss: 0.4686, train_acc: 0.8410 test_loss: 4.9662, test_acc: 0.6430, best: 0.6784, time: 0:01:45
 Epoch: 294, lr: 8.0e-05, train_loss: 0.4660, train_acc: 0.8458 test_loss: 10.8479, test_acc: 0.6185, best: 0.6784, time: 0:01:45
 Epoch: 295, lr: 8.0e-05, train_loss: 0.4588, train_acc: 0.8414 test_loss: 13.4535, test_acc: 0.6146, best: 0.6784, time: 0:01:46
 Epoch: 296, lr: 8.0e-05, train_loss: 0.4500, train_acc: 0.8482 test_loss: 6.4738, test_acc: 0.6375, best: 0.6784, time: 0:01:45
 Epoch: 297, lr: 8.0e-05, train_loss: 0.4855, train_acc: 0.8278 test_loss: 6.8048, test_acc: 0.6395, best: 0.6784, time: 0:01:46
 Epoch: 298, lr: 8.0e-05, train_loss: 0.4841, train_acc: 0.8332 test_loss: 3.8525, test_acc: 0.6541, best: 0.6784, time: 0:01:47
 Epoch: 299, lr: 8.0e-05, train_loss: 0.4597, train_acc: 0.8434 test_loss: 3.3014, test_acc: 0.6560, best: 0.6784, time: 0:01:47
 Highest accuracy: 0.6784